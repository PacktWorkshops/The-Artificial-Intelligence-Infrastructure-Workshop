{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kevin/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11233a610>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import module\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "# make game\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# seed the experiment\n",
    "env.seed(9)\n",
    "np.random.seed(9)\n",
    "random.seed(9) \n",
    "torch.manual_seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our policy\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(self.observation_space, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, self.action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# define our agent\n",
    "class Agent:\n",
    "    def __init__(self, policy_net, target_net):\n",
    "        MEMORY_SIZE = 10000\n",
    "        GAMMA = 0.6\n",
    "        BATCH_SIZE = 128\n",
    "        EXPLORATION_MAX = 0.9\n",
    "        EXPLORATION_MIN = 0.05\n",
    "        EXPLORATION_DECAY = 0.95\n",
    "        # 1, 2, 3 \n",
    "        TARGET_UPDATE = 1\n",
    "\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.target_net.load_state_dict(policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.RMSprop(policy_net.parameters(), lr=1e-3)\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.exploration_min = EXPLORATION_MIN\n",
    "        self.exploration_decay = EXPLORATION_DECAY\n",
    "        self.target_update = TARGET_UPDATE\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return torch.tensor([[random.randrange(self.policy_net.action_space)]])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state)\n",
    "            return q_values.max(1)[1].view(1,1)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def update_target_net(self, i_episode):\n",
    "        if i_episode % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        # fetch data\n",
    "        print(\"[ Experience replay ] starts\")\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        batch_state = torch.cat(tuple([x[0] for x in batch]))\n",
    "        batch_action = torch.cat(tuple([x[1] for x in batch]))\n",
    "        batch_reward = torch.cat(tuple([x[2] for x in batch]))\n",
    "        batch_next_state = [x[3] for x in batch]\n",
    "        non_final_next_states = torch.cat(tuple([s for s in batch_next_state if s is not None]))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), dtype=torch.bool)\n",
    "        \n",
    "        # policy net forward pass to get Q values\n",
    "        state_action_values = self.policy_net(batch_state).gather(1, batch_action)\n",
    "        # target net forward pass to get Q values of next state\n",
    "        next_state_values = torch.zeros(self.batch_size)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * self.gamma).unsqueeze(1) + batch_reward\n",
    "\n",
    "        # train policy\n",
    "        policy_loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # the more policy gets replayed, the less the agent explores\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 1 ] state=tensor([[-0.0055,  0.0210,  0.0088,  0.0255]])\n",
      "[ episode 1 ][ timestamp 1 ] state=tensor([[-0.0055,  0.0210,  0.0088,  0.0255]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0051,  0.2160,  0.0094, -0.2644]])\n",
      "[ episode 1 ][ timestamp 2 ] state=tensor([[-0.0051,  0.2160,  0.0094, -0.2644]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0008,  0.4110,  0.0041, -0.5541]])\n",
      "[ episode 1 ][ timestamp 3 ] state=tensor([[-0.0008,  0.4110,  0.0041, -0.5541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0074,  0.6061, -0.0070, -0.8455]])\n",
      "[ episode 1 ][ timestamp 4 ] state=tensor([[ 0.0074,  0.6061, -0.0070, -0.8455]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0196,  0.4110, -0.0239, -0.5551]])\n",
      "[ episode 1 ][ timestamp 5 ] state=tensor([[ 0.0196,  0.4110, -0.0239, -0.5551]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0278,  0.2163, -0.0350, -0.2700]])\n",
      "[ episode 1 ][ timestamp 6 ] state=tensor([[ 0.0278,  0.2163, -0.0350, -0.2700]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0321,  0.0217, -0.0404,  0.0114]])\n",
      "[ episode 1 ][ timestamp 7 ] state=tensor([[ 0.0321,  0.0217, -0.0404,  0.0114]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0325,  0.2173, -0.0402, -0.2938]])\n",
      "[ episode 1 ][ timestamp 8 ] state=tensor([[ 0.0325,  0.2173, -0.0402, -0.2938]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0369,  0.4130, -0.0461, -0.5989]])\n",
      "[ episode 1 ][ timestamp 9 ] state=tensor([[ 0.0369,  0.4130, -0.0461, -0.5989]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0452,  0.2186, -0.0581, -0.3210]])\n",
      "[ episode 1 ][ timestamp 10 ] state=tensor([[ 0.0452,  0.2186, -0.0581, -0.3210]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0495,  0.4145, -0.0645, -0.6314]])\n",
      "[ episode 1 ][ timestamp 11 ] state=tensor([[ 0.0495,  0.4145, -0.0645, -0.6314]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0578,  0.2203, -0.0771, -0.3597]])\n",
      "[ episode 1 ][ timestamp 12 ] state=tensor([[ 0.0578,  0.2203, -0.0771, -0.3597]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0622,  0.4164, -0.0843, -0.6757]])\n",
      "[ episode 1 ][ timestamp 13 ] state=tensor([[ 0.0622,  0.4164, -0.0843, -0.6757]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0705,  0.6126, -0.0978, -0.9937]])\n",
      "[ episode 1 ][ timestamp 14 ] state=tensor([[ 0.0705,  0.6126, -0.0978, -0.9937]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0828,  0.4189, -0.1177, -0.7333]])\n",
      "[ episode 1 ][ timestamp 15 ] state=tensor([[ 0.0828,  0.4189, -0.1177, -0.7333]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0912,  0.6155, -0.1324, -1.0606]])\n",
      "[ episode 1 ][ timestamp 16 ] state=tensor([[ 0.0912,  0.6155, -0.1324, -1.0606]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1035,  0.8121, -0.1536, -1.3917]])\n",
      "[ episode 1 ][ timestamp 17 ] state=tensor([[ 0.1035,  0.8121, -0.1536, -1.3917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1197,  0.6191, -0.1814, -1.1507]])\n",
      "[ episode 1 ][ timestamp 18 ] state=tensor([[ 0.1197,  0.6191, -0.1814, -1.1507]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1321,  0.4268, -0.2044, -0.9199]])\n",
      "[ episode 1 ][ timestamp 19 ] state=tensor([[ 0.1321,  0.4268, -0.2044, -0.9199]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Ended! ] Episode 1: Exploration_rate=0.9. Score=19.\n",
      "[ episode 2 ] state=tensor([[ 0.0453, -0.0062, -0.0333,  0.0044]])\n",
      "[ episode 2 ][ timestamp 1 ] state=tensor([[ 0.0453, -0.0062, -0.0333,  0.0044]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0451, -0.2008, -0.0333,  0.2864]])\n",
      "[ episode 2 ][ timestamp 2 ] state=tensor([[ 0.0451, -0.2008, -0.0333,  0.2864]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0411, -0.3954, -0.0275,  0.5684]])\n",
      "[ episode 2 ][ timestamp 3 ] state=tensor([[ 0.0411, -0.3954, -0.0275,  0.5684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0332, -0.5901, -0.0162,  0.8523]])\n",
      "[ episode 2 ][ timestamp 4 ] state=tensor([[ 0.0332, -0.5901, -0.0162,  0.8523]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 2.1412e-02, -7.8503e-01,  8.8797e-04,  1.1399e+00]])\n",
      "[ episode 2 ][ timestamp 5 ] state=tensor([[ 2.1412e-02, -7.8503e-01,  8.8797e-04,  1.1399e+00]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0057, -0.5899,  0.0237,  0.8474]])\n",
      "[ episode 2 ][ timestamp 6 ] state=tensor([[ 0.0057, -0.5899,  0.0237,  0.8474]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0061, -0.3951,  0.0406,  0.5623]])\n",
      "[ episode 2 ][ timestamp 7 ] state=tensor([[-0.0061, -0.3951,  0.0406,  0.5623]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0140, -0.5908,  0.0519,  0.8675]])\n",
      "[ episode 2 ][ timestamp 8 ] state=tensor([[-0.0140, -0.5908,  0.0519,  0.8675]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0258, -0.3964,  0.0692,  0.5916]])\n",
      "[ episode 2 ][ timestamp 9 ] state=tensor([[-0.0258, -0.3964,  0.0692,  0.5916]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0337, -0.2023,  0.0811,  0.3215]])\n",
      "[ episode 2 ][ timestamp 10 ] state=tensor([[-0.0337, -0.2023,  0.0811,  0.3215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0378, -0.3985,  0.0875,  0.6386]])\n",
      "[ episode 2 ][ timestamp 11 ] state=tensor([[-0.0378, -0.3985,  0.0875,  0.6386]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0458, -0.2047,  0.1003,  0.3747]])\n",
      "[ episode 2 ][ timestamp 12 ] state=tensor([[-0.0458, -0.2047,  0.1003,  0.3747]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0498, -0.4011,  0.1078,  0.6972]])\n",
      "[ episode 2 ][ timestamp 13 ] state=tensor([[-0.0498, -0.4011,  0.1078,  0.6972]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0579, -0.2076,  0.1217,  0.4403]])\n",
      "[ episode 2 ][ timestamp 14 ] state=tensor([[-0.0579, -0.2076,  0.1217,  0.4403]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0620, -0.0144,  0.1305,  0.1883]])\n",
      "[ episode 2 ][ timestamp 15 ] state=tensor([[-0.0620, -0.0144,  0.1305,  0.1883]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0623, -0.2111,  0.1343,  0.5192]])\n",
      "[ episode 2 ][ timestamp 16 ] state=tensor([[-0.0623, -0.2111,  0.1343,  0.5192]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0665, -0.0181,  0.1447,  0.2716]])\n",
      "[ episode 2 ][ timestamp 17 ] state=tensor([[-0.0665, -0.0181,  0.1447,  0.2716]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0669, -0.2150,  0.1501,  0.6062]])\n",
      "[ episode 2 ][ timestamp 18 ] state=tensor([[-0.0669, -0.2150,  0.1501,  0.6062]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0712, -0.0223,  0.1622,  0.3643]])\n",
      "[ episode 2 ][ timestamp 19 ] state=tensor([[-0.0712, -0.0223,  0.1622,  0.3643]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0716,  0.1702,  0.1695,  0.1269]])\n",
      "[ episode 2 ][ timestamp 20 ] state=tensor([[-0.0716,  0.1702,  0.1695,  0.1269]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0682,  0.3626,  0.1720, -0.1079]])\n",
      "[ episode 2 ][ timestamp 21 ] state=tensor([[-0.0682,  0.3626,  0.1720, -0.1079]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0610,  0.5549,  0.1699, -0.3418]])\n",
      "[ episode 2 ][ timestamp 22 ] state=tensor([[-0.0610,  0.5549,  0.1699, -0.3418]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0499,  0.3578,  0.1630, -0.0007]])\n",
      "[ episode 2 ][ timestamp 23 ] state=tensor([[-0.0499,  0.3578,  0.1630, -0.0007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0427,  0.1607,  0.1630,  0.3387]])\n",
      "[ episode 2 ][ timestamp 24 ] state=tensor([[-0.0427,  0.1607,  0.1630,  0.3387]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0395, -0.0363,  0.1698,  0.6780]])\n",
      "[ episode 2 ][ timestamp 25 ] state=tensor([[-0.0395, -0.0363,  0.1698,  0.6780]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0402,  0.1561,  0.1834,  0.4432]])\n",
      "[ episode 2 ][ timestamp 26 ] state=tensor([[-0.0402,  0.1561,  0.1834,  0.4432]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0371, -0.0411,  0.1922,  0.7876]])\n",
      "[ episode 2 ][ timestamp 27 ] state=tensor([[-0.0371, -0.0411,  0.1922,  0.7876]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0379,  0.1510,  0.2080,  0.5611]])\n",
      "[ episode 2 ][ timestamp 28 ] state=tensor([[-0.0379,  0.1510,  0.2080,  0.5611]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Ended! ] Episode 2: Exploration_rate=0.9. Score=28.\n",
      "[ episode 3 ] state=tensor([[-0.0080,  0.0045, -0.0432,  0.0064]])\n",
      "[ episode 3 ][ timestamp 1 ] state=tensor([[-0.0080,  0.0045, -0.0432,  0.0064]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0079, -0.1900, -0.0431,  0.2851]])\n",
      "[ episode 3 ][ timestamp 2 ] state=tensor([[-0.0079, -0.1900, -0.0431,  0.2851]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0117,  0.0057, -0.0374, -0.0208]])\n",
      "[ episode 3 ][ timestamp 3 ] state=tensor([[-0.0117,  0.0057, -0.0374, -0.0208]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0116, -0.1888, -0.0378,  0.2598]])\n",
      "[ episode 3 ][ timestamp 4 ] state=tensor([[-0.0116, -0.1888, -0.0378,  0.2598]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0153, -0.3834, -0.0326,  0.5404]])\n",
      "[ episode 3 ][ timestamp 5 ] state=tensor([[-0.0153, -0.3834, -0.0326,  0.5404]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0230, -0.1878, -0.0218,  0.2376]])\n",
      "[ episode 3 ][ timestamp 6 ] state=tensor([[-0.0230, -0.1878, -0.0218,  0.2376]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0268, -0.3826, -0.0170,  0.5233]])\n",
      "[ episode 3 ][ timestamp 7 ] state=tensor([[-0.0268, -0.3826, -0.0170,  0.5233]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0344, -0.5775, -0.0066,  0.8106]])\n",
      "[ episode 3 ][ timestamp 8 ] state=tensor([[-0.0344, -0.5775, -0.0066,  0.8106]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0460, -0.7726,  0.0097,  1.1012]])\n",
      "[ episode 3 ][ timestamp 9 ] state=tensor([[-0.0460, -0.7726,  0.0097,  1.1012]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0614, -0.9678,  0.0317,  1.3969]])\n",
      "[ episode 3 ][ timestamp 10 ] state=tensor([[-0.0614, -0.9678,  0.0317,  1.3969]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0808, -1.1633,  0.0596,  1.6993]])\n",
      "[ episode 3 ][ timestamp 11 ] state=tensor([[-0.0808, -1.1633,  0.0596,  1.6993]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1040, -0.9689,  0.0936,  1.4258]])\n",
      "[ episode 3 ][ timestamp 12 ] state=tensor([[-0.1040, -0.9689,  0.0936,  1.4258]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1234, -1.1651,  0.1221,  1.7462]])\n",
      "[ episode 3 ][ timestamp 13 ] state=tensor([[-0.1234, -1.1651,  0.1221,  1.7462]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1467, -1.3613,  0.1570,  2.0742]])\n",
      "[ episode 3 ][ timestamp 14 ] state=tensor([[-0.1467, -1.3613,  0.1570,  2.0742]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1739, -1.5577,  0.1985,  2.4111]])\n",
      "[ episode 3 ][ timestamp 15 ] state=tensor([[-0.1739, -1.5577,  0.1985,  2.4111]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Ended! ] Episode 3: Exploration_rate=0.9. Score=15.\n",
      "[ episode 4 ] state=tensor([[ 0.0352,  0.0226, -0.0025,  0.0490]])\n",
      "[ episode 4 ][ timestamp 1 ] state=tensor([[ 0.0352,  0.0226, -0.0025,  0.0490]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0356, -0.1725, -0.0015,  0.3409]])\n",
      "[ episode 4 ][ timestamp 2 ] state=tensor([[ 0.0356, -0.1725, -0.0015,  0.3409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[0.0322, 0.0226, 0.0053, 0.0478]])\n",
      "[ episode 4 ][ timestamp 3 ] state=tensor([[0.0322, 0.0226, 0.0053, 0.0478]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0326,  0.2177,  0.0062, -0.2433]])\n",
      "[ episode 4 ][ timestamp 4 ] state=tensor([[ 0.0326,  0.2177,  0.0062, -0.2433]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[0.0370, 0.0225, 0.0014, 0.0514]])\n",
      "[ episode 4 ][ timestamp 5 ] state=tensor([[0.0370, 0.0225, 0.0014, 0.0514]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0374,  0.2176,  0.0024, -0.2409]])\n",
      "[ episode 4 ][ timestamp 6 ] state=tensor([[ 0.0374,  0.2176,  0.0024, -0.2409]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0418,  0.0224, -0.0024,  0.0526]])\n",
      "[ episode 4 ][ timestamp 7 ] state=tensor([[ 0.0418,  0.0224, -0.0024,  0.0526]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0422,  0.2176, -0.0014, -0.2409]])\n",
      "[ episode 4 ][ timestamp 8 ] state=tensor([[ 0.0422,  0.2176, -0.0014, -0.2409]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0466,  0.0225, -0.0062,  0.0514]])\n",
      "[ episode 4 ][ timestamp 9 ] state=tensor([[ 0.0466,  0.0225, -0.0062,  0.0514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0470, -0.1726, -0.0052,  0.3421]])\n",
      "[ episode 4 ][ timestamp 10 ] state=tensor([[ 0.0470, -0.1726, -0.0052,  0.3421]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0436, -0.3676,  0.0017,  0.6332]])\n",
      "[ episode 4 ][ timestamp 11 ] state=tensor([[ 0.0436, -0.3676,  0.0017,  0.6332]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0362, -0.5628,  0.0143,  0.9264]])\n",
      "[ episode 4 ][ timestamp 12 ] state=tensor([[ 0.0362, -0.5628,  0.0143,  0.9264]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0250, -0.7581,  0.0329,  1.2235]])\n",
      "[ episode 4 ][ timestamp 13 ] state=tensor([[ 0.0250, -0.7581,  0.0329,  1.2235]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0098, -0.9536,  0.0573,  1.5263]])\n",
      "[ episode 4 ][ timestamp 14 ] state=tensor([[ 0.0098, -0.9536,  0.0573,  1.5263]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0093, -0.7592,  0.0879,  1.2521]])\n",
      "[ episode 4 ][ timestamp 15 ] state=tensor([[-0.0093, -0.7592,  0.0879,  1.2521]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0245, -0.9553,  0.1129,  1.5709]])\n",
      "[ episode 4 ][ timestamp 16 ] state=tensor([[-0.0245, -0.9553,  0.1129,  1.5709]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0436, -0.7617,  0.1443,  1.3155]])\n",
      "[ episode 4 ][ timestamp 17 ] state=tensor([[-0.0436, -0.7617,  0.1443,  1.3155]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0588, -0.9584,  0.1706,  1.6497]])\n",
      "[ episode 4 ][ timestamp 18 ] state=tensor([[-0.0588, -0.9584,  0.1706,  1.6497]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0780, -1.1550,  0.2036,  1.9903]])\n",
      "[ episode 4 ][ timestamp 19 ] state=tensor([[-0.0780, -1.1550,  0.2036,  1.9903]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Ended! ] Episode 4: Exploration_rate=0.9. Score=19.\n",
      "[ episode 5 ] state=tensor([[-0.0393,  0.0026, -0.0217, -0.0007]])\n",
      "[ episode 5 ][ timestamp 1 ] state=tensor([[-0.0393,  0.0026, -0.0217, -0.0007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0392, -0.1922, -0.0217,  0.2851]])\n",
      "[ episode 5 ][ timestamp 2 ] state=tensor([[-0.0392, -0.1922, -0.0217,  0.2851]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0431, -0.3870, -0.0160,  0.5708]])\n",
      "[ episode 5 ][ timestamp 3 ] state=tensor([[-0.0431, -0.3870, -0.0160,  0.5708]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0508, -0.1917, -0.0046,  0.2732]])\n",
      "[ episode 5 ][ timestamp 4 ] state=tensor([[-0.0508, -0.1917, -0.0046,  0.2732]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0546, -0.3867,  0.0009,  0.5644]])\n",
      "[ episode 5 ][ timestamp 5 ] state=tensor([[-0.0546, -0.3867,  0.0009,  0.5644]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0624, -0.1916,  0.0122,  0.2720]])\n",
      "[ episode 5 ][ timestamp 6 ] state=tensor([[-0.0624, -0.1916,  0.0122,  0.2720]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0662, -0.3869,  0.0176,  0.5685]])\n",
      "[ episode 5 ][ timestamp 7 ] state=tensor([[-0.0662, -0.3869,  0.0176,  0.5685]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0739, -0.1920,  0.0290,  0.2814]])\n",
      "[ episode 5 ][ timestamp 8 ] state=tensor([[-0.0739, -0.1920,  0.0290,  0.2814]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0778, -0.3876,  0.0346,  0.5831]])\n",
      "[ episode 5 ][ timestamp 9 ] state=tensor([[-0.0778, -0.3876,  0.0346,  0.5831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0855, -0.1929,  0.0463,  0.3016]])\n",
      "[ episode 5 ][ timestamp 10 ] state=tensor([[-0.0855, -0.1929,  0.0463,  0.3016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0894,  0.0015,  0.0523,  0.0238]])\n",
      "[ episode 5 ][ timestamp 11 ] state=tensor([[-0.0894,  0.0015,  0.0523,  0.0238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0894, -0.1943,  0.0528,  0.3325]])\n",
      "[ episode 5 ][ timestamp 12 ] state=tensor([[-0.0894, -0.1943,  0.0528,  0.3325]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-9.3255e-02, -3.1600e-06,  5.9458e-02,  5.6973e-02]])\n",
      "[ episode 5 ][ timestamp 13 ] state=tensor([[-9.3255e-02, -3.1600e-06,  5.9458e-02,  5.6973e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0933, -0.1959,  0.0606,  0.3678]])\n",
      "[ episode 5 ][ timestamp 14 ] state=tensor([[-0.0933, -0.1959,  0.0606,  0.3678]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0972, -0.3919,  0.0680,  0.6790]])\n",
      "[ episode 5 ][ timestamp 15 ] state=tensor([[-0.0972, -0.3919,  0.0680,  0.6790]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1050, -0.1977,  0.0815,  0.4084]])\n",
      "[ episode 5 ][ timestamp 16 ] state=tensor([[-0.1050, -0.1977,  0.0815,  0.4084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1090, -0.0039,  0.0897,  0.1425]])\n",
      "[ episode 5 ][ timestamp 17 ] state=tensor([[-0.1090, -0.0039,  0.0897,  0.1425]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1090, -0.2001,  0.0926,  0.4621]])\n",
      "[ episode 5 ][ timestamp 18 ] state=tensor([[-0.1090, -0.2001,  0.0926,  0.4621]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1130, -0.0064,  0.1018,  0.2000]])\n",
      "[ episode 5 ][ timestamp 19 ] state=tensor([[-0.1130, -0.0064,  0.1018,  0.2000]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1132, -0.2029,  0.1058,  0.5229]])\n",
      "[ episode 5 ][ timestamp 20 ] state=tensor([[-0.1132, -0.2029,  0.1058,  0.5229]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1172, -0.3993,  0.1163,  0.8470]])\n",
      "[ episode 5 ][ timestamp 21 ] state=tensor([[-0.1172, -0.3993,  0.1163,  0.8470]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1252, -0.5958,  0.1332,  1.1739]])\n",
      "[ episode 5 ][ timestamp 22 ] state=tensor([[-0.1252, -0.5958,  0.1332,  1.1739]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1371, -0.4026,  0.1567,  0.9257]])\n",
      "[ episode 5 ][ timestamp 23 ] state=tensor([[-0.1371, -0.4026,  0.1567,  0.9257]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1452, -0.2099,  0.1752,  0.6861]])\n",
      "[ episode 5 ][ timestamp 24 ] state=tensor([[-0.1452, -0.2099,  0.1752,  0.6861]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1494, -0.0176,  0.1889,  0.4533]])\n",
      "[ episode 5 ][ timestamp 25 ] state=tensor([[-0.1494, -0.0176,  0.1889,  0.4533]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1497,  0.1744,  0.1980,  0.2256]])\n",
      "[ episode 5 ][ timestamp 26 ] state=tensor([[-0.1497,  0.1744,  0.1980,  0.2256]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1463, -0.0229,  0.2025,  0.5736]])\n",
      "[ episode 5 ][ timestamp 27 ] state=tensor([[-0.1463, -0.0229,  0.2025,  0.5736]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Ended! ] Episode 5: Exploration_rate=0.9. Score=27.\n",
      "[ episode 6 ] state=tensor([[ 0.0214,  0.0055, -0.0184, -0.0325]])\n",
      "[ episode 6 ][ timestamp 1 ] state=tensor([[ 0.0214,  0.0055, -0.0184, -0.0325]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0215,  0.2008, -0.0190, -0.3309]])\n",
      "[ episode 6 ][ timestamp 2 ] state=tensor([[ 0.0215,  0.2008, -0.0190, -0.3309]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0255,  0.0060, -0.0256, -0.0443]])\n",
      "[ episode 6 ][ timestamp 3 ] state=tensor([[ 0.0255,  0.0060, -0.0256, -0.0443]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0256, -0.1888, -0.0265,  0.2402]])\n",
      "[ episode 6 ][ timestamp 4 ] state=tensor([[ 0.0256, -0.1888, -0.0265,  0.2402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0219, -0.3835, -0.0217,  0.5244]])\n",
      "[ episode 6 ][ timestamp 5 ] state=tensor([[ 0.0219, -0.3835, -0.0217,  0.5244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0142, -0.1881, -0.0112,  0.2250]])\n",
      "[ episode 6 ][ timestamp 6 ] state=tensor([[ 0.0142, -0.1881, -0.0112,  0.2250]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0104,  0.0072, -0.0067, -0.0712]])\n",
      "[ episode 6 ][ timestamp 7 ] state=tensor([[ 0.0104,  0.0072, -0.0067, -0.0712]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0106,  0.2024, -0.0081, -0.3660]])\n",
      "[ episode 6 ][ timestamp 8 ] state=tensor([[ 0.0106,  0.2024, -0.0081, -0.3660]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0146,  0.3977, -0.0155, -0.6612]])\n",
      "[ episode 6 ][ timestamp 9 ] state=tensor([[ 0.0146,  0.3977, -0.0155, -0.6612]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0226,  0.5930, -0.0287, -0.9587]])\n",
      "[ episode 6 ][ timestamp 10 ] state=tensor([[ 0.0226,  0.5930, -0.0287, -0.9587]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0344,  0.7885, -0.0479, -1.2603]])\n",
      "[ episode 6 ][ timestamp 11 ] state=tensor([[ 0.0344,  0.7885, -0.0479, -1.2603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0502,  0.5940, -0.0731, -0.9830]])\n",
      "[ episode 6 ][ timestamp 12 ] state=tensor([[ 0.0502,  0.5940, -0.0731, -0.9830]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0621,  0.3999, -0.0927, -0.7141]])\n",
      "[ episode 6 ][ timestamp 13 ] state=tensor([[ 0.0621,  0.3999, -0.0927, -0.7141]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0701,  0.5962, -0.1070, -1.0345]])\n",
      "[ episode 6 ][ timestamp 14 ] state=tensor([[ 0.0701,  0.5962, -0.1070, -1.0345]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0820,  0.4027, -0.1277, -0.7772]])\n",
      "[ episode 6 ][ timestamp 15 ] state=tensor([[ 0.0820,  0.4027, -0.1277, -0.7772]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0901,  0.2095, -0.1432, -0.5273]])\n",
      "[ episode 6 ][ timestamp 16 ] state=tensor([[ 0.0901,  0.2095, -0.1432, -0.5273]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0943,  0.4063, -0.1538, -0.8615]])\n",
      "[ episode 6 ][ timestamp 17 ] state=tensor([[ 0.0943,  0.4063, -0.1538, -0.8615]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1024,  0.6032, -0.1710, -1.1983]])\n",
      "[ episode 6 ][ timestamp 18 ] state=tensor([[ 0.1024,  0.6032, -0.1710, -1.1983]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1145,  0.8000, -0.1950, -1.5393]])\n",
      "[ episode 6 ][ timestamp 19 ] state=tensor([[ 0.1145,  0.8000, -0.1950, -1.5393]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Ended! ] Episode 6: Exploration_rate=0.9. Score=19.\n",
      "[ episode 7 ] state=tensor([[-0.0253, -0.0215,  0.0073, -0.0012]])\n",
      "[ episode 7 ][ timestamp 1 ] state=tensor([[-0.0253, -0.0215,  0.0073, -0.0012]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0257,  0.1735,  0.0072, -0.2916]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 2 ] state=tensor([[-0.0257,  0.1735,  0.0072, -0.2916]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0223, -0.0217,  0.0014,  0.0033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 3 ] state=tensor([[-0.0223, -0.0217,  0.0014,  0.0033]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0227,  0.1734,  0.0015, -0.2889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 4 ] state=tensor([[-0.0227,  0.1734,  0.0015, -0.2889]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0192, -0.0218, -0.0043,  0.0043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 5 ] state=tensor([[-0.0192, -0.0218, -0.0043,  0.0043]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0197, -0.2168, -0.0042,  0.2956]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 6 ] state=tensor([[-0.0197, -0.2168, -0.0042,  0.2956]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0240, -0.0216,  0.0017,  0.0016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 7 ] state=tensor([[-0.0240, -0.0216,  0.0017,  0.0016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0244,  0.1735,  0.0017, -0.2906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 8 ] state=tensor([[-0.0244,  0.1735,  0.0017, -0.2906]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0210,  0.3686, -0.0041, -0.5827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 9 ] state=tensor([[-0.0210,  0.3686, -0.0041, -0.5827]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0136,  0.5637, -0.0157, -0.8767]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 10 ] state=tensor([[-0.0136,  0.5637, -0.0157, -0.8767]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0023,  0.3688, -0.0333, -0.5890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 11 ] state=tensor([[-0.0023,  0.3688, -0.0333, -0.5890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0051,  0.5644, -0.0451, -0.8920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 12 ] state=tensor([[ 0.0051,  0.5644, -0.0451, -0.8920]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0163,  0.3699, -0.0629, -0.6138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 13 ] state=tensor([[ 0.0163,  0.3699, -0.0629, -0.6138]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0237,  0.5659, -0.0752, -0.9256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 14 ] state=tensor([[ 0.0237,  0.5659, -0.0752, -0.9256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0351,  0.7619, -0.0937, -1.2409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 15 ] state=tensor([[ 0.0351,  0.7619, -0.0937, -1.2409]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0503,  0.5681, -0.1185, -0.9790]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 16 ] state=tensor([[ 0.0503,  0.5681, -0.1185, -0.9790]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0617,  0.3748, -0.1381, -0.7258]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 17 ] state=tensor([[ 0.0617,  0.3748, -0.1381, -0.7258]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0692,  0.1818, -0.1526, -0.4795]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 18 ] state=tensor([[ 0.0692,  0.1818, -0.1526, -0.4795]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0728,  0.3787, -0.1622, -0.8162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 19 ] state=tensor([[ 0.0728,  0.3787, -0.1622, -0.8162]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0804,  0.1861, -0.1785, -0.5786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 20 ] state=tensor([[ 0.0804,  0.1861, -0.1785, -0.5786]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0841,  0.3832, -0.1901, -0.9218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 21 ] state=tensor([[ 0.0841,  0.3832, -0.1901, -0.9218]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0918,  0.1911, -0.2085, -0.6943]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 7 ][ timestamp 22 ] state=tensor([[ 0.0918,  0.1911, -0.2085, -0.6943]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 7: Exploration_rate=0.29118019047633814. Score=22.\n",
      "[ episode 8 ] state=tensor([[-0.0088,  0.0037, -0.0035,  0.0264]])\n",
      "[ episode 8 ][ timestamp 1 ] state=tensor([[-0.0088,  0.0037, -0.0035,  0.0264]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0088,  0.1989, -0.0030, -0.2674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 2 ] state=tensor([[-0.0088,  0.1989, -0.0030, -0.2674]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0048,  0.0038, -0.0084,  0.0243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 3 ] state=tensor([[-0.0048,  0.0038, -0.0084,  0.0243]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0047,  0.1990, -0.0079, -0.2710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 4 ] state=tensor([[-0.0047,  0.1990, -0.0079, -0.2710]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0007,  0.0040, -0.0133,  0.0192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 5 ] state=tensor([[-0.0007,  0.0040, -0.0133,  0.0192]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0006,  0.1994, -0.0129, -0.2777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 6 ] state=tensor([[-0.0006,  0.1994, -0.0129, -0.2777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0033,  0.0044, -0.0185,  0.0109]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 8 ][ timestamp 7 ] state=tensor([[ 0.0033,  0.0044, -0.0185,  0.0109]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0034, -0.1904, -0.0183,  0.2977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 8 ] state=tensor([[ 0.0034, -0.1904, -0.0183,  0.2977]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0004,  0.0049, -0.0123, -0.0007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 9 ] state=tensor([[-0.0004,  0.0049, -0.0123, -0.0007]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.7817e-04,  2.0024e-01, -1.2311e-02, -2.9721e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 10 ] state=tensor([[-2.7817e-04,  2.0024e-01, -1.2311e-02, -2.9721e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0037,  0.0053, -0.0183, -0.0084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 11 ] state=tensor([[ 0.0037,  0.0053, -0.0183, -0.0084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0038,  0.2007, -0.0184, -0.3068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 12 ] state=tensor([[ 0.0038,  0.2007, -0.0184, -0.3068]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0078,  0.0058, -0.0246, -0.0200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 13 ] state=tensor([[ 0.0078,  0.0058, -0.0246, -0.0200]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0080,  0.2013, -0.0250, -0.3203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 14 ] state=tensor([[ 0.0080,  0.2013, -0.0250, -0.3203]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0120,  0.0065, -0.0314, -0.0356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 15 ] state=tensor([[ 0.0120,  0.0065, -0.0314, -0.0356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0121,  0.2021, -0.0321, -0.3380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 16 ] state=tensor([[ 0.0121,  0.2021, -0.0321, -0.3380]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0162,  0.0074, -0.0388, -0.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 17 ] state=tensor([[ 0.0162,  0.0074, -0.0388, -0.0556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0163,  0.2031, -0.0400, -0.3603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 18 ] state=tensor([[ 0.0163,  0.2031, -0.0400, -0.3603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0204,  0.0086, -0.0472, -0.0805]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 19 ] state=tensor([[ 0.0204,  0.0086, -0.0472, -0.0805]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0205,  0.2043, -0.0488, -0.3877]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 20 ] state=tensor([[ 0.0205,  0.2043, -0.0488, -0.3877]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0246,  0.4001, -0.0565, -0.6953]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 21 ] state=tensor([[ 0.0246,  0.4001, -0.0565, -0.6953]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0326,  0.5960, -0.0704, -1.0053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 22 ] state=tensor([[ 0.0326,  0.5960, -0.0704, -1.0053]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0445,  0.7919, -0.0905, -1.3192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 23 ] state=tensor([[ 0.0445,  0.7919, -0.0905, -1.3192]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0604,  0.9881, -0.1169, -1.6388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 24 ] state=tensor([[ 0.0604,  0.9881, -0.1169, -1.6388]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0801,  1.1844, -0.1497, -1.9655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 25 ] state=tensor([[ 0.0801,  1.1844, -0.1497, -1.9655]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1038,  1.3807, -0.1890, -2.3006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 8 ][ timestamp 26 ] state=tensor([[ 0.1038,  1.3807, -0.1890, -2.3006]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 8: Exploration_rate=0.07673183130087738. Score=26.\n",
      "[ episode 9 ] state=tensor([[-0.0166, -0.0489, -0.0032, -0.0319]])\n",
      "[ episode 9 ][ timestamp 1 ] state=tensor([[-0.0166, -0.0489, -0.0032, -0.0319]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0176,  0.1463, -0.0038, -0.3256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 2 ] state=tensor([[-0.0176,  0.1463, -0.0038, -0.3256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0147,  0.3415, -0.0103, -0.6195]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 3 ] state=tensor([[-0.0147,  0.3415, -0.0103, -0.6195]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0078,  0.5367, -0.0227, -0.9154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 4 ] state=tensor([[-0.0078,  0.5367, -0.0227, -0.9154]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0029,  0.7322, -0.0410, -1.2152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 5 ] state=tensor([[ 0.0029,  0.7322, -0.0410, -1.2152]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0175,  0.9278, -0.0653, -1.5204]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 6 ] state=tensor([[ 0.0175,  0.9278, -0.0653, -1.5204]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0361,  1.1236, -0.0958, -1.8328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 7 ] state=tensor([[ 0.0361,  1.1236, -0.0958, -1.8328]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0586,  1.3197, -0.1324, -2.1536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 8 ] state=tensor([[ 0.0586,  1.3197, -0.1324, -2.1536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0850,  1.5158, -0.1755, -2.4841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 9 ][ timestamp 9 ] state=tensor([[ 0.0850,  1.5158, -0.1755, -2.4841]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 9: Exploration_rate=0.05. Score=9.\n",
      "[ episode 10 ] state=tensor([[-0.0098,  0.0255, -0.0084,  0.0413]])\n",
      "[ episode 10 ][ timestamp 1 ] state=tensor([[-0.0098,  0.0255, -0.0084,  0.0413]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0093,  0.2208, -0.0076, -0.2541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 2 ] state=tensor([[-0.0093,  0.2208, -0.0076, -0.2541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0049,  0.4160, -0.0127, -0.5491]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 3 ] state=tensor([[-0.0049,  0.4160, -0.0127, -0.5491]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0034,  0.6113, -0.0237, -0.8458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 4 ] state=tensor([[ 0.0034,  0.6113, -0.0237, -0.8458]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0156,  0.8067, -0.0406, -1.1458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 5 ] state=tensor([[ 0.0156,  0.8067, -0.0406, -1.1458]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0318,  1.0024, -0.0635, -1.4510]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 6 ] state=tensor([[ 0.0318,  1.0024, -0.0635, -1.4510]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0518,  1.1982, -0.0925, -1.7628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 7 ] state=tensor([[ 0.0518,  1.1982, -0.0925, -1.7628]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0758,  1.3943, -0.1278, -2.0827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 8 ] state=tensor([[ 0.0758,  1.3943, -0.1278, -2.0827]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1037,  1.5904, -0.1694, -2.4121]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 10 ][ timestamp 9 ] state=tensor([[ 0.1037,  1.5904, -0.1694, -2.4121]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 10: Exploration_rate=0.05. Score=9.\n",
      "[ episode 11 ] state=tensor([[-0.0408,  0.0063,  0.0437,  0.0060]])\n",
      "[ episode 11 ][ timestamp 1 ] state=tensor([[-0.0408,  0.0063,  0.0437,  0.0060]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0407,  0.2008,  0.0438, -0.2725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 2 ] state=tensor([[-0.0407,  0.2008,  0.0438, -0.2725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0367,  0.3953,  0.0384, -0.5511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 3 ] state=tensor([[-0.0367,  0.3953,  0.0384, -0.5511]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0288,  0.5898,  0.0273, -0.8314]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 11 ][ timestamp 4 ] state=tensor([[-0.0288,  0.5898,  0.0273, -0.8314]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0170,  0.7846,  0.0107, -1.1154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 5 ] state=tensor([[-0.0170,  0.7846,  0.0107, -1.1154]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3122e-03,  9.7955e-01, -1.1601e-02, -1.4047e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 6 ] state=tensor([[-1.3122e-03,  9.7955e-01, -1.1601e-02, -1.4047e+00]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0183,  0.7846, -0.0397, -1.1157]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 7 ] state=tensor([[ 0.0183,  0.7846, -0.0397, -1.1157]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0340,  0.9802, -0.0620, -1.4205]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 8 ] state=tensor([[ 0.0340,  0.9802, -0.0620, -1.4205]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0536,  1.1760, -0.0904, -1.7319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 9 ] state=tensor([[ 0.0536,  1.1760, -0.0904, -1.7319]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0771,  1.3721, -0.1251, -2.0513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 10 ] state=tensor([[ 0.0771,  1.3721, -0.1251, -2.0513]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1045,  1.1784, -0.1661, -1.7998]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 11 ] state=tensor([[ 0.1045,  1.1784, -0.1661, -1.7998]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1281,  1.3750, -0.2021, -2.1392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 11 ][ timestamp 12 ] state=tensor([[ 0.1281,  1.3750, -0.2021, -2.1392]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 11: Exploration_rate=0.05. Score=12.\n",
      "[ episode 12 ] state=tensor([[ 0.0202, -0.0475,  0.0480, -0.0268]])\n",
      "[ episode 12 ][ timestamp 1 ] state=tensor([[ 0.0202, -0.0475,  0.0480, -0.0268]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0192,  0.1469,  0.0475, -0.3040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 2 ] state=tensor([[ 0.0192,  0.1469,  0.0475, -0.3040]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0222,  0.3413,  0.0414, -0.5814]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 3 ] state=tensor([[ 0.0222,  0.3413,  0.0414, -0.5814]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0290,  0.5358,  0.0298, -0.8607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 4 ] state=tensor([[ 0.0290,  0.5358,  0.0298, -0.8607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0397,  0.7305,  0.0125, -1.1439]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 5 ] state=tensor([[ 0.0397,  0.7305,  0.0125, -1.1439]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0543,  0.9255, -0.0103, -1.4326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 6 ] state=tensor([[ 0.0543,  0.9255, -0.0103, -1.4326]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0728,  1.1207, -0.0390, -1.7285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 7 ] state=tensor([[ 0.0728,  1.1207, -0.0390, -1.7285]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0952,  1.3163, -0.0736, -2.0331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 8 ] state=tensor([[ 0.0952,  1.3163, -0.0736, -2.0331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1216,  1.1220, -0.1142, -1.7640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 9 ] state=tensor([[ 0.1216,  1.1220, -0.1142, -1.7640]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1440,  1.3182, -0.1495, -2.0899]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 10 ] state=tensor([[ 0.1440,  1.3182, -0.1495, -2.0899]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1704,  1.1249, -0.1913, -1.8470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 12 ][ timestamp 11 ] state=tensor([[ 0.1704,  1.1249, -0.1913, -1.8470]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 12: Exploration_rate=0.05. Score=11.\n",
      "[ episode 13 ] state=tensor([[ 0.0128,  0.0384,  0.0055, -0.0304]])\n",
      "[ episode 13 ][ timestamp 1 ] state=tensor([[ 0.0128,  0.0384,  0.0055, -0.0304]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0136,  0.2334,  0.0049, -0.3213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 2 ] state=tensor([[ 0.0136,  0.2334,  0.0049, -0.3213]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0182,  0.4285, -0.0016, -0.6125]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 3 ] state=tensor([[ 0.0182,  0.4285, -0.0016, -0.6125]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0268,  0.6236, -0.0138, -0.9057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 4 ] state=tensor([[ 0.0268,  0.6236, -0.0138, -0.9057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0393,  0.8189, -0.0319, -1.2027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 5 ] state=tensor([[ 0.0393,  0.8189, -0.0319, -1.2027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0556,  1.0145, -0.0560, -1.5052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 6 ] state=tensor([[ 0.0556,  1.0145, -0.0560, -1.5052]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0759,  0.8201, -0.0861, -1.2305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 7 ] state=tensor([[ 0.0759,  0.8201, -0.0861, -1.2305]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0923,  1.0162, -0.1107, -1.5488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 8 ] state=tensor([[ 0.0923,  1.0162, -0.1107, -1.5488]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1127,  0.8225, -0.1417, -1.2926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 9 ] state=tensor([[ 0.1127,  0.8225, -0.1417, -1.2926]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1291,  1.0192, -0.1675, -1.6261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 10 ] state=tensor([[ 0.1291,  1.0192, -0.1675, -1.6261]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1495,  1.2158, -0.2000, -1.9660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 13 ][ timestamp 11 ] state=tensor([[ 0.1495,  1.2158, -0.2000, -1.9660]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 13: Exploration_rate=0.05. Score=11.\n",
      "[ episode 14 ] state=tensor([[-0.0375, -0.0062, -0.0041, -0.0405]])\n",
      "[ episode 14 ][ timestamp 1 ] state=tensor([[-0.0375, -0.0062, -0.0041, -0.0405]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0376,  0.1889, -0.0049, -0.3345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 2 ] state=tensor([[-0.0376,  0.1889, -0.0049, -0.3345]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0339,  0.3841, -0.0116, -0.6287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 3 ] state=tensor([[-0.0339,  0.3841, -0.0116, -0.6287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0262,  0.5794, -0.0241, -0.9250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 4 ] state=tensor([[-0.0262,  0.5794, -0.0241, -0.9250]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0146,  0.7749, -0.0426, -1.2252]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 5 ] state=tensor([[-0.0146,  0.7749, -0.0426, -1.2252]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 9.0700e-04,  9.7050e-01, -6.7142e-02, -1.5309e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 6 ] state=tensor([[ 9.0700e-04,  9.7050e-01, -6.7142e-02, -1.5309e+00]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0203,  1.1664, -0.0978, -1.8437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 7 ] state=tensor([[ 0.0203,  1.1664, -0.0978, -1.8437]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0436,  0.9724, -0.1346, -1.5830]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 8 ] state=tensor([[ 0.0436,  0.9724, -0.1346, -1.5830]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0631,  0.7792, -0.1663, -1.3351]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 14 ][ timestamp 9 ] state=tensor([[ 0.0631,  0.7792, -0.1663, -1.3351]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0787,  0.9759, -0.1930, -1.6749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 14 ][ timestamp 10 ] state=tensor([[ 0.0787,  0.9759, -0.1930, -1.6749]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 14: Exploration_rate=0.05. Score=10.\n",
      "[ episode 15 ] state=tensor([[ 0.0171, -0.0254,  0.0036,  0.0462]])\n",
      "[ episode 15 ][ timestamp 1 ] state=tensor([[ 0.0171, -0.0254,  0.0036,  0.0462]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0166,  0.1696,  0.0045, -0.2454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 2 ] state=tensor([[ 0.0166,  0.1696,  0.0045, -0.2454]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0200, -0.0255, -0.0004,  0.0487]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 3 ] state=tensor([[ 0.0200, -0.0255, -0.0004,  0.0487]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0195,  0.1696,  0.0006, -0.2441]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 4 ] state=tensor([[ 0.0195,  0.1696,  0.0006, -0.2441]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0229, -0.0255, -0.0043,  0.0488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 5 ] state=tensor([[ 0.0229, -0.0255, -0.0043,  0.0488]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0224,  0.1696, -0.0033, -0.2452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 6 ] state=tensor([[ 0.0224,  0.1696, -0.0033, -0.2452]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0258,  0.3648, -0.0082, -0.5389]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 7 ] state=tensor([[ 0.0258,  0.3648, -0.0082, -0.5389]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0331,  0.5600, -0.0190, -0.8342]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 8 ] state=tensor([[ 0.0331,  0.5600, -0.0190, -0.8342]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0443,  0.7554, -0.0357, -1.1328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 9 ] state=tensor([[ 0.0443,  0.7554, -0.0357, -1.1328]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0594,  0.9510, -0.0583, -1.4365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 10 ] state=tensor([[ 0.0594,  0.9510, -0.0583, -1.4365]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0784,  0.7566, -0.0871, -1.1626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 11 ] state=tensor([[ 0.0784,  0.7566, -0.0871, -1.1626]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0935,  0.5627, -0.1103, -0.8984]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 12 ] state=tensor([[ 0.0935,  0.5627, -0.1103, -0.8984]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1048,  0.7592, -0.1283, -1.2236]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 13 ] state=tensor([[ 0.1048,  0.7592, -0.1283, -1.2236]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1200,  0.5659, -0.1527, -0.9737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 14 ] state=tensor([[ 0.1200,  0.5659, -0.1527, -0.9737]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1313,  0.7627, -0.1722, -1.3102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 15 ] state=tensor([[ 0.1313,  0.7627, -0.1722, -1.3102]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1465,  0.5701, -0.1984, -1.0760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 15 ][ timestamp 16 ] state=tensor([[ 0.1465,  0.5701, -0.1984, -1.0760]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 15: Exploration_rate=0.05. Score=16.\n",
      "[ episode 16 ] state=tensor([[-0.0004, -0.0089,  0.0483, -0.0198]])\n",
      "[ episode 16 ][ timestamp 1 ] state=tensor([[-0.0004, -0.0089,  0.0483, -0.0198]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0006,  0.1855,  0.0479, -0.2969]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 2 ] state=tensor([[-0.0006,  0.1855,  0.0479, -0.2969]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0031,  0.3799,  0.0420, -0.5741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 3 ] state=tensor([[ 0.0031,  0.3799,  0.0420, -0.5741]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0107,  0.5744,  0.0305, -0.8533]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 4 ] state=tensor([[ 0.0107,  0.5744,  0.0305, -0.8533]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0222,  0.7691,  0.0134, -1.1362]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 5 ] state=tensor([[ 0.0222,  0.7691,  0.0134, -1.1362]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0376,  0.5738, -0.0093, -0.8394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 6 ] state=tensor([[ 0.0376,  0.5738, -0.0093, -0.8394]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0490,  0.7690, -0.0261, -1.1350]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 7 ] state=tensor([[ 0.0490,  0.7690, -0.0261, -1.1350]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0644,  0.9645, -0.0488, -1.4357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 8 ] state=tensor([[ 0.0644,  0.9645, -0.0488, -1.4357]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0837,  0.7700, -0.0775, -1.1587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 9 ] state=tensor([[ 0.0837,  0.7700, -0.0775, -1.1587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0991,  0.5760, -0.1007, -0.8913]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 10 ] state=tensor([[ 0.0991,  0.5760, -0.1007, -0.8913]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1106,  0.7723, -0.1185, -1.2138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 11 ] state=tensor([[ 0.1106,  0.7723, -0.1185, -1.2138]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1261,  0.9687, -0.1428, -1.5412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 12 ] state=tensor([[ 0.1261,  0.9687, -0.1428, -1.5412]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1455,  0.7756, -0.1736, -1.2963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 13 ] state=tensor([[ 0.1455,  0.7756, -0.1736, -1.2963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1610,  0.5830, -0.1995, -1.0626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 16 ][ timestamp 14 ] state=tensor([[ 0.1610,  0.5830, -0.1995, -1.0626]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 16: Exploration_rate=0.05. Score=14.\n",
      "[ episode 17 ] state=tensor([[ 0.0197,  0.0174,  0.0295, -0.0450]])\n",
      "[ episode 17 ][ timestamp 1 ] state=tensor([[ 0.0197,  0.0174,  0.0295, -0.0450]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0201,  0.2121,  0.0286, -0.3282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 2 ] state=tensor([[ 0.0201,  0.2121,  0.0286, -0.3282]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0243,  0.4068,  0.0220, -0.6118]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 3 ] state=tensor([[ 0.0243,  0.4068,  0.0220, -0.6118]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0324,  0.6016,  0.0098, -0.8974]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 4 ] state=tensor([[ 0.0324,  0.6016,  0.0098, -0.8974]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0445,  0.7966, -0.0082, -1.1870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 5 ] state=tensor([[ 0.0445,  0.7966, -0.0082, -1.1870]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0604,  0.6016, -0.0319, -0.8969]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 6 ] state=tensor([[ 0.0604,  0.6016, -0.0319, -0.8969]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0724,  0.4069, -0.0499, -0.6145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 7 ] state=tensor([[ 0.0724,  0.4069, -0.0499, -0.6145]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0806,  0.6027, -0.0622, -0.9224]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 8 ] state=tensor([[ 0.0806,  0.6027, -0.0622, -0.9224]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0926,  0.7986, -0.0806, -1.2340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 9 ] state=tensor([[ 0.0926,  0.7986, -0.0806, -1.2340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1086,  0.6046, -0.1053, -0.9676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 10 ] state=tensor([[ 0.1086,  0.6046, -0.1053, -0.9676]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1207,  0.4110, -0.1246, -0.7098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 11 ] state=tensor([[ 0.1207,  0.4110, -0.1246, -0.7098]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1289,  0.6076, -0.1388, -1.0389]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 12 ] state=tensor([[ 0.1289,  0.6076, -0.1388, -1.0389]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1411,  0.4146, -0.1596, -0.7929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 13 ] state=tensor([[ 0.1411,  0.4146, -0.1596, -0.7929]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1494,  0.6115, -0.1755, -1.1312]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 17 ][ timestamp 14 ] state=tensor([[ 0.1494,  0.6115, -0.1755, -1.1312]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1616,  0.4191, -0.1981, -0.8983]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 17 ][ timestamp 15 ] state=tensor([[ 0.1616,  0.4191, -0.1981, -0.8983]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 17: Exploration_rate=0.05. Score=15.\n",
      "[ episode 18 ] state=tensor([[-0.0382,  0.0317, -0.0049, -0.0367]])\n",
      "[ episode 18 ][ timestamp 1 ] state=tensor([[-0.0382,  0.0317, -0.0049, -0.0367]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0375,  0.2269, -0.0057, -0.3309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 2 ] state=tensor([[-0.0375,  0.2269, -0.0057, -0.3309]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0330,  0.4221, -0.0123, -0.6254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 3 ] state=tensor([[-0.0330,  0.4221, -0.0123, -0.6254]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0246,  0.6174, -0.0248, -0.9219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 4 ] state=tensor([[-0.0246,  0.6174, -0.0248, -0.9219]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0122,  0.8128, -0.0432, -1.2222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 5 ] state=tensor([[-0.0122,  0.8128, -0.0432, -1.2222]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0041,  0.6183, -0.0677, -0.9434]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 6 ] state=tensor([[ 0.0041,  0.6183, -0.0677, -0.9434]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0164,  0.4241, -0.0865, -0.6727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 7 ] state=tensor([[ 0.0164,  0.4241, -0.0865, -0.6727]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0249,  0.6203, -0.1000, -0.9914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 8 ] state=tensor([[ 0.0249,  0.6203, -0.1000, -0.9914]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0373,  0.4267, -0.1198, -0.7317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 9 ] state=tensor([[ 0.0373,  0.4267, -0.1198, -0.7317]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0458,  0.6232, -0.1344, -1.0595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 10 ] state=tensor([[ 0.0458,  0.6232, -0.1344, -1.0595]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0583,  0.4301, -0.1556, -0.8119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 11 ] state=tensor([[ 0.0583,  0.4301, -0.1556, -0.8119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0669,  0.6270, -0.1719, -1.1492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 12 ] state=tensor([[ 0.0669,  0.6270, -0.1719, -1.1492]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0794,  0.4345, -0.1949, -0.9150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 18 ][ timestamp 13 ] state=tensor([[ 0.0794,  0.4345, -0.1949, -0.9150]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 18: Exploration_rate=0.05. Score=13.\n",
      "[ episode 19 ] state=tensor([[0.0339, 0.0257, 0.0215, 0.0011]])\n",
      "[ episode 19 ][ timestamp 1 ] state=tensor([[0.0339, 0.0257, 0.0215, 0.0011]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0344,  0.2205,  0.0215, -0.2847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 2 ] state=tensor([[ 0.0344,  0.2205,  0.0215, -0.2847]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0388,  0.4153,  0.0158, -0.5705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 3 ] state=tensor([[ 0.0388,  0.4153,  0.0158, -0.5705]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0471,  0.6102,  0.0044, -0.8582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 4 ] state=tensor([[ 0.0471,  0.6102,  0.0044, -0.8582]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0593,  0.4150, -0.0128, -0.5641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 5 ] state=tensor([[ 0.0593,  0.4150, -0.0128, -0.5641]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0676,  0.6103, -0.0240, -0.8608]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 6 ] state=tensor([[ 0.0676,  0.6103, -0.0240, -0.8608]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0798,  0.4156, -0.0413, -0.5758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 7 ] state=tensor([[ 0.0798,  0.4156, -0.0413, -0.5758]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0881,  0.6112, -0.0528, -0.8811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 8 ] state=tensor([[ 0.0881,  0.6112, -0.0528, -0.8811]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1004,  0.4169, -0.0704, -0.6055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 9 ] state=tensor([[ 0.1004,  0.4169, -0.0704, -0.6055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1087,  0.6129, -0.0825, -0.9195]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 10 ] state=tensor([[ 0.1087,  0.6129, -0.0825, -0.9195]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1209,  0.4190, -0.1009, -0.6538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 11 ] state=tensor([[ 0.1209,  0.4190, -0.1009, -0.6538]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1293,  0.6154, -0.1140, -0.9765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 12 ] state=tensor([[ 0.1293,  0.6154, -0.1140, -0.9765]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1416,  0.4219, -0.1335, -0.7217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 13 ] state=tensor([[ 0.1416,  0.4219, -0.1335, -0.7217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1501,  0.6186, -0.1479, -1.0532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 14 ] state=tensor([[ 0.1501,  0.6186, -0.1479, -1.0532]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1624,  0.4257, -0.1690, -0.8104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 15 ] state=tensor([[ 0.1624,  0.4257, -0.1690, -0.8104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1710,  0.6227, -0.1852, -1.1511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 16 ] state=tensor([[ 0.1710,  0.6227, -0.1852, -1.1511]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1834,  0.4304, -0.2082, -0.9218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 19 ][ timestamp 17 ] state=tensor([[ 0.1834,  0.4304, -0.2082, -0.9218]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 19: Exploration_rate=0.05. Score=17.\n",
      "[ episode 20 ] state=tensor([[-0.0417,  0.0216,  0.0429, -0.0304]])\n",
      "[ episode 20 ][ timestamp 1 ] state=tensor([[-0.0417,  0.0216,  0.0429, -0.0304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0412, -0.1741,  0.0423,  0.2756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 2 ] state=tensor([[-0.0412, -0.1741,  0.0423,  0.2756]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0447, -0.3698,  0.0478,  0.5813]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 3 ] state=tensor([[-0.0447, -0.3698,  0.0478,  0.5813]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0521, -0.5656,  0.0595,  0.8886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 4 ] state=tensor([[-0.0521, -0.5656,  0.0595,  0.8886]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0634, -0.7615,  0.0772,  1.1994]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 5 ] state=tensor([[-0.0634, -0.7615,  0.0772,  1.1994]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0787, -0.5674,  0.1012,  0.9319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 6 ] state=tensor([[-0.0787, -0.5674,  0.1012,  0.9319]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0900, -0.7638,  0.1198,  1.2546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 7 ] state=tensor([[-0.0900, -0.7638,  0.1198,  1.2546]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1053, -0.9602,  0.1449,  1.5823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 8 ] state=tensor([[-0.1053, -0.9602,  0.1449,  1.5823]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1245, -1.1567,  0.1766,  1.9164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 20 ][ timestamp 9 ] state=tensor([[-0.1245, -1.1567,  0.1766,  1.9164]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 20: Exploration_rate=0.05. Score=9.\n",
      "[ episode 21 ] state=tensor([[-0.0201, -0.0006,  0.0207,  0.0008]])\n",
      "[ episode 21 ][ timestamp 1 ] state=tensor([[-0.0201, -0.0006,  0.0207,  0.0008]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0202, -0.1960,  0.0207,  0.2999]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 2 ] state=tensor([[-0.0202, -0.1960,  0.0207,  0.2999]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0241, -0.3914,  0.0267,  0.5991]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 3 ] state=tensor([[-0.0241, -0.3914,  0.0267,  0.5991]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0319, -0.5869,  0.0387,  0.9000]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 4 ] state=tensor([[-0.0319, -0.5869,  0.0387,  0.9000]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0436, -0.7825,  0.0567,  1.2046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 5 ] state=tensor([[-0.0436, -0.7825,  0.0567,  1.2046]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0593, -0.5882,  0.0808,  0.9302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 6 ] state=tensor([[-0.0593, -0.5882,  0.0808,  0.9302]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0711, -0.7843,  0.0994,  1.2471]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 7 ] state=tensor([[-0.0711, -0.7843,  0.0994,  1.2471]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0867, -0.9805,  0.1243,  1.5692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 8 ] state=tensor([[-0.0867, -0.9805,  0.1243,  1.5692]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1064, -1.1769,  0.1557,  1.8979]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 9 ] state=tensor([[-0.1064, -1.1769,  0.1557,  1.8979]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1299, -1.3733,  0.1936,  2.2346]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 21 ][ timestamp 10 ] state=tensor([[-0.1299, -1.3733,  0.1936,  2.2346]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 21: Exploration_rate=0.05. Score=10.\n",
      "[ episode 22 ] state=tensor([[-0.0198, -0.0091,  0.0434, -0.0008]])\n",
      "[ episode 22 ][ timestamp 1 ] state=tensor([[-0.0198, -0.0091,  0.0434, -0.0008]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0200, -0.2049,  0.0434,  0.3053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 2 ] state=tensor([[-0.0200, -0.2049,  0.0434,  0.3053]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0241, -0.4006,  0.0495,  0.6113]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 3 ] state=tensor([[-0.0241, -0.4006,  0.0495,  0.6113]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0321, -0.5964,  0.0618,  0.9192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 4 ] state=tensor([[-0.0321, -0.5964,  0.0618,  0.9192]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0440, -0.7922,  0.0801,  1.2306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 5 ] state=tensor([[-0.0440, -0.7922,  0.0801,  1.2306]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0599, -0.9883,  0.1048,  1.5473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 6 ] state=tensor([[-0.0599, -0.9883,  0.1048,  1.5473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0796, -1.1845,  0.1357,  1.8708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 7 ] state=tensor([[-0.0796, -1.1845,  0.1357,  1.8708]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1033, -1.3808,  0.1731,  2.2023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 22 ][ timestamp 8 ] state=tensor([[-0.1033, -1.3808,  0.1731,  2.2023]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 22: Exploration_rate=0.05. Score=8.\n",
      "[ episode 23 ] state=tensor([[-0.0472, -0.0413,  0.0064, -0.0313]])\n",
      "[ episode 23 ][ timestamp 1 ] state=tensor([[-0.0472, -0.0413,  0.0064, -0.0313]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0480,  0.1537,  0.0057, -0.3219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 2 ] state=tensor([[-0.0480,  0.1537,  0.0057, -0.3219]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0449,  0.3487, -0.0007, -0.6128]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 3 ] state=tensor([[-0.0449,  0.3487, -0.0007, -0.6128]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0379,  0.5439, -0.0130, -0.9057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 4 ] state=tensor([[-0.0379,  0.5439, -0.0130, -0.9057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0271,  0.7392, -0.0311, -1.2024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 5 ] state=tensor([[-0.0271,  0.7392, -0.0311, -1.2024]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0123,  0.5445, -0.0551, -0.9196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 6 ] state=tensor([[-0.0123,  0.5445, -0.0551, -0.9196]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0014,  0.3501, -0.0735, -0.6448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 7 ] state=tensor([[-0.0014,  0.3501, -0.0735, -0.6448]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0056,  0.5462, -0.0864, -0.9597]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 8 ] state=tensor([[ 0.0056,  0.5462, -0.0864, -0.9597]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0165,  0.3523, -0.1056, -0.6953]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 9 ] state=tensor([[ 0.0165,  0.3523, -0.1056, -0.6953]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0236,  0.1588, -0.1195, -0.4377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 10 ] state=tensor([[ 0.0236,  0.1588, -0.1195, -0.4377]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0268,  0.3554, -0.1283, -0.7655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 11 ] state=tensor([[ 0.0268,  0.3554, -0.1283, -0.7655]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0339,  0.1623, -0.1436, -0.5158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 12 ] state=tensor([[ 0.0339,  0.1623, -0.1436, -0.5158]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0371,  0.3591, -0.1539, -0.8500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 13 ] state=tensor([[ 0.0371,  0.3591, -0.1539, -0.8500]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0443,  0.1664, -0.1709, -0.6094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 14 ] state=tensor([[ 0.0443,  0.1664, -0.1709, -0.6094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0476,  0.3634, -0.1831, -0.9507]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 15 ] state=tensor([[ 0.0476,  0.3634, -0.1831, -0.9507]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0549,  0.1711, -0.2021, -0.7207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 23 ][ timestamp 16 ] state=tensor([[ 0.0549,  0.1711, -0.2021, -0.7207]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 23: Exploration_rate=0.05. Score=16.\n",
      "[ episode 24 ] state=tensor([[-0.0020, -0.0187,  0.0470,  0.0360]])\n",
      "[ episode 24 ][ timestamp 1 ] state=tensor([[-0.0020, -0.0187,  0.0470,  0.0360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0024,  0.1757,  0.0477, -0.2415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 2 ] state=tensor([[-0.0024,  0.1757,  0.0477, -0.2415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0011,  0.3701,  0.0429, -0.5187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 3 ] state=tensor([[ 0.0011,  0.3701,  0.0429, -0.5187]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0085,  0.5646,  0.0325, -0.7976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 4 ] state=tensor([[ 0.0085,  0.5646,  0.0325, -0.7976]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0198,  0.3690,  0.0166, -0.4948]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 5 ] state=tensor([[ 0.0198,  0.3690,  0.0166, -0.4948]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0272,  0.1737,  0.0067, -0.1970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 6 ] state=tensor([[ 0.0272,  0.1737,  0.0067, -0.1970]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0306,  0.3687,  0.0027, -0.4875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 7 ] state=tensor([[ 0.0306,  0.3687,  0.0027, -0.4875]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0380,  0.5638, -0.0070, -0.7794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 8 ] state=tensor([[ 0.0380,  0.5638, -0.0070, -0.7794]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0493,  0.3688, -0.0226, -0.4889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 9 ] state=tensor([[ 0.0493,  0.3688, -0.0226, -0.4889]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0567,  0.5642, -0.0324, -0.7886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 10 ] state=tensor([[ 0.0567,  0.5642, -0.0324, -0.7886]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0680,  0.3695, -0.0481, -0.5063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 11 ] state=tensor([[ 0.0680,  0.3695, -0.0481, -0.5063]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0754,  0.5653, -0.0583, -0.8137]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 12 ] state=tensor([[ 0.0754,  0.5653, -0.0583, -0.8137]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0867,  0.3710, -0.0745, -0.5399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 13 ] state=tensor([[ 0.0867,  0.3710, -0.0745, -0.5399]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0941,  0.1770, -0.0853, -0.2716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 14 ] state=tensor([[ 0.0941,  0.1770, -0.0853, -0.2716]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0976,  0.3733, -0.0908, -0.5900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 15 ] state=tensor([[ 0.0976,  0.3733, -0.0908, -0.5900]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1051,  0.5695, -0.1026, -0.9098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 16 ] state=tensor([[ 0.1051,  0.5695, -0.1026, -0.9098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1165,  0.3759, -0.1208, -0.6510]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 17 ] state=tensor([[ 0.1165,  0.3759, -0.1208, -0.6510]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1240,  0.1827, -0.1338, -0.3987]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 18 ] state=tensor([[ 0.1240,  0.1827, -0.1338, -0.3987]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1276,  0.3794, -0.1418, -0.7304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 19 ] state=tensor([[ 0.1276,  0.3794, -0.1418, -0.7304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1352,  0.1865, -0.1564, -0.4855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 20 ] state=tensor([[ 0.1352,  0.1865, -0.1564, -0.4855]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1390,  0.3835, -0.1661, -0.8231]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 21 ] state=tensor([[ 0.1390,  0.3835, -0.1661, -0.8231]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1466,  0.1909, -0.1825, -0.5869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 22 ] state=tensor([[ 0.1466,  0.1909, -0.1825, -0.5869]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1505,  0.3881, -0.1943, -0.9311]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 24 ][ timestamp 23 ] state=tensor([[ 0.1505,  0.3881, -0.1943, -0.9311]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 24: Exploration_rate=0.05. Score=23.\n",
      "[ episode 25 ] state=tensor([[ 0.0401, -0.0458,  0.0243,  0.0173]])\n",
      "[ episode 25 ][ timestamp 1 ] state=tensor([[ 0.0401, -0.0458,  0.0243,  0.0173]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0392,  0.1490,  0.0246, -0.2676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 2 ] state=tensor([[ 0.0392,  0.1490,  0.0246, -0.2676]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0422,  0.3438,  0.0193, -0.5524]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 3 ] state=tensor([[ 0.0422,  0.3438,  0.0193, -0.5524]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0490,  0.5386,  0.0082, -0.8390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 4 ] state=tensor([[ 0.0490,  0.5386,  0.0082, -0.8390]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0598,  0.3434, -0.0085, -0.5437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 5 ] state=tensor([[ 0.0598,  0.3434, -0.0085, -0.5437]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0667,  0.5386, -0.0194, -0.8391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 6 ] state=tensor([[ 0.0667,  0.5386, -0.0194, -0.8391]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0774,  0.3438, -0.0362, -0.5526]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 7 ] state=tensor([[ 0.0774,  0.3438, -0.0362, -0.5526]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0843,  0.1492, -0.0473, -0.2715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 8 ] state=tensor([[ 0.0843,  0.1492, -0.0473, -0.2715]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0873,  0.3449, -0.0527, -0.5787]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 9 ] state=tensor([[ 0.0873,  0.3449, -0.0527, -0.5787]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0942,  0.1506, -0.0643, -0.3031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 10 ] state=tensor([[ 0.0942,  0.1506, -0.0643, -0.3031]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0972,  0.3466, -0.0703, -0.6153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 11 ] state=tensor([[ 0.0972,  0.3466, -0.0703, -0.6153]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1041,  0.1525, -0.0826, -0.3456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 12 ] state=tensor([[ 0.1041,  0.1525, -0.0826, -0.3456]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1072,  0.3487, -0.0895, -0.6632]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 25 ][ timestamp 13 ] state=tensor([[ 0.1072,  0.3487, -0.0895, -0.6632]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1142,  0.1549, -0.1028, -0.4000]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 14 ] state=tensor([[ 0.1142,  0.1549, -0.1028, -0.4000]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1173,  0.3513, -0.1108, -0.7232]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 15 ] state=tensor([[ 0.1173,  0.3513, -0.1108, -0.7232]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1243,  0.1579, -0.1253, -0.4673]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 16 ] state=tensor([[ 0.1243,  0.1579, -0.1253, -0.4673]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1274,  0.3546, -0.1346, -0.7967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 17 ] state=tensor([[ 0.1274,  0.3546, -0.1346, -0.7967]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1345,  0.1615, -0.1505, -0.5493]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 18 ] state=tensor([[ 0.1345,  0.1615, -0.1505, -0.5493]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1378, -0.0312, -0.1615, -0.3075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 19 ] state=tensor([[ 0.1378, -0.0312, -0.1615, -0.3075]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1371,  0.1658, -0.1677, -0.6465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 20 ] state=tensor([[ 0.1371,  0.1658, -0.1677, -0.6465]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1405,  0.3628, -0.1806, -0.9869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 21 ] state=tensor([[ 0.1405,  0.3628, -0.1806, -0.9869]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1477,  0.1705, -0.2004, -0.7560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 25 ][ timestamp 22 ] state=tensor([[ 0.1477,  0.1705, -0.2004, -0.7560]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 25: Exploration_rate=0.05. Score=22.\n",
      "[ episode 26 ] state=tensor([[ 0.0161, -0.0146, -0.0021, -0.0143]])\n",
      "[ episode 26 ][ timestamp 1 ] state=tensor([[ 0.0161, -0.0146, -0.0021, -0.0143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0158,  0.1806, -0.0024, -0.3077]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 2 ] state=tensor([[ 0.0158,  0.1806, -0.0024, -0.3077]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0194,  0.3757, -0.0086, -0.6011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 3 ] state=tensor([[ 0.0194,  0.3757, -0.0086, -0.6011]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0270,  0.5710, -0.0206, -0.8965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 4 ] state=tensor([[ 0.0270,  0.5710, -0.0206, -0.8965]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0384,  0.3762, -0.0385, -0.6103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 5 ] state=tensor([[ 0.0384,  0.3762, -0.0385, -0.6103]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0459,  0.5718, -0.0507, -0.9149]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 6 ] state=tensor([[ 0.0459,  0.5718, -0.0507, -0.9149]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0573,  0.3774, -0.0690, -0.6386]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 7 ] state=tensor([[ 0.0573,  0.3774, -0.0690, -0.6386]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0649,  0.1833, -0.0818, -0.3684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 8 ] state=tensor([[ 0.0649,  0.1833, -0.0818, -0.3684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0685, -0.0106, -0.0892, -0.1026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 9 ] state=tensor([[ 0.0685, -0.0106, -0.0892, -0.1026]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0683,  0.1857, -0.0912, -0.4220]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 10 ] state=tensor([[ 0.0683,  0.1857, -0.0912, -0.4220]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0720,  0.3820, -0.0997, -0.7420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 11 ] state=tensor([[ 0.0720,  0.3820, -0.0997, -0.7420]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0797,  0.1884, -0.1145, -0.4823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 12 ] state=tensor([[ 0.0797,  0.1884, -0.1145, -0.4823]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0835,  0.3849, -0.1242, -0.8088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 13 ] state=tensor([[ 0.0835,  0.3849, -0.1242, -0.8088]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0912,  0.1917, -0.1403, -0.5576]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 14 ] state=tensor([[ 0.0912,  0.1917, -0.1403, -0.5576]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0950,  0.3885, -0.1515, -0.8910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 15 ] state=tensor([[ 0.0950,  0.3885, -0.1515, -0.8910]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1028,  0.1957, -0.1693, -0.6495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 16 ] state=tensor([[ 0.1028,  0.1957, -0.1693, -0.6495]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1067,  0.3927, -0.1823, -0.9903]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 17 ] state=tensor([[ 0.1067,  0.3927, -0.1823, -0.9903]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1145,  0.2004, -0.2021, -0.7600]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 26 ][ timestamp 18 ] state=tensor([[ 0.1145,  0.2004, -0.2021, -0.7600]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 26: Exploration_rate=0.05. Score=18.\n",
      "[ episode 27 ] state=tensor([[ 0.0076, -0.0180,  0.0341,  0.0072]])\n",
      "[ episode 27 ][ timestamp 1 ] state=tensor([[ 0.0076, -0.0180,  0.0341,  0.0072]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0073,  0.1766,  0.0342, -0.2745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 2 ] state=tensor([[ 0.0073,  0.1766,  0.0342, -0.2745]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0108,  0.3712,  0.0287, -0.5562]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 3 ] state=tensor([[ 0.0108,  0.3712,  0.0287, -0.5562]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0182,  0.5659,  0.0176, -0.8397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 4 ] state=tensor([[ 0.0182,  0.5659,  0.0176, -0.8397]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0295,  0.3706,  0.0008, -0.5415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 5 ] state=tensor([[ 0.0295,  0.3706,  0.0008, -0.5415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0369,  0.5657, -0.0100, -0.8340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 6 ] state=tensor([[ 0.0369,  0.5657, -0.0100, -0.8340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0483,  0.3707, -0.0267, -0.5445]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 7 ] state=tensor([[ 0.0483,  0.3707, -0.0267, -0.5445]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0557,  0.1760, -0.0376, -0.2603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 8 ] state=tensor([[ 0.0557,  0.1760, -0.0376, -0.2603]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0592,  0.3716, -0.0428, -0.5646]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 9 ] state=tensor([[ 0.0592,  0.3716, -0.0428, -0.5646]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0666,  0.1771, -0.0541, -0.2857]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 10 ] state=tensor([[ 0.0666,  0.1771, -0.0541, -0.2857]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0702,  0.3730, -0.0598, -0.5949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 11 ] state=tensor([[ 0.0702,  0.3730, -0.0598, -0.5949]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0776,  0.1787, -0.0717, -0.3217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 12 ] state=tensor([[ 0.0776,  0.1787, -0.0717, -0.3217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0812,  0.3748, -0.0781, -0.6361]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 13 ] state=tensor([[ 0.0812,  0.3748, -0.0781, -0.6361]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0887,  0.1809, -0.0909, -0.3690]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 14 ] state=tensor([[ 0.0887,  0.1809, -0.0909, -0.3690]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0923,  0.3771, -0.0982, -0.6889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 15 ] state=tensor([[ 0.0923,  0.3771, -0.0982, -0.6889]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0999,  0.5735, -0.1120, -1.0108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 16 ] state=tensor([[ 0.0999,  0.5735, -0.1120, -1.0108]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1113,  0.3800, -0.1322, -0.7553]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 17 ] state=tensor([[ 0.1113,  0.3800, -0.1322, -0.7553]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1189,  0.1869, -0.1473, -0.5070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 18 ] state=tensor([[ 0.1189,  0.1869, -0.1473, -0.5070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1227,  0.3838, -0.1575, -0.8422]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 19 ] state=tensor([[ 0.1227,  0.3838, -0.1575, -0.8422]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1303,  0.1911, -0.1743, -0.6029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 20 ] state=tensor([[ 0.1303,  0.1911, -0.1743, -0.6029]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1342,  0.3882, -0.1864, -0.9451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 21 ] state=tensor([[ 0.1342,  0.3882, -0.1864, -0.9451]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1419,  0.1960, -0.2053, -0.7163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 27 ][ timestamp 22 ] state=tensor([[ 0.1419,  0.1960, -0.2053, -0.7163]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 27: Exploration_rate=0.05. Score=22.\n",
      "[ episode 28 ] state=tensor([[-0.0144, -0.0184,  0.0075, -0.0398]])\n",
      "[ episode 28 ][ timestamp 1 ] state=tensor([[-0.0144, -0.0184,  0.0075, -0.0398]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0148, -0.2136,  0.0067,  0.2552]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 2 ] state=tensor([[-0.0148, -0.2136,  0.0067,  0.2552]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0191, -0.0186,  0.0118, -0.0353]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 3 ] state=tensor([[-0.0191, -0.0186,  0.0118, -0.0353]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0194,  0.1764,  0.0111, -0.3243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 4 ] state=tensor([[-0.0194,  0.1764,  0.0111, -0.3243]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0159,  0.3713,  0.0046, -0.6134]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 5 ] state=tensor([[-0.0159,  0.3713,  0.0046, -0.6134]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0085,  0.5664, -0.0077, -0.9047]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 6 ] state=tensor([[-0.0085,  0.5664, -0.0077, -0.9047]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0029,  0.3714, -0.0258, -0.6144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 7 ] state=tensor([[ 0.0029,  0.3714, -0.0258, -0.6144]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0103,  0.1766, -0.0381, -0.3300]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 8 ] state=tensor([[ 0.0103,  0.1766, -0.0381, -0.3300]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0138,  0.3722, -0.0447, -0.6344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 9 ] state=tensor([[ 0.0138,  0.3722, -0.0447, -0.6344]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0213,  0.1778, -0.0573, -0.3561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 10 ] state=tensor([[ 0.0213,  0.1778, -0.0573, -0.3561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0248,  0.3737, -0.0645, -0.6663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 11 ] state=tensor([[ 0.0248,  0.3737, -0.0645, -0.6663]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0323,  0.1795, -0.0778, -0.3946]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 12 ] state=tensor([[ 0.0323,  0.1795, -0.0778, -0.3946]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0359,  0.3756, -0.0857, -0.7108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 13 ] state=tensor([[ 0.0359,  0.3756, -0.0857, -0.7108]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0434,  0.5718, -0.0999, -1.0291]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 28 ][ timestamp 14 ] state=tensor([[ 0.0434,  0.5718, -0.0999, -1.0291]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0548,  0.3782, -0.1205, -0.7694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 15 ] state=tensor([[ 0.0548,  0.3782, -0.1205, -0.7694]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0624,  0.1849, -0.1359, -0.5169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 16 ] state=tensor([[ 0.0624,  0.1849, -0.1359, -0.5169]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0661,  0.3816, -0.1462, -0.8492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 17 ] state=tensor([[ 0.0661,  0.3816, -0.1462, -0.8492]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0737,  0.1888, -0.1632, -0.6058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 18 ] state=tensor([[ 0.0737,  0.1888, -0.1632, -0.6058]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0775,  0.3858, -0.1753, -0.9451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 19 ] state=tensor([[ 0.0775,  0.3858, -0.1753, -0.9451]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0852,  0.5828, -0.1942, -1.2874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 28 ][ timestamp 20 ] state=tensor([[ 0.0852,  0.5828, -0.1942, -1.2874]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 28: Exploration_rate=0.05. Score=20.\n",
      "[ episode 29 ] state=tensor([[-0.0139,  0.0218,  0.0464, -0.0242]])\n",
      "[ episode 29 ][ timestamp 1 ] state=tensor([[-0.0139,  0.0218,  0.0464, -0.0242]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0134,  0.2162,  0.0459, -0.3019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 2 ] state=tensor([[-0.0134,  0.2162,  0.0459, -0.3019]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0091,  0.0205,  0.0398,  0.0049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 3 ] state=tensor([[-0.0091,  0.0205,  0.0398,  0.0049]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0087,  0.2150,  0.0399, -0.2750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 4 ] state=tensor([[-0.0087,  0.2150,  0.0399, -0.2750]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0044,  0.4096,  0.0344, -0.5548]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 5 ] state=tensor([[-0.0044,  0.4096,  0.0344, -0.5548]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0038,  0.6042,  0.0233, -0.8365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 6 ] state=tensor([[ 0.0038,  0.6042,  0.0233, -0.8365]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0159,  0.4087,  0.0066, -0.5365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 7 ] state=tensor([[ 0.0159,  0.4087,  0.0066, -0.5365]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0240,  0.6038, -0.0041, -0.8271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 8 ] state=tensor([[ 0.0240,  0.6038, -0.0041, -0.8271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0361,  0.4087, -0.0207, -0.5358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 9 ] state=tensor([[ 0.0361,  0.4087, -0.0207, -0.5358]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0443,  0.6041, -0.0314, -0.8349]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 10 ] state=tensor([[ 0.0443,  0.6041, -0.0314, -0.8349]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0564,  0.4094, -0.0481, -0.5522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 11 ] state=tensor([[ 0.0564,  0.4094, -0.0481, -0.5522]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0646,  0.2150, -0.0591, -0.2751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 12 ] state=tensor([[ 0.0646,  0.2150, -0.0591, -0.2751]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0689,  0.4109, -0.0646, -0.5858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 13 ] state=tensor([[ 0.0689,  0.4109, -0.0646, -0.5858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0771,  0.2168, -0.0763, -0.3142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 14 ] state=tensor([[ 0.0771,  0.2168, -0.0763, -0.3142]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0814,  0.4129, -0.0826, -0.6299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 15 ] state=tensor([[ 0.0814,  0.4129, -0.0826, -0.6299]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0897,  0.6091, -0.0952, -0.9474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 16 ] state=tensor([[ 0.0897,  0.6091, -0.0952, -0.9474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1019,  0.4153, -0.1142, -0.6861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 17 ] state=tensor([[ 0.1019,  0.4153, -0.1142, -0.6861]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1102,  0.6118, -0.1279, -1.0125]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 18 ] state=tensor([[ 0.1102,  0.6118, -0.1279, -1.0125]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1224,  0.4186, -0.1482, -0.7625]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 19 ] state=tensor([[ 0.1224,  0.4186, -0.1482, -0.7625]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1308,  0.6155, -0.1634, -1.0979]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 20 ] state=tensor([[ 0.1308,  0.6155, -0.1634, -1.0979]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1431,  0.4228, -0.1854, -0.8606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 21 ] state=tensor([[ 0.1431,  0.4228, -0.1854, -0.8606]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1515,  0.2306, -0.2026, -0.6315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 29 ][ timestamp 22 ] state=tensor([[ 0.1515,  0.2306, -0.2026, -0.6315]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 29: Exploration_rate=0.05. Score=22.\n",
      "[ episode 30 ] state=tensor([[-0.0125, -0.0010, -0.0431,  0.0157]])\n",
      "[ episode 30 ][ timestamp 1 ] state=tensor([[-0.0125, -0.0010, -0.0431,  0.0157]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0125,  0.1947, -0.0428, -0.2902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 2 ] state=tensor([[-0.0125,  0.1947, -0.0428, -0.2902]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0086,  0.3904, -0.0486, -0.5961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 3 ] state=tensor([[-0.0086,  0.3904, -0.0486, -0.5961]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0008,  0.1960, -0.0605, -0.3191]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 4 ] state=tensor([[-0.0008,  0.1960, -0.0605, -0.3191]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0031,  0.0018, -0.0669, -0.0461]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 5 ] state=tensor([[ 0.0031,  0.0018, -0.0669, -0.0461]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0032,  0.1978, -0.0678, -0.3591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 6 ] state=tensor([[ 0.0032,  0.1978, -0.0678, -0.3591]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0071,  0.3939, -0.0750, -0.6724]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 7 ] state=tensor([[ 0.0071,  0.3939, -0.0750, -0.6724]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0150,  0.1999, -0.0884, -0.4042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 8 ] state=tensor([[ 0.0150,  0.1999, -0.0884, -0.4042]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0190,  0.3961, -0.0965, -0.7234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 9 ] state=tensor([[ 0.0190,  0.3961, -0.0965, -0.7234]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0269,  0.2024, -0.1110, -0.4626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 10 ] state=tensor([[ 0.0269,  0.2024, -0.1110, -0.4626]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0310,  0.3989, -0.1202, -0.7881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 11 ] state=tensor([[ 0.0310,  0.3989, -0.1202, -0.7881]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0390,  0.5955, -0.1360, -1.1161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 12 ] state=tensor([[ 0.0390,  0.5955, -0.1360, -1.1161]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0509,  0.4024, -0.1583, -0.8689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 13 ] state=tensor([[ 0.0509,  0.4024, -0.1583, -0.8689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0589,  0.2097, -0.1757, -0.6299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 14 ] state=tensor([[ 0.0589,  0.2097, -0.1757, -0.6299]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0631,  0.0175, -0.1883, -0.3973]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 15 ] state=tensor([[ 0.0631,  0.0175, -0.1883, -0.3973]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0635,  0.2147, -0.1962, -0.7430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 30 ][ timestamp 16 ] state=tensor([[ 0.0635,  0.2147, -0.1962, -0.7430]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 30: Exploration_rate=0.05. Score=16.\n",
      "[ episode 31 ] state=tensor([[-0.0264, -0.0412,  0.0433,  0.0077]])\n",
      "[ episode 31 ][ timestamp 1 ] state=tensor([[-0.0264, -0.0412,  0.0433,  0.0077]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0272,  0.1533,  0.0435, -0.2710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 2 ] state=tensor([[-0.0272,  0.1533,  0.0435, -0.2710]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0241,  0.3478,  0.0381, -0.5497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 3 ] state=tensor([[-0.0241,  0.3478,  0.0381, -0.5497]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0172,  0.1522,  0.0271, -0.2452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 4 ] state=tensor([[-0.0172,  0.1522,  0.0271, -0.2452]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0141,  0.3469,  0.0222, -0.5292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 5 ] state=tensor([[-0.0141,  0.3469,  0.0222, -0.5292]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0072,  0.1515,  0.0116, -0.2297]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 6 ] state=tensor([[-0.0072,  0.1515,  0.0116, -0.2297]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0042,  0.3464,  0.0070, -0.5187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 7 ] state=tensor([[-0.0042,  0.3464,  0.0070, -0.5187]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0028,  0.1512, -0.0034, -0.2238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 8 ] state=tensor([[ 0.0028,  0.1512, -0.0034, -0.2238]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0058,  0.3464, -0.0078, -0.5175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 9 ] state=tensor([[ 0.0058,  0.3464, -0.0078, -0.5175]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0127,  0.5416, -0.0182, -0.8127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 10 ] state=tensor([[ 0.0127,  0.5416, -0.0182, -0.8127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0235,  0.3467, -0.0344, -0.5257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 11 ] state=tensor([[ 0.0235,  0.3467, -0.0344, -0.5257]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0305,  0.1521, -0.0450, -0.2441]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 31 ][ timestamp 12 ] state=tensor([[ 0.0305,  0.1521, -0.0450, -0.2441]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0335,  0.3478, -0.0498, -0.5506]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 13 ] state=tensor([[ 0.0335,  0.3478, -0.0498, -0.5506]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0405,  0.1535, -0.0609, -0.2741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 14 ] state=tensor([[ 0.0405,  0.1535, -0.0609, -0.2741]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0435,  0.3494, -0.0663, -0.5853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 15 ] state=tensor([[ 0.0435,  0.3494, -0.0663, -0.5853]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0505,  0.1553, -0.0780, -0.3142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 16 ] state=tensor([[ 0.0505,  0.1553, -0.0780, -0.3142]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0536,  0.3514, -0.0843, -0.6305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 17 ] state=tensor([[ 0.0536,  0.3514, -0.0843, -0.6305]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0607,  0.1575, -0.0969, -0.3655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 18 ] state=tensor([[ 0.0607,  0.1575, -0.0969, -0.3655]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0638,  0.3539, -0.1042, -0.6871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 19 ] state=tensor([[ 0.0638,  0.3539, -0.1042, -0.6871]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0709,  0.5503, -0.1180, -1.0107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 20 ] state=tensor([[ 0.0709,  0.5503, -0.1180, -1.0107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0819,  0.3569, -0.1382, -0.7573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 21 ] state=tensor([[ 0.0819,  0.3569, -0.1382, -0.7573]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0890,  0.5537, -0.1533, -1.0901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 22 ] state=tensor([[ 0.0890,  0.5537, -0.1533, -1.0901]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1001,  0.3609, -0.1751, -0.8491]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 23 ] state=tensor([[ 0.1001,  0.3609, -0.1751, -0.8491]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1073,  0.1685, -0.1921, -0.6163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 24 ] state=tensor([[ 0.1073,  0.1685, -0.1921, -0.6163]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1107,  0.3657, -0.2045, -0.9628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 31 ][ timestamp 25 ] state=tensor([[ 0.1107,  0.3657, -0.2045, -0.9628]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 31: Exploration_rate=0.05. Score=25.\n",
      "[ episode 32 ] state=tensor([[-0.0098, -0.0356, -0.0191, -0.0324]])\n",
      "[ episode 32 ][ timestamp 1 ] state=tensor([[-0.0098, -0.0356, -0.0191, -0.0324]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0105,  0.1598, -0.0197, -0.3310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 2 ] state=tensor([[-0.0105,  0.1598, -0.0197, -0.3310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0073,  0.3552, -0.0264, -0.6298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 3 ] state=tensor([[-0.0073,  0.3552, -0.0264, -0.6298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9932e-04,  1.6048e-01, -3.8955e-02, -3.4558e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 4 ] state=tensor([[-1.9932e-04,  1.6048e-01, -3.8955e-02, -3.4558e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0030,  0.3561, -0.0459, -0.6503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 5 ] state=tensor([[ 0.0030,  0.3561, -0.0459, -0.6503]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0101,  0.1617, -0.0589, -0.3724]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 6 ] state=tensor([[ 0.0101,  0.1617, -0.0589, -0.3724]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0134,  0.3576, -0.0663, -0.6830]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 7 ] state=tensor([[ 0.0134,  0.3576, -0.0663, -0.6830]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0205,  0.1634, -0.0800, -0.4120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 8 ] state=tensor([[ 0.0205,  0.1634, -0.0800, -0.4120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0238,  0.3596, -0.0882, -0.7287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 9 ] state=tensor([[ 0.0238,  0.3596, -0.0882, -0.7287]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0310,  0.1658, -0.1028, -0.4651]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 10 ] state=tensor([[ 0.0310,  0.1658, -0.1028, -0.4651]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0343,  0.3622, -0.1121, -0.7883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 11 ] state=tensor([[ 0.0343,  0.3622, -0.1121, -0.7883]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0415,  0.5587, -0.1279, -1.1140]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 12 ] state=tensor([[ 0.0415,  0.5587, -0.1279, -1.1140]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0527,  0.3655, -0.1501, -0.8641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 13 ] state=tensor([[ 0.0527,  0.3655, -0.1501, -0.8641]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0600,  0.1727, -0.1674, -0.6221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 14 ] state=tensor([[ 0.0600,  0.1727, -0.1674, -0.6221]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0635,  0.3697, -0.1799, -0.9625]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 15 ] state=tensor([[ 0.0635,  0.3697, -0.1799, -0.9625]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0709,  0.1774, -0.1991, -0.7313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 32 ][ timestamp 16 ] state=tensor([[ 0.0709,  0.1774, -0.1991, -0.7313]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 32: Exploration_rate=0.05. Score=16.\n",
      "[ episode 33 ] state=tensor([[-0.0380,  0.0161, -0.0012,  0.0388]])\n",
      "[ episode 33 ][ timestamp 1 ] state=tensor([[-0.0380,  0.0161, -0.0012,  0.0388]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0377,  0.2112, -0.0004, -0.2542]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 2 ] state=tensor([[-0.0377,  0.2112, -0.0004, -0.2542]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0335,  0.4064, -0.0055, -0.5470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 3 ] state=tensor([[-0.0335,  0.4064, -0.0055, -0.5470]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0254,  0.2113, -0.0164, -0.2561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 4 ] state=tensor([[-0.0254,  0.2113, -0.0164, -0.2561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0211,  0.0164, -0.0216,  0.0314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 5 ] state=tensor([[-0.0211,  0.0164, -0.0216,  0.0314]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0208, -0.1784, -0.0209,  0.3172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 6 ] state=tensor([[-0.0208, -0.1784, -0.0209,  0.3172]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0244,  0.0170, -0.0146,  0.0180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 7 ] state=tensor([[-0.0244,  0.0170, -0.0146,  0.0180]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0240,  0.2124, -0.0142, -0.2793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 8 ] state=tensor([[-0.0240,  0.2124, -0.0142, -0.2793]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0198,  0.4077, -0.0198, -0.5764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 9 ] state=tensor([[-0.0198,  0.4077, -0.0198, -0.5764]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0116,  0.2129, -0.0313, -0.2901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 10 ] state=tensor([[-0.0116,  0.2129, -0.0313, -0.2901]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0074,  0.4084, -0.0371, -0.5925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 11 ] state=tensor([[-0.0074,  0.4084, -0.0371, -0.5925]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0008,  0.2138, -0.0490, -0.3117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 12 ] state=tensor([[ 0.0008,  0.2138, -0.0490, -0.3117]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0051,  0.4096, -0.0552, -0.6194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 13 ] state=tensor([[ 0.0051,  0.4096, -0.0552, -0.6194]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0133,  0.6055, -0.0676, -0.9290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 14 ] state=tensor([[ 0.0133,  0.6055, -0.0676, -0.9290]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0254,  0.4113, -0.0862, -0.6583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 15 ] state=tensor([[ 0.0254,  0.4113, -0.0862, -0.6583]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0336,  0.6075, -0.0994, -0.9768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 16 ] state=tensor([[ 0.0336,  0.6075, -0.0994, -0.9768]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0457,  0.4139, -0.1189, -0.7169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 17 ] state=tensor([[ 0.0457,  0.4139, -0.1189, -0.7169]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0540,  0.2206, -0.1332, -0.4639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 18 ] state=tensor([[ 0.0540,  0.2206, -0.1332, -0.4639]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0584,  0.4173, -0.1425, -0.7954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 19 ] state=tensor([[ 0.0584,  0.4173, -0.1425, -0.7954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0668,  0.2244, -0.1584, -0.5508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 20 ] state=tensor([[ 0.0668,  0.2244, -0.1584, -0.5508]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0713,  0.4213, -0.1694, -0.8889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 21 ] state=tensor([[ 0.0713,  0.4213, -0.1694, -0.8889]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0797,  0.2289, -0.1872, -0.6539]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 22 ] state=tensor([[ 0.0797,  0.2289, -0.1872, -0.6539]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0843,  0.0368, -0.2003, -0.4255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 23 ] state=tensor([[ 0.0843,  0.0368, -0.2003, -0.4255]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0850,  0.2341, -0.2088, -0.7741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 33 ][ timestamp 24 ] state=tensor([[ 0.0850,  0.2341, -0.2088, -0.7741]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 33: Exploration_rate=0.05. Score=24.\n",
      "[ episode 34 ] state=tensor([[ 0.0035,  0.0193, -0.0147,  0.0290]])\n",
      "[ episode 34 ][ timestamp 1 ] state=tensor([[ 0.0035,  0.0193, -0.0147,  0.0290]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0039, -0.1756, -0.0141,  0.3170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 2 ] state=tensor([[ 0.0039, -0.1756, -0.0141,  0.3170]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0003,  0.0197, -0.0078,  0.0199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 3 ] state=tensor([[ 0.0003,  0.0197, -0.0078,  0.0199]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0007,  0.2149, -0.0074, -0.2753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 4 ] state=tensor([[ 0.0007,  0.2149, -0.0074, -0.2753]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0050,  0.4102, -0.0129, -0.5703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 5 ] state=tensor([[ 0.0050,  0.4102, -0.0129, -0.5703]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0132,  0.2152, -0.0243, -0.2817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 6 ] state=tensor([[ 0.0132,  0.2152, -0.0243, -0.2817]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0175,  0.0204, -0.0299,  0.0032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 7 ] state=tensor([[ 0.0175,  0.0204, -0.0299,  0.0032]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0180,  0.2160, -0.0299, -0.2987]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 34 ][ timestamp 8 ] state=tensor([[ 0.0180,  0.2160, -0.0299, -0.2987]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0223,  0.4115, -0.0359, -0.6007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 9 ] state=tensor([[ 0.0223,  0.4115, -0.0359, -0.6007]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0305,  0.6071, -0.0479, -0.9044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 10 ] state=tensor([[ 0.0305,  0.6071, -0.0479, -0.9044]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0427,  0.4127, -0.0660, -0.6272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 11 ] state=tensor([[ 0.0427,  0.4127, -0.0660, -0.6272]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0509,  0.2185, -0.0785, -0.3560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 12 ] state=tensor([[ 0.0509,  0.2185, -0.0785, -0.3560]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0553,  0.4147, -0.0856, -0.6723]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 13 ] state=tensor([[ 0.0553,  0.4147, -0.0856, -0.6723]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0636,  0.6109, -0.0991, -0.9907]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 14 ] state=tensor([[ 0.0636,  0.6109, -0.0991, -0.9907]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0758,  0.4172, -0.1189, -0.7307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 15 ] state=tensor([[ 0.0758,  0.4172, -0.1189, -0.7307]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0841,  0.2239, -0.1335, -0.4777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 16 ] state=tensor([[ 0.0841,  0.2239, -0.1335, -0.4777]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0886,  0.4207, -0.1430, -0.8093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 17 ] state=tensor([[ 0.0886,  0.4207, -0.1430, -0.8093]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0970,  0.6174, -0.1592, -1.1433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 18 ] state=tensor([[ 0.0970,  0.6174, -0.1592, -1.1433]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1094,  0.4247, -0.1821, -0.9045]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 19 ] state=tensor([[ 0.1094,  0.4247, -0.1821, -0.9045]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1179,  0.2324, -0.2002, -0.6741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 34 ][ timestamp 20 ] state=tensor([[ 0.1179,  0.2324, -0.2002, -0.6741]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 34: Exploration_rate=0.05. Score=20.\n",
      "[ episode 35 ] state=tensor([[0.0433, 0.0441, 0.0398, 0.0066]])\n",
      "[ episode 35 ][ timestamp 1 ] state=tensor([[0.0433, 0.0441, 0.0398, 0.0066]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0442,  0.2386,  0.0399, -0.2733]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 2 ] state=tensor([[ 0.0442,  0.2386,  0.0399, -0.2733]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0490,  0.4331,  0.0344, -0.5531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 3 ] state=tensor([[ 0.0490,  0.4331,  0.0344, -0.5531]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0576,  0.2375,  0.0234, -0.2498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 4 ] state=tensor([[ 0.0576,  0.2375,  0.0234, -0.2498]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0624,  0.4323,  0.0184, -0.5350]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 5 ] state=tensor([[ 0.0624,  0.4323,  0.0184, -0.5350]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0710,  0.2369,  0.0077, -0.2366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 6 ] state=tensor([[ 0.0710,  0.2369,  0.0077, -0.2366]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0758,  0.4319,  0.0030, -0.5268]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 7 ] state=tensor([[ 0.0758,  0.4319,  0.0030, -0.5268]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0844,  0.2368, -0.0076, -0.2332]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 8 ] state=tensor([[ 0.0844,  0.2368, -0.0076, -0.2332]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0892,  0.4320, -0.0122, -0.5283]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 9 ] state=tensor([[ 0.0892,  0.4320, -0.0122, -0.5283]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0978,  0.6273, -0.0228, -0.8248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 10 ] state=tensor([[ 0.0978,  0.6273, -0.0228, -0.8248]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1103,  0.4325, -0.0393, -0.5394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 11 ] state=tensor([[ 0.1103,  0.4325, -0.0393, -0.5394]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1190,  0.2379, -0.0501, -0.2594]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 12 ] state=tensor([[ 0.1190,  0.2379, -0.0501, -0.2594]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1238,  0.4337, -0.0553, -0.5674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 13 ] state=tensor([[ 0.1238,  0.4337, -0.0553, -0.5674]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1324,  0.6296, -0.0666, -0.8770]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 14 ] state=tensor([[ 0.1324,  0.6296, -0.0666, -0.8770]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1450,  0.4354, -0.0842, -0.6060]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 15 ] state=tensor([[ 0.1450,  0.4354, -0.0842, -0.6060]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1537,  0.2416, -0.0963, -0.3409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 16 ] state=tensor([[ 0.1537,  0.2416, -0.0963, -0.3409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1586,  0.4379, -0.1031, -0.6624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 17 ] state=tensor([[ 0.1586,  0.4379, -0.1031, -0.6624]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1673,  0.2444, -0.1164, -0.4038]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 18 ] state=tensor([[ 0.1673,  0.2444, -0.1164, -0.4038]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1722,  0.4410, -0.1244, -0.7308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 19 ] state=tensor([[ 0.1722,  0.4410, -0.1244, -0.7308]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1810,  0.2478, -0.1391, -0.4798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 20 ] state=tensor([[ 0.1810,  0.2478, -0.1391, -0.4798]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1860,  0.4445, -0.1486, -0.8128]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 21 ] state=tensor([[ 0.1860,  0.4445, -0.1486, -0.8128]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1949,  0.2517, -0.1649, -0.5704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 22 ] state=tensor([[ 0.1949,  0.2517, -0.1649, -0.5704]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1999,  0.0593, -0.1763, -0.3338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 23 ] state=tensor([[ 0.1999,  0.0593, -0.1763, -0.3338]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.2011,  0.2564, -0.1830, -0.6765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 24 ] state=tensor([[ 0.2011,  0.2564, -0.1830, -0.6765]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2062,  0.0642, -0.1965, -0.4466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 25 ] state=tensor([[ 0.2062,  0.0642, -0.1965, -0.4466]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.2075,  0.2615, -0.2054, -0.7942]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 35 ][ timestamp 26 ] state=tensor([[ 0.2075,  0.2615, -0.2054, -0.7942]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 35: Exploration_rate=0.05. Score=26.\n",
      "[ episode 36 ] state=tensor([[-0.0202,  0.0160,  0.0235, -0.0219]])\n",
      "[ episode 36 ][ timestamp 1 ] state=tensor([[-0.0202,  0.0160,  0.0235, -0.0219]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0199,  0.2107,  0.0230, -0.3071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 2 ] state=tensor([[-0.0199,  0.2107,  0.0230, -0.3071]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0156,  0.4055,  0.0169, -0.5925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 3 ] state=tensor([[-0.0156,  0.4055,  0.0169, -0.5925]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0075,  0.6004,  0.0050, -0.8798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 4 ] state=tensor([[-0.0075,  0.6004,  0.0050, -0.8798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0045,  0.4052, -0.0125, -0.5855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 5 ] state=tensor([[ 0.0045,  0.4052, -0.0125, -0.5855]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0126,  0.6005, -0.0243, -0.8821]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 6 ] state=tensor([[ 0.0126,  0.6005, -0.0243, -0.8821]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0246,  0.4057, -0.0419, -0.5971]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 7 ] state=tensor([[ 0.0246,  0.4057, -0.0419, -0.5971]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0327,  0.2112, -0.0538, -0.3180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 8 ] state=tensor([[ 0.0327,  0.2112, -0.0538, -0.3180]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0369,  0.4071, -0.0602, -0.6271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 9 ] state=tensor([[ 0.0369,  0.4071, -0.0602, -0.6271]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0451,  0.6030, -0.0727, -0.9381]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 10 ] state=tensor([[ 0.0451,  0.6030, -0.0727, -0.9381]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0571,  0.4089, -0.0915, -0.6692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 11 ] state=tensor([[ 0.0571,  0.4089, -0.0915, -0.6692]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0653,  0.6052, -0.1049, -0.9892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 12 ] state=tensor([[ 0.0653,  0.6052, -0.1049, -0.9892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0774,  0.4116, -0.1247, -0.7312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 13 ] state=tensor([[ 0.0774,  0.4116, -0.1247, -0.7312]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0856,  0.6082, -0.1393, -1.0604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 14 ] state=tensor([[ 0.0856,  0.6082, -0.1393, -1.0604]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0978,  0.4152, -0.1605, -0.8145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 15 ] state=tensor([[ 0.0978,  0.4152, -0.1605, -0.8145]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1061,  0.2226, -0.1768, -0.5763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 16 ] state=tensor([[ 0.1061,  0.2226, -0.1768, -0.5763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1106,  0.0303, -0.1883, -0.3441]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 36 ][ timestamp 17 ] state=tensor([[ 0.1106,  0.0303, -0.1883, -0.3441]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1112,  0.2275, -0.1952, -0.6897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 18 ] state=tensor([[ 0.1112,  0.2275, -0.1952, -0.6897]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1157,  0.4248, -0.2090, -1.0370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 36 ][ timestamp 19 ] state=tensor([[ 0.1157,  0.4248, -0.2090, -1.0370]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 36: Exploration_rate=0.05. Score=19.\n",
      "[ episode 37 ] state=tensor([[ 0.0050,  0.0341,  0.0070, -0.0339]])\n",
      "[ episode 37 ][ timestamp 1 ] state=tensor([[ 0.0050,  0.0341,  0.0070, -0.0339]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0057,  0.2291,  0.0064, -0.3243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 2 ] state=tensor([[ 0.0057,  0.2291,  0.0064, -0.3243]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 1.0264e-02,  4.2413e-01, -1.3176e-04, -6.1498e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 3 ] state=tensor([[ 1.0264e-02,  4.2413e-01, -1.3176e-04, -6.1498e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0187,  0.2290, -0.0124, -0.3223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 4 ] state=tensor([[ 0.0187,  0.2290, -0.0124, -0.3223]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0233,  0.0341, -0.0189, -0.0336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 5 ] state=tensor([[ 0.0233,  0.0341, -0.0189, -0.0336]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0240,  0.2295, -0.0196, -0.3322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 6 ] state=tensor([[ 0.0240,  0.2295, -0.0196, -0.3322]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0286,  0.4249, -0.0262, -0.6310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 7 ] state=tensor([[ 0.0286,  0.4249, -0.0262, -0.6310]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0371,  0.2301, -0.0388, -0.3466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 8 ] state=tensor([[ 0.0371,  0.2301, -0.0388, -0.3466]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0417,  0.4258, -0.0457, -0.6513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 9 ] state=tensor([[ 0.0417,  0.4258, -0.0457, -0.6513]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0502,  0.2313, -0.0588, -0.3734]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 10 ] state=tensor([[ 0.0502,  0.2313, -0.0588, -0.3734]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0548,  0.4272, -0.0662, -0.6840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 11 ] state=tensor([[ 0.0548,  0.4272, -0.0662, -0.6840]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0634,  0.2331, -0.0799, -0.4129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 12 ] state=tensor([[ 0.0634,  0.2331, -0.0799, -0.4129]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0680,  0.4292, -0.0882, -0.7297]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 13 ] state=tensor([[ 0.0680,  0.4292, -0.0882, -0.7297]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0766,  0.2354, -0.1028, -0.4660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 14 ] state=tensor([[ 0.0766,  0.2354, -0.1028, -0.4660]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0813,  0.4318, -0.1121, -0.7892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 15 ] state=tensor([[ 0.0813,  0.4318, -0.1121, -0.7892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0900,  0.2384, -0.1279, -0.5338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 16 ] state=tensor([[ 0.0900,  0.2384, -0.1279, -0.5338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0947,  0.0453, -0.1385, -0.2840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 17 ] state=tensor([[ 0.0947,  0.0453, -0.1385, -0.2840]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0956,  0.2421, -0.1442, -0.6169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 18 ] state=tensor([[ 0.0956,  0.2421, -0.1442, -0.6169]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1005,  0.0493, -0.1566, -0.3729]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 19 ] state=tensor([[ 0.1005,  0.0493, -0.1566, -0.3729]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1015,  0.2462, -0.1640, -0.7106]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 20 ] state=tensor([[ 0.1015,  0.2462, -0.1640, -0.7106]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1064,  0.4432, -0.1782, -1.0501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 21 ] state=tensor([[ 0.1064,  0.4432, -0.1782, -1.0501]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1153,  0.2508, -0.1992, -0.8182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 37 ][ timestamp 22 ] state=tensor([[ 0.1153,  0.2508, -0.1992, -0.8182]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 37: Exploration_rate=0.05. Score=22.\n",
      "[ episode 38 ] state=tensor([[-0.0072, -0.0038, -0.0141,  0.0405]])\n",
      "[ episode 38 ][ timestamp 1 ] state=tensor([[-0.0072, -0.0038, -0.0141,  0.0405]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0073,  0.1915, -0.0133, -0.2566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 2 ] state=tensor([[-0.0073,  0.1915, -0.0133, -0.2566]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0035,  0.3868, -0.0184, -0.5535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 3 ] state=tensor([[-0.0035,  0.3868, -0.0184, -0.5535]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0043,  0.1920, -0.0295, -0.2667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 4 ] state=tensor([[ 0.0043,  0.1920, -0.0295, -0.2667]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0081,  0.3875, -0.0348, -0.5685]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 5 ] state=tensor([[ 0.0081,  0.3875, -0.0348, -0.5685]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0159,  0.5831, -0.0462, -0.8719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 6 ] state=tensor([[ 0.0159,  0.5831, -0.0462, -0.8719]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0275,  0.3886, -0.0636, -0.5941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 7 ] state=tensor([[ 0.0275,  0.3886, -0.0636, -0.5941]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0353,  0.1945, -0.0755, -0.3221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 8 ] state=tensor([[ 0.0353,  0.1945, -0.0755, -0.3221]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0392,  0.3906, -0.0820, -0.6376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 9 ] state=tensor([[ 0.0392,  0.3906, -0.0820, -0.6376]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0470,  0.1967, -0.0947, -0.3719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 10 ] state=tensor([[ 0.0470,  0.1967, -0.0947, -0.3719]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0509,  0.3930, -0.1021, -0.6928]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 11 ] state=tensor([[ 0.0509,  0.3930, -0.1021, -0.6928]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0588,  0.1995, -0.1160, -0.4340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 12 ] state=tensor([[ 0.0588,  0.1995, -0.1160, -0.4340]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0628,  0.3960, -0.1247, -0.7609]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 13 ] state=tensor([[ 0.0628,  0.3960, -0.1247, -0.7609]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0707,  0.2028, -0.1399, -0.5099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 14 ] state=tensor([[ 0.0707,  0.2028, -0.1399, -0.5099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0748,  0.3996, -0.1501, -0.8432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 15 ] state=tensor([[ 0.0748,  0.3996, -0.1501, -0.8432]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0828,  0.2068, -0.1670, -0.6012]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 16 ] state=tensor([[ 0.0828,  0.2068, -0.1670, -0.6012]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0869,  0.4038, -0.1790, -0.9415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 17 ] state=tensor([[ 0.0869,  0.4038, -0.1790, -0.9415]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0950,  0.2115, -0.1978, -0.7099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 38 ][ timestamp 18 ] state=tensor([[ 0.0950,  0.2115, -0.1978, -0.7099]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 38: Exploration_rate=0.05. Score=18.\n",
      "[ episode 39 ] state=tensor([[ 0.0092, -0.0267,  0.0003, -0.0345]])\n",
      "[ episode 39 ][ timestamp 1 ] state=tensor([[ 0.0092, -0.0267,  0.0003, -0.0345]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0087,  0.1684, -0.0004, -0.3271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 2 ] state=tensor([[ 0.0087,  0.1684, -0.0004, -0.3271]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0120,  0.3635, -0.0069, -0.6199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 3 ] state=tensor([[ 0.0120,  0.3635, -0.0069, -0.6199]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0193,  0.1685, -0.0193, -0.3294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 4 ] state=tensor([[ 0.0193,  0.1685, -0.0193, -0.3294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0227,  0.3639, -0.0259, -0.6281]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 5 ] state=tensor([[ 0.0227,  0.3639, -0.0259, -0.6281]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0300,  0.1691, -0.0385, -0.3437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 6 ] state=tensor([[ 0.0300,  0.1691, -0.0385, -0.3437]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0333,  0.3648, -0.0453, -0.6483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 7 ] state=tensor([[ 0.0333,  0.3648, -0.0453, -0.6483]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0406,  0.1703, -0.0583, -0.3702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 8 ] state=tensor([[ 0.0406,  0.1703, -0.0583, -0.3702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0440, -0.0239, -0.0657, -0.0965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 9 ] state=tensor([[ 0.0440, -0.0239, -0.0657, -0.0965]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0436,  0.1721, -0.0676, -0.4091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 10 ] state=tensor([[ 0.0436,  0.1721, -0.0676, -0.4091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0470, -0.0220, -0.0758, -0.1385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 11 ] state=tensor([[ 0.0470, -0.0220, -0.0758, -0.1385]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0466,  0.1741, -0.0786, -0.4541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 12 ] state=tensor([[ 0.0466,  0.1741, -0.0786, -0.4541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0500,  0.3702, -0.0877, -0.7705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 13 ] state=tensor([[ 0.0500,  0.3702, -0.0877, -0.7705]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0575,  0.1764, -0.1031, -0.5067]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 14 ] state=tensor([[ 0.0575,  0.1764, -0.1031, -0.5067]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0610,  0.3728, -0.1132, -0.8300]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 15 ] state=tensor([[ 0.0610,  0.3728, -0.1132, -0.8300]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0684,  0.1794, -0.1298, -0.5749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 16 ] state=tensor([[ 0.0684,  0.1794, -0.1298, -0.5749]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0720,  0.3761, -0.1413, -0.9055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 17 ] state=tensor([[ 0.0720,  0.3761, -0.1413, -0.9055]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0795,  0.1831, -0.1594, -0.6604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 18 ] state=tensor([[ 0.0795,  0.1831, -0.1594, -0.6604]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0832,  0.3801, -0.1726, -0.9987]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 19 ] state=tensor([[ 0.0832,  0.3801, -0.1726, -0.9987]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0908,  0.1876, -0.1926, -0.7649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 20 ] state=tensor([[ 0.0908,  0.1876, -0.1926, -0.7649]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0946,  0.3848, -0.2079, -1.1114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 39 ][ timestamp 21 ] state=tensor([[ 0.0946,  0.3848, -0.2079, -1.1114]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 39: Exploration_rate=0.05. Score=21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 40 ] state=tensor([[-0.0352,  0.0489,  0.0319, -0.0375]])\n",
      "[ episode 40 ][ timestamp 1 ] state=tensor([[-0.0352,  0.0489,  0.0319, -0.0375]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0343,  0.2436,  0.0312, -0.3199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 2 ] state=tensor([[-0.0343,  0.2436,  0.0312, -0.3199]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0294,  0.4382,  0.0248, -0.6026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 3 ] state=tensor([[-0.0294,  0.4382,  0.0248, -0.6026]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0206,  0.6330,  0.0127, -0.8874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 4 ] state=tensor([[-0.0206,  0.6330,  0.0127, -0.8874]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0080,  0.4377, -0.0050, -0.5907]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 5 ] state=tensor([[-0.0080,  0.4377, -0.0050, -0.5907]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0008,  0.2427, -0.0168, -0.2996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 6 ] state=tensor([[ 0.0008,  0.2427, -0.0168, -0.2996]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0056,  0.4380, -0.0228, -0.5976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 7 ] state=tensor([[ 0.0056,  0.4380, -0.0228, -0.5976]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0144,  0.6334, -0.0348, -0.8974]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 8 ] state=tensor([[ 0.0144,  0.6334, -0.0348, -0.8974]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0271,  0.4388, -0.0527, -0.6158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 9 ] state=tensor([[ 0.0271,  0.4388, -0.0527, -0.6158]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0358,  0.2445, -0.0650, -0.3402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 10 ] state=tensor([[ 0.0358,  0.2445, -0.0650, -0.3402]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0407,  0.4405, -0.0719, -0.6527]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 11 ] state=tensor([[ 0.0407,  0.4405, -0.0719, -0.6527]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0495,  0.2464, -0.0849, -0.3834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 12 ] state=tensor([[ 0.0495,  0.2464, -0.0849, -0.3834]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0545,  0.4426, -0.0926, -0.7016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 13 ] state=tensor([[ 0.0545,  0.4426, -0.0926, -0.7016]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0633,  0.2489, -0.1066, -0.4395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 14 ] state=tensor([[ 0.0633,  0.2489, -0.1066, -0.4395]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0683,  0.4454, -0.1154, -0.7638]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 15 ] state=tensor([[ 0.0683,  0.4454, -0.1154, -0.7638]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0772,  0.2520, -0.1307, -0.5095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 16 ] state=tensor([[ 0.0772,  0.2520, -0.1307, -0.5095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0822,  0.4487, -0.1409, -0.8403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 17 ] state=tensor([[ 0.0822,  0.4487, -0.1409, -0.8403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0912,  0.2557, -0.1577, -0.5951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 18 ] state=tensor([[ 0.0912,  0.2557, -0.1577, -0.5951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0963,  0.0631, -0.1696, -0.3559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 19 ] state=tensor([[ 0.0963,  0.0631, -0.1696, -0.3559]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0976,  0.2602, -0.1767, -0.6969]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 20 ] state=tensor([[ 0.0976,  0.2602, -0.1767, -0.6969]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1028,  0.0679, -0.1906, -0.4646]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 21 ] state=tensor([[ 0.1028,  0.0679, -0.1906, -0.4646]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1042,  0.2652, -0.1999, -0.8108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 40 ][ timestamp 22 ] state=tensor([[ 0.1042,  0.2652, -0.1999, -0.8108]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 40: Exploration_rate=0.05. Score=22.\n",
      "[ episode 41 ] state=tensor([[-0.0080,  0.0311, -0.0463, -0.0477]])\n",
      "[ episode 41 ][ timestamp 1 ] state=tensor([[-0.0080,  0.0311, -0.0463, -0.0477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0074,  0.2268, -0.0472, -0.3546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 2 ] state=tensor([[-0.0074,  0.2268, -0.0472, -0.3546]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0029,  0.4226, -0.0543, -0.6618]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 3 ] state=tensor([[-0.0029,  0.4226, -0.0543, -0.6618]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0056,  0.2283, -0.0675, -0.3867]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 4 ] state=tensor([[ 0.0056,  0.2283, -0.0675, -0.3867]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0101,  0.4243, -0.0753, -0.6999]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 5 ] state=tensor([[ 0.0101,  0.4243, -0.0753, -0.6999]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0186,  0.2303, -0.0893, -0.4318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 6 ] state=tensor([[ 0.0186,  0.2303, -0.0893, -0.4318]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0232,  0.4266, -0.0979, -0.7512]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 7 ] state=tensor([[ 0.0232,  0.4266, -0.0979, -0.7512]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0318,  0.2329, -0.1129, -0.4909]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 8 ] state=tensor([[ 0.0318,  0.2329, -0.1129, -0.4909]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0364,  0.4294, -0.1228, -0.8169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 9 ] state=tensor([[ 0.0364,  0.4294, -0.1228, -0.8169]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0450,  0.2362, -0.1391, -0.5653]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 10 ] state=tensor([[ 0.0450,  0.2362, -0.1391, -0.5653]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0497,  0.0433, -0.1504, -0.3194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 11 ] state=tensor([[ 0.0497,  0.0433, -0.1504, -0.3194]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0506,  0.2402, -0.1568, -0.6555]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 12 ] state=tensor([[ 0.0506,  0.2402, -0.1568, -0.6555]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0554,  0.0475, -0.1699, -0.4160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 13 ] state=tensor([[ 0.0554,  0.0475, -0.1699, -0.4160]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0563,  0.2446, -0.1782, -0.7571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 14 ] state=tensor([[ 0.0563,  0.2446, -0.1782, -0.7571]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0612,  0.4417, -0.1934, -1.1001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 41 ][ timestamp 15 ] state=tensor([[ 0.0612,  0.4417, -0.1934, -1.1001]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 41: Exploration_rate=0.05. Score=15.\n",
      "[ episode 42 ] state=tensor([[ 0.0103, -0.0217, -0.0001,  0.0409]])\n",
      "[ episode 42 ][ timestamp 1 ] state=tensor([[ 0.0103, -0.0217, -0.0001,  0.0409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0099,  0.1734,  0.0007, -0.2518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 2 ] state=tensor([[ 0.0099,  0.1734,  0.0007, -0.2518]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0134,  0.3686, -0.0043, -0.5443]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 3 ] state=tensor([[ 0.0134,  0.3686, -0.0043, -0.5443]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0207,  0.5637, -0.0152, -0.8383]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 4 ] state=tensor([[ 0.0207,  0.5637, -0.0152, -0.8383]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0320,  0.3688, -0.0320, -0.5505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 5 ] state=tensor([[ 0.0320,  0.3688, -0.0320, -0.5505]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0394,  0.1742, -0.0430, -0.2680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 6 ] state=tensor([[ 0.0394,  0.1742, -0.0430, -0.2680]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0429,  0.3699, -0.0483, -0.5740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 7 ] state=tensor([[ 0.0429,  0.3699, -0.0483, -0.5740]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0503,  0.5656, -0.0598, -0.8815]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 8 ] state=tensor([[ 0.0503,  0.5656, -0.0598, -0.8815]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0616,  0.3714, -0.0775, -0.6082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 9 ] state=tensor([[ 0.0616,  0.3714, -0.0775, -0.6082]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0690,  0.1774, -0.0896, -0.3409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 10 ] state=tensor([[ 0.0690,  0.1774, -0.0896, -0.3409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0725,  0.3737, -0.0964, -0.6604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 11 ] state=tensor([[ 0.0725,  0.3737, -0.0964, -0.6604]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0800,  0.1800, -0.1096, -0.3996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 12 ] state=tensor([[ 0.0800,  0.1800, -0.1096, -0.3996]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0836,  0.3765, -0.1176, -0.7247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 13 ] state=tensor([[ 0.0836,  0.3765, -0.1176, -0.7247]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0912,  0.1832, -0.1321, -0.4713]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 14 ] state=tensor([[ 0.0912,  0.1832, -0.1321, -0.4713]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0948,  0.3799, -0.1416, -0.8025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 15 ] state=tensor([[ 0.0948,  0.3799, -0.1416, -0.8025]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1024,  0.1870, -0.1576, -0.5575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 16 ] state=tensor([[ 0.1024,  0.1870, -0.1576, -0.5575]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1062,  0.3840, -0.1688, -0.8954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 17 ] state=tensor([[ 0.1062,  0.3840, -0.1688, -0.8954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1138,  0.1915, -0.1867, -0.6601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 18 ] state=tensor([[ 0.1138,  0.1915, -0.1867, -0.6601]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1177,  0.3886, -0.1999, -1.0053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 42 ][ timestamp 19 ] state=tensor([[ 0.1177,  0.3886, -0.1999, -1.0053]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 42: Exploration_rate=0.05. Score=19.\n",
      "[ episode 43 ] state=tensor([[ 0.0339, -0.0291,  0.0183,  0.0010]])\n",
      "[ episode 43 ][ timestamp 1 ] state=tensor([[ 0.0339, -0.0291,  0.0183,  0.0010]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0333,  0.1657,  0.0183, -0.2859]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 2 ] state=tensor([[ 0.0333,  0.1657,  0.0183, -0.2859]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0366,  0.3606,  0.0126, -0.5728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 3 ] state=tensor([[ 0.0366,  0.3606,  0.0126, -0.5728]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0439,  0.1653,  0.0011, -0.2762]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 43 ][ timestamp 4 ] state=tensor([[ 0.0439,  0.1653,  0.0011, -0.2762]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0472,  0.3604, -0.0044, -0.5685]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 5 ] state=tensor([[ 0.0472,  0.3604, -0.0044, -0.5685]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0544,  0.1653, -0.0158, -0.2772]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 6 ] state=tensor([[ 0.0544,  0.1653, -0.0158, -0.2772]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0577,  0.3607, -0.0213, -0.5748]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 7 ] state=tensor([[ 0.0577,  0.3607, -0.0213, -0.5748]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0649,  0.1659, -0.0328, -0.2890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 8 ] state=tensor([[ 0.0649,  0.1659, -0.0328, -0.2890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0682,  0.3614, -0.0386, -0.5918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 9 ] state=tensor([[ 0.0682,  0.3614, -0.0386, -0.5918]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0754,  0.5571, -0.0505, -0.8964]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 10 ] state=tensor([[ 0.0754,  0.5571, -0.0505, -0.8964]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0866,  0.3627, -0.0684, -0.6200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 11 ] state=tensor([[ 0.0866,  0.3627, -0.0684, -0.6200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0938,  0.1686, -0.0808, -0.3496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 12 ] state=tensor([[ 0.0938,  0.1686, -0.0808, -0.3496]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0972,  0.3647, -0.0878, -0.6666]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 13 ] state=tensor([[ 0.0972,  0.3647, -0.0878, -0.6666]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1045,  0.1709, -0.1011, -0.4028]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 14 ] state=tensor([[ 0.1045,  0.1709, -0.1011, -0.4028]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1079,  0.3673, -0.1092, -0.7256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 15 ] state=tensor([[ 0.1079,  0.3673, -0.1092, -0.7256]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1153,  0.1739, -0.1237, -0.4692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 16 ] state=tensor([[ 0.1153,  0.1739, -0.1237, -0.4692]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1187,  0.3705, -0.1331, -0.7981]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 17 ] state=tensor([[ 0.1187,  0.3705, -0.1331, -0.7981]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1262,  0.1774, -0.1490, -0.5501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 18 ] state=tensor([[ 0.1262,  0.1774, -0.1490, -0.5501]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1297,  0.3743, -0.1600, -0.8858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 19 ] state=tensor([[ 0.1297,  0.3743, -0.1600, -0.8858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1372,  0.1817, -0.1777, -0.6474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 20 ] state=tensor([[ 0.1372,  0.1817, -0.1777, -0.6474]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1408,  0.3788, -0.1907, -0.9903]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 43 ][ timestamp 21 ] state=tensor([[ 0.1408,  0.3788, -0.1907, -0.9903]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 43: Exploration_rate=0.05. Score=21.\n",
      "[ episode 44 ] state=tensor([[ 0.0314,  0.0050, -0.0373,  0.0317]])\n",
      "[ episode 44 ][ timestamp 1 ] state=tensor([[ 0.0314,  0.0050, -0.0373,  0.0317]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0315,  0.2006, -0.0367, -0.2725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 2 ] state=tensor([[ 0.0315,  0.2006, -0.0367, -0.2725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0355,  0.3963, -0.0421, -0.5765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 3 ] state=tensor([[ 0.0355,  0.3963, -0.0421, -0.5765]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0434,  0.2017, -0.0537, -0.2974]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 4 ] state=tensor([[ 0.0434,  0.2017, -0.0537, -0.2974]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0475,  0.3976, -0.0596, -0.6065]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 5 ] state=tensor([[ 0.0475,  0.3976, -0.0596, -0.6065]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0554,  0.2033, -0.0717, -0.3332]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 6 ] state=tensor([[ 0.0554,  0.2033, -0.0717, -0.3332]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0595,  0.3994, -0.0784, -0.6476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 7 ] state=tensor([[ 0.0595,  0.3994, -0.0784, -0.6476]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0675,  0.2055, -0.0913, -0.3806]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 8 ] state=tensor([[ 0.0675,  0.2055, -0.0913, -0.3806]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0716,  0.4018, -0.0990, -0.7006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 9 ] state=tensor([[ 0.0716,  0.4018, -0.0990, -0.7006]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0796,  0.2081, -0.1130, -0.4406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 10 ] state=tensor([[ 0.0796,  0.2081, -0.1130, -0.4406]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0838,  0.0148, -0.1218, -0.1856]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 11 ] state=tensor([[ 0.0838,  0.0148, -0.1218, -0.1856]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0841,  0.2114, -0.1255, -0.5141]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 12 ] state=tensor([[ 0.0841,  0.2114, -0.1255, -0.5141]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0883,  0.0183, -0.1358, -0.2634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 13 ] state=tensor([[ 0.0883,  0.0183, -0.1358, -0.2634]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0887,  0.2150, -0.1410, -0.5957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 14 ] state=tensor([[ 0.0887,  0.2150, -0.1410, -0.5957]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0930,  0.0221, -0.1530, -0.3505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 15 ] state=tensor([[ 0.0930,  0.0221, -0.1530, -0.3505]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0934,  0.2191, -0.1600, -0.6873]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 16 ] state=tensor([[ 0.0934,  0.2191, -0.1600, -0.6873]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0978,  0.0265, -0.1737, -0.4489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 17 ] state=tensor([[ 0.0978,  0.0265, -0.1737, -0.4489]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0983,  0.2236, -0.1827, -0.7909]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 18 ] state=tensor([[ 0.0983,  0.2236, -0.1827, -0.7909]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1028,  0.4207, -0.1985, -1.1351]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 44 ][ timestamp 19 ] state=tensor([[ 0.1028,  0.4207, -0.1985, -1.1351]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 44: Exploration_rate=0.05. Score=19.\n",
      "[ episode 45 ] state=tensor([[ 0.0482,  0.0432, -0.0084, -0.0190]])\n",
      "[ episode 45 ][ timestamp 1 ] state=tensor([[ 0.0482,  0.0432, -0.0084, -0.0190]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0491,  0.2384, -0.0088, -0.3143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 2 ] state=tensor([[ 0.0491,  0.2384, -0.0088, -0.3143]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0539,  0.0434, -0.0151, -0.0244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 3 ] state=tensor([[ 0.0539,  0.0434, -0.0151, -0.0244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0547,  0.2387, -0.0156, -0.3218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 4 ] state=tensor([[ 0.0547,  0.2387, -0.0156, -0.3218]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0595,  0.4341, -0.0220, -0.6194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 5 ] state=tensor([[ 0.0595,  0.4341, -0.0220, -0.6194]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0682,  0.2393, -0.0344, -0.3337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 6 ] state=tensor([[ 0.0682,  0.2393, -0.0344, -0.3337]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0730,  0.4349, -0.0411, -0.6370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 7 ] state=tensor([[ 0.0730,  0.4349, -0.0411, -0.6370]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0817,  0.2403, -0.0538, -0.3575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 8 ] state=tensor([[ 0.0817,  0.2403, -0.0538, -0.3575]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0865,  0.4362, -0.0609, -0.6667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 9 ] state=tensor([[ 0.0865,  0.4362, -0.0609, -0.6667]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0952,  0.6321, -0.0743, -0.9779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 10 ] state=tensor([[ 0.0952,  0.6321, -0.0743, -0.9779]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1078,  0.4380, -0.0938, -0.7095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 11 ] state=tensor([[ 0.1078,  0.4380, -0.0938, -0.7095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1166,  0.2443, -0.1080, -0.4477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 12 ] state=tensor([[ 0.1166,  0.2443, -0.1080, -0.4477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1215,  0.4408, -0.1170, -0.7724]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 13 ] state=tensor([[ 0.1215,  0.4408, -0.1170, -0.7724]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1303,  0.2475, -0.1324, -0.5187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 14 ] state=tensor([[ 0.1303,  0.2475, -0.1324, -0.5187]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1353,  0.4442, -0.1428, -0.8500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 15 ] state=tensor([[ 0.1353,  0.4442, -0.1428, -0.8500]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1441,  0.2513, -0.1598, -0.6054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 16 ] state=tensor([[ 0.1441,  0.2513, -0.1598, -0.6054]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1492,  0.4482, -0.1719, -0.9439]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 17 ] state=tensor([[ 0.1492,  0.4482, -0.1719, -0.9439]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1581,  0.2558, -0.1908, -0.7097]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 18 ] state=tensor([[ 0.1581,  0.2558, -0.1908, -0.7097]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1632,  0.4530, -0.2050, -1.0559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 45 ][ timestamp 19 ] state=tensor([[ 0.1632,  0.4530, -0.2050, -1.0559]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 45: Exploration_rate=0.05. Score=19.\n",
      "[ episode 46 ] state=tensor([[-0.0169, -0.0469, -0.0112, -0.0031]])\n",
      "[ episode 46 ][ timestamp 1 ] state=tensor([[-0.0169, -0.0469, -0.0112, -0.0031]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0179,  0.1484, -0.0113, -0.2993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 2 ] state=tensor([[-0.0179,  0.1484, -0.0113, -0.2993]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0149,  0.3437, -0.0173, -0.5956]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 3 ] state=tensor([[-0.0149,  0.3437, -0.0173, -0.5956]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0080,  0.5390, -0.0292, -0.8936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 4 ] state=tensor([[-0.0080,  0.5390, -0.0292, -0.8936]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0028,  0.3443, -0.0470, -0.6103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 5 ] state=tensor([[ 0.0028,  0.3443, -0.0470, -0.6103]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0097,  0.1499, -0.0593, -0.3328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 6 ] state=tensor([[ 0.0097,  0.1499, -0.0593, -0.3328]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0126,  0.3458, -0.0659, -0.6435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 7 ] state=tensor([[ 0.0126,  0.3458, -0.0659, -0.6435]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0196,  0.1516, -0.0788, -0.3723]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 8 ] state=tensor([[ 0.0196,  0.1516, -0.0788, -0.3723]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0226,  0.3478, -0.0862, -0.6887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 9 ] state=tensor([[ 0.0226,  0.3478, -0.0862, -0.6887]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0296,  0.1540, -0.1000, -0.4244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 10 ] state=tensor([[ 0.0296,  0.1540, -0.1000, -0.4244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0326,  0.3503, -0.1085, -0.7469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 11 ] state=tensor([[ 0.0326,  0.3503, -0.1085, -0.7469]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0396,  0.1569, -0.1234, -0.4902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 12 ] state=tensor([[ 0.0396,  0.1569, -0.1234, -0.4902]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0428,  0.3535, -0.1332, -0.8191]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 13 ] state=tensor([[ 0.0428,  0.3535, -0.1332, -0.8191]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0498,  0.1604, -0.1496, -0.5711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 14 ] state=tensor([[ 0.0498,  0.1604, -0.1496, -0.5711]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0531,  0.3573, -0.1610, -0.9069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 15 ] state=tensor([[ 0.0531,  0.3573, -0.1610, -0.9069]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0602,  0.1647, -0.1792, -0.6689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 46 ][ timestamp 16 ] state=tensor([[ 0.0602,  0.1647, -0.1792, -0.6689]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0635,  0.3618, -0.1926, -1.0122]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 46 ][ timestamp 17 ] state=tensor([[ 0.0635,  0.3618, -0.1926, -1.0122]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 46: Exploration_rate=0.05. Score=17.\n",
      "[ episode 47 ] state=tensor([[ 0.0477, -0.0147,  0.0048,  0.0270]])\n",
      "[ episode 47 ][ timestamp 1 ] state=tensor([[ 0.0477, -0.0147,  0.0048,  0.0270]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0474,  0.1804,  0.0053, -0.2642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 2 ] state=tensor([[ 0.0474,  0.1804,  0.0053, -0.2642]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 5.1040e-02,  3.7544e-01,  2.3159e-05, -5.5521e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 3 ] state=tensor([[ 5.1040e-02,  3.7544e-01,  2.3159e-05, -5.5521e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0585,  0.5706, -0.0111, -0.8479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 4 ] state=tensor([[ 0.0585,  0.5706, -0.0111, -0.8479]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0700,  0.3756, -0.0280, -0.5587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 5 ] state=tensor([[ 0.0700,  0.3756, -0.0280, -0.5587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0775,  0.1809, -0.0392, -0.2750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 6 ] state=tensor([[ 0.0775,  0.1809, -0.0392, -0.2750]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0811,  0.3765, -0.0447, -0.5798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 7 ] state=tensor([[ 0.0811,  0.3765, -0.0447, -0.5798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0886,  0.1821, -0.0563, -0.3015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 8 ] state=tensor([[ 0.0886,  0.1821, -0.0563, -0.3015]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0923,  0.3779, -0.0623, -0.6114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 9 ] state=tensor([[ 0.0923,  0.3779, -0.0623, -0.6114]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0998,  0.1837, -0.0746, -0.3390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 10 ] state=tensor([[ 0.0998,  0.1837, -0.0746, -0.3390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1035,  0.3798, -0.0813, -0.6542]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 11 ] state=tensor([[ 0.1035,  0.3798, -0.0813, -0.6542]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1111,  0.1859, -0.0944, -0.3882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 12 ] state=tensor([[ 0.1111,  0.1859, -0.0944, -0.3882]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1148,  0.3823, -0.1022, -0.7091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 13 ] state=tensor([[ 0.1148,  0.3823, -0.1022, -0.7091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1225,  0.1887, -0.1164, -0.4503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 14 ] state=tensor([[ 0.1225,  0.1887, -0.1164, -0.4503]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1262,  0.3853, -0.1254, -0.7773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 15 ] state=tensor([[ 0.1262,  0.3853, -0.1254, -0.7773]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1339,  0.1921, -0.1409, -0.5265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 16 ] state=tensor([[ 0.1339,  0.1921, -0.1409, -0.5265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1378,  0.3889, -0.1515, -0.8601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 17 ] state=tensor([[ 0.1378,  0.3889, -0.1515, -0.8601]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1456,  0.1961, -0.1687, -0.6186]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 18 ] state=tensor([[ 0.1456,  0.1961, -0.1687, -0.6186]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1495,  0.3931, -0.1810, -0.9593]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 19 ] state=tensor([[ 0.1495,  0.3931, -0.1810, -0.9593]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1573,  0.2008, -0.2002, -0.7285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 47 ][ timestamp 20 ] state=tensor([[ 0.1573,  0.2008, -0.2002, -0.7285]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 47: Exploration_rate=0.05. Score=20.\n",
      "[ episode 48 ] state=tensor([[-0.0365, -0.0296,  0.0432,  0.0392]])\n",
      "[ episode 48 ][ timestamp 1 ] state=tensor([[-0.0365, -0.0296,  0.0432,  0.0392]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0371,  0.1649,  0.0440, -0.2396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 2 ] state=tensor([[-0.0371,  0.1649,  0.0440, -0.2396]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0338,  0.3594,  0.0392, -0.5181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 3 ] state=tensor([[-0.0338,  0.3594,  0.0392, -0.5181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0266,  0.5539,  0.0288, -0.7981]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 4 ] state=tensor([[-0.0266,  0.5539,  0.0288, -0.7981]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0155,  0.7487,  0.0129, -1.0816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 5 ] state=tensor([[-0.0155,  0.7487,  0.0129, -1.0816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-5.1809e-04,  5.5336e-01, -8.7519e-03, -7.8491e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 6 ] state=tensor([[-5.1809e-04,  5.5336e-01, -8.7519e-03, -7.8491e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0105,  0.3584, -0.0245, -0.4950]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 7 ] state=tensor([[ 0.0105,  0.3584, -0.0245, -0.4950]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0177,  0.5538, -0.0344, -0.7953]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 8 ] state=tensor([[ 0.0177,  0.5538, -0.0344, -0.7953]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0288,  0.3592, -0.0503, -0.5136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 9 ] state=tensor([[ 0.0288,  0.3592, -0.0503, -0.5136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0360,  0.5550, -0.0605, -0.8217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 10 ] state=tensor([[ 0.0360,  0.5550, -0.0605, -0.8217]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0471,  0.3607, -0.0770, -0.5486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 11 ] state=tensor([[ 0.0471,  0.3607, -0.0770, -0.5486]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0543,  0.1668, -0.0879, -0.2812]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 12 ] state=tensor([[ 0.0543,  0.1668, -0.0879, -0.2812]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0576,  0.3630, -0.0936, -0.6002]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 13 ] state=tensor([[ 0.0576,  0.3630, -0.0936, -0.6002]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0649,  0.1693, -0.1056, -0.3384]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 14 ] state=tensor([[ 0.0649,  0.1693, -0.1056, -0.3384]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0683,  0.3658, -0.1123, -0.6624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 15 ] state=tensor([[ 0.0683,  0.3658, -0.1123, -0.6624]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0756,  0.1724, -0.1256, -0.4071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 16 ] state=tensor([[ 0.0756,  0.1724, -0.1256, -0.4071]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0790,  0.3691, -0.1337, -0.7366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 17 ] state=tensor([[ 0.0790,  0.3691, -0.1337, -0.7366]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0864,  0.1760, -0.1485, -0.4888]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 18 ] state=tensor([[ 0.0864,  0.1760, -0.1485, -0.4888]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0899,  0.3729, -0.1582, -0.8244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 19 ] state=tensor([[ 0.0899,  0.3729, -0.1582, -0.8244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0974,  0.1802, -0.1747, -0.5853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 20 ] state=tensor([[ 0.0974,  0.1802, -0.1747, -0.5853]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1010, -0.0121, -0.1864, -0.3524]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 21 ] state=tensor([[ 0.1010, -0.0121, -0.1864, -0.3524]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1008,  0.1851, -0.1935, -0.6976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 22 ] state=tensor([[ 0.1008,  0.1851, -0.1935, -0.6976]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1045, -0.0068, -0.2074, -0.4715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 48 ][ timestamp 23 ] state=tensor([[ 0.1045, -0.0068, -0.2074, -0.4715]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 48: Exploration_rate=0.05. Score=23.\n",
      "[ episode 49 ] state=tensor([[-0.0231,  0.0435, -0.0337, -0.0249]])\n",
      "[ episode 49 ][ timestamp 1 ] state=tensor([[-0.0231,  0.0435, -0.0337, -0.0249]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0222,  0.2391, -0.0342, -0.3280]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 2 ] state=tensor([[-0.0222,  0.2391, -0.0342, -0.3280]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0174,  0.4347, -0.0407, -0.6313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 3 ] state=tensor([[-0.0174,  0.4347, -0.0407, -0.6313]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0087,  0.2402, -0.0533, -0.3517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 4 ] state=tensor([[-0.0087,  0.2402, -0.0533, -0.3517]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0039,  0.4360, -0.0604, -0.6607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 5 ] state=tensor([[-0.0039,  0.4360, -0.0604, -0.6607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0048,  0.6319, -0.0736, -0.9718]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 6 ] state=tensor([[ 0.0048,  0.6319, -0.0736, -0.9718]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0174,  0.4378, -0.0930, -0.7031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 7 ] state=tensor([[ 0.0174,  0.4378, -0.0930, -0.7031]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0262,  0.2441, -0.1071, -0.4411]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 8 ] state=tensor([[ 0.0262,  0.2441, -0.1071, -0.4411]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0311,  0.4406, -0.1159, -0.7655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 9 ] state=tensor([[ 0.0311,  0.4406, -0.1159, -0.7655]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0399,  0.2472, -0.1312, -0.5114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 10 ] state=tensor([[ 0.0399,  0.2472, -0.1312, -0.5114]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0448,  0.4439, -0.1415, -0.8424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 11 ] state=tensor([[ 0.0448,  0.4439, -0.1415, -0.8424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0537,  0.6407, -0.1583, -1.1760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 12 ] state=tensor([[ 0.0537,  0.6407, -0.1583, -1.1760]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0665,  0.4479, -0.1818, -0.9369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 13 ] state=tensor([[ 0.0665,  0.4479, -0.1818, -0.9369]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0755,  0.2557, -0.2006, -0.7064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 49 ][ timestamp 14 ] state=tensor([[ 0.0755,  0.2557, -0.2006, -0.7064]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 49: Exploration_rate=0.05. Score=14.\n",
      "[ episode 50 ] state=tensor([[ 0.0448, -0.0398,  0.0048,  0.0291]])\n",
      "[ episode 50 ][ timestamp 1 ] state=tensor([[ 0.0448, -0.0398,  0.0048,  0.0291]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0440,  0.1552,  0.0054, -0.2621]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 2 ] state=tensor([[ 0.0440,  0.1552,  0.0054, -0.2621]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 4.7106e-02,  3.5026e-01,  1.7073e-04, -5.5306e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 3 ] state=tensor([[ 4.7106e-02,  3.5026e-01,  1.7073e-04, -5.5306e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0541,  0.1551, -0.0109, -0.2603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 4 ] state=tensor([[ 0.0541,  0.1551, -0.0109, -0.2603]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0572,  0.3504, -0.0161, -0.5564]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 50 ][ timestamp 5 ] state=tensor([[ 0.0572,  0.3504, -0.0161, -0.5564]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0642,  0.1555, -0.0272, -0.2689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 6 ] state=tensor([[ 0.0642,  0.1555, -0.0272, -0.2689]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0673,  0.3510, -0.0326, -0.5700]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 7 ] state=tensor([[ 0.0673,  0.3510, -0.0326, -0.5700]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0744,  0.1564, -0.0440, -0.2878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 8 ] state=tensor([[ 0.0744,  0.1564, -0.0440, -0.2878]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0775,  0.3521, -0.0498, -0.5940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 9 ] state=tensor([[ 0.0775,  0.3521, -0.0498, -0.5940]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0845,  0.1577, -0.0616, -0.3174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 10 ] state=tensor([[ 0.0845,  0.1577, -0.0616, -0.3174]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0877,  0.3536, -0.0680, -0.6289]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 11 ] state=tensor([[ 0.0877,  0.3536, -0.0680, -0.6289]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0947,  0.1595, -0.0806, -0.3583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 12 ] state=tensor([[ 0.0947,  0.1595, -0.0806, -0.3583]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0979,  0.3557, -0.0877, -0.6753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 13 ] state=tensor([[ 0.0979,  0.3557, -0.0877, -0.6753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1051,  0.1619, -0.1012, -0.4115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 14 ] state=tensor([[ 0.1051,  0.1619, -0.1012, -0.4115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1083,  0.3583, -0.1095, -0.7343]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 15 ] state=tensor([[ 0.1083,  0.3583, -0.1095, -0.7343]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1155,  0.1648, -0.1241, -0.4779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 16 ] state=tensor([[ 0.1155,  0.1648, -0.1241, -0.4779]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1188,  0.3615, -0.1337, -0.8070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 17 ] state=tensor([[ 0.1188,  0.3615, -0.1337, -0.8070]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1260,  0.1684, -0.1498, -0.5592]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 18 ] state=tensor([[ 0.1260,  0.1684, -0.1498, -0.5592]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1294,  0.3653, -0.1610, -0.8951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 19 ] state=tensor([[ 0.1294,  0.3653, -0.1610, -0.8951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1367,  0.1727, -0.1789, -0.6571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 20 ] state=tensor([[ 0.1367,  0.1727, -0.1789, -0.6571]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1401,  0.3698, -0.1921, -1.0003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 50 ][ timestamp 21 ] state=tensor([[ 0.1401,  0.3698, -0.1921, -1.0003]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 50: Exploration_rate=0.05. Score=21.\n",
      "[ episode 51 ] state=tensor([[0.0221, 0.0332, 0.0168, 0.0413]])\n",
      "[ episode 51 ][ timestamp 1 ] state=tensor([[0.0221, 0.0332, 0.0168, 0.0413]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0228,  0.2281,  0.0176, -0.2461]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 2 ] state=tensor([[ 0.0228,  0.2281,  0.0176, -0.2461]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0273,  0.4230,  0.0127, -0.5332]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 3 ] state=tensor([[ 0.0273,  0.4230,  0.0127, -0.5332]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0358,  0.2277,  0.0020, -0.2365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 4 ] state=tensor([[ 0.0358,  0.2277,  0.0020, -0.2365]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0403,  0.4228, -0.0027, -0.5286]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 5 ] state=tensor([[ 0.0403,  0.4228, -0.0027, -0.5286]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0488,  0.2277, -0.0133, -0.2367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 6 ] state=tensor([[ 0.0488,  0.2277, -0.0133, -0.2367]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0533,  0.4230, -0.0180, -0.5336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 7 ] state=tensor([[ 0.0533,  0.4230, -0.0180, -0.5336]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0618,  0.2281, -0.0287, -0.2466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 8 ] state=tensor([[ 0.0618,  0.2281, -0.0287, -0.2466]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0664,  0.4237, -0.0336, -0.5482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 9 ] state=tensor([[ 0.0664,  0.4237, -0.0336, -0.5482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0748,  0.6192, -0.0446, -0.8513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 10 ] state=tensor([[ 0.0748,  0.6192, -0.0446, -0.8513]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0872,  0.4247, -0.0616, -0.5730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 11 ] state=tensor([[ 0.0872,  0.4247, -0.0616, -0.5730]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0957,  0.2305, -0.0731, -0.3003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 12 ] state=tensor([[ 0.0957,  0.2305, -0.0731, -0.3003]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1003,  0.4266, -0.0791, -0.6151]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 13 ] state=tensor([[ 0.1003,  0.4266, -0.0791, -0.6151]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1089,  0.2327, -0.0914, -0.3484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 14 ] state=tensor([[ 0.1089,  0.2327, -0.0914, -0.3484]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1135,  0.4290, -0.0984, -0.6684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 15 ] state=tensor([[ 0.1135,  0.4290, -0.0984, -0.6684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1221,  0.2354, -0.1117, -0.4083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 16 ] state=tensor([[ 0.1221,  0.2354, -0.1117, -0.4083]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1268,  0.0420, -0.1199, -0.1528]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 17 ] state=tensor([[ 0.1268,  0.0420, -0.1199, -0.1528]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1276,  0.2386, -0.1229, -0.4808]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 18 ] state=tensor([[ 0.1276,  0.2386, -0.1229, -0.4808]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1324,  0.4352, -0.1326, -0.8095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 19 ] state=tensor([[ 0.1324,  0.4352, -0.1326, -0.8095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1411,  0.6319, -0.1488, -1.1408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 20 ] state=tensor([[ 0.1411,  0.6319, -0.1488, -1.1408]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1538,  0.4390, -0.1716, -0.8982]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 21 ] state=tensor([[ 0.1538,  0.4390, -0.1716, -0.8982]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1625,  0.2466, -0.1895, -0.6640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 22 ] state=tensor([[ 0.1625,  0.2466, -0.1895, -0.6640]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1675,  0.0545, -0.2028, -0.4365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 51 ][ timestamp 23 ] state=tensor([[ 0.1675,  0.0545, -0.2028, -0.4365]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 51: Exploration_rate=0.05. Score=23.\n",
      "[ episode 52 ] state=tensor([[ 0.0168, -0.0377, -0.0029,  0.0078]])\n",
      "[ episode 52 ][ timestamp 1 ] state=tensor([[ 0.0168, -0.0377, -0.0029,  0.0078]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0160,  0.1574, -0.0027, -0.2858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 2 ] state=tensor([[ 0.0160,  0.1574, -0.0027, -0.2858]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0192,  0.3526, -0.0085, -0.5794]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 52 ][ timestamp 3 ] state=tensor([[ 0.0192,  0.3526, -0.0085, -0.5794]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0262,  0.5478, -0.0200, -0.8747]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 4 ] state=tensor([[ 0.0262,  0.5478, -0.0200, -0.8747]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0372,  0.3530, -0.0375, -0.5884]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 5 ] state=tensor([[ 0.0372,  0.3530, -0.0375, -0.5884]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0442,  0.1584, -0.0493, -0.3078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 6 ] state=tensor([[ 0.0442,  0.1584, -0.0493, -0.3078]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0474,  0.3542, -0.0555, -0.6156]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 7 ] state=tensor([[ 0.0474,  0.3542, -0.0555, -0.6156]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0545,  0.5500, -0.0678, -0.9252]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 8 ] state=tensor([[ 0.0545,  0.5500, -0.0678, -0.9252]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0655,  0.3559, -0.0863, -0.6545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 9 ] state=tensor([[ 0.0655,  0.3559, -0.0863, -0.6545]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0726,  0.1621, -0.0994, -0.3902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 10 ] state=tensor([[ 0.0726,  0.1621, -0.0994, -0.3902]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0759, -0.0315, -0.1072, -0.1305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 11 ] state=tensor([[ 0.0759, -0.0315, -0.1072, -0.1305]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0752,  0.1650, -0.1098, -0.4549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 12 ] state=tensor([[ 0.0752,  0.1650, -0.1098, -0.4549]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0785, -0.0284, -0.1189, -0.1988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 13 ] state=tensor([[ 0.0785, -0.0284, -0.1189, -0.1988]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0780,  0.1682, -0.1229, -0.5265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 14 ] state=tensor([[ 0.0780,  0.1682, -0.1229, -0.5265]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0813, -0.0250, -0.1334, -0.2749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 15 ] state=tensor([[ 0.0813, -0.0250, -0.1334, -0.2749]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0808,  0.1717, -0.1389, -0.6065]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 16 ] state=tensor([[ 0.0808,  0.1717, -0.1389, -0.6065]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0842, -0.0212, -0.1510, -0.3606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 17 ] state=tensor([[ 0.0842, -0.0212, -0.1510, -0.3606]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0838,  0.1757, -0.1582, -0.6968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 18 ] state=tensor([[ 0.0838,  0.1757, -0.1582, -0.6968]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0873,  0.3726, -0.1722, -1.0348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 19 ] state=tensor([[ 0.0873,  0.3726, -0.1722, -1.0348]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0948,  0.1801, -0.1928, -0.8007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 20 ] state=tensor([[ 0.0948,  0.1801, -0.1928, -0.8007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0984, -0.0119, -0.2089, -0.5744]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 52 ][ timestamp 21 ] state=tensor([[ 0.0984, -0.0119, -0.2089, -0.5744]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 52: Exploration_rate=0.05. Score=21.\n",
      "[ episode 53 ] state=tensor([[ 0.0061, -0.0276,  0.0396,  0.0119]])\n",
      "[ episode 53 ][ timestamp 1 ] state=tensor([[ 0.0061, -0.0276,  0.0396,  0.0119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0056,  0.1669,  0.0398, -0.2681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 2 ] state=tensor([[ 0.0056,  0.1669,  0.0398, -0.2681]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0089,  0.3615,  0.0345, -0.5479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 3 ] state=tensor([[ 0.0089,  0.3615,  0.0345, -0.5479]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0162,  0.5561,  0.0235, -0.8295]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 4 ] state=tensor([[ 0.0162,  0.5561,  0.0235, -0.8295]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0273,  0.3606,  0.0069, -0.5296]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 5 ] state=tensor([[ 0.0273,  0.3606,  0.0069, -0.5296]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0345,  0.1654, -0.0037, -0.2347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 6 ] state=tensor([[ 0.0345,  0.1654, -0.0037, -0.2347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0378, -0.0297, -0.0084,  0.0568]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 7 ] state=tensor([[ 0.0378, -0.0297, -0.0084,  0.0568]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0372,  0.1656, -0.0072, -0.2385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 8 ] state=tensor([[ 0.0372,  0.1656, -0.0072, -0.2385]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0405,  0.3608, -0.0120, -0.5334]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 9 ] state=tensor([[ 0.0405,  0.3608, -0.0120, -0.5334]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0477,  0.5561, -0.0227, -0.8299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 10 ] state=tensor([[ 0.0477,  0.5561, -0.0227, -0.8299]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0589,  0.3613, -0.0393, -0.5444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 11 ] state=tensor([[ 0.0589,  0.3613, -0.0393, -0.5444]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0661,  0.5569, -0.0502, -0.8492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 12 ] state=tensor([[ 0.0661,  0.5569, -0.0502, -0.8492]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0772,  0.3625, -0.0671, -0.5727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 13 ] state=tensor([[ 0.0772,  0.3625, -0.0671, -0.5727]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0845,  0.1684, -0.0786, -0.3019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 14 ] state=tensor([[ 0.0845,  0.1684, -0.0786, -0.3019]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0878,  0.3646, -0.0846, -0.6183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 15 ] state=tensor([[ 0.0878,  0.3646, -0.0846, -0.6183]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0951,  0.1707, -0.0970, -0.3534]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 16 ] state=tensor([[ 0.0951,  0.1707, -0.0970, -0.3534]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0986,  0.3671, -0.1041, -0.6750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 17 ] state=tensor([[ 0.0986,  0.3671, -0.1041, -0.6750]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1059,  0.1736, -0.1176, -0.4169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 18 ] state=tensor([[ 0.1059,  0.1736, -0.1176, -0.4169]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1094,  0.3701, -0.1259, -0.7442]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 19 ] state=tensor([[ 0.1094,  0.3701, -0.1259, -0.7442]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1168,  0.1769, -0.1408, -0.4936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 20 ] state=tensor([[ 0.1168,  0.1769, -0.1408, -0.4936]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1203,  0.3737, -0.1507, -0.8271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 21 ] state=tensor([[ 0.1203,  0.3737, -0.1507, -0.8271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1278,  0.1810, -0.1672, -0.5854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 22 ] state=tensor([[ 0.1278,  0.1810, -0.1672, -0.5854]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1314, -0.0115, -0.1789, -0.3497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 23 ] state=tensor([[ 0.1314, -0.0115, -0.1789, -0.3497]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1312,  0.1857, -0.1859, -0.6930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 24 ] state=tensor([[ 0.1312,  0.1857, -0.1859, -0.6930]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1349, -0.0064, -0.1998, -0.4641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 25 ] state=tensor([[ 0.1349, -0.0064, -0.1998, -0.4641]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1348,  0.1909, -0.2090, -0.8125]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 53 ][ timestamp 26 ] state=tensor([[ 0.1348,  0.1909, -0.2090, -0.8125]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 53: Exploration_rate=0.05. Score=26.\n",
      "[ episode 54 ] state=tensor([[-1.7468e-02, -2.4283e-03,  1.0245e-06,  6.1355e-03]])\n",
      "[ episode 54 ][ timestamp 1 ] state=tensor([[-1.7468e-02, -2.4283e-03,  1.0245e-06,  6.1355e-03]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7516e-02,  1.9269e-01,  1.2374e-04, -2.8655e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 2 ] state=tensor([[-1.7516e-02,  1.9269e-01,  1.2374e-04, -2.8655e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0137,  0.3878, -0.0056, -0.5792]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 3 ] state=tensor([[-0.0137,  0.3878, -0.0056, -0.5792]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0059,  0.1928, -0.0172, -0.2883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 4 ] state=tensor([[-0.0059,  0.1928, -0.0172, -0.2883]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0021,  0.3881, -0.0230, -0.5863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 5 ] state=tensor([[-0.0021,  0.3881, -0.0230, -0.5863]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0057,  0.1933, -0.0347, -0.3010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 6 ] state=tensor([[ 0.0057,  0.1933, -0.0347, -0.3010]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0096,  0.3889, -0.0407, -0.6044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 7 ] state=tensor([[ 0.0096,  0.3889, -0.0407, -0.6044]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0174,  0.1944, -0.0528, -0.3248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 8 ] state=tensor([[ 0.0174,  0.1944, -0.0528, -0.3248]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 2.1246e-02,  7.7527e-05, -5.9286e-02, -4.9218e-02]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 54 ][ timestamp 9 ] state=tensor([[ 2.1246e-02,  7.7527e-05, -5.9286e-02, -4.9218e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0212,  0.1960, -0.0603, -0.3600]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 10 ] state=tensor([[ 0.0212,  0.1960, -0.0603, -0.3600]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0252,  0.3919, -0.0675, -0.6711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 11 ] state=tensor([[ 0.0252,  0.3919, -0.0675, -0.6711]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0330,  0.1978, -0.0809, -0.4004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 12 ] state=tensor([[ 0.0330,  0.1978, -0.0809, -0.4004]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0370,  0.3940, -0.0889, -0.7174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 13 ] state=tensor([[ 0.0370,  0.3940, -0.0889, -0.7174]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0448,  0.2002, -0.1032, -0.4540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 14 ] state=tensor([[ 0.0448,  0.2002, -0.1032, -0.4540]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0488,  0.3966, -0.1123, -0.7773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 15 ] state=tensor([[ 0.0488,  0.3966, -0.1123, -0.7773]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0568,  0.2032, -0.1279, -0.5220]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 16 ] state=tensor([[ 0.0568,  0.2032, -0.1279, -0.5220]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0608,  0.0101, -0.1383, -0.2722]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 17 ] state=tensor([[ 0.0608,  0.0101, -0.1383, -0.2722]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0610,  0.2069, -0.1438, -0.6051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 18 ] state=tensor([[ 0.0610,  0.2069, -0.1438, -0.6051]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0652,  0.0140, -0.1559, -0.3609]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 19 ] state=tensor([[ 0.0652,  0.0140, -0.1559, -0.3609]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0655,  0.2110, -0.1631, -0.6984]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 20 ] state=tensor([[ 0.0655,  0.2110, -0.1631, -0.6984]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0697,  0.0184, -0.1770, -0.4612]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 21 ] state=tensor([[ 0.0697,  0.0184, -0.1770, -0.4612]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0700,  0.2156, -0.1863, -0.8040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 22 ] state=tensor([[ 0.0700,  0.2156, -0.1863, -0.8040]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0744,  0.4127, -0.2024, -1.1491]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 54 ][ timestamp 23 ] state=tensor([[ 0.0744,  0.4127, -0.2024, -1.1491]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 54: Exploration_rate=0.05. Score=23.\n",
      "[ episode 55 ] state=tensor([[-0.0124, -0.0113, -0.0444,  0.0007]])\n",
      "[ episode 55 ][ timestamp 1 ] state=tensor([[-0.0124, -0.0113, -0.0444,  0.0007]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0127,  0.1844, -0.0444, -0.3057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 2 ] state=tensor([[-0.0127,  0.1844, -0.0444, -0.3057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0090,  0.3801, -0.0505, -0.6120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 3 ] state=tensor([[-0.0090,  0.3801, -0.0505, -0.6120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0014,  0.5759, -0.0627, -0.9201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 4 ] state=tensor([[-0.0014,  0.5759, -0.0627, -0.9201]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0101,  0.3817, -0.0811, -0.6478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 5 ] state=tensor([[ 0.0101,  0.3817, -0.0811, -0.6478]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0178,  0.5779, -0.0941, -0.9649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 6 ] state=tensor([[ 0.0178,  0.5779, -0.0941, -0.9649]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0293,  0.3841, -0.1134, -0.7032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 7 ] state=tensor([[ 0.0293,  0.3841, -0.1134, -0.7032]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0370,  0.1907, -0.1274, -0.4482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 8 ] state=tensor([[ 0.0370,  0.1907, -0.1274, -0.4482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0408,  0.3874, -0.1364, -0.7782]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 9 ] state=tensor([[ 0.0408,  0.3874, -0.1364, -0.7782]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0486,  0.1944, -0.1520, -0.5314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 10 ] state=tensor([[ 0.0486,  0.1944, -0.1520, -0.5314]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0525,  0.3913, -0.1626, -0.8678]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 11 ] state=tensor([[ 0.0525,  0.3913, -0.1626, -0.8678]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0603,  0.1987, -0.1800, -0.6304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 12 ] state=tensor([[ 0.0603,  0.1987, -0.1800, -0.6304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0643,  0.0065, -0.1926, -0.3993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 13 ] state=tensor([[ 0.0643,  0.0065, -0.1926, -0.3993]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0644,  0.2038, -0.2006, -0.7460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 55 ][ timestamp 14 ] state=tensor([[ 0.0644,  0.2038, -0.2006, -0.7460]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 55: Exploration_rate=0.05. Score=14.\n",
      "[ episode 56 ] state=tensor([[-0.0447,  0.0494, -0.0075, -0.0203]])\n",
      "[ episode 56 ][ timestamp 1 ] state=tensor([[-0.0447,  0.0494, -0.0075, -0.0203]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0437,  0.2446, -0.0079, -0.3154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 2 ] state=tensor([[-0.0437,  0.2446, -0.0079, -0.3154]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0388,  0.0496, -0.0142, -0.0252]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 3 ] state=tensor([[-0.0388,  0.0496, -0.0142, -0.0252]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0379,  0.2449, -0.0147, -0.3223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 4 ] state=tensor([[-0.0379,  0.2449, -0.0147, -0.3223]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0330,  0.4403, -0.0212, -0.6196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 5 ] state=tensor([[-0.0330,  0.4403, -0.0212, -0.6196]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0242,  0.6357, -0.0336, -0.9189]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 6 ] state=tensor([[-0.0242,  0.6357, -0.0336, -0.9189]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0114,  0.4410, -0.0520, -0.6370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 7 ] state=tensor([[-0.0114,  0.4410, -0.0520, -0.6370]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0026,  0.6368, -0.0647, -0.9456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 8 ] state=tensor([[-0.0026,  0.6368, -0.0647, -0.9456]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0101,  0.4426, -0.0836, -0.6739]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 9 ] state=tensor([[ 0.0101,  0.4426, -0.0836, -0.6739]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0190,  0.6388, -0.0971, -0.9917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 10 ] state=tensor([[ 0.0190,  0.6388, -0.0971, -0.9917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0317,  0.4451, -0.1169, -0.7310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 11 ] state=tensor([[ 0.0317,  0.4451, -0.1169, -0.7310]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0407,  0.2518, -0.1315, -0.4773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 12 ] state=tensor([[ 0.0407,  0.2518, -0.1315, -0.4773]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0457,  0.4485, -0.1411, -0.8084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 13 ] state=tensor([[ 0.0457,  0.4485, -0.1411, -0.8084]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0547,  0.2556, -0.1573, -0.5632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 14 ] state=tensor([[ 0.0547,  0.2556, -0.1573, -0.5632]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0598,  0.0630, -0.1685, -0.3239]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 15 ] state=tensor([[ 0.0598,  0.0630, -0.1685, -0.3239]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0610,  0.2600, -0.1750, -0.6646]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 16 ] state=tensor([[ 0.0610,  0.2600, -0.1750, -0.6646]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0662,  0.0677, -0.1883, -0.4317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 17 ] state=tensor([[ 0.0662,  0.0677, -0.1883, -0.4317]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0676,  0.2649, -0.1969, -0.7773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 56 ][ timestamp 18 ] state=tensor([[ 0.0676,  0.2649, -0.1969, -0.7773]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 56: Exploration_rate=0.05. Score=18.\n",
      "[ episode 57 ] state=tensor([[0.0364, 0.0409, 0.0181, 0.0215]])\n",
      "[ episode 57 ][ timestamp 1 ] state=tensor([[0.0364, 0.0409, 0.0181, 0.0215]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0372,  0.2358,  0.0185, -0.2654]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 2 ] state=tensor([[ 0.0372,  0.2358,  0.0185, -0.2654]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0419,  0.4306,  0.0132, -0.5522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 3 ] state=tensor([[ 0.0419,  0.4306,  0.0132, -0.5522]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0505,  0.2353,  0.0021, -0.2554]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 4 ] state=tensor([[ 0.0505,  0.2353,  0.0021, -0.2554]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0552,  0.4304, -0.0030, -0.5475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 5 ] state=tensor([[ 0.0552,  0.4304, -0.0030, -0.5475]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0638,  0.2353, -0.0139, -0.2557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 6 ] state=tensor([[ 0.0638,  0.2353, -0.0139, -0.2557]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0685,  0.4306, -0.0190, -0.5528]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 7 ] state=tensor([[ 0.0685,  0.4306, -0.0190, -0.5528]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0772,  0.2358, -0.0301, -0.2661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 8 ] state=tensor([[ 0.0772,  0.2358, -0.0301, -0.2661]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0819,  0.4313, -0.0354, -0.5682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 9 ] state=tensor([[ 0.0819,  0.4313, -0.0354, -0.5682]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0905,  0.6269, -0.0468, -0.8718]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 10 ] state=tensor([[ 0.0905,  0.6269, -0.0468, -0.8718]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1030,  0.4325, -0.0642, -0.5942]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 11 ] state=tensor([[ 0.1030,  0.4325, -0.0642, -0.5942]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1117,  0.2383, -0.0761, -0.3224]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 12 ] state=tensor([[ 0.1117,  0.2383, -0.0761, -0.3224]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1165,  0.0443, -0.0826, -0.0546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 13 ] state=tensor([[ 0.1165,  0.0443, -0.0826, -0.0546]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1173,  0.2405, -0.0836, -0.3722]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 14 ] state=tensor([[ 0.1173,  0.2405, -0.0836, -0.3722]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1221,  0.4367, -0.0911, -0.6900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 15 ] state=tensor([[ 0.1221,  0.4367, -0.0911, -0.6900]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1309,  0.2430, -0.1049, -0.4274]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 16 ] state=tensor([[ 0.1309,  0.2430, -0.1049, -0.4274]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1357,  0.4394, -0.1134, -0.7512]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 57 ][ timestamp 17 ] state=tensor([[ 0.1357,  0.4394, -0.1134, -0.7512]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1445,  0.2460, -0.1285, -0.4962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 18 ] state=tensor([[ 0.1445,  0.2460, -0.1285, -0.4962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1495,  0.0529, -0.1384, -0.2466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 19 ] state=tensor([[ 0.1495,  0.0529, -0.1384, -0.2466]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1505,  0.2497, -0.1433, -0.5796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 20 ] state=tensor([[ 0.1505,  0.2497, -0.1433, -0.5796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1555,  0.0569, -0.1549, -0.3352]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 21 ] state=tensor([[ 0.1555,  0.0569, -0.1549, -0.3352]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1566,  0.2538, -0.1616, -0.6725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 22 ] state=tensor([[ 0.1566,  0.2538, -0.1616, -0.6725]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1617,  0.0613, -0.1751, -0.4347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 23 ] state=tensor([[ 0.1617,  0.0613, -0.1751, -0.4347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1629,  0.2584, -0.1838, -0.7771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 24 ] state=tensor([[ 0.1629,  0.2584, -0.1838, -0.7771]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1681,  0.0662, -0.1993, -0.5474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 57 ][ timestamp 25 ] state=tensor([[ 0.1681,  0.0662, -0.1993, -0.5474]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 57: Exploration_rate=0.05. Score=25.\n",
      "[ episode 58 ] state=tensor([[ 0.0029,  0.0272, -0.0113, -0.0332]])\n",
      "[ episode 58 ][ timestamp 1 ] state=tensor([[ 0.0029,  0.0272, -0.0113, -0.0332]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0035,  0.2225, -0.0120, -0.3294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 2 ] state=tensor([[ 0.0035,  0.2225, -0.0120, -0.3294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0079,  0.4178, -0.0186, -0.6259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 3 ] state=tensor([[ 0.0079,  0.4178, -0.0186, -0.6259]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0163,  0.2230, -0.0311, -0.3391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 4 ] state=tensor([[ 0.0163,  0.2230, -0.0311, -0.3391]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0207,  0.0283, -0.0379, -0.0564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 5 ] state=tensor([[ 0.0207,  0.0283, -0.0379, -0.0564]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0213,  0.2239, -0.0390, -0.3607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 6 ] state=tensor([[ 0.0213,  0.2239, -0.0390, -0.3607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0258,  0.4196, -0.0462, -0.6655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 7 ] state=tensor([[ 0.0258,  0.4196, -0.0462, -0.6655]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0342,  0.6153, -0.0595, -0.9723]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 8 ] state=tensor([[ 0.0342,  0.6153, -0.0595, -0.9723]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0465,  0.4211, -0.0790, -0.6989]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 9 ] state=tensor([[ 0.0465,  0.4211, -0.0790, -0.6989]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0549,  0.2271, -0.0929, -0.4321]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 10 ] state=tensor([[ 0.0549,  0.2271, -0.0929, -0.4321]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0594,  0.0334, -0.1016, -0.1701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 11 ] state=tensor([[ 0.0594,  0.0334, -0.1016, -0.1701]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0601,  0.2298, -0.1050, -0.4930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 12 ] state=tensor([[ 0.0601,  0.2298, -0.1050, -0.4930]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0647,  0.4263, -0.1148, -0.8168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 13 ] state=tensor([[ 0.0647,  0.4263, -0.1148, -0.8168]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0732,  0.2329, -0.1312, -0.5624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 14 ] state=tensor([[ 0.0732,  0.2329, -0.1312, -0.5624]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0779,  0.4296, -0.1424, -0.8933]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 15 ] state=tensor([[ 0.0779,  0.4296, -0.1424, -0.8933]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0865,  0.2367, -0.1603, -0.6486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 16 ] state=tensor([[ 0.0865,  0.2367, -0.1603, -0.6486]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0912,  0.0441, -0.1733, -0.4104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 17 ] state=tensor([[ 0.0912,  0.0441, -0.1733, -0.4104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0921,  0.2412, -0.1815, -0.7523]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 18 ] state=tensor([[ 0.0921,  0.2412, -0.1815, -0.7523]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0969,  0.0490, -0.1965, -0.5217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 19 ] state=tensor([[ 0.0969,  0.0490, -0.1965, -0.5217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0979,  0.2462, -0.2069, -0.8693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 58 ][ timestamp 20 ] state=tensor([[ 0.0979,  0.2462, -0.2069, -0.8693]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 58: Exploration_rate=0.05. Score=20.\n",
      "[ episode 59 ] state=tensor([[ 0.0053, -0.0444, -0.0270, -0.0329]])\n",
      "[ episode 59 ][ timestamp 1 ] state=tensor([[ 0.0053, -0.0444, -0.0270, -0.0329]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0044,  0.1511, -0.0277, -0.3340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 2 ] state=tensor([[ 0.0044,  0.1511, -0.0277, -0.3340]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0075,  0.3466, -0.0344, -0.6353]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 3 ] state=tensor([[ 0.0075,  0.3466, -0.0344, -0.6353]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0144,  0.1519, -0.0471, -0.3536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 4 ] state=tensor([[ 0.0144,  0.1519, -0.0471, -0.3536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0174,  0.3477, -0.0542, -0.6608]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 5 ] state=tensor([[ 0.0174,  0.3477, -0.0542, -0.6608]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0244,  0.5435, -0.0674, -0.9700]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 6 ] state=tensor([[ 0.0244,  0.5435, -0.0674, -0.9700]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0353,  0.7395, -0.0868, -1.2831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 7 ] state=tensor([[ 0.0353,  0.7395, -0.0868, -1.2831]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0501,  0.5456, -0.1124, -1.0188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 8 ] state=tensor([[ 0.0501,  0.5456, -0.1124, -1.0188]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0610,  0.3521, -0.1328, -0.7634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 9 ] state=tensor([[ 0.0610,  0.3521, -0.1328, -0.7634]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0680,  0.1591, -0.1481, -0.5153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 10 ] state=tensor([[ 0.0680,  0.1591, -0.1481, -0.5153]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0712, -0.0337, -0.1584, -0.2727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 11 ] state=tensor([[ 0.0712, -0.0337, -0.1584, -0.2727]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0705,  0.1633, -0.1638, -0.6108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 12 ] state=tensor([[ 0.0705,  0.1633, -0.1638, -0.6108]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0738, -0.0292, -0.1761, -0.3739]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 13 ] state=tensor([[ 0.0738, -0.0292, -0.1761, -0.3739]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0732,  0.1679, -0.1835, -0.7165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 14 ] state=tensor([[ 0.0732,  0.1679, -0.1835, -0.7165]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0766, -0.0243, -0.1979, -0.4868]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 15 ] state=tensor([[ 0.0766, -0.0243, -0.1979, -0.4868]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0761,  0.1730, -0.2076, -0.8347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 59 ][ timestamp 16 ] state=tensor([[ 0.0761,  0.1730, -0.2076, -0.8347]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 59: Exploration_rate=0.05. Score=16.\n",
      "[ episode 60 ] state=tensor([[-0.0264,  0.0022, -0.0044,  0.0229]])\n",
      "[ episode 60 ][ timestamp 1 ] state=tensor([[-0.0264,  0.0022, -0.0044,  0.0229]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0263,  0.1974, -0.0039, -0.2712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 2 ] state=tensor([[-0.0263,  0.1974, -0.0039, -0.2712]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0224,  0.3926, -0.0094, -0.5651]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 3 ] state=tensor([[-0.0224,  0.3926, -0.0094, -0.5651]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0145,  0.1976, -0.0207, -0.2754]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 4 ] state=tensor([[-0.0145,  0.1976, -0.0207, -0.2754]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0106,  0.0027, -0.0262,  0.0107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 5 ] state=tensor([[-0.0106,  0.0027, -0.0262,  0.0107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0105, -0.1920, -0.0259,  0.2951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 6 ] state=tensor([[-0.0105, -0.1920, -0.0259,  0.2951]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0144,  0.0035, -0.0200, -0.0057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 7 ] state=tensor([[-0.0144,  0.0035, -0.0200, -0.0057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0143,  0.1989, -0.0202, -0.3046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 8 ] state=tensor([[-0.0143,  0.1989, -0.0202, -0.3046]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0103,  0.3943, -0.0263, -0.6036]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 9 ] state=tensor([[-0.0103,  0.3943, -0.0263, -0.6036]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0024,  0.1996, -0.0383, -0.3193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 10 ] state=tensor([[-0.0024,  0.1996, -0.0383, -0.3193]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0016,  0.3952, -0.0447, -0.6238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 11 ] state=tensor([[ 0.0016,  0.3952, -0.0447, -0.6238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0095,  0.2007, -0.0572, -0.3456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 12 ] state=tensor([[ 0.0095,  0.2007, -0.0572, -0.3456]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0135,  0.3966, -0.0641, -0.6557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 13 ] state=tensor([[ 0.0135,  0.3966, -0.0641, -0.6557]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0214,  0.2024, -0.0772, -0.3839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 14 ] state=tensor([[ 0.0214,  0.2024, -0.0772, -0.3839]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0255,  0.0085, -0.0849, -0.1165]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 60 ][ timestamp 15 ] state=tensor([[ 0.0255,  0.0085, -0.0849, -0.1165]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0256,  0.2047, -0.0872, -0.4347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 16 ] state=tensor([[ 0.0256,  0.2047, -0.0872, -0.4347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0297,  0.4010, -0.0959, -0.7536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 17 ] state=tensor([[ 0.0297,  0.4010, -0.0959, -0.7536]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0377,  0.2073, -0.1110, -0.4925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 18 ] state=tensor([[ 0.0377,  0.2073, -0.1110, -0.4925]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0419,  0.0139, -0.1208, -0.2368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 19 ] state=tensor([[ 0.0419,  0.0139, -0.1208, -0.2368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0422, -0.1793, -0.1256,  0.0155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 20 ] state=tensor([[ 0.0422, -0.1793, -0.1256,  0.0155]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0386,  0.0174, -0.1253, -0.3141]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 21 ] state=tensor([[ 0.0386,  0.0174, -0.1253, -0.3141]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0389,  0.2140, -0.1315, -0.6435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 22 ] state=tensor([[ 0.0389,  0.2140, -0.1315, -0.6435]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0432,  0.0210, -0.1444, -0.3949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 23 ] state=tensor([[ 0.0432,  0.0210, -0.1444, -0.3949]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0436,  0.2178, -0.1523, -0.7294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 24 ] state=tensor([[ 0.0436,  0.2178, -0.1523, -0.7294]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0480,  0.0251, -0.1669, -0.4883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 25 ] state=tensor([[ 0.0480,  0.0251, -0.1669, -0.4883]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0485,  0.2221, -0.1767, -0.8286]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 26 ] state=tensor([[ 0.0485,  0.2221, -0.1767, -0.8286]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0529,  0.0298, -0.1932, -0.5963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 27 ] state=tensor([[ 0.0529,  0.0298, -0.1932, -0.5963]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0535,  0.2270, -0.2052, -0.9431]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 60 ][ timestamp 28 ] state=tensor([[ 0.0535,  0.2270, -0.2052, -0.9431]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 60: Exploration_rate=0.05. Score=28.\n",
      "[ episode 61 ] state=tensor([[ 0.0169, -0.0124,  0.0326,  0.0016]])\n",
      "[ episode 61 ][ timestamp 1 ] state=tensor([[ 0.0169, -0.0124,  0.0326,  0.0016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0167,  0.1822,  0.0327, -0.2806]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 2 ] state=tensor([[ 0.0167,  0.1822,  0.0327, -0.2806]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0203,  0.3769,  0.0271, -0.5628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 3 ] state=tensor([[ 0.0203,  0.3769,  0.0271, -0.5628]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0279,  0.5716,  0.0158, -0.8468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 4 ] state=tensor([[ 0.0279,  0.5716,  0.0158, -0.8468]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0393,  0.3763, -0.0011, -0.5492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 5 ] state=tensor([[ 0.0393,  0.3763, -0.0011, -0.5492]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0468,  0.1812, -0.0121, -0.2568]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 6 ] state=tensor([[ 0.0468,  0.1812, -0.0121, -0.2568]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0504,  0.3765, -0.0172, -0.5533]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 7 ] state=tensor([[ 0.0504,  0.3765, -0.0172, -0.5533]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0580,  0.5718, -0.0283, -0.8514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 8 ] state=tensor([[ 0.0580,  0.5718, -0.0283, -0.8514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0694,  0.3771, -0.0453, -0.5677]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 9 ] state=tensor([[ 0.0694,  0.3771, -0.0453, -0.5677]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0769,  0.1826, -0.0567, -0.2897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 10 ] state=tensor([[ 0.0769,  0.1826, -0.0567, -0.2897]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0806,  0.3785, -0.0625, -0.5997]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 11 ] state=tensor([[ 0.0806,  0.3785, -0.0625, -0.5997]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0882,  0.1843, -0.0745, -0.3273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 12 ] state=tensor([[ 0.0882,  0.1843, -0.0745, -0.3273]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0919,  0.3804, -0.0810, -0.6425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 13 ] state=tensor([[ 0.0919,  0.3804, -0.0810, -0.6425]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0995,  0.1865, -0.0939, -0.3764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 14 ] state=tensor([[ 0.0995,  0.1865, -0.0939, -0.3764]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1032,  0.3828, -0.1014, -0.6972]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 15 ] state=tensor([[ 0.1032,  0.3828, -0.1014, -0.6972]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1109,  0.1893, -0.1153, -0.4380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 16 ] state=tensor([[ 0.1109,  0.1893, -0.1153, -0.4380]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1146, -0.0041, -0.1241, -0.1838]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 17 ] state=tensor([[ 0.1146, -0.0041, -0.1241, -0.1838]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1146,  0.1926, -0.1278, -0.5129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 18 ] state=tensor([[ 0.1146,  0.1926, -0.1278, -0.5129]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1184, -0.0005, -0.1380, -0.2631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 19 ] state=tensor([[ 0.1184, -0.0005, -0.1380, -0.2631]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1184,  0.1963, -0.1433, -0.5959]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 20 ] state=tensor([[ 0.1184,  0.1963, -0.1433, -0.5959]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1223,  0.0034, -0.1552, -0.3516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 21 ] state=tensor([[ 0.1223,  0.0034, -0.1552, -0.3516]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1224,  0.2004, -0.1623, -0.6889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 22 ] state=tensor([[ 0.1224,  0.2004, -0.1623, -0.6889]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1264,  0.0078, -0.1760, -0.4514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 23 ] state=tensor([[ 0.1264,  0.0078, -0.1760, -0.4514]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1266,  0.2050, -0.1851, -0.7940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 24 ] state=tensor([[ 0.1266,  0.2050, -0.1851, -0.7940]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1307,  0.0128, -0.2009, -0.5648]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 61 ][ timestamp 25 ] state=tensor([[ 0.1307,  0.0128, -0.2009, -0.5648]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 61: Exploration_rate=0.05. Score=25.\n",
      "[ episode 62 ] state=tensor([[ 0.0333,  0.0355, -0.0430,  0.0445]])\n",
      "[ episode 62 ][ timestamp 1 ] state=tensor([[ 0.0333,  0.0355, -0.0430,  0.0445]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0341,  0.2312, -0.0421, -0.2614]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 2 ] state=tensor([[ 0.0341,  0.2312, -0.0421, -0.2614]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0387,  0.4269, -0.0473, -0.5671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 3 ] state=tensor([[ 0.0387,  0.4269, -0.0473, -0.5671]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0472,  0.2325, -0.0587, -0.2897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 4 ] state=tensor([[ 0.0472,  0.2325, -0.0587, -0.2897]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0519,  0.0382, -0.0644, -0.0160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 5 ] state=tensor([[ 0.0519,  0.0382, -0.0644, -0.0160]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0526, -0.1559, -0.0648,  0.2556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 6 ] state=tensor([[ 0.0526, -0.1559, -0.0648,  0.2556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0495,  0.0401, -0.0597, -0.0567]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 7 ] state=tensor([[ 0.0495,  0.0401, -0.0597, -0.0567]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0503,  0.2360, -0.0608, -0.3676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 8 ] state=tensor([[ 0.0503,  0.2360, -0.0608, -0.3676]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0550,  0.4319, -0.0681, -0.6788]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 9 ] state=tensor([[ 0.0550,  0.4319, -0.0681, -0.6788]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0637,  0.2378, -0.0817, -0.4084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 10 ] state=tensor([[ 0.0637,  0.2378, -0.0817, -0.4084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0684,  0.4340, -0.0899, -0.7257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 11 ] state=tensor([[ 0.0684,  0.4340, -0.0899, -0.7257]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0771,  0.2402, -0.1044, -0.4626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 12 ] state=tensor([[ 0.0771,  0.2402, -0.1044, -0.4626]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0819,  0.0467, -0.1136, -0.2045]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 13 ] state=tensor([[ 0.0819,  0.0467, -0.1136, -0.2045]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0829,  0.2433, -0.1177, -0.5308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 14 ] state=tensor([[ 0.0829,  0.2433, -0.1177, -0.5308]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0877,  0.0500, -0.1284, -0.2774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 15 ] state=tensor([[ 0.0877,  0.0500, -0.1284, -0.2774]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0887,  0.2467, -0.1339, -0.6076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 16 ] state=tensor([[ 0.0887,  0.2467, -0.1339, -0.6076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0936,  0.0537, -0.1461, -0.3600]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 17 ] state=tensor([[ 0.0936,  0.0537, -0.1461, -0.3600]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0947,  0.2505, -0.1533, -0.6949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 18 ] state=tensor([[ 0.0947,  0.2505, -0.1533, -0.6949]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0997,  0.0578, -0.1672, -0.4541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 19 ] state=tensor([[ 0.0997,  0.0578, -0.1672, -0.4541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1009,  0.2549, -0.1762, -0.7945]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 20 ] state=tensor([[ 0.1009,  0.2549, -0.1762, -0.7945]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1060,  0.0625, -0.1921, -0.5620]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 21 ] state=tensor([[ 0.1060,  0.0625, -0.1921, -0.5620]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1072,  0.2598, -0.2034, -0.9085]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 62 ][ timestamp 22 ] state=tensor([[ 0.1072,  0.2598, -0.2034, -0.9085]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 62: Exploration_rate=0.05. Score=22.\n",
      "[ episode 63 ] state=tensor([[ 0.0499,  0.0288,  0.0288, -0.0021]])\n",
      "[ episode 63 ][ timestamp 1 ] state=tensor([[ 0.0499,  0.0288,  0.0288, -0.0021]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0504,  0.2235,  0.0287, -0.2856]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 2 ] state=tensor([[ 0.0504,  0.2235,  0.0287, -0.2856]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0549,  0.4182,  0.0230, -0.5690]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 3 ] state=tensor([[ 0.0549,  0.4182,  0.0230, -0.5690]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0633,  0.2227,  0.0116, -0.2692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 4 ] state=tensor([[ 0.0633,  0.2227,  0.0116, -0.2692]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0677,  0.4177,  0.0063, -0.5582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 5 ] state=tensor([[ 0.0677,  0.4177,  0.0063, -0.5582]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0761,  0.6127, -0.0049, -0.8489]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 63 ][ timestamp 6 ] state=tensor([[ 0.0761,  0.6127, -0.0049, -0.8489]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0883,  0.4177, -0.0219, -0.5578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 7 ] state=tensor([[ 0.0883,  0.4177, -0.0219, -0.5578]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0967,  0.2229, -0.0330, -0.2720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 8 ] state=tensor([[ 0.0967,  0.2229, -0.0330, -0.2720]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1011,  0.4185, -0.0385, -0.5750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 9 ] state=tensor([[ 0.1011,  0.4185, -0.0385, -0.5750]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1095,  0.2239, -0.0500, -0.2946]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 10 ] state=tensor([[ 0.1095,  0.2239, -0.0500, -0.2946]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1140,  0.0295, -0.0559, -0.0181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 11 ] state=tensor([[ 0.1140,  0.0295, -0.0559, -0.0181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1146,  0.2254, -0.0562, -0.3279]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 12 ] state=tensor([[ 0.1146,  0.2254, -0.0562, -0.3279]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1191,  0.0311, -0.0628, -0.0535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 13 ] state=tensor([[ 0.1191,  0.0311, -0.0628, -0.0535]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1197,  0.2271, -0.0639, -0.3653]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 14 ] state=tensor([[ 0.1197,  0.2271, -0.0639, -0.3653]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1242,  0.0329, -0.0712, -0.0934]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 15 ] state=tensor([[ 0.1242,  0.0329, -0.0712, -0.0934]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1249, -0.1611, -0.0730,  0.1760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 16 ] state=tensor([[ 0.1249, -0.1611, -0.0730,  0.1760]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1217,  0.0350, -0.0695, -0.1388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 17 ] state=tensor([[ 0.1217,  0.0350, -0.0695, -0.1388]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1224,  0.2310, -0.0723, -0.4526]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 18 ] state=tensor([[ 0.1224,  0.2310, -0.0723, -0.4526]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1270,  0.0370, -0.0813, -0.1835]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 19 ] state=tensor([[ 0.1270,  0.0370, -0.0813, -0.1835]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1277,  0.2332, -0.0850, -0.5007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 20 ] state=tensor([[ 0.1277,  0.2332, -0.0850, -0.5007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1324,  0.0393, -0.0950, -0.2360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 21 ] state=tensor([[ 0.1324,  0.0393, -0.0950, -0.2360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1332,  0.2357, -0.0998, -0.5571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 22 ] state=tensor([[ 0.1332,  0.2357, -0.0998, -0.5571]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1379,  0.0421, -0.1109, -0.2974]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 23 ] state=tensor([[ 0.1379,  0.0421, -0.1109, -0.2974]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1388,  0.2386, -0.1168, -0.6229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 24 ] state=tensor([[ 0.1388,  0.2386, -0.1168, -0.6229]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1435,  0.0453, -0.1293, -0.3692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 25 ] state=tensor([[ 0.1435,  0.0453, -0.1293, -0.3692]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1444, -0.1478, -0.1367, -0.1199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 26 ] state=tensor([[ 0.1444, -0.1478, -0.1367, -0.1199]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1415,  0.0490, -0.1391, -0.4524]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 27 ] state=tensor([[ 0.1415,  0.0490, -0.1391, -0.4524]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1425,  0.2458, -0.1481, -0.7855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 28 ] state=tensor([[ 0.1425,  0.2458, -0.1481, -0.7855]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1474,  0.0530, -0.1638, -0.5428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 29 ] state=tensor([[ 0.1474,  0.0530, -0.1638, -0.5428]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1484, -0.1395, -0.1747, -0.3059]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 30 ] state=tensor([[ 0.1484, -0.1395, -0.1747, -0.3059]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1456,  0.0576, -0.1808, -0.6482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 31 ] state=tensor([[ 0.1456,  0.0576, -0.1808, -0.6482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1468,  0.2547, -0.1938, -0.9919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 63 ][ timestamp 32 ] state=tensor([[ 0.1468,  0.2547, -0.1938, -0.9919]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 63: Exploration_rate=0.05. Score=32.\n",
      "[ episode 64 ] state=tensor([[ 0.0490,  0.0470, -0.0341,  0.0367]])\n",
      "[ episode 64 ][ timestamp 1 ] state=tensor([[ 0.0490,  0.0470, -0.0341,  0.0367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0499, -0.1476, -0.0334,  0.3184]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 2 ] state=tensor([[ 0.0499, -0.1476, -0.0334,  0.3184]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0470,  0.0480, -0.0270,  0.0154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 3 ] state=tensor([[ 0.0470,  0.0480, -0.0270,  0.0154]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0479,  0.2435, -0.0267, -0.2856]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 4 ] state=tensor([[ 0.0479,  0.2435, -0.0267, -0.2856]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0528,  0.0488, -0.0324, -0.0015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 5 ] state=tensor([[ 0.0528,  0.0488, -0.0324, -0.0015]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0538,  0.2444, -0.0324, -0.3042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 6 ] state=tensor([[ 0.0538,  0.2444, -0.0324, -0.3042]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0587,  0.4399, -0.0385, -0.6069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 7 ] state=tensor([[ 0.0587,  0.4399, -0.0385, -0.6069]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0675,  0.2454, -0.0506, -0.3266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 8 ] state=tensor([[ 0.0675,  0.2454, -0.0506, -0.3266]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0724,  0.4412, -0.0572, -0.6348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 9 ] state=tensor([[ 0.0724,  0.4412, -0.0572, -0.6348]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0812,  0.2469, -0.0699, -0.3607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 10 ] state=tensor([[ 0.0812,  0.2469, -0.0699, -0.3607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0861,  0.4429, -0.0771, -0.6746]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 11 ] state=tensor([[ 0.0861,  0.4429, -0.0771, -0.6746]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0950,  0.2490, -0.0906, -0.4071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 12 ] state=tensor([[ 0.0950,  0.2490, -0.0906, -0.4071]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1000,  0.0552, -0.0987, -0.1443]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 13 ] state=tensor([[ 0.1000,  0.0552, -0.0987, -0.1443]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1011,  0.2516, -0.1016, -0.4664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 14 ] state=tensor([[ 0.1011,  0.2516, -0.1016, -0.4664]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1061,  0.0581, -0.1109, -0.2074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 15 ] state=tensor([[ 0.1061,  0.0581, -0.1109, -0.2074]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1073,  0.2546, -0.1151, -0.5329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 16 ] state=tensor([[ 0.1073,  0.2546, -0.1151, -0.5329]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1124,  0.0612, -0.1257, -0.2786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 17 ] state=tensor([[ 0.1124,  0.0612, -0.1257, -0.2786]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1136,  0.2579, -0.1313, -0.6081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 18 ] state=tensor([[ 0.1136,  0.2579, -0.1313, -0.6081]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1188,  0.0649, -0.1435, -0.3595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 19 ] state=tensor([[ 0.1188,  0.0649, -0.1435, -0.3595]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1200,  0.2617, -0.1507, -0.6938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 20 ] state=tensor([[ 0.1200,  0.2617, -0.1507, -0.6938]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1253,  0.0689, -0.1645, -0.4521]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 21 ] state=tensor([[ 0.1253,  0.0689, -0.1645, -0.4521]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1267, -0.1235, -0.1736, -0.2154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 22 ] state=tensor([[ 0.1267, -0.1235, -0.1736, -0.2154]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1242,  0.0736, -0.1779, -0.5575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 23 ] state=tensor([[ 0.1242,  0.0736, -0.1779, -0.5575]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1257,  0.2707, -0.1890, -0.9005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 24 ] state=tensor([[ 0.1257,  0.2707, -0.1890, -0.9005]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1311,  0.0786, -0.2070, -0.6727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 64 ][ timestamp 25 ] state=tensor([[ 0.1311,  0.0786, -0.2070, -0.6727]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 64: Exploration_rate=0.05. Score=25.\n",
      "[ episode 65 ] state=tensor([[-0.0178,  0.0255, -0.0044,  0.0081]])\n",
      "[ episode 65 ][ timestamp 1 ] state=tensor([[-0.0178,  0.0255, -0.0044,  0.0081]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0173,  0.2207, -0.0042, -0.2860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 2 ] state=tensor([[-0.0173,  0.2207, -0.0042, -0.2860]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0129,  0.4159, -0.0100, -0.5800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 3 ] state=tensor([[-0.0129,  0.4159, -0.0100, -0.5800]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0046,  0.6112, -0.0216, -0.8758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 4 ] state=tensor([[-0.0046,  0.6112, -0.0216, -0.8758]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0077,  0.4163, -0.0391, -0.5900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 5 ] state=tensor([[ 0.0077,  0.4163, -0.0391, -0.5900]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0160,  0.2218, -0.0509, -0.3099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 6 ] state=tensor([[ 0.0160,  0.2218, -0.0509, -0.3099]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0204,  0.0274, -0.0571, -0.0336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 7 ] state=tensor([[ 0.0204,  0.0274, -0.0571, -0.0336]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0210,  0.2233, -0.0577, -0.3438]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 8 ] state=tensor([[ 0.0210,  0.2233, -0.0577, -0.3438]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0254,  0.0291, -0.0646, -0.0698]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 9 ] state=tensor([[ 0.0254,  0.0291, -0.0646, -0.0698]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0260,  0.2250, -0.0660, -0.3822]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 65 ][ timestamp 10 ] state=tensor([[ 0.0260,  0.2250, -0.0660, -0.3822]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0305,  0.0309, -0.0737, -0.1110]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 11 ] state=tensor([[ 0.0305,  0.0309, -0.0737, -0.1110]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0311,  0.2270, -0.0759, -0.4260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 12 ] state=tensor([[ 0.0311,  0.2270, -0.0759, -0.4260]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0357,  0.0330, -0.0844, -0.1582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 13 ] state=tensor([[ 0.0357,  0.0330, -0.0844, -0.1582]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0363,  0.2293, -0.0876, -0.4763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 14 ] state=tensor([[ 0.0363,  0.2293, -0.0876, -0.4763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0409,  0.0355, -0.0971, -0.2124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 15 ] state=tensor([[ 0.0409,  0.0355, -0.0971, -0.2124]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0416,  0.2318, -0.1013, -0.5341]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 16 ] state=tensor([[ 0.0416,  0.2318, -0.1013, -0.5341]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0463,  0.0383, -0.1120, -0.2750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 17 ] state=tensor([[ 0.0463,  0.0383, -0.1120, -0.2750]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0470, -0.1551, -0.1175, -0.0196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 18 ] state=tensor([[ 0.0470, -0.1551, -0.1175, -0.0196]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0439,  0.0415, -0.1179, -0.3469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 19 ] state=tensor([[ 0.0439,  0.0415, -0.1179, -0.3469]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0448,  0.2381, -0.1249, -0.6743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 20 ] state=tensor([[ 0.0448,  0.2381, -0.1249, -0.6743]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0495,  0.0449, -0.1383, -0.4234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 21 ] state=tensor([[ 0.0495,  0.0449, -0.1383, -0.4234]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0504, -0.1480, -0.1468, -0.1774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 22 ] state=tensor([[ 0.0504, -0.1480, -0.1468, -0.1774]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0475,  0.0489, -0.1504, -0.5125]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 23 ] state=tensor([[ 0.0475,  0.0489, -0.1504, -0.5125]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0484,  0.2458, -0.1606, -0.8486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 24 ] state=tensor([[ 0.0484,  0.2458, -0.1606, -0.8486]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0534,  0.0532, -0.1776, -0.6104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 25 ] state=tensor([[ 0.0534,  0.0532, -0.1776, -0.6104]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0544, -0.1391, -0.1898, -0.3785]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 26 ] state=tensor([[ 0.0544, -0.1391, -0.1898, -0.3785]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0516,  0.0581, -0.1974, -0.7245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 65 ][ timestamp 27 ] state=tensor([[ 0.0516,  0.0581, -0.1974, -0.7245]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 65: Exploration_rate=0.05. Score=27.\n",
      "[ episode 66 ] state=tensor([[ 0.0108, -0.0379,  0.0058, -0.0335]])\n",
      "[ episode 66 ][ timestamp 1 ] state=tensor([[ 0.0108, -0.0379,  0.0058, -0.0335]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0101, -0.2331,  0.0051,  0.2610]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 2 ] state=tensor([[ 0.0101, -0.2331,  0.0051,  0.2610]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0054, -0.0381,  0.0103, -0.0301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 3 ] state=tensor([[ 0.0054, -0.0381,  0.0103, -0.0301]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0046,  0.1569,  0.0097, -0.3195]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 4 ] state=tensor([[ 0.0046,  0.1569,  0.0097, -0.3195]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0078,  0.3519,  0.0033, -0.6091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 5 ] state=tensor([[ 0.0078,  0.3519,  0.0033, -0.6091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0148,  0.1567, -0.0089, -0.3154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 6 ] state=tensor([[ 0.0148,  0.1567, -0.0089, -0.3154]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0179, -0.0383, -0.0152, -0.0255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 7 ] state=tensor([[ 0.0179, -0.0383, -0.0152, -0.0255]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0172,  0.1570, -0.0157, -0.3230]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 8 ] state=tensor([[ 0.0172,  0.1570, -0.0157, -0.3230]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0203,  0.3524, -0.0221, -0.6205]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 9 ] state=tensor([[ 0.0203,  0.3524, -0.0221, -0.6205]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0274,  0.5478, -0.0346, -0.9201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 10 ] state=tensor([[ 0.0274,  0.5478, -0.0346, -0.9201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0383,  0.7434, -0.0530, -1.2235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 11 ] state=tensor([[ 0.0383,  0.7434, -0.0530, -1.2235]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0532,  0.5490, -0.0774, -0.9478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 12 ] state=tensor([[ 0.0532,  0.5490, -0.0774, -0.9478]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0642,  0.3550, -0.0964, -0.6804]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 13 ] state=tensor([[ 0.0642,  0.3550, -0.0964, -0.6804]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0713,  0.1613, -0.1100, -0.4196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 14 ] state=tensor([[ 0.0713,  0.1613, -0.1100, -0.4196]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0745,  0.3578, -0.1184, -0.7448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 15 ] state=tensor([[ 0.0745,  0.3578, -0.1184, -0.7448]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0817,  0.1645, -0.1333, -0.4916]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 16 ] state=tensor([[ 0.0817,  0.1645, -0.1333, -0.4916]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0849, -0.0285, -0.1431, -0.2437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 17 ] state=tensor([[ 0.0849, -0.0285, -0.1431, -0.2437]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0844,  0.1683, -0.1480, -0.5779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 18 ] state=tensor([[ 0.0844,  0.1683, -0.1480, -0.5779]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0877, -0.0244, -0.1595, -0.3353]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 19 ] state=tensor([[ 0.0877, -0.0244, -0.1595, -0.3353]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0872,  0.1725, -0.1663, -0.6737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 20 ] state=tensor([[ 0.0872,  0.1725, -0.1663, -0.6737]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0907, -0.0199, -0.1797, -0.4376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 21 ] state=tensor([[ 0.0907, -0.0199, -0.1797, -0.4376]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0903,  0.1772, -0.1885, -0.7812]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 22 ] state=tensor([[ 0.0903,  0.1772, -0.1885, -0.7812]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0938, -0.0149, -0.2041, -0.5532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 66 ][ timestamp 23 ] state=tensor([[ 0.0938, -0.0149, -0.2041, -0.5532]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 66: Exploration_rate=0.05. Score=23.\n",
      "[ episode 67 ] state=tensor([[-0.0485,  0.0343,  0.0120,  0.0281]])\n",
      "[ episode 67 ][ timestamp 1 ] state=tensor([[-0.0485,  0.0343,  0.0120,  0.0281]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0478,  0.2293,  0.0126, -0.2607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 2 ] state=tensor([[-0.0478,  0.2293,  0.0126, -0.2607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0432,  0.4242,  0.0074, -0.5494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 3 ] state=tensor([[-0.0432,  0.4242,  0.0074, -0.5494]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0347,  0.6192, -0.0036, -0.8398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 4 ] state=tensor([[-0.0347,  0.6192, -0.0036, -0.8398]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0223,  0.8144, -0.0204, -1.1336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 5 ] state=tensor([[-0.0223,  0.8144, -0.0204, -1.1336]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0061,  1.0098, -0.0431, -1.4326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 6 ] state=tensor([[-0.0061,  1.0098, -0.0431, -1.4326]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0141,  0.8152, -0.0717, -1.1537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 7 ] state=tensor([[ 0.0141,  0.8152, -0.0717, -1.1537]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0304,  0.6211, -0.0948, -0.8844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 8 ] state=tensor([[ 0.0304,  0.6211, -0.0948, -0.8844]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0429,  0.4274, -0.1125, -0.6229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 9 ] state=tensor([[ 0.0429,  0.4274, -0.1125, -0.6229]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0514,  0.2340, -0.1250, -0.3677]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 10 ] state=tensor([[ 0.0514,  0.2340, -0.1250, -0.3677]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0561,  0.0408, -0.1323, -0.1169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 11 ] state=tensor([[ 0.0561,  0.0408, -0.1323, -0.1169]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0569,  0.2376, -0.1347, -0.4482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 12 ] state=tensor([[ 0.0569,  0.2376, -0.1347, -0.4482]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0617,  0.0446, -0.1436, -0.2008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 13 ] state=tensor([[ 0.0617,  0.0446, -0.1436, -0.2008]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0626,  0.2415, -0.1476, -0.5352]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 14 ] state=tensor([[ 0.0626,  0.2415, -0.1476, -0.5352]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0674,  0.0487, -0.1583, -0.2924]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 15 ] state=tensor([[ 0.0674,  0.0487, -0.1583, -0.2924]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0684,  0.2457, -0.1642, -0.6305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 16 ] state=tensor([[ 0.0684,  0.2457, -0.1642, -0.6305]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0733,  0.0532, -0.1768, -0.3937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 17 ] state=tensor([[ 0.0733,  0.0532, -0.1768, -0.3937]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0743,  0.2503, -0.1847, -0.7365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 18 ] state=tensor([[ 0.0743,  0.2503, -0.1847, -0.7365]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0793,  0.0582, -0.1994, -0.5072]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 67 ][ timestamp 19 ] state=tensor([[ 0.0793,  0.0582, -0.1994, -0.5072]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 67: Exploration_rate=0.05. Score=19.\n",
      "[ episode 68 ] state=tensor([[ 0.0123, -0.0415, -0.0101, -0.0116]])\n",
      "[ episode 68 ][ timestamp 1 ] state=tensor([[ 0.0123, -0.0415, -0.0101, -0.0116]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0115,  0.1538, -0.0104, -0.3075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 2 ] state=tensor([[ 0.0115,  0.1538, -0.0104, -0.3075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0146, -0.0412, -0.0165, -0.0181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 3 ] state=tensor([[ 0.0146, -0.0412, -0.0165, -0.0181]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0138, -0.2361, -0.0169,  0.2693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 4 ] state=tensor([[ 0.0138, -0.2361, -0.0169,  0.2693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0090, -0.4309, -0.0115,  0.5566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 5 ] state=tensor([[ 0.0090, -0.4309, -0.0115,  0.5566]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0004, -0.2356, -0.0004,  0.2603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 6 ] state=tensor([[ 0.0004, -0.2356, -0.0004,  0.2603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0043, -0.4308,  0.0048,  0.5529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 7 ] state=tensor([[-0.0043, -0.4308,  0.0048,  0.5529]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0129, -0.2357,  0.0159,  0.2617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 8 ] state=tensor([[-0.0129, -0.2357,  0.0159,  0.2617]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0176, -0.0408,  0.0211, -0.0259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 9 ] state=tensor([[-0.0176, -0.0408,  0.0211, -0.0259]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0184,  0.1540,  0.0206, -0.3118]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 10 ] state=tensor([[-0.0184,  0.1540,  0.0206, -0.3118]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0154,  0.3488,  0.0144, -0.5980]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 11 ] state=tensor([[-0.0154,  0.3488,  0.0144, -0.5980]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0084,  0.5437,  0.0024, -0.8861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 12 ] state=tensor([[-0.0084,  0.5437,  0.0024, -0.8861]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0025,  0.3486, -0.0153, -0.5926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 13 ] state=tensor([[ 0.0025,  0.3486, -0.0153, -0.5926]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0095,  0.5439, -0.0272, -0.8901]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 68 ][ timestamp 14 ] state=tensor([[ 0.0095,  0.5439, -0.0272, -0.8901]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0203,  0.3492, -0.0450, -0.6061]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 15 ] state=tensor([[ 0.0203,  0.3492, -0.0450, -0.6061]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0273,  0.1547, -0.0571, -0.3279]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 16 ] state=tensor([[ 0.0273,  0.1547, -0.0571, -0.3279]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0304,  0.3506, -0.0636, -0.6380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 17 ] state=tensor([[ 0.0304,  0.3506, -0.0636, -0.6380]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0374,  0.1564, -0.0764, -0.3661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 18 ] state=tensor([[ 0.0374,  0.1564, -0.0764, -0.3661]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0406, -0.0375, -0.0837, -0.0984]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 19 ] state=tensor([[ 0.0406, -0.0375, -0.0837, -0.0984]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0398,  0.1587, -0.0857, -0.4163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 20 ] state=tensor([[ 0.0398,  0.1587, -0.0857, -0.4163]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0430,  0.3549, -0.0940, -0.7347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 21 ] state=tensor([[ 0.0430,  0.3549, -0.0940, -0.7347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0501,  0.1612, -0.1087, -0.4730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 22 ] state=tensor([[ 0.0501,  0.1612, -0.1087, -0.4730]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0533,  0.3577, -0.1182, -0.7979]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 23 ] state=tensor([[ 0.0533,  0.3577, -0.1182, -0.7979]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0605,  0.1643, -0.1341, -0.5446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 24 ] state=tensor([[ 0.0605,  0.1643, -0.1341, -0.5446]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0637, -0.0287, -0.1450, -0.2970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 25 ] state=tensor([[ 0.0637, -0.0287, -0.1450, -0.2970]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0632,  0.1682, -0.1510, -0.6317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 26 ] state=tensor([[ 0.0632,  0.1682, -0.1510, -0.6317]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0665, -0.0245, -0.1636, -0.3901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 27 ] state=tensor([[ 0.0665, -0.0245, -0.1636, -0.3901]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0660,  0.1725, -0.1714, -0.7296]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 28 ] state=tensor([[ 0.0660,  0.1725, -0.1714, -0.7296]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0695, -0.0199, -0.1860, -0.4954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 29 ] state=tensor([[ 0.0695, -0.0199, -0.1860, -0.4954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0691, -0.2120, -0.1959, -0.2666]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 30 ] state=tensor([[ 0.0691, -0.2120, -0.1959, -0.2666]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0649, -0.0147, -0.2012, -0.6141]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 68 ][ timestamp 31 ] state=tensor([[ 0.0649, -0.0147, -0.2012, -0.6141]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 68: Exploration_rate=0.05. Score=31.\n",
      "[ episode 69 ] state=tensor([[-0.0046, -0.0472, -0.0105,  0.0444]])\n",
      "[ episode 69 ][ timestamp 1 ] state=tensor([[-0.0046, -0.0472, -0.0105,  0.0444]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0055, -0.2421, -0.0097,  0.3338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 2 ] state=tensor([[-0.0055, -0.2421, -0.0097,  0.3338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0104, -0.4371, -0.0030,  0.6234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 3 ] state=tensor([[-0.0104, -0.4371, -0.0030,  0.6234]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0191, -0.2419,  0.0095,  0.3298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 4 ] state=tensor([[-0.0191, -0.2419,  0.0095,  0.3298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0240, -0.4372,  0.0161,  0.6254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 5 ] state=tensor([[-0.0240, -0.4372,  0.0161,  0.6254]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0327, -0.2423,  0.0286,  0.3379]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 6 ] state=tensor([[-0.0327, -0.2423,  0.0286,  0.3379]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0376, -0.4378,  0.0353,  0.6394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 7 ] state=tensor([[-0.0376, -0.4378,  0.0353,  0.6394]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0463, -0.2432,  0.0481,  0.3581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 8 ] state=tensor([[-0.0463, -0.2432,  0.0481,  0.3581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0512, -0.0488,  0.0553,  0.0810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 9 ] state=tensor([[-0.0512, -0.0488,  0.0553,  0.0810]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0522,  0.1455,  0.0569, -0.1938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 10 ] state=tensor([[-0.0522,  0.1455,  0.0569, -0.1938]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0492,  0.3398,  0.0530, -0.4680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 11 ] state=tensor([[-0.0492,  0.3398,  0.0530, -0.4680]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0424,  0.5341,  0.0437, -0.7435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 12 ] state=tensor([[-0.0424,  0.5341,  0.0437, -0.7435]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0318,  0.7286,  0.0288, -1.0221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 13 ] state=tensor([[-0.0318,  0.7286,  0.0288, -1.0221]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0172,  0.5331,  0.0084, -0.7205]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 14 ] state=tensor([[-0.0172,  0.5331,  0.0084, -0.7205]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0065,  0.3378, -0.0060, -0.4252]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 15 ] state=tensor([[-0.0065,  0.3378, -0.0060, -0.4252]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 2.2192e-04,  5.3305e-01, -1.4543e-02, -7.1979e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 16 ] state=tensor([[ 2.2192e-04,  5.3305e-01, -1.4543e-02, -7.1979e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0109,  0.3381, -0.0289, -0.4317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 17 ] state=tensor([[ 0.0109,  0.3381, -0.0289, -0.4317]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0176,  0.1434, -0.0376, -0.1483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 18 ] state=tensor([[ 0.0176,  0.1434, -0.0376, -0.1483]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0205, -0.0511, -0.0405,  0.1323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 19 ] state=tensor([[ 0.0205, -0.0511, -0.0405,  0.1323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0195,  0.1445, -0.0379, -0.1729]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 20 ] state=tensor([[ 0.0195,  0.1445, -0.0379, -0.1729]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0224,  0.3402, -0.0414, -0.4773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 21 ] state=tensor([[ 0.0224,  0.3402, -0.0414, -0.4773]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0292,  0.5359, -0.0509, -0.7827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 22 ] state=tensor([[ 0.0292,  0.5359, -0.0509, -0.7827]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0399,  0.3415, -0.0666, -0.5065]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 23 ] state=tensor([[ 0.0399,  0.3415, -0.0666, -0.5065]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0467,  0.1474, -0.0767, -0.2355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 24 ] state=tensor([[ 0.0467,  0.1474, -0.0767, -0.2355]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0497,  0.3435, -0.0814, -0.5513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 25 ] state=tensor([[ 0.0497,  0.3435, -0.0814, -0.5513]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0566,  0.1496, -0.0924, -0.2853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 26 ] state=tensor([[ 0.0566,  0.1496, -0.0924, -0.2853]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0595,  0.3459, -0.0981, -0.6057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 27 ] state=tensor([[ 0.0595,  0.3459, -0.0981, -0.6057]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0665,  0.1523, -0.1102, -0.3455]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 28 ] state=tensor([[ 0.0665,  0.1523, -0.1102, -0.3455]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0695, -0.0411, -0.1171, -0.0895]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 29 ] state=tensor([[ 0.0695, -0.0411, -0.1171, -0.0895]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0687,  0.1555, -0.1189, -0.4167]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 30 ] state=tensor([[ 0.0687,  0.1555, -0.1189, -0.4167]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0718, -0.0378, -0.1273, -0.1637]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 31 ] state=tensor([[ 0.0718, -0.0378, -0.1273, -0.1637]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0710, -0.2309, -0.1305,  0.0862]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 32 ] state=tensor([[ 0.0710, -0.2309, -0.1305,  0.0862]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0664, -0.0341, -0.1288, -0.2446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 33 ] state=tensor([[ 0.0664, -0.0341, -0.1288, -0.2446]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0657,  0.1626, -0.1337, -0.5750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 34 ] state=tensor([[ 0.0657,  0.1626, -0.1337, -0.5750]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0690, -0.0305, -0.1452, -0.3272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 35 ] state=tensor([[ 0.0690, -0.0305, -0.1452, -0.3272]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0684,  0.1664, -0.1518, -0.6620]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 36 ] state=tensor([[ 0.0684,  0.1664, -0.1518, -0.6620]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0717, -0.0263, -0.1650, -0.4206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 37 ] state=tensor([[ 0.0717, -0.0263, -0.1650, -0.4206]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0712,  0.1707, -0.1734, -0.7605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 38 ] state=tensor([[ 0.0712,  0.1707, -0.1734, -0.7605]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0746, -0.0216, -0.1886, -0.5270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 39 ] state=tensor([[ 0.0746, -0.0216, -0.1886, -0.5270]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0742,  0.1756, -0.1992, -0.8727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 69 ][ timestamp 40 ] state=tensor([[ 0.0742,  0.1756, -0.1992, -0.8727]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 69: Exploration_rate=0.05. Score=40.\n",
      "[ episode 70 ] state=tensor([[ 0.0162, -0.0458, -0.0085, -0.0056]])\n",
      "[ episode 70 ][ timestamp 1 ] state=tensor([[ 0.0162, -0.0458, -0.0085, -0.0056]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0153,  0.1494, -0.0086, -0.3010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 2 ] state=tensor([[ 0.0153,  0.1494, -0.0086, -0.3010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0183, -0.0456, -0.0146, -0.0110]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 3 ] state=tensor([[ 0.0183, -0.0456, -0.0146, -0.0110]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0173, -0.2405, -0.0148,  0.2770]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 70 ][ timestamp 4 ] state=tensor([[ 0.0173, -0.2405, -0.0148,  0.2770]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0125, -0.0452, -0.0093, -0.0203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 5 ] state=tensor([[ 0.0125, -0.0452, -0.0093, -0.0203]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0116, -0.2402, -0.0097,  0.2694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 6 ] state=tensor([[ 0.0116, -0.2402, -0.0097,  0.2694]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0068, -0.4352, -0.0043,  0.5590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 7 ] state=tensor([[ 0.0068, -0.4352, -0.0043,  0.5590]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0019, -0.2400,  0.0069,  0.2650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 8 ] state=tensor([[-0.0019, -0.2400,  0.0069,  0.2650]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0067, -0.0449,  0.0122, -0.0255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 9 ] state=tensor([[-0.0067, -0.0449,  0.0122, -0.0255]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0076,  0.1500,  0.0116, -0.3143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 10 ] state=tensor([[-0.0076,  0.1500,  0.0116, -0.3143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0046,  0.3450,  0.0054, -0.6033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 11 ] state=tensor([[-0.0046,  0.3450,  0.0054, -0.6033]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0023,  0.5400, -0.0067, -0.8943]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 12 ] state=tensor([[ 0.0023,  0.5400, -0.0067, -0.8943]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0131,  0.7352, -0.0246, -1.1891]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 13 ] state=tensor([[ 0.0131,  0.7352, -0.0246, -1.1891]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0278,  0.5404, -0.0484, -0.9042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 14 ] state=tensor([[ 0.0278,  0.5404, -0.0484, -0.9042]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0386,  0.3460, -0.0665, -0.6271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 15 ] state=tensor([[ 0.0386,  0.3460, -0.0665, -0.6271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0456,  0.1518, -0.0790, -0.3561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 16 ] state=tensor([[ 0.0456,  0.1518, -0.0790, -0.3561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0486,  0.3480, -0.0861, -0.6726]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 17 ] state=tensor([[ 0.0486,  0.3480, -0.0861, -0.6726]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0556,  0.1542, -0.0996, -0.4082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 18 ] state=tensor([[ 0.0556,  0.1542, -0.0996, -0.4082]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0586,  0.3506, -0.1077, -0.7306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 19 ] state=tensor([[ 0.0586,  0.3506, -0.1077, -0.7306]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0656,  0.1571, -0.1224, -0.4737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 20 ] state=tensor([[ 0.0656,  0.1571, -0.1224, -0.4737]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0688, -0.0361, -0.1318, -0.2219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 21 ] state=tensor([[ 0.0688, -0.0361, -0.1318, -0.2219]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0681,  0.1606, -0.1363, -0.5531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 22 ] state=tensor([[ 0.0681,  0.1606, -0.1363, -0.5531]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0713, -0.0324, -0.1473, -0.3062]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 23 ] state=tensor([[ 0.0713, -0.0324, -0.1473, -0.3062]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0706,  0.1645, -0.1534, -0.6415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 24 ] state=tensor([[ 0.0706,  0.1645, -0.1534, -0.6415]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0739, -0.0282, -0.1663, -0.4008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 25 ] state=tensor([[ 0.0739, -0.0282, -0.1663, -0.4008]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0734,  0.1689, -0.1743, -0.7410]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 26 ] state=tensor([[ 0.0734,  0.1689, -0.1743, -0.7410]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0767, -0.0235, -0.1891, -0.5078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 27 ] state=tensor([[ 0.0767, -0.0235, -0.1891, -0.5078]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0763, -0.2155, -0.1993, -0.2802]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 28 ] state=tensor([[ 0.0763, -0.2155, -0.1993, -0.2802]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0720, -0.0182, -0.2049, -0.6285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 70 ][ timestamp 29 ] state=tensor([[ 0.0720, -0.0182, -0.2049, -0.6285]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 70: Exploration_rate=0.05. Score=29.\n",
      "[ episode 71 ] state=tensor([[-0.0175, -0.0491,  0.0370, -0.0442]])\n",
      "[ episode 71 ][ timestamp 1 ] state=tensor([[-0.0175, -0.0491,  0.0370, -0.0442]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0185,  0.1455,  0.0361, -0.3250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 2 ] state=tensor([[-0.0185,  0.1455,  0.0361, -0.3250]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0156,  0.3401,  0.0296, -0.6061]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 3 ] state=tensor([[-0.0156,  0.3401,  0.0296, -0.6061]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0088,  0.5348,  0.0175, -0.8893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 4 ] state=tensor([[-0.0088,  0.5348,  0.0175, -0.8893]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 1.9381e-03,  3.3942e-01, -2.8650e-04, -5.9114e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 5 ] state=tensor([[ 1.9381e-03,  3.3942e-01, -2.8650e-04, -5.9114e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0087,  0.5345, -0.0121, -0.8839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 6 ] state=tensor([[ 0.0087,  0.5345, -0.0121, -0.8839]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0194,  0.3396, -0.0298, -0.5951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 7 ] state=tensor([[ 0.0194,  0.3396, -0.0298, -0.5951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0262,  0.1449, -0.0417, -0.3119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 8 ] state=tensor([[ 0.0262,  0.1449, -0.0417, -0.3119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0291,  0.3406, -0.0479, -0.6174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 9 ] state=tensor([[ 0.0291,  0.3406, -0.0479, -0.6174]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0359,  0.1462, -0.0603, -0.3402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 10 ] state=tensor([[ 0.0359,  0.1462, -0.0603, -0.3402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0388, -0.0480, -0.0671, -0.0671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 11 ] state=tensor([[ 0.0388, -0.0480, -0.0671, -0.0671]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0379,  0.1480, -0.0684, -0.3802]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 12 ] state=tensor([[ 0.0379,  0.1480, -0.0684, -0.3802]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0408,  0.3440, -0.0760, -0.6937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 13 ] state=tensor([[ 0.0408,  0.3440, -0.0760, -0.6937]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0477,  0.1500, -0.0899, -0.4259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 14 ] state=tensor([[ 0.0477,  0.1500, -0.0899, -0.4259]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0507, -0.0437, -0.0984, -0.1628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 15 ] state=tensor([[ 0.0507, -0.0437, -0.0984, -0.1628]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0498, -0.2373, -0.1017,  0.0973]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 16 ] state=tensor([[ 0.0498, -0.2373, -0.1017,  0.0973]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0451, -0.0409, -0.0997, -0.2257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 17 ] state=tensor([[ 0.0451, -0.0409, -0.0997, -0.2257]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0443,  0.1555, -0.1042, -0.5481]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 18 ] state=tensor([[ 0.0443,  0.1555, -0.1042, -0.5481]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0474, -0.0380, -0.1152, -0.2900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 19 ] state=tensor([[ 0.0474, -0.0380, -0.1152, -0.2900]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0466,  0.1585, -0.1210, -0.6167]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 20 ] state=tensor([[ 0.0466,  0.1585, -0.1210, -0.6167]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0498, -0.0347, -0.1333, -0.3644]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 21 ] state=tensor([[ 0.0498, -0.0347, -0.1333, -0.3644]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0491, -0.2277, -0.1406, -0.1166]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 22 ] state=tensor([[ 0.0491, -0.2277, -0.1406, -0.1166]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0446, -0.0309, -0.1430, -0.4501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 23 ] state=tensor([[ 0.0446, -0.0309, -0.1430, -0.4501]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0439,  0.1659, -0.1520, -0.7842]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 24 ] state=tensor([[ 0.0439,  0.1659, -0.1520, -0.7842]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0473, -0.0268, -0.1676, -0.5429]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 25 ] state=tensor([[ 0.0473, -0.0268, -0.1676, -0.5429]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0467, -0.2192, -0.1785, -0.3074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 26 ] state=tensor([[ 0.0467, -0.2192, -0.1785, -0.3074]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0423, -0.0221, -0.1846, -0.6506]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 27 ] state=tensor([[ 0.0423, -0.0221, -0.1846, -0.6506]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0419,  0.1751, -0.1977, -0.9953]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 71 ][ timestamp 28 ] state=tensor([[ 0.0419,  0.1751, -0.1977, -0.9953]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 71: Exploration_rate=0.05. Score=28.\n",
      "[ episode 72 ] state=tensor([[ 0.0255,  0.0011, -0.0103, -0.0056]])\n",
      "[ episode 72 ][ timestamp 1 ] state=tensor([[ 0.0255,  0.0011, -0.0103, -0.0056]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0255,  0.1964, -0.0104, -0.3015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 2 ] state=tensor([[ 0.0255,  0.1964, -0.0104, -0.3015]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0294,  0.0014, -0.0164, -0.0121]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 3 ] state=tensor([[ 0.0294,  0.0014, -0.0164, -0.0121]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0295, -0.1934, -0.0167,  0.2753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 4 ] state=tensor([[ 0.0295, -0.1934, -0.0167,  0.2753]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0256,  0.0019, -0.0112, -0.0226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 5 ] state=tensor([[ 0.0256,  0.0019, -0.0112, -0.0226]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0256,  0.1972, -0.0116, -0.3188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 6 ] state=tensor([[ 0.0256,  0.1972, -0.0116, -0.3188]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0296,  0.0022, -0.0180, -0.0298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 7 ] state=tensor([[ 0.0296,  0.0022, -0.0180, -0.0298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0296, -0.1926, -0.0186,  0.2572]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 8 ] state=tensor([[ 0.0296, -0.1926, -0.0186,  0.2572]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0258,  0.0028, -0.0134, -0.0413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 9 ] state=tensor([[ 0.0258,  0.0028, -0.0134, -0.0413]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0258,  0.1981, -0.0143, -0.3382]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 10 ] state=tensor([[ 0.0258,  0.1981, -0.0143, -0.3382]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0298,  0.3934, -0.0210, -0.6353]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 72 ][ timestamp 11 ] state=tensor([[ 0.0298,  0.3934, -0.0210, -0.6353]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0377,  0.1986, -0.0337, -0.3493]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 12 ] state=tensor([[ 0.0377,  0.1986, -0.0337, -0.3493]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0416,  0.0039, -0.0407, -0.0675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 13 ] state=tensor([[ 0.0416,  0.0039, -0.0407, -0.0675]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0417,  0.1996, -0.0421, -0.3727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 14 ] state=tensor([[ 0.0417,  0.1996, -0.0421, -0.3727]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0457,  0.0051, -0.0495, -0.0936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 15 ] state=tensor([[ 0.0457,  0.0051, -0.0495, -0.0936]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0458, -0.1892, -0.0514,  0.1831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 16 ] state=tensor([[ 0.0458, -0.1892, -0.0514,  0.1831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0420,  0.0066, -0.0477, -0.1254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 17 ] state=tensor([[ 0.0420,  0.0066, -0.0477, -0.1254]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0422,  0.2023, -0.0502, -0.4327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 18 ] state=tensor([[ 0.0422,  0.2023, -0.0502, -0.4327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0462,  0.3981, -0.0589, -0.7408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 19 ] state=tensor([[ 0.0462,  0.3981, -0.0589, -0.7408]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0542,  0.2039, -0.0737, -0.4673]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 20 ] state=tensor([[ 0.0542,  0.2039, -0.0737, -0.4673]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0582,  0.0099, -0.0831, -0.1987]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 21 ] state=tensor([[ 0.0582,  0.0099, -0.0831, -0.1987]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0584, -0.1840, -0.0870,  0.0667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 22 ] state=tensor([[ 0.0584, -0.1840, -0.0870,  0.0667]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0548, -0.3777, -0.0857,  0.3307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 23 ] state=tensor([[ 0.0548, -0.3777, -0.0857,  0.3307]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0472, -0.1815, -0.0791,  0.0123]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 24 ] state=tensor([[ 0.0472, -0.1815, -0.0791,  0.0123]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0436,  0.0146, -0.0788, -0.3043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 25 ] state=tensor([[ 0.0436,  0.0146, -0.0788, -0.3043]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0439, -0.1793, -0.0849, -0.0375]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 26 ] state=tensor([[ 0.0439, -0.1793, -0.0849, -0.0375]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0403,  0.0170, -0.0857, -0.3557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 27 ] state=tensor([[ 0.0403,  0.0170, -0.0857, -0.3557]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0406,  0.2132, -0.0928, -0.6741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 28 ] state=tensor([[ 0.0406,  0.2132, -0.0928, -0.6741]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0449,  0.0195, -0.1063, -0.4120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 29 ] state=tensor([[ 0.0449,  0.0195, -0.1063, -0.4120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0453,  0.2159, -0.1145, -0.7362]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 30 ] state=tensor([[ 0.0453,  0.2159, -0.1145, -0.7362]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0496,  0.0226, -0.1292, -0.4817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 31 ] state=tensor([[ 0.0496,  0.0226, -0.1292, -0.4817]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0500, -0.1705, -0.1389, -0.2324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 32 ] state=tensor([[ 0.0500, -0.1705, -0.1389, -0.2324]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0466,  0.0263, -0.1435, -0.5654]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 33 ] state=tensor([[ 0.0466,  0.0263, -0.1435, -0.5654]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0472, -0.1666, -0.1548, -0.3212]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 34 ] state=tensor([[ 0.0472, -0.1666, -0.1548, -0.3212]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0438,  0.0304, -0.1613, -0.6584]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 35 ] state=tensor([[ 0.0438,  0.0304, -0.1613, -0.6584]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0444, -0.1622, -0.1744, -0.4205]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 36 ] state=tensor([[ 0.0444, -0.1622, -0.1744, -0.4205]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0412,  0.0349, -0.1828, -0.7627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 37 ] state=tensor([[ 0.0412,  0.0349, -0.1828, -0.7627]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0419, -0.1573, -0.1981, -0.5327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 38 ] state=tensor([[ 0.0419, -0.1573, -0.1981, -0.5327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0387,  0.0400, -0.2087, -0.8807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 72 ][ timestamp 39 ] state=tensor([[ 0.0387,  0.0400, -0.2087, -0.8807]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 72: Exploration_rate=0.05. Score=39.\n",
      "[ episode 73 ] state=tensor([[-0.0104, -0.0174, -0.0147, -0.0100]])\n",
      "[ episode 73 ][ timestamp 1 ] state=tensor([[-0.0104, -0.0174, -0.0147, -0.0100]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0108,  0.1779, -0.0149, -0.3073]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 2 ] state=tensor([[-0.0108,  0.1779, -0.0149, -0.3073]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0072, -0.0170, -0.0210, -0.0193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 3 ] state=tensor([[-0.0072, -0.0170, -0.0210, -0.0193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0076, -0.2118, -0.0214,  0.2667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 4 ] state=tensor([[-0.0076, -0.2118, -0.0214,  0.2667]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0118, -0.4066, -0.0161,  0.5525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 5 ] state=tensor([[-0.0118, -0.4066, -0.0161,  0.5525]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0199, -0.2113, -0.0050,  0.2548]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 6 ] state=tensor([[-0.0199, -0.2113, -0.0050,  0.2548]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.4149e-02, -1.6102e-02,  4.9296e-05, -3.9480e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 7 ] state=tensor([[-2.4149e-02, -1.6102e-02,  4.9296e-05, -3.9480e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0245, -0.2112, -0.0007,  0.2532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 8 ] state=tensor([[-0.0245, -0.2112, -0.0007,  0.2532]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0287, -0.0161,  0.0043, -0.0397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 9 ] state=tensor([[-0.0287, -0.0161,  0.0043, -0.0397]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0290,  0.1790,  0.0035, -0.3310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 10 ] state=tensor([[-0.0290,  0.1790,  0.0035, -0.3310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0254,  0.3740, -0.0031, -0.6226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 11 ] state=tensor([[-0.0254,  0.3740, -0.0031, -0.6226]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0180,  0.5692, -0.0155, -0.9162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 12 ] state=tensor([[-0.0180,  0.5692, -0.0155, -0.9162]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0066,  0.3743, -0.0339, -0.6285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 13 ] state=tensor([[-0.0066,  0.3743, -0.0339, -0.6285]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 9.1249e-04,  5.6987e-01, -4.6436e-02, -9.3163e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 14 ] state=tensor([[ 9.1249e-04,  5.6987e-01, -4.6436e-02, -9.3163e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0123,  0.3754, -0.0651, -0.6539]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 15 ] state=tensor([[ 0.0123,  0.3754, -0.0651, -0.6539]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0198,  0.1812, -0.0781, -0.3824]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 16 ] state=tensor([[ 0.0198,  0.1812, -0.0781, -0.3824]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0234, -0.0127, -0.0858, -0.1153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 17 ] state=tensor([[ 0.0234, -0.0127, -0.0858, -0.1153]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0232,  0.1836, -0.0881, -0.4338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 18 ] state=tensor([[ 0.0232,  0.1836, -0.0881, -0.4338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0269, -0.0102, -0.0968, -0.1701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 19 ] state=tensor([[ 0.0269, -0.0102, -0.0968, -0.1701]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0267, -0.2038, -0.1002,  0.0905]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 20 ] state=tensor([[ 0.0267, -0.2038, -0.1002,  0.0905]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0226, -0.3974, -0.0984,  0.3500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 21 ] state=tensor([[ 0.0226, -0.3974, -0.0984,  0.3500]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0146, -0.2010, -0.0914,  0.0280]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 22 ] state=tensor([[ 0.0146, -0.2010, -0.0914,  0.0280]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0106, -0.0047, -0.0908, -0.2921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 23 ] state=tensor([[ 0.0106, -0.0047, -0.0908, -0.2921]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0105, -0.1984, -0.0967, -0.0294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 24 ] state=tensor([[ 0.0105, -0.1984, -0.0967, -0.0294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0065, -0.0021, -0.0972, -0.3509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 25 ] state=tensor([[ 0.0065, -0.0021, -0.0972, -0.3509]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0065,  0.1943, -0.1043, -0.6726]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 26 ] state=tensor([[ 0.0065,  0.1943, -0.1043, -0.6726]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0104,  0.0008, -0.1177, -0.4145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 27 ] state=tensor([[ 0.0104,  0.0008, -0.1177, -0.4145]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0104, -0.1925, -0.1260, -0.1611]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 28 ] state=tensor([[ 0.0104, -0.1925, -0.1260, -0.1611]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0066, -0.3856, -0.1292,  0.0893]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 73 ][ timestamp 29 ] state=tensor([[ 0.0066, -0.3856, -0.1292,  0.0893]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0012, -0.1889, -0.1274, -0.2412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 30 ] state=tensor([[-0.0012, -0.1889, -0.1274, -0.2412]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0049,  0.0078, -0.1323, -0.5712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 31 ] state=tensor([[-0.0049,  0.0078, -0.1323, -0.5712]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0048, -0.1852, -0.1437, -0.3229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 32 ] state=tensor([[-0.0048, -0.1852, -0.1437, -0.3229]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0085,  0.0116, -0.1501, -0.6572]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 33 ] state=tensor([[-0.0085,  0.0116, -0.1501, -0.6572]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0082, -0.1812, -0.1633, -0.4153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 34 ] state=tensor([[-0.0082, -0.1812, -0.1633, -0.4153]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0119,  0.0159, -0.1716, -0.7547]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 35 ] state=tensor([[-0.0119,  0.0159, -0.1716, -0.7547]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0116,  0.2129, -0.1867, -1.0961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 36 ] state=tensor([[-0.0116,  0.2129, -0.1867, -1.0961]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0073,  0.0206, -0.2086, -0.8673]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 73 ][ timestamp 37 ] state=tensor([[-0.0073,  0.0206, -0.2086, -0.8673]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 73: Exploration_rate=0.05. Score=37.\n",
      "[ episode 74 ] state=tensor([[ 0.0168, -0.0391,  0.0286,  0.0367]])\n",
      "[ episode 74 ][ timestamp 1 ] state=tensor([[ 0.0168, -0.0391,  0.0286,  0.0367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0161, -0.2347,  0.0294,  0.3382]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 2 ] state=tensor([[ 0.0161, -0.2347,  0.0294,  0.3382]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0114, -0.4302,  0.0361,  0.6400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 3 ] state=tensor([[ 0.0114, -0.4302,  0.0361,  0.6400]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0028, -0.2356,  0.0489,  0.3589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 4 ] state=tensor([[ 0.0028, -0.2356,  0.0489,  0.3589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0019, -0.4314,  0.0561,  0.6666]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 5 ] state=tensor([[-0.0019, -0.4314,  0.0561,  0.6666]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0106, -0.6272,  0.0694,  0.9765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 6 ] state=tensor([[-0.0106, -0.6272,  0.0694,  0.9765]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0231, -0.4331,  0.0890,  0.7064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 7 ] state=tensor([[-0.0231, -0.4331,  0.0890,  0.7064]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0318, -0.2393,  0.1031,  0.4430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 8 ] state=tensor([[-0.0318, -0.2393,  0.1031,  0.4430]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0366, -0.4357,  0.1120,  0.7663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 9 ] state=tensor([[-0.0366, -0.4357,  0.1120,  0.7663]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0453, -0.6322,  0.1273,  1.0920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 10 ] state=tensor([[-0.0453, -0.6322,  0.1273,  1.0920]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0579, -0.4390,  0.1491,  0.8418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 11 ] state=tensor([[-0.0579, -0.4390,  0.1491,  0.8418]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0667, -0.6358,  0.1660,  1.1774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 12 ] state=tensor([[-0.0667, -0.6358,  0.1660,  1.1774]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0794, -0.4432,  0.1895,  0.9410]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 13 ] state=tensor([[-0.0794, -0.4432,  0.1895,  0.9410]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0883, -0.6403,  0.2083,  1.2868]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 74 ][ timestamp 14 ] state=tensor([[-0.0883, -0.6403,  0.2083,  1.2868]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 74: Exploration_rate=0.05. Score=14.\n",
      "[ episode 75 ] state=tensor([[-0.0226,  0.0200,  0.0167, -0.0360]])\n",
      "[ episode 75 ][ timestamp 1 ] state=tensor([[-0.0226,  0.0200,  0.0167, -0.0360]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0222, -0.1754,  0.0159,  0.2619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 2 ] state=tensor([[-0.0222, -0.1754,  0.0159,  0.2619]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0257,  0.0195,  0.0212, -0.0257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 3 ] state=tensor([[-0.0257,  0.0195,  0.0212, -0.0257]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0253, -0.1759,  0.0207,  0.2735]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 4 ] state=tensor([[-0.0253, -0.1759,  0.0207,  0.2735]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0288, -0.3713,  0.0261,  0.5727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 5 ] state=tensor([[-0.0288, -0.3713,  0.0261,  0.5727]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0362, -0.1765,  0.0376,  0.2883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 6 ] state=tensor([[-0.0362, -0.1765,  0.0376,  0.2883]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0398, -0.3722,  0.0434,  0.5926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 7 ] state=tensor([[-0.0398, -0.3722,  0.0434,  0.5926]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0472, -0.1777,  0.0552,  0.3139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 8 ] state=tensor([[-0.0472, -0.1777,  0.0552,  0.3139]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0507,  0.0166,  0.0615,  0.0391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 9 ] state=tensor([[-0.0507,  0.0166,  0.0615,  0.0391]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0504,  0.2108,  0.0623, -0.2335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 10 ] state=tensor([[-0.0504,  0.2108,  0.0623, -0.2335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0462,  0.4050,  0.0576, -0.5059]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 11 ] state=tensor([[-0.0462,  0.4050,  0.0576, -0.5059]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0381,  0.5992,  0.0475, -0.7799]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 12 ] state=tensor([[-0.0381,  0.5992,  0.0475, -0.7799]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0261,  0.7937,  0.0319, -1.0573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 13 ] state=tensor([[-0.0261,  0.7937,  0.0319, -1.0573]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0102,  0.9884,  0.0107, -1.3398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 14 ] state=tensor([[-0.0102,  0.9884,  0.0107, -1.3398]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0095,  1.1833, -0.0161, -1.6291]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 15 ] state=tensor([[ 0.0095,  1.1833, -0.0161, -1.6291]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0332,  0.9884, -0.0486, -1.3415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 16 ] state=tensor([[ 0.0332,  0.9884, -0.0486, -1.3415]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0530,  0.7939, -0.0755, -1.0644]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 17 ] state=tensor([[ 0.0530,  0.7939, -0.0755, -1.0644]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0688,  0.5999, -0.0968, -0.7963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 18 ] state=tensor([[ 0.0688,  0.5999, -0.0968, -0.7963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0808,  0.4062, -0.1127, -0.5356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 19 ] state=tensor([[ 0.0808,  0.4062, -0.1127, -0.5356]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0890,  0.2128, -0.1234, -0.2804]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 20 ] state=tensor([[ 0.0890,  0.2128, -0.1234, -0.2804]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0932,  0.0197, -0.1290, -0.0291]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 21 ] state=tensor([[ 0.0932,  0.0197, -0.1290, -0.0291]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0936,  0.2164, -0.1296, -0.3595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 22 ] state=tensor([[ 0.0936,  0.2164, -0.1296, -0.3595]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0979,  0.0233, -0.1368, -0.1103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 23 ] state=tensor([[ 0.0979,  0.0233, -0.1368, -0.1103]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0984, -0.1696, -0.1390,  0.1363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 24 ] state=tensor([[ 0.0984, -0.1696, -0.1390,  0.1363]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0950, -0.3625, -0.1363,  0.3821]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 25 ] state=tensor([[ 0.0950, -0.3625, -0.1363,  0.3821]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0878, -0.1657, -0.1286,  0.0497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 26 ] state=tensor([[ 0.0878, -0.1657, -0.1286,  0.0497]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0844, -0.3588, -0.1276,  0.2992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 27 ] state=tensor([[ 0.0844, -0.3588, -0.1276,  0.2992]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0773, -0.1621, -0.1216, -0.0308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 28 ] state=tensor([[ 0.0773, -0.1621, -0.1216, -0.0308]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0740, -0.3553, -0.1223,  0.2211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 29 ] state=tensor([[ 0.0740, -0.3553, -0.1223,  0.2211]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0669, -0.1586, -0.1178, -0.1075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 30 ] state=tensor([[ 0.0669, -0.1586, -0.1178, -0.1075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0638, -0.3519, -0.1200,  0.1458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 31 ] state=tensor([[ 0.0638, -0.3519, -0.1200,  0.1458]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0567, -0.1553, -0.1171, -0.1821]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 32 ] state=tensor([[ 0.0567, -0.1553, -0.1171, -0.1821]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0536,  0.0413, -0.1207, -0.5093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 33 ] state=tensor([[ 0.0536,  0.0413, -0.1207, -0.5093]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0544, -0.1519, -0.1309, -0.2570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 34 ] state=tensor([[ 0.0544, -0.1519, -0.1309, -0.2570]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0514, -0.3450, -0.1360, -0.0083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 35 ] state=tensor([[ 0.0514, -0.3450, -0.1360, -0.0083]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0445, -0.1482, -0.1362, -0.3406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 36 ] state=tensor([[ 0.0445, -0.1482, -0.1362, -0.3406]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0415,  0.0486, -0.1430, -0.6730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 37 ] state=tensor([[ 0.0415,  0.0486, -0.1430, -0.6730]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0425, -0.1443, -0.1565, -0.4285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 38 ] state=tensor([[ 0.0425, -0.1443, -0.1565, -0.4285]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0396, -0.3369, -0.1650, -0.1889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 39 ] state=tensor([[ 0.0396, -0.3369, -0.1650, -0.1889]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0329, -0.1398, -0.1688, -0.5288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 40 ] state=tensor([[ 0.0329, -0.1398, -0.1688, -0.5288]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0301,  0.0572, -0.1794, -0.8696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 41 ] state=tensor([[ 0.0301,  0.0572, -0.1794, -0.8696]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0312, -0.1351, -0.1968, -0.6382]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 75 ][ timestamp 42 ] state=tensor([[ 0.0312, -0.1351, -0.1968, -0.6382]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 75: Exploration_rate=0.05. Score=42.\n",
      "[ episode 76 ] state=tensor([[-0.0082, -0.0388,  0.0194,  0.0263]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 76 ][ timestamp 1 ] state=tensor([[-0.0082, -0.0388,  0.0194,  0.0263]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0090, -0.2342,  0.0199,  0.3250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 2 ] state=tensor([[-0.0090, -0.2342,  0.0199,  0.3250]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0137, -0.0394,  0.0264,  0.0387]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 3 ] state=tensor([[-0.0137, -0.0394,  0.0264,  0.0387]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0145, -0.2349,  0.0272,  0.3396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 4 ] state=tensor([[-0.0145, -0.2349,  0.0272,  0.3396]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0192, -0.0402,  0.0340,  0.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 5 ] state=tensor([[-0.0192, -0.0402,  0.0340,  0.0556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0200,  0.1545,  0.0351, -0.2262]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 6 ] state=tensor([[-0.0200,  0.1545,  0.0351, -0.2262]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0169,  0.3491,  0.0306, -0.5076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 7 ] state=tensor([[-0.0169,  0.3491,  0.0306, -0.5076]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0099,  0.5437,  0.0204, -0.7905]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 8 ] state=tensor([[-0.0099,  0.5437,  0.0204, -0.7905]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 9.5591e-04,  7.3858e-01,  4.6032e-03, -1.0767e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 9 ] state=tensor([[ 9.5591e-04,  7.3858e-01,  4.6032e-03, -1.0767e+00]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0157,  0.9336, -0.0169, -1.3679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 10 ] state=tensor([[ 0.0157,  0.9336, -0.0169, -1.3679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0344,  1.1290, -0.0443, -1.6658]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 11 ] state=tensor([[ 0.0344,  1.1290, -0.0443, -1.6658]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0570,  0.9344, -0.0776, -1.3873]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 12 ] state=tensor([[ 0.0570,  0.9344, -0.0776, -1.3873]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0757,  0.7403, -0.1053, -1.1198]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 13 ] state=tensor([[ 0.0757,  0.7403, -0.1053, -1.1198]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0905,  0.5467, -0.1277, -0.8619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 14 ] state=tensor([[ 0.0905,  0.5467, -0.1277, -0.8619]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1014,  0.3536, -0.1450, -0.6120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 15 ] state=tensor([[ 0.1014,  0.3536, -0.1450, -0.6120]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1085,  0.1607, -0.1572, -0.3683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 16 ] state=tensor([[ 0.1085,  0.1607, -0.1572, -0.3683]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1117, -0.0319, -0.1646, -0.1290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 17 ] state=tensor([[ 0.1117, -0.0319, -0.1646, -0.1290]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1111, -0.2243, -0.1672,  0.1076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 18 ] state=tensor([[ 0.1111, -0.2243, -0.1672,  0.1076]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1066, -0.0272, -0.1650, -0.2328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 19 ] state=tensor([[ 0.1066, -0.0272, -0.1650, -0.2328]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1060,  0.1698, -0.1697, -0.5727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 20 ] state=tensor([[ 0.1060,  0.1698, -0.1697, -0.5727]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1094, -0.0226, -0.1811, -0.3379]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 21 ] state=tensor([[ 0.1094, -0.0226, -0.1811, -0.3379]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1090, -0.2147, -0.1879, -0.1074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 22 ] state=tensor([[ 0.1090, -0.2147, -0.1879, -0.1074]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1047, -0.0174, -0.1900, -0.4529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 23 ] state=tensor([[ 0.1047, -0.0174, -0.1900, -0.4529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1043, -0.2094, -0.1991, -0.2257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 24 ] state=tensor([[ 0.1043, -0.2094, -0.1991, -0.2257]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1001, -0.0121, -0.2036, -0.5740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 76 ][ timestamp 25 ] state=tensor([[ 0.1001, -0.0121, -0.2036, -0.5740]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 76: Exploration_rate=0.05. Score=25.\n",
      "[ episode 77 ] state=tensor([[-0.0440,  0.0192, -0.0391,  0.0155]])\n",
      "[ episode 77 ][ timestamp 1 ] state=tensor([[-0.0440,  0.0192, -0.0391,  0.0155]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0436,  0.2148, -0.0388, -0.2892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 2 ] state=tensor([[-0.0436,  0.2148, -0.0388, -0.2892]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0393,  0.4105, -0.0446, -0.5939]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 3 ] state=tensor([[-0.0393,  0.4105, -0.0446, -0.5939]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0311,  0.2160, -0.0564, -0.3156]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 4 ] state=tensor([[-0.0311,  0.2160, -0.0564, -0.3156]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0268,  0.0217, -0.0627, -0.0412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 5 ] state=tensor([[-0.0268,  0.0217, -0.0627, -0.0412]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0263, -0.1724, -0.0636,  0.2310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 6 ] state=tensor([[-0.0263, -0.1724, -0.0636,  0.2310]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0298, -0.3666, -0.0589,  0.5030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 7 ] state=tensor([[-0.0298, -0.3666, -0.0589,  0.5030]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0371, -0.1707, -0.0489,  0.1924]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 8 ] state=tensor([[-0.0371, -0.1707, -0.0489,  0.1924]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0405, -0.3651, -0.0450,  0.4692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 9 ] state=tensor([[-0.0405, -0.3651, -0.0450,  0.4692]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0478, -0.5595, -0.0357,  0.7474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 10 ] state=tensor([[-0.0478, -0.5595, -0.0357,  0.7474]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0590, -0.3639, -0.0207,  0.4437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 11 ] state=tensor([[-0.0590, -0.3639, -0.0207,  0.4437]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0663, -0.1685, -0.0118,  0.1446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 12 ] state=tensor([[-0.0663, -0.1685, -0.0118,  0.1446]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0697,  0.0268, -0.0089, -0.1518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 13 ] state=tensor([[-0.0697,  0.0268, -0.0089, -0.1518]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0691,  0.2220, -0.0120, -0.4473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 14 ] state=tensor([[-0.0691,  0.2220, -0.0120, -0.4473]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0647,  0.4173, -0.0209, -0.7437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 15 ] state=tensor([[-0.0647,  0.4173, -0.0209, -0.7437]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0564,  0.6127, -0.0358, -1.0429]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 16 ] state=tensor([[-0.0564,  0.6127, -0.0358, -1.0429]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0441,  0.4181, -0.0567, -0.7617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 17 ] state=tensor([[-0.0441,  0.4181, -0.0567, -0.7617]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0357,  0.2238, -0.0719, -0.4874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 18 ] state=tensor([[-0.0357,  0.2238, -0.0719, -0.4874]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0313,  0.0297, -0.0816, -0.2182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 19 ] state=tensor([[-0.0313,  0.0297, -0.0816, -0.2182]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0307,  0.2259, -0.0860, -0.5355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 20 ] state=tensor([[-0.0307,  0.2259, -0.0860, -0.5355]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0262,  0.0321, -0.0967, -0.2711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 21 ] state=tensor([[-0.0262,  0.0321, -0.0967, -0.2711]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0255, -0.1615, -0.1021, -0.0104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 22 ] state=tensor([[-0.0255, -0.1615, -0.1021, -0.0104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0287,  0.0349, -0.1023, -0.3335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 23 ] state=tensor([[-0.0287,  0.0349, -0.1023, -0.3335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0280,  0.2313, -0.1090, -0.6566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 24 ] state=tensor([[-0.0280,  0.2313, -0.1090, -0.6566]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0234,  0.0379, -0.1221, -0.4001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 25 ] state=tensor([[-0.0234,  0.0379, -0.1221, -0.4001]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0227,  0.2345, -0.1301, -0.7287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 26 ] state=tensor([[-0.0227,  0.2345, -0.1301, -0.7287]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0180,  0.0414, -0.1447, -0.4796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 27 ] state=tensor([[-0.0180,  0.0414, -0.1447, -0.4796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0171, -0.1514, -0.1543, -0.2358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 28 ] state=tensor([[-0.0171, -0.1514, -0.1543, -0.2358]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0202, -0.3440, -0.1590,  0.0045]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 29 ] state=tensor([[-0.0202, -0.3440, -0.1590,  0.0045]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0271, -0.5365, -0.1589,  0.2431]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 30 ] state=tensor([[-0.0271, -0.5365, -0.1589,  0.2431]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0378, -0.3396, -0.1541, -0.0952]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 31 ] state=tensor([[-0.0378, -0.3396, -0.1541, -0.0952]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0446, -0.1426, -0.1560, -0.4323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 32 ] state=tensor([[-0.0446, -0.1426, -0.1560, -0.4323]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0474, -0.3352, -0.1646, -0.1925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 33 ] state=tensor([[-0.0474, -0.3352, -0.1646, -0.1925]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0541, -0.1382, -0.1685, -0.5323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 34 ] state=tensor([[-0.0541, -0.1382, -0.1685, -0.5323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0569,  0.0589, -0.1791, -0.8730]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 77 ][ timestamp 35 ] state=tensor([[-0.0569,  0.0589, -0.1791, -0.8730]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0557, -0.1334, -0.1966, -0.6415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 36 ] state=tensor([[-0.0557, -0.1334, -0.1966, -0.6415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0584,  0.0638, -0.2094, -0.9891]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 77 ][ timestamp 37 ] state=tensor([[-0.0584,  0.0638, -0.2094, -0.9891]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 77: Exploration_rate=0.05. Score=37.\n",
      "[ episode 78 ] state=tensor([[-0.0255, -0.0176, -0.0242,  0.0392]])\n",
      "[ episode 78 ][ timestamp 1 ] state=tensor([[-0.0255, -0.0176, -0.0242,  0.0392]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0259, -0.2124, -0.0234,  0.3242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 2 ] state=tensor([[-0.0259, -0.2124, -0.0234,  0.3242]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0301, -0.4072, -0.0169,  0.6094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 3 ] state=tensor([[-0.0301, -0.4072, -0.0169,  0.6094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0383, -0.2118, -0.0047,  0.3115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 4 ] state=tensor([[-0.0383, -0.2118, -0.0047,  0.3115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0425, -0.0166,  0.0015,  0.0173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 5 ] state=tensor([[-0.0425, -0.0166,  0.0015,  0.0173]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0428, -0.2118,  0.0019,  0.3105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 6 ] state=tensor([[-0.0428, -0.2118,  0.0019,  0.3105]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0471, -0.4069,  0.0081,  0.6037]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 7 ] state=tensor([[-0.0471, -0.4069,  0.0081,  0.6037]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0552, -0.2119,  0.0202,  0.3136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 8 ] state=tensor([[-0.0552, -0.2119,  0.0202,  0.3136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0594, -0.0171,  0.0264,  0.0274]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 9 ] state=tensor([[-0.0594, -0.0171,  0.0264,  0.0274]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0598,  0.1776,  0.0270, -0.2569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 10 ] state=tensor([[-0.0598,  0.1776,  0.0270, -0.2569]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0562,  0.3724,  0.0218, -0.5409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 11 ] state=tensor([[-0.0562,  0.3724,  0.0218, -0.5409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0488,  0.5672,  0.0110, -0.8266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 12 ] state=tensor([[-0.0488,  0.5672,  0.0110, -0.8266]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0374,  0.7621, -0.0055, -1.1158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 13 ] state=tensor([[-0.0374,  0.7621, -0.0055, -1.1158]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0222,  0.9573, -0.0278, -1.4102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 14 ] state=tensor([[-0.0222,  0.9573, -0.0278, -1.4102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0030,  1.1528, -0.0560, -1.7115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 15 ] state=tensor([[-0.0030,  1.1528, -0.0560, -1.7115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0200,  0.9584, -0.0903, -1.4368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 16 ] state=tensor([[ 0.0200,  0.9584, -0.0903, -1.4368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0392,  0.7645, -0.1190, -1.1736]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 17 ] state=tensor([[ 0.0392,  0.7645, -0.1190, -1.1736]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0545,  0.5711, -0.1425, -0.9205]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 18 ] state=tensor([[ 0.0545,  0.5711, -0.1425, -0.9205]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0659,  0.3781, -0.1609, -0.6757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 19 ] state=tensor([[ 0.0659,  0.3781, -0.1609, -0.6757]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0735,  0.1856, -0.1744, -0.4377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 20 ] state=tensor([[ 0.0735,  0.1856, -0.1744, -0.4377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0772, -0.0067, -0.1832, -0.2047]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 21 ] state=tensor([[ 0.0772, -0.0067, -0.1832, -0.2047]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0770, -0.1988, -0.1872,  0.0251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 22 ] state=tensor([[ 0.0770, -0.1988, -0.1872,  0.0251]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0731, -0.3908, -0.1867,  0.2533]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 23 ] state=tensor([[ 0.0731, -0.3908, -0.1867,  0.2533]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0652, -0.1936, -0.1817, -0.0919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 24 ] state=tensor([[ 0.0652, -0.1936, -0.1817, -0.0919]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0614,  0.0036, -0.1835, -0.4360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 25 ] state=tensor([[ 0.0614,  0.0036, -0.1835, -0.4360]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0614, -0.1885, -0.1922, -0.2063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 26 ] state=tensor([[ 0.0614, -0.1885, -0.1922, -0.2063]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0577, -0.3804, -0.1964,  0.0201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 27 ] state=tensor([[ 0.0577, -0.3804, -0.1964,  0.0201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0501, -0.1831, -0.1960, -0.3275]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 28 ] state=tensor([[ 0.0501, -0.1831, -0.1960, -0.3275]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0464,  0.0142, -0.2025, -0.6751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 78 ][ timestamp 29 ] state=tensor([[ 0.0464,  0.0142, -0.2025, -0.6751]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 78: Exploration_rate=0.05. Score=29.\n",
      "[ episode 79 ] state=tensor([[-0.0453,  0.0016,  0.0330, -0.0130]])\n",
      "[ episode 79 ][ timestamp 1 ] state=tensor([[-0.0453,  0.0016,  0.0330, -0.0130]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0453, -0.1939,  0.0328,  0.2899]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 2 ] state=tensor([[-0.0453, -0.1939,  0.0328,  0.2899]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0492, -0.3895,  0.0386,  0.5927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 3 ] state=tensor([[-0.0492, -0.3895,  0.0386,  0.5927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0570, -0.1950,  0.0504,  0.3124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 4 ] state=tensor([[-0.0570, -0.1950,  0.0504,  0.3124]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0609, -0.3908,  0.0567,  0.6206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 5 ] state=tensor([[-0.0609, -0.3908,  0.0567,  0.6206]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0687, -0.1965,  0.0691,  0.3463]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 6 ] state=tensor([[-0.0687, -0.1965,  0.0691,  0.3463]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0726, -0.0024,  0.0760,  0.0761]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 7 ] state=tensor([[-0.0726, -0.0024,  0.0760,  0.0761]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0727,  0.1916,  0.0775, -0.1916]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 8 ] state=tensor([[-0.0727,  0.1916,  0.0775, -0.1916]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0688,  0.3855,  0.0737, -0.4589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 9 ] state=tensor([[-0.0688,  0.3855,  0.0737, -0.4589]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0611,  0.5795,  0.0645, -0.7275]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 10 ] state=tensor([[-0.0611,  0.5795,  0.0645, -0.7275]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0495,  0.7737,  0.0500, -0.9992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 11 ] state=tensor([[-0.0495,  0.7737,  0.0500, -0.9992]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0340,  0.9681,  0.0300, -1.2757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 12 ] state=tensor([[-0.0340,  0.9681,  0.0300, -1.2757]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0147,  1.1628,  0.0045, -1.5589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 13 ] state=tensor([[-0.0147,  1.1628,  0.0045, -1.5589]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0086,  1.3579, -0.0267, -1.8502]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 14 ] state=tensor([[ 0.0086,  1.3579, -0.0267, -1.8502]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0357,  1.1631, -0.0637, -1.5659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 15 ] state=tensor([[ 0.0357,  1.1631, -0.0637, -1.5659]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0590,  0.9688, -0.0950, -1.2938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 16 ] state=tensor([[ 0.0590,  0.9688, -0.0950, -1.2938]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0784,  0.7750, -0.1209, -1.0323]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 79 ][ timestamp 17 ] state=tensor([[ 0.0784,  0.7750, -0.1209, -1.0323]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0939,  0.5816, -0.1416, -0.7799]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 18 ] state=tensor([[ 0.0939,  0.5816, -0.1416, -0.7799]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1055,  0.3887, -0.1572, -0.5349]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 19 ] state=tensor([[ 0.1055,  0.3887, -0.1572, -0.5349]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1133,  0.1961, -0.1679, -0.2955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 20 ] state=tensor([[ 0.1133,  0.1961, -0.1679, -0.2955]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1172,  0.0037, -0.1738, -0.0601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 21 ] state=tensor([[ 0.1172,  0.0037, -0.1738, -0.0601]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1173, -0.1885, -0.1750,  0.1731]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 22 ] state=tensor([[ 0.1173, -0.1885, -0.1750,  0.1731]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1135, -0.3808, -0.1715,  0.4058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 23 ] state=tensor([[ 0.1135, -0.3808, -0.1715,  0.4058]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1059, -0.1837, -0.1634,  0.0644]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 24 ] state=tensor([[ 0.1059, -0.1837, -0.1634,  0.0644]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1022, -0.3761, -0.1621,  0.3014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 25 ] state=tensor([[ 0.1022, -0.3761, -0.1621,  0.3014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0947, -0.1791, -0.1561, -0.0377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 26 ] state=tensor([[ 0.0947, -0.1791, -0.1561, -0.0377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0911, -0.3717, -0.1568,  0.2019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 27 ] state=tensor([[ 0.0911, -0.3717, -0.1568,  0.2019]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0837, -0.5643, -0.1528,  0.4413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 28 ] state=tensor([[ 0.0837, -0.5643, -0.1528,  0.4413]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0724, -0.3673, -0.1440,  0.1047]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 29 ] state=tensor([[ 0.0724, -0.3673, -0.1440,  0.1047]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0650, -0.5601, -0.1419,  0.3487]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 30 ] state=tensor([[ 0.0650, -0.5601, -0.1419,  0.3487]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0538, -0.3633, -0.1349,  0.0148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 31 ] state=tensor([[ 0.0538, -0.3633, -0.1349,  0.0148]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0466, -0.1665, -0.1346, -0.3172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 32 ] state=tensor([[ 0.0466, -0.1665, -0.1346, -0.3172]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0432,  0.0302, -0.1409, -0.6491]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 33 ] state=tensor([[ 0.0432,  0.0302, -0.1409, -0.6491]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0438, -0.1627, -0.1539, -0.4039]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 34 ] state=tensor([[ 0.0438, -0.1627, -0.1539, -0.4039]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0406, -0.3553, -0.1620, -0.1634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 35 ] state=tensor([[ 0.0406, -0.3553, -0.1620, -0.1634]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0335, -0.1583, -0.1653, -0.5025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 36 ] state=tensor([[ 0.0335, -0.1583, -0.1653, -0.5025]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0303, -0.3508, -0.1753, -0.2661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 37 ] state=tensor([[ 0.0303, -0.3508, -0.1753, -0.2661]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0233, -0.1536, -0.1806, -0.6086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 38 ] state=tensor([[ 0.0233, -0.1536, -0.1806, -0.6086]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0202, -0.3458, -0.1928, -0.3778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 39 ] state=tensor([[ 0.0202, -0.3458, -0.1928, -0.3778]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0133, -0.1486, -0.2004, -0.7246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 79 ][ timestamp 40 ] state=tensor([[ 0.0133, -0.1486, -0.2004, -0.7246]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 79: Exploration_rate=0.05. Score=40.\n",
      "[ episode 80 ] state=tensor([[ 0.0489,  0.0460, -0.0031, -0.0173]])\n",
      "[ episode 80 ][ timestamp 1 ] state=tensor([[ 0.0489,  0.0460, -0.0031, -0.0173]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0499, -0.1491, -0.0034,  0.2744]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 2 ] state=tensor([[ 0.0499, -0.1491, -0.0034,  0.2744]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0469,  0.0461,  0.0021, -0.0194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 3 ] state=tensor([[ 0.0469,  0.0461,  0.0021, -0.0194]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0478, -0.1491,  0.0017,  0.2740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 4 ] state=tensor([[ 0.0478, -0.1491,  0.0017,  0.2740]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0448, -0.3442,  0.0072,  0.5672]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 5 ] state=tensor([[ 0.0448, -0.3442,  0.0072,  0.5672]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0379, -0.1492,  0.0185,  0.2768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 6 ] state=tensor([[ 0.0379, -0.1492,  0.0185,  0.2768]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0349, -0.3446,  0.0240,  0.5752]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 7 ] state=tensor([[ 0.0349, -0.3446,  0.0240,  0.5752]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0280, -0.1498,  0.0355,  0.2902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 8 ] state=tensor([[ 0.0280, -0.1498,  0.0355,  0.2902]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0251, -0.3454,  0.0413,  0.5939]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 9 ] state=tensor([[ 0.0251, -0.3454,  0.0413,  0.5939]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0181, -0.1509,  0.0532,  0.3145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 10 ] state=tensor([[ 0.0181, -0.1509,  0.0532,  0.3145]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0151, -0.3467,  0.0595,  0.6235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 11 ] state=tensor([[ 0.0151, -0.3467,  0.0595,  0.6235]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0082, -0.1525,  0.0720,  0.3501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 12 ] state=tensor([[ 0.0082, -0.1525,  0.0720,  0.3501]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[0.0051, 0.0415, 0.0790, 0.0810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 13 ] state=tensor([[0.0051, 0.0415, 0.0790, 0.0810]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0060, -0.1546,  0.0806,  0.3975]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 14 ] state=tensor([[ 0.0060, -0.1546,  0.0806,  0.3975]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0029, -0.3508,  0.0886,  0.7145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 15 ] state=tensor([[ 0.0029, -0.3508,  0.0886,  0.7145]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0041, -0.1570,  0.1028,  0.4509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 16 ] state=tensor([[-0.0041, -0.1570,  0.1028,  0.4509]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0073, -0.3534,  0.1119,  0.7742]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 17 ] state=tensor([[-0.0073, -0.3534,  0.1119,  0.7742]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0143, -0.1600,  0.1273,  0.5187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 18 ] state=tensor([[-0.0143, -0.1600,  0.1273,  0.5187]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0175, -0.3567,  0.1377,  0.8486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 19 ] state=tensor([[-0.0175, -0.3567,  0.1377,  0.8486]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0247, -0.1637,  0.1547,  0.6022]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 20 ] state=tensor([[-0.0247, -0.1637,  0.1547,  0.6022]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0280, -0.3606,  0.1667,  0.9394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 21 ] state=tensor([[-0.0280, -0.3606,  0.1667,  0.9394]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0352, -0.1680,  0.1855,  0.7034]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 80 ][ timestamp 22 ] state=tensor([[-0.0352, -0.1680,  0.1855,  0.7034]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0385, -0.3652,  0.1996,  1.0482]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 80 ][ timestamp 23 ] state=tensor([[-0.0385, -0.3652,  0.1996,  1.0482]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 80: Exploration_rate=0.05. Score=23.\n",
      "[ episode 81 ] state=tensor([[-0.0129,  0.0473, -0.0476,  0.0283]])\n",
      "[ episode 81 ][ timestamp 1 ] state=tensor([[-0.0129,  0.0473, -0.0476,  0.0283]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0120, -0.1471, -0.0470,  0.3057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 2 ] state=tensor([[-0.0120, -0.1471, -0.0470,  0.3057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0149,  0.0487, -0.0409, -0.0015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 3 ] state=tensor([[-0.0149,  0.0487, -0.0409, -0.0015]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0139, -0.1459, -0.0409,  0.2780]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 4 ] state=tensor([[-0.0139, -0.1459, -0.0409,  0.2780]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0169, -0.3404, -0.0353,  0.5575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 5 ] state=tensor([[-0.0169, -0.3404, -0.0353,  0.5575]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0237, -0.1448, -0.0242,  0.2539]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 6 ] state=tensor([[-0.0237, -0.1448, -0.0242,  0.2539]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0266, -0.3395, -0.0191,  0.5389]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 7 ] state=tensor([[-0.0266, -0.3395, -0.0191,  0.5389]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0334, -0.1441, -0.0083,  0.2403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 8 ] state=tensor([[-0.0334, -0.1441, -0.0083,  0.2403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0362, -0.3392, -0.0035,  0.5303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 9 ] state=tensor([[-0.0362, -0.3392, -0.0035,  0.5303]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0430, -0.1440,  0.0071,  0.2365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 10 ] state=tensor([[-0.0430, -0.1440,  0.0071,  0.2365]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0459, -0.3392,  0.0118,  0.5314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 11 ] state=tensor([[-0.0459, -0.3392,  0.0118,  0.5314]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0527, -0.1442,  0.0224,  0.2425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 12 ] state=tensor([[-0.0527, -0.1442,  0.0224,  0.2425]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0556, -0.3397,  0.0273,  0.5421]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 13 ] state=tensor([[-0.0556, -0.3397,  0.0273,  0.5421]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0624, -0.1450,  0.0381,  0.2582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 14 ] state=tensor([[-0.0624, -0.1450,  0.0381,  0.2582]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0653, -0.3406,  0.0433,  0.5626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 15 ] state=tensor([[-0.0653, -0.3406,  0.0433,  0.5626]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0721, -0.1461,  0.0545,  0.2839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 16 ] state=tensor([[-0.0721, -0.1461,  0.0545,  0.2839]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0750, -0.3420,  0.0602,  0.5933]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 17 ] state=tensor([[-0.0750, -0.3420,  0.0602,  0.5933]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0818, -0.5379,  0.0721,  0.9043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 18 ] state=tensor([[-0.0818, -0.5379,  0.0721,  0.9043]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0926, -0.3438,  0.0902,  0.6351]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 19 ] state=tensor([[-0.0926, -0.3438,  0.0902,  0.6351]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0995, -0.1500,  0.1029,  0.3721]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 20 ] state=tensor([[-0.0995, -0.1500,  0.1029,  0.3721]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1025, -0.3465,  0.1103,  0.6954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 21 ] state=tensor([[-0.1025, -0.3465,  0.1103,  0.6954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1094, -0.5429,  0.1242,  1.0207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 22 ] state=tensor([[-0.1094, -0.5429,  0.1242,  1.0207]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1203, -0.7395,  0.1446,  1.3496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 23 ] state=tensor([[-0.1203, -0.7395,  0.1446,  1.3496]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1350, -0.9361,  0.1716,  1.6839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 24 ] state=tensor([[-0.1350, -0.9361,  0.1716,  1.6839]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1538, -1.1327,  0.2053,  2.0247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 81 ][ timestamp 25 ] state=tensor([[-0.1538, -1.1327,  0.2053,  2.0247]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 81: Exploration_rate=0.05. Score=25.\n",
      "[ episode 82 ] state=tensor([[ 0.0135,  0.0466, -0.0050,  0.0052]])\n",
      "[ episode 82 ][ timestamp 1 ] state=tensor([[ 0.0135,  0.0466, -0.0050,  0.0052]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0145, -0.1485, -0.0049,  0.2963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 2 ] state=tensor([[ 0.0145, -0.1485, -0.0049,  0.2963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0115, -0.3436,  0.0010,  0.5874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 3 ] state=tensor([[ 0.0115, -0.3436,  0.0010,  0.5874]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0046, -0.1484,  0.0128,  0.2951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 4 ] state=tensor([[ 0.0046, -0.1484,  0.0128,  0.2951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0017, -0.3437,  0.0187,  0.5918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 5 ] state=tensor([[ 0.0017, -0.3437,  0.0187,  0.5918]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0052, -0.5391,  0.0305,  0.8903]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 6 ] state=tensor([[-0.0052, -0.5391,  0.0305,  0.8903]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0160, -0.3444,  0.0483,  0.6073]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 7 ] state=tensor([[-0.0160, -0.3444,  0.0483,  0.6073]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0229, -0.5402,  0.0605,  0.9148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 8 ] state=tensor([[-0.0229, -0.5402,  0.0605,  0.9148]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0337, -0.3459,  0.0788,  0.6417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 9 ] state=tensor([[-0.0337, -0.3459,  0.0788,  0.6417]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0406, -0.5421,  0.0916,  0.9582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 10 ] state=tensor([[-0.0406, -0.5421,  0.0916,  0.9582]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0514, -0.3483,  0.1108,  0.6956]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 11 ] state=tensor([[-0.0514, -0.3483,  0.1108,  0.6956]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0584, -0.5448,  0.1247,  1.0210]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 12 ] state=tensor([[-0.0584, -0.5448,  0.1247,  1.0210]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0693, -0.7413,  0.1451,  1.3501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 13 ] state=tensor([[-0.0693, -0.7413,  0.1451,  1.3501]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0841, -0.9379,  0.1721,  1.6844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 14 ] state=tensor([[-0.0841, -0.9379,  0.1721,  1.6844]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1029, -1.1346,  0.2058,  2.0254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 82 ][ timestamp 15 ] state=tensor([[-0.1029, -1.1346,  0.2058,  2.0254]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 82: Exploration_rate=0.05. Score=15.\n",
      "[ episode 83 ] state=tensor([[-0.0013, -0.0264,  0.0258,  0.0389]])\n",
      "[ episode 83 ][ timestamp 1 ] state=tensor([[-0.0013, -0.0264,  0.0258,  0.0389]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0019,  0.1684,  0.0266, -0.2456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 2 ] state=tensor([[-0.0019,  0.1684,  0.0266, -0.2456]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0015, -0.0271,  0.0217,  0.0554]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 3 ] state=tensor([[ 0.0015, -0.0271,  0.0217,  0.0554]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0010, -0.2226,  0.0228,  0.3548]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 4 ] state=tensor([[ 0.0010, -0.2226,  0.0228,  0.3548]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0035, -0.4180,  0.0299,  0.6546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 5 ] state=tensor([[-0.0035, -0.4180,  0.0299,  0.6546]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0119, -0.2233,  0.0430,  0.3715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 6 ] state=tensor([[-0.0119, -0.2233,  0.0430,  0.3715]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0163, -0.4190,  0.0504,  0.6774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 7 ] state=tensor([[-0.0163, -0.4190,  0.0504,  0.6774]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0247, -0.2246,  0.0639,  0.4010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 8 ] state=tensor([[-0.0247, -0.2246,  0.0639,  0.4010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0292, -0.4206,  0.0720,  0.7131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 9 ] state=tensor([[-0.0292, -0.4206,  0.0720,  0.7131]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0376, -0.6166,  0.0862,  1.0276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 10 ] state=tensor([[-0.0376, -0.6166,  0.0862,  1.0276]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0499, -0.4227,  0.1068,  0.7632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 11 ] state=tensor([[-0.0499, -0.4227,  0.1068,  0.7632]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0584, -0.2292,  0.1220,  0.5059]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 12 ] state=tensor([[-0.0584, -0.2292,  0.1220,  0.5059]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0630, -0.4259,  0.1322,  0.8344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 13 ] state=tensor([[-0.0630, -0.4259,  0.1322,  0.8344]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0715, -0.2328,  0.1489,  0.5860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 14 ] state=tensor([[-0.0715, -0.2328,  0.1489,  0.5860]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0761, -0.4296,  0.1606,  0.9217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 15 ] state=tensor([[-0.0761, -0.4296,  0.1606,  0.9217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0847, -0.2370,  0.1790,  0.6834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 16 ] state=tensor([[-0.0847, -0.2370,  0.1790,  0.6834]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0895, -0.4341,  0.1927,  1.0267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 83 ][ timestamp 17 ] state=tensor([[-0.0895, -0.4341,  0.1927,  1.0267]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 83: Exploration_rate=0.05. Score=17.\n",
      "[ episode 84 ] state=tensor([[-0.0285, -0.0301, -0.0498,  0.0044]])\n",
      "[ episode 84 ][ timestamp 1 ] state=tensor([[-0.0285, -0.0301, -0.0498,  0.0044]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0291,  0.1657, -0.0498, -0.3036]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 2 ] state=tensor([[-0.0291,  0.1657, -0.0498, -0.3036]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0258,  0.3615, -0.0558, -0.6115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 3 ] state=tensor([[-0.0258,  0.3615, -0.0558, -0.6115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0186,  0.1672, -0.0681, -0.3369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 4 ] state=tensor([[-0.0186,  0.1672, -0.0681, -0.3369]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0152,  0.3632, -0.0748, -0.6503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 5 ] state=tensor([[-0.0152,  0.3632, -0.0748, -0.6503]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0080,  0.1692, -0.0878, -0.3821]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 6 ] state=tensor([[-0.0080,  0.1692, -0.0878, -0.3821]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0046, -0.0245, -0.0954, -0.1183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 7 ] state=tensor([[-0.0046, -0.0245, -0.0954, -0.1183]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0051, -0.2182, -0.0978,  0.1428]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 84 ][ timestamp 8 ] state=tensor([[-0.0051, -0.2182, -0.0978,  0.1428]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0094, -0.4118, -0.0950,  0.4031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 9 ] state=tensor([[-0.0094, -0.4118, -0.0950,  0.4031]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0177, -0.2154, -0.0869,  0.0821]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 10 ] state=tensor([[-0.0177, -0.2154, -0.0869,  0.0821]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0220, -0.4092, -0.0852,  0.3461]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 11 ] state=tensor([[-0.0220, -0.4092, -0.0852,  0.3461]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0302, -0.6030, -0.0783,  0.6107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 12 ] state=tensor([[-0.0302, -0.6030, -0.0783,  0.6107]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0422, -0.4069, -0.0661,  0.2945]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 13 ] state=tensor([[-0.0422, -0.4069, -0.0661,  0.2945]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0504, -0.2109, -0.0602, -0.0183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 14 ] state=tensor([[-0.0504, -0.2109, -0.0602, -0.0183]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0546, -0.4051, -0.0606,  0.2548]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 15 ] state=tensor([[-0.0546, -0.4051, -0.0606,  0.2548]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0627, -0.5993, -0.0555,  0.5277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 16 ] state=tensor([[-0.0627, -0.5993, -0.0555,  0.5277]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0747, -0.7936, -0.0449,  0.8024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 17 ] state=tensor([[-0.0747, -0.7936, -0.0449,  0.8024]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0905, -0.5979, -0.0289,  0.4960]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 18 ] state=tensor([[-0.0905, -0.5979, -0.0289,  0.4960]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1025, -0.4024, -0.0190,  0.1943]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 19 ] state=tensor([[-0.1025, -0.4024, -0.0190,  0.1943]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1106, -0.5972, -0.0151,  0.4810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 20 ] state=tensor([[-0.1106, -0.5972, -0.0151,  0.4810]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1225, -0.4019, -0.0055,  0.1836]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 21 ] state=tensor([[-0.1225, -0.4019, -0.0055,  0.1836]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1305, -0.2067, -0.0018, -0.1108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 22 ] state=tensor([[-0.1305, -0.2067, -0.0018, -0.1108]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1347, -0.0116, -0.0040, -0.4041]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 23 ] state=tensor([[-0.1347, -0.0116, -0.0040, -0.4041]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1349,  0.1836, -0.0121, -0.6980]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 24 ] state=tensor([[-0.1349,  0.1836, -0.0121, -0.6980]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1312,  0.3789, -0.0261, -0.9945]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 25 ] state=tensor([[-0.1312,  0.3789, -0.0261, -0.9945]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1237,  0.1841, -0.0459, -0.7101]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 26 ] state=tensor([[-0.1237,  0.1841, -0.0459, -0.7101]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1200, -0.0103, -0.0601, -0.4322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 27 ] state=tensor([[-0.1200, -0.0103, -0.0601, -0.4322]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1202, -0.2045, -0.0688, -0.1591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 28 ] state=tensor([[-0.1202, -0.2045, -0.0688, -0.1591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1243, -0.3986, -0.0720,  0.1111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 29 ] state=tensor([[-0.1243, -0.3986, -0.0720,  0.1111]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1322, -0.5926, -0.0698,  0.3802]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 30 ] state=tensor([[-0.1322, -0.5926, -0.0698,  0.3802]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1441, -0.3966, -0.0621,  0.0664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 31 ] state=tensor([[-0.1441, -0.3966, -0.0621,  0.0664]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1520, -0.5908, -0.0608,  0.3388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 32 ] state=tensor([[-0.1520, -0.5908, -0.0608,  0.3388]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1638, -0.3948, -0.0540,  0.0276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 33 ] state=tensor([[-0.1638, -0.3948, -0.0540,  0.0276]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1717, -0.5891, -0.0535,  0.3028]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 34 ] state=tensor([[-0.1717, -0.5891, -0.0535,  0.3028]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1835, -0.3933, -0.0474, -0.0063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 35 ] state=tensor([[-0.1835, -0.3933, -0.0474, -0.0063]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1914, -0.5877, -0.0476,  0.2711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 36 ] state=tensor([[-0.1914, -0.5877, -0.0476,  0.2711]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2031, -0.3919, -0.0421, -0.0362]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 37 ] state=tensor([[-0.2031, -0.3919, -0.0421, -0.0362]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2110, -0.5864, -0.0429,  0.2429]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 38 ] state=tensor([[-0.2110, -0.5864, -0.0429,  0.2429]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2227, -0.3907, -0.0380, -0.0630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 39 ] state=tensor([[-0.2227, -0.3907, -0.0380, -0.0630]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2305, -0.5853, -0.0393,  0.2174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 40 ] state=tensor([[-0.2305, -0.5853, -0.0393,  0.2174]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2422, -0.3896, -0.0349, -0.0874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 41 ] state=tensor([[-0.2422, -0.3896, -0.0349, -0.0874]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2500, -0.1940, -0.0367, -0.3909]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 42 ] state=tensor([[-0.2500, -0.1940, -0.0367, -0.3909]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2539,  0.0016, -0.0445, -0.6949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 43 ] state=tensor([[-0.2539,  0.0016, -0.0445, -0.6949]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2539, -0.1929, -0.0584, -0.4165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 44 ] state=tensor([[-0.2539, -0.1929, -0.0584, -0.4165]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2577,  0.0030, -0.0667, -0.7270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 45 ] state=tensor([[-0.2577,  0.0030, -0.0667, -0.7270]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2577,  0.1990, -0.0813, -1.0400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 46 ] state=tensor([[-0.2577,  0.1990, -0.0813, -1.0400]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2537,  0.3951, -0.1021, -1.3570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 47 ] state=tensor([[-0.2537,  0.3951, -0.1021, -1.3570]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2458,  0.2014, -0.1292, -1.0979]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 48 ] state=tensor([[-0.2458,  0.2014, -0.1292, -1.0979]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2418,  0.0082, -0.1512, -0.8484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 49 ] state=tensor([[-0.2418,  0.0082, -0.1512, -0.8484]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2416, -0.1846, -0.1681, -0.6068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 50 ] state=tensor([[-0.2416, -0.1846, -0.1681, -0.6068]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2453,  0.0124, -0.1803, -0.9474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 51 ] state=tensor([[-0.2453,  0.0124, -0.1803, -0.9474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2450, -0.1799, -0.1992, -0.7163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 84 ][ timestamp 52 ] state=tensor([[-0.2450, -0.1799, -0.1992, -0.7163]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 84: Exploration_rate=0.05. Score=52.\n",
      "[ episode 85 ] state=tensor([[ 0.0034,  0.0291,  0.0059, -0.0338]])\n",
      "[ episode 85 ][ timestamp 1 ] state=tensor([[ 0.0034,  0.0291,  0.0059, -0.0338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0040, -0.1661,  0.0053,  0.2607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 2 ] state=tensor([[ 0.0040, -0.1661,  0.0053,  0.2607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0007,  0.0290,  0.0105, -0.0303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 3 ] state=tensor([[ 0.0007,  0.0290,  0.0105, -0.0303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0012, -0.1663,  0.0099,  0.2657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 4 ] state=tensor([[ 0.0012, -0.1663,  0.0099,  0.2657]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0021, -0.3616,  0.0152,  0.5614]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 85 ][ timestamp 5 ] state=tensor([[-0.0021, -0.3616,  0.0152,  0.5614]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0093, -0.1667,  0.0264,  0.2736]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 6 ] state=tensor([[-0.0093, -0.1667,  0.0264,  0.2736]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0127, -0.3622,  0.0319,  0.5745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 7 ] state=tensor([[-0.0127, -0.3622,  0.0319,  0.5745]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0199, -0.1675,  0.0434,  0.2920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 8 ] state=tensor([[-0.0199, -0.1675,  0.0434,  0.2920]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0233, -0.3632,  0.0492,  0.5980]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 9 ] state=tensor([[-0.0233, -0.3632,  0.0492,  0.5980]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0305, -0.1688,  0.0612,  0.3212]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 10 ] state=tensor([[-0.0305, -0.1688,  0.0612,  0.3212]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0339, -0.3647,  0.0676,  0.6326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 11 ] state=tensor([[-0.0339, -0.3647,  0.0676,  0.6326]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0412, -0.5607,  0.0802,  0.9457]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 12 ] state=tensor([[-0.0412, -0.5607,  0.0802,  0.9457]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0524, -0.3668,  0.0992,  0.6793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 13 ] state=tensor([[-0.0524, -0.3668,  0.0992,  0.6793]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0597, -0.1732,  0.1127,  0.4194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 14 ] state=tensor([[-0.0597, -0.1732,  0.1127,  0.4194]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0632, -0.3697,  0.1211,  0.7454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 15 ] state=tensor([[-0.0632, -0.3697,  0.1211,  0.7454]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0706, -0.1764,  0.1360,  0.4932]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 16 ] state=tensor([[-0.0706, -0.1764,  0.1360,  0.4932]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0741, -0.3732,  0.1459,  0.8254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 17 ] state=tensor([[-0.0741, -0.3732,  0.1459,  0.8254]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0816, -0.5700,  0.1624,  1.1602]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 18 ] state=tensor([[-0.0816, -0.5700,  0.1624,  1.1602]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0930, -0.3773,  0.1856,  0.9225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 19 ] state=tensor([[-0.0930, -0.3773,  0.1856,  0.9225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1005, -0.5744,  0.2041,  1.2673]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 85 ][ timestamp 20 ] state=tensor([[-0.1005, -0.5744,  0.2041,  1.2673]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 85: Exploration_rate=0.05. Score=20.\n",
      "[ episode 86 ] state=tensor([[ 0.0186, -0.0334, -0.0017, -0.0424]])\n",
      "[ episode 86 ][ timestamp 1 ] state=tensor([[ 0.0186, -0.0334, -0.0017, -0.0424]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0180, -0.2285, -0.0026,  0.2498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 2 ] state=tensor([[ 0.0180, -0.2285, -0.0026,  0.2498]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0134, -0.4235,  0.0024,  0.5416]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 3 ] state=tensor([[ 0.0134, -0.4235,  0.0024,  0.5416]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0049, -0.6187,  0.0133,  0.8351]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 4 ] state=tensor([[ 0.0049, -0.6187,  0.0133,  0.8351]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0074, -0.4238,  0.0300,  0.5466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 5 ] state=tensor([[-0.0074, -0.4238,  0.0300,  0.5466]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0159, -0.2291,  0.0409,  0.2635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 6 ] state=tensor([[-0.0159, -0.2291,  0.0409,  0.2635]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0205, -0.4248,  0.0462,  0.5688]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 7 ] state=tensor([[-0.0205, -0.4248,  0.0462,  0.5688]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0290, -0.2303,  0.0575,  0.2910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 8 ] state=tensor([[-0.0290, -0.2303,  0.0575,  0.2910]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0336, -0.4262,  0.0634,  0.6013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 9 ] state=tensor([[-0.0336, -0.4262,  0.0634,  0.6013]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0421, -0.2320,  0.0754,  0.3292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 10 ] state=tensor([[-0.0421, -0.2320,  0.0754,  0.3292]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0468, -0.4281,  0.0820,  0.6447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 11 ] state=tensor([[-0.0468, -0.4281,  0.0820,  0.6447]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0553, -0.2342,  0.0949,  0.3789]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 12 ] state=tensor([[-0.0553, -0.2342,  0.0949,  0.3789]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0600, -0.4306,  0.1024,  0.6999]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 13 ] state=tensor([[-0.0600, -0.4306,  0.1024,  0.6999]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0686, -0.2370,  0.1164,  0.4411]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 14 ] state=tensor([[-0.0686, -0.2370,  0.1164,  0.4411]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0734, -0.4336,  0.1253,  0.7681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 15 ] state=tensor([[-0.0734, -0.4336,  0.1253,  0.7681]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0820, -0.2404,  0.1406,  0.5173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 16 ] state=tensor([[-0.0820, -0.2404,  0.1406,  0.5173]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0868, -0.4372,  0.1510,  0.8508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 17 ] state=tensor([[-0.0868, -0.4372,  0.1510,  0.8508]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0956, -0.2444,  0.1680,  0.6092]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 18 ] state=tensor([[-0.0956, -0.2444,  0.1680,  0.6092]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1005, -0.4414,  0.1802,  0.9497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 19 ] state=tensor([[-0.1005, -0.4414,  0.1802,  0.9497]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1093, -0.6384,  0.1992,  1.2931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 86 ][ timestamp 20 ] state=tensor([[-0.1093, -0.6384,  0.1992,  1.2931]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 86: Exploration_rate=0.05. Score=20.\n",
      "[ episode 87 ] state=tensor([[ 0.0027,  0.0301,  0.0260, -0.0113]])\n",
      "[ episode 87 ][ timestamp 1 ] state=tensor([[ 0.0027,  0.0301,  0.0260, -0.0113]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0033, -0.1654,  0.0258,  0.2895]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 2 ] state=tensor([[ 0.0033, -0.1654,  0.0258,  0.2895]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-3.5880e-05, -3.6085e-01,  3.1553e-02,  5.9019e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 3 ] state=tensor([[-3.5880e-05, -3.6085e-01,  3.1553e-02,  5.9019e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0073, -0.1662,  0.0434,  0.3076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 4 ] state=tensor([[-0.0073, -0.1662,  0.0434,  0.3076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0106, -0.3619,  0.0495,  0.6136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 5 ] state=tensor([[-0.0106, -0.3619,  0.0495,  0.6136]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0178, -0.5577,  0.0618,  0.9215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 6 ] state=tensor([[-0.0178, -0.5577,  0.0618,  0.9215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0290, -0.7536,  0.0802,  1.2329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 7 ] state=tensor([[-0.0290, -0.7536,  0.0802,  1.2329]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0440, -0.5596,  0.1049,  0.9664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 8 ] state=tensor([[-0.0440, -0.5596,  0.1049,  0.9664]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0552, -0.3660,  0.1242,  0.7084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 9 ] state=tensor([[-0.0552, -0.3660,  0.1242,  0.7084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0626, -0.1728,  0.1384,  0.4573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 10 ] state=tensor([[-0.0626, -0.1728,  0.1384,  0.4573]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0660, -0.3696,  0.1475,  0.7902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 11 ] state=tensor([[-0.0660, -0.3696,  0.1475,  0.7902]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0734, -0.5664,  0.1633,  1.1254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 12 ] state=tensor([[-0.0734, -0.5664,  0.1633,  1.1254]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0847, -0.3737,  0.1858,  0.8881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 13 ] state=tensor([[-0.0847, -0.3737,  0.1858,  0.8881]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0922, -0.1816,  0.2036,  0.6591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 87 ][ timestamp 14 ] state=tensor([[-0.0922, -0.1816,  0.2036,  0.6591]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 87: Exploration_rate=0.05. Score=14.\n",
      "[ episode 88 ] state=tensor([[-0.0110, -0.0034,  0.0119,  0.0125]])\n",
      "[ episode 88 ][ timestamp 1 ] state=tensor([[-0.0110, -0.0034,  0.0119,  0.0125]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0110, -0.1987,  0.0121,  0.3089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 2 ] state=tensor([[-0.0110, -0.1987,  0.0121,  0.3089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0150, -0.3940,  0.0183,  0.6054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 3 ] state=tensor([[-0.0150, -0.3940,  0.0183,  0.6054]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0229, -0.1991,  0.0304,  0.3185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 4 ] state=tensor([[-0.0229, -0.1991,  0.0304,  0.3185]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0269, -0.3946,  0.0368,  0.6207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 5 ] state=tensor([[-0.0269, -0.3946,  0.0368,  0.6207]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0348, -0.5903,  0.0492,  0.9247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 6 ] state=tensor([[-0.0348, -0.5903,  0.0492,  0.9247]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0466, -0.3958,  0.0677,  0.6479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 7 ] state=tensor([[-0.0466, -0.3958,  0.0677,  0.6479]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0545, -0.2017,  0.0807,  0.3773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 8 ] state=tensor([[-0.0545, -0.2017,  0.0807,  0.3773]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0585, -0.3979,  0.0882,  0.6943]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 9 ] state=tensor([[-0.0585, -0.3979,  0.0882,  0.6943]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0665, -0.2041,  0.1021,  0.4306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 10 ] state=tensor([[-0.0665, -0.2041,  0.1021,  0.4306]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0706, -0.4005,  0.1107,  0.7536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 11 ] state=tensor([[-0.0706, -0.4005,  0.1107,  0.7536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0786, -0.2071,  0.1258,  0.4977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 12 ] state=tensor([[-0.0786, -0.2071,  0.1258,  0.4977]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0827, -0.4037,  0.1357,  0.8273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 13 ] state=tensor([[-0.0827, -0.4037,  0.1357,  0.8273]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0908, -0.2107,  0.1523,  0.5802]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 14 ] state=tensor([[-0.0908, -0.2107,  0.1523,  0.5802]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0950, -0.4076,  0.1639,  0.9167]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 15 ] state=tensor([[-0.0950, -0.4076,  0.1639,  0.9167]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1032, -0.6045,  0.1822,  1.2561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 16 ] state=tensor([[-0.1032, -0.6045,  0.1822,  1.2561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1152, -0.8014,  0.2073,  1.5998]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 88 ][ timestamp 17 ] state=tensor([[-0.1152, -0.8014,  0.2073,  1.5998]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 88: Exploration_rate=0.05. Score=17.\n",
      "[ episode 89 ] state=tensor([[-0.0405,  0.0002,  0.0009,  0.0337]])\n",
      "[ episode 89 ][ timestamp 1 ] state=tensor([[-0.0405,  0.0002,  0.0009,  0.0337]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0405, -0.1949,  0.0016,  0.3266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 2 ] state=tensor([[-0.0405, -0.1949,  0.0016,  0.3266]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0444,  0.0002,  0.0081,  0.0345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 3 ] state=tensor([[-0.0444,  0.0002,  0.0081,  0.0345]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0444,  0.1952,  0.0088, -0.2556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 4 ] state=tensor([[-0.0444,  0.1952,  0.0088, -0.2556]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-4.0523e-02, -4.7889e-05,  3.7225e-03,  3.9830e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 5 ] state=tensor([[-4.0523e-02, -4.7889e-05,  3.7225e-03,  3.9830e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0405,  0.1950,  0.0045, -0.2517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 6 ] state=tensor([[-0.0405,  0.1950,  0.0045, -0.2517]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0366, -0.0002, -0.0005,  0.0424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 7 ] state=tensor([[-0.0366, -0.0002, -0.0005,  0.0424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0366,  0.1950,  0.0003, -0.2504]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 8 ] state=tensor([[-0.0366,  0.1950,  0.0003, -0.2504]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0327,  0.3901, -0.0047, -0.5430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 9 ] state=tensor([[-0.0327,  0.3901, -0.0047, -0.5430]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0249,  0.1950, -0.0155, -0.2518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 10 ] state=tensor([[-0.0249,  0.1950, -0.0155, -0.2518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0210,  0.0001, -0.0206,  0.0360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 11 ] state=tensor([[-0.0210,  0.0001, -0.0206,  0.0360]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0210, -0.1947, -0.0199,  0.3221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 12 ] state=tensor([[-0.0210, -0.1947, -0.0199,  0.3221]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0249, -0.3895, -0.0134,  0.6084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 13 ] state=tensor([[-0.0249, -0.3895, -0.0134,  0.6084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0327, -0.1942, -0.0012,  0.3116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 14 ] state=tensor([[-0.0327, -0.1942, -0.0012,  0.3116]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0366, -0.3893,  0.0050,  0.6039]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 15 ] state=tensor([[-0.0366, -0.3893,  0.0050,  0.6039]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0444, -0.1943,  0.0171,  0.3127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 16 ] state=tensor([[-0.0444, -0.1943,  0.0171,  0.3127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0483, -0.3896,  0.0233,  0.6108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 17 ] state=tensor([[-0.0483, -0.3896,  0.0233,  0.6108]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0561, -0.1948,  0.0355,  0.3255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 18 ] state=tensor([[-0.0561, -0.1948,  0.0355,  0.3255]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0600, -0.3905,  0.0420,  0.6292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 19 ] state=tensor([[-0.0600, -0.3905,  0.0420,  0.6292]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0678, -0.1959,  0.0546,  0.3500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 20 ] state=tensor([[-0.0678, -0.1959,  0.0546,  0.3500]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0717, -0.3918,  0.0616,  0.6594]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 21 ] state=tensor([[-0.0717, -0.3918,  0.0616,  0.6594]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0795, -0.1976,  0.0748,  0.3868]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 22 ] state=tensor([[-0.0795, -0.1976,  0.0748,  0.3868]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0835, -0.3937,  0.0826,  0.7021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 23 ] state=tensor([[-0.0835, -0.3937,  0.0826,  0.7021]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0913, -0.1998,  0.0966,  0.4365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 24 ] state=tensor([[-0.0913, -0.1998,  0.0966,  0.4365]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0953, -0.3961,  0.1053,  0.7580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 25 ] state=tensor([[-0.0953, -0.3961,  0.1053,  0.7580]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1033, -0.2026,  0.1205,  0.5002]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 26 ] state=tensor([[-0.1033, -0.2026,  0.1205,  0.5002]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1073, -0.0094,  0.1305,  0.2478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 27 ] state=tensor([[-0.1073, -0.0094,  0.1305,  0.2478]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1075, -0.2061,  0.1354,  0.5786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 28 ] state=tensor([[-0.1075, -0.2061,  0.1354,  0.5786]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1116, -0.4028,  0.1470,  0.9107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 29 ] state=tensor([[-0.1116, -0.4028,  0.1470,  0.9107]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1197, -0.2100,  0.1652,  0.6676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 30 ] state=tensor([[-0.1197, -0.2100,  0.1652,  0.6676]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1239, -0.0175,  0.1786,  0.4312]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 89 ][ timestamp 31 ] state=tensor([[-0.1239, -0.0175,  0.1786,  0.4312]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1242, -0.2146,  0.1872,  0.7744]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 32 ] state=tensor([[-0.1242, -0.2146,  0.1872,  0.7744]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1285, -0.0225,  0.2027,  0.5460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 89 ][ timestamp 33 ] state=tensor([[-0.1285, -0.0225,  0.2027,  0.5460]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 89: Exploration_rate=0.05. Score=33.\n",
      "[ episode 90 ] state=tensor([[ 0.0186, -0.0200, -0.0178,  0.0336]])\n",
      "[ episode 90 ][ timestamp 1 ] state=tensor([[ 0.0186, -0.0200, -0.0178,  0.0336]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0182, -0.2149, -0.0171,  0.3206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 2 ] state=tensor([[ 0.0182, -0.2149, -0.0171,  0.3206]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0139, -0.0195, -0.0107,  0.0226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 3 ] state=tensor([[ 0.0139, -0.0195, -0.0107,  0.0226]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0135,  0.1757, -0.0103, -0.2735]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 4 ] state=tensor([[ 0.0135,  0.1757, -0.0103, -0.2735]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0170, -0.0192, -0.0158,  0.0159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 5 ] state=tensor([[ 0.0170, -0.0192, -0.0158,  0.0159]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0166,  0.1761, -0.0154, -0.2817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 6 ] state=tensor([[ 0.0166,  0.1761, -0.0154, -0.2817]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0202, -0.0188, -0.0211,  0.0061]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 7 ] state=tensor([[ 0.0202, -0.0188, -0.0211,  0.0061]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0198, -0.2136, -0.0209,  0.2921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 8 ] state=tensor([[ 0.0198, -0.2136, -0.0209,  0.2921]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0155, -0.4084, -0.0151,  0.5781]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 9 ] state=tensor([[ 0.0155, -0.4084, -0.0151,  0.5781]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0073, -0.2131, -0.0035,  0.2807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 10 ] state=tensor([[ 0.0073, -0.2131, -0.0035,  0.2807]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0031, -0.4082,  0.0021,  0.5722]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 11 ] state=tensor([[ 0.0031, -0.4082,  0.0021,  0.5722]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0051, -0.2131,  0.0135,  0.2802]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 12 ] state=tensor([[-0.0051, -0.2131,  0.0135,  0.2802]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0093, -0.4084,  0.0191,  0.5771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 13 ] state=tensor([[-0.0093, -0.4084,  0.0191,  0.5771]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0175, -0.2135,  0.0307,  0.2905]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 14 ] state=tensor([[-0.0175, -0.2135,  0.0307,  0.2905]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0218, -0.4091,  0.0365,  0.5927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 15 ] state=tensor([[-0.0218, -0.4091,  0.0365,  0.5927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0300, -0.2145,  0.0483,  0.3118]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 16 ] state=tensor([[-0.0300, -0.2145,  0.0483,  0.3118]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0342, -0.0201,  0.0546,  0.0347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 17 ] state=tensor([[-0.0342, -0.0201,  0.0546,  0.0347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0346,  0.1742,  0.0553, -0.2403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 18 ] state=tensor([[-0.0346,  0.1742,  0.0553, -0.2403]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0312,  0.3685,  0.0505, -0.5150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 19 ] state=tensor([[-0.0312,  0.3685,  0.0505, -0.5150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0238,  0.5629,  0.0402, -0.7914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 20 ] state=tensor([[-0.0238,  0.5629,  0.0402, -0.7914]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0125,  0.7574,  0.0243, -1.0712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 21 ] state=tensor([[-0.0125,  0.7574,  0.0243, -1.0712]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0026,  0.9522,  0.0029, -1.3561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 22 ] state=tensor([[ 0.0026,  0.9522,  0.0029, -1.3561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0217,  1.1473, -0.0242, -1.6479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 23 ] state=tensor([[ 0.0217,  1.1473, -0.0242, -1.6479]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0446,  0.9525, -0.0572, -1.3629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 24 ] state=tensor([[ 0.0446,  0.9525, -0.0572, -1.3629]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0637,  0.7581, -0.0844, -1.0886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 25 ] state=tensor([[ 0.0637,  0.7581, -0.0844, -1.0886]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0788,  0.5642, -0.1062, -0.8236]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 26 ] state=tensor([[ 0.0788,  0.5642, -0.1062, -0.8236]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0901,  0.3707, -0.1227, -0.5661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 27 ] state=tensor([[ 0.0901,  0.3707, -0.1227, -0.5661]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0975,  0.1775, -0.1340, -0.3144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 28 ] state=tensor([[ 0.0975,  0.1775, -0.1340, -0.3144]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1011, -0.0155, -0.1403, -0.0668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 29 ] state=tensor([[ 0.1011, -0.0155, -0.1403, -0.0668]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1008, -0.2084, -0.1416,  0.1785]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 30 ] state=tensor([[ 0.1008, -0.2084, -0.1416,  0.1785]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0966, -0.4012, -0.1381,  0.4234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 31 ] state=tensor([[ 0.0966, -0.4012, -0.1381,  0.4234]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0886, -0.2044, -0.1296,  0.0905]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 32 ] state=tensor([[ 0.0886, -0.2044, -0.1296,  0.0905]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0845, -0.3975, -0.1278,  0.3397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 33 ] state=tensor([[ 0.0845, -0.3975, -0.1278,  0.3397]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0765, -0.5906, -0.1210,  0.5895]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 34 ] state=tensor([[ 0.0765, -0.5906, -0.1210,  0.5895]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0647, -0.3940, -0.1092,  0.2613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 35 ] state=tensor([[ 0.0647, -0.3940, -0.1092,  0.2613]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0568, -0.5874, -0.1040,  0.5176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 36 ] state=tensor([[ 0.0568, -0.5874, -0.1040,  0.5176]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0451, -0.3910, -0.0936,  0.1941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 37 ] state=tensor([[ 0.0451, -0.3910, -0.0936,  0.1941]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0373, -0.5846, -0.0897,  0.4558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 38 ] state=tensor([[ 0.0373, -0.5846, -0.0897,  0.4558]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0256, -0.3884, -0.0806,  0.1363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 39 ] state=tensor([[ 0.0256, -0.3884, -0.0806,  0.1363]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0178, -0.5822, -0.0779,  0.4025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 40 ] state=tensor([[ 0.0178, -0.5822, -0.0779,  0.4025]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0062, -0.3861, -0.0698,  0.0863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 41 ] state=tensor([[ 0.0062, -0.3861, -0.0698,  0.0863]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0016, -0.5802, -0.0681,  0.3561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 42 ] state=tensor([[-0.0016, -0.5802, -0.0681,  0.3561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0132, -0.3841, -0.0610,  0.0428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 43 ] state=tensor([[-0.0132, -0.3841, -0.0610,  0.0428]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0209, -0.5783, -0.0601,  0.3156]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 44 ] state=tensor([[-0.0209, -0.5783, -0.0601,  0.3156]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0324, -0.7726, -0.0538,  0.5887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 45 ] state=tensor([[-0.0324, -0.7726, -0.0538,  0.5887]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0479, -0.5767, -0.0421,  0.2796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 46 ] state=tensor([[-0.0479, -0.5767, -0.0421,  0.2796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0594, -0.7712, -0.0365,  0.5587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 47 ] state=tensor([[-0.0594, -0.7712, -0.0365,  0.5587]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0748, -0.5756, -0.0253,  0.2548]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 48 ] state=tensor([[-0.0748, -0.5756, -0.0253,  0.2548]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0863, -0.3801, -0.0202, -0.0458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 49 ] state=tensor([[-0.0863, -0.3801, -0.0202, -0.0458]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0939, -0.5750, -0.0211,  0.2404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 50 ] state=tensor([[-0.0939, -0.5750, -0.0211,  0.2404]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1054, -0.7698, -0.0163,  0.5264]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 51 ] state=tensor([[-0.1054, -0.7698, -0.0163,  0.5264]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1208, -0.5744, -0.0058,  0.2286]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 52 ] state=tensor([[-0.1208, -0.5744, -0.0058,  0.2286]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1323, -0.3792, -0.0012, -0.0659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 53 ] state=tensor([[-0.1323, -0.3792, -0.0012, -0.0659]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1399, -0.5743, -0.0025,  0.2264]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 54 ] state=tensor([[-0.1399, -0.5743, -0.0025,  0.2264]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1514, -0.3792,  0.0020, -0.0670]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 55 ] state=tensor([[-0.1514, -0.3792,  0.0020, -0.0670]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1590, -0.1841,  0.0007, -0.3591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 56 ] state=tensor([[-0.1590, -0.1841,  0.0007, -0.3591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1627, -0.3792, -0.0065, -0.0662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 57 ] state=tensor([[-0.1627, -0.3792, -0.0065, -0.0662]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1702, -0.5742, -0.0078,  0.2244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 58 ] state=tensor([[-0.1702, -0.5742, -0.0078,  0.2244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1817, -0.3790, -0.0034, -0.0707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 59 ] state=tensor([[-0.1817, -0.3790, -0.0034, -0.0707]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1893, -0.5741, -0.0048,  0.2209]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 90 ][ timestamp 60 ] state=tensor([[-0.1893, -0.5741, -0.0048,  0.2209]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0079e-01, -3.7889e-01, -3.4714e-04, -7.3287e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 61 ] state=tensor([[-2.0079e-01, -3.7889e-01, -3.4714e-04, -7.3287e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2084, -0.1838, -0.0018, -0.3661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 62 ] state=tensor([[-0.2084, -0.1838, -0.0018, -0.3661]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2120, -0.3789, -0.0091, -0.0740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 63 ] state=tensor([[-0.2120, -0.3789, -0.0091, -0.0740]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2196, -0.5738, -0.0106,  0.2158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 64 ] state=tensor([[-0.2196, -0.5738, -0.0106,  0.2158]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2311, -0.3786, -0.0063, -0.0802]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 65 ] state=tensor([[-0.2311, -0.3786, -0.0063, -0.0802]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2387, -0.1834, -0.0079, -0.3749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 66 ] state=tensor([[-0.2387, -0.1834, -0.0079, -0.3749]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2423,  0.0119, -0.0154, -0.6700]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 67 ] state=tensor([[-0.2423,  0.0119, -0.0154, -0.6700]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2421, -0.1830, -0.0288, -0.3822]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 68 ] state=tensor([[-0.2421, -0.1830, -0.0288, -0.3822]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2458,  0.0125, -0.0364, -0.6838]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 69 ] state=tensor([[-0.2458,  0.0125, -0.0364, -0.6838]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2455,  0.2081, -0.0501, -0.9878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 70 ] state=tensor([[-0.2455,  0.2081, -0.0501, -0.9878]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2414,  0.4038, -0.0699, -1.2958]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 71 ] state=tensor([[-0.2414,  0.4038, -0.0699, -1.2958]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2333,  0.5998, -0.0958, -1.6095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 72 ] state=tensor([[-0.2333,  0.5998, -0.0958, -1.6095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2213,  0.7959, -0.1280, -1.9304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 73 ] state=tensor([[-0.2213,  0.7959, -0.1280, -1.9304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2054,  0.6024, -0.1666, -1.6800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 74 ] state=tensor([[-0.2054,  0.6024, -0.1666, -1.6800]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1933,  0.4095, -0.2002, -1.4435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 90 ][ timestamp 75 ] state=tensor([[-0.1933,  0.4095, -0.2002, -1.4435]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 90: Exploration_rate=0.05. Score=75.\n",
      "[ episode 91 ] state=tensor([[ 0.0132, -0.0299, -0.0042, -0.0230]])\n",
      "[ episode 91 ][ timestamp 1 ] state=tensor([[ 0.0132, -0.0299, -0.0042, -0.0230]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0126, -0.2250, -0.0047,  0.2684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 2 ] state=tensor([[ 0.0126, -0.2250, -0.0047,  0.2684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0081, -0.4200,  0.0007,  0.5596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 3 ] state=tensor([[ 0.0081, -0.4200,  0.0007,  0.5596]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0003, -0.2249,  0.0119,  0.2671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 4 ] state=tensor([[-0.0003, -0.2249,  0.0119,  0.2671]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0048, -0.0300,  0.0172, -0.0218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 5 ] state=tensor([[-0.0048, -0.0300,  0.0172, -0.0218]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0054, -0.2253,  0.0168,  0.2762]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 6 ] state=tensor([[-0.0054, -0.2253,  0.0168,  0.2762]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0099, -0.4207,  0.0223,  0.5742]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 7 ] state=tensor([[-0.0099, -0.4207,  0.0223,  0.5742]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0183, -0.2259,  0.0338,  0.2886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 8 ] state=tensor([[-0.0183, -0.2259,  0.0338,  0.2886]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0229, -0.4215,  0.0395,  0.5917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 9 ] state=tensor([[-0.0229, -0.4215,  0.0395,  0.5917]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0313, -0.2269,  0.0514,  0.3117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 10 ] state=tensor([[-0.0313, -0.2269,  0.0514,  0.3117]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0358, -0.0326,  0.0576,  0.0357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 11 ] state=tensor([[-0.0358, -0.0326,  0.0576,  0.0357]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0365, -0.2285,  0.0583,  0.3460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 12 ] state=tensor([[-0.0365, -0.2285,  0.0583,  0.3460]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0410, -0.0342,  0.0652,  0.0722]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 13 ] state=tensor([[-0.0410, -0.0342,  0.0652,  0.0722]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0417,  0.1599,  0.0667, -0.1992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 14 ] state=tensor([[-0.0417,  0.1599,  0.0667, -0.1992]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0385, -0.0361,  0.0627,  0.1138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 15 ] state=tensor([[-0.0385, -0.0361,  0.0627,  0.1138]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0392, -0.2321,  0.0650,  0.4256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 16 ] state=tensor([[-0.0392, -0.2321,  0.0650,  0.4256]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0439, -0.4281,  0.0735,  0.7380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 17 ] state=tensor([[-0.0439, -0.4281,  0.0735,  0.7380]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0525, -0.2340,  0.0882,  0.4693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 18 ] state=tensor([[-0.0525, -0.2340,  0.0882,  0.4693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0571, -0.4303,  0.0976,  0.7885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 19 ] state=tensor([[-0.0571, -0.4303,  0.0976,  0.7885]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0657, -0.6266,  0.1134,  1.1102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 20 ] state=tensor([[-0.0657, -0.6266,  0.1134,  1.1102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0783, -0.4331,  0.1356,  0.8551]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 21 ] state=tensor([[-0.0783, -0.4331,  0.1356,  0.8551]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0869, -0.2401,  0.1527,  0.6080]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 22 ] state=tensor([[-0.0869, -0.2401,  0.1527,  0.6080]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0917, -0.0474,  0.1649,  0.3670]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 23 ] state=tensor([[-0.0917, -0.0474,  0.1649,  0.3670]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0927, -0.2444,  0.1722,  0.7068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 24 ] state=tensor([[-0.0927, -0.2444,  0.1722,  0.7068]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0976, -0.4415,  0.1863,  1.0484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 25 ] state=tensor([[-0.0976, -0.4415,  0.1863,  1.0484]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1064, -0.6385,  0.2073,  1.3933]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 91 ][ timestamp 26 ] state=tensor([[-0.1064, -0.6385,  0.2073,  1.3933]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 91: Exploration_rate=0.05. Score=26.\n",
      "[ episode 92 ] state=tensor([[ 0.0422,  0.0094, -0.0058,  0.0193]])\n",
      "[ episode 92 ][ timestamp 1 ] state=tensor([[ 0.0422,  0.0094, -0.0058,  0.0193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0424, -0.1856, -0.0054,  0.3102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 2 ] state=tensor([[ 0.0424, -0.1856, -0.0054,  0.3102]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0386, -0.3807,  0.0008,  0.6011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 3 ] state=tensor([[ 0.0386, -0.3807,  0.0008,  0.6011]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0310, -0.1856,  0.0129,  0.3087]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 4 ] state=tensor([[ 0.0310, -0.1856,  0.0129,  0.3087]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0273, -0.3809,  0.0190,  0.6054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 5 ] state=tensor([[ 0.0273, -0.3809,  0.0190,  0.6054]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0197, -0.1860,  0.0311,  0.3188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 6 ] state=tensor([[ 0.0197, -0.1860,  0.0311,  0.3188]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0160, -0.3816,  0.0375,  0.6211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 7 ] state=tensor([[ 0.0160, -0.3816,  0.0375,  0.6211]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0084, -0.1870,  0.0499,  0.3405]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 8 ] state=tensor([[ 0.0084, -0.1870,  0.0499,  0.3405]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0046, -0.3828,  0.0567,  0.6485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 9 ] state=tensor([[ 0.0046, -0.3828,  0.0567,  0.6485]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0030, -0.1885,  0.0697,  0.3742]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 10 ] state=tensor([[-0.0030, -0.1885,  0.0697,  0.3742]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0068, -0.3845,  0.0772,  0.6880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 11 ] state=tensor([[-0.0068, -0.3845,  0.0772,  0.6880]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0145, -0.1906,  0.0910,  0.4206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 12 ] state=tensor([[-0.0145, -0.1906,  0.0910,  0.4206]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0183, -0.3868,  0.0994,  0.7405]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 13 ] state=tensor([[-0.0183, -0.3868,  0.0994,  0.7405]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0260, -0.1932,  0.1142,  0.4807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 14 ] state=tensor([[-0.0260, -0.1932,  0.1142,  0.4807]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0299, -0.3898,  0.1238,  0.8071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 15 ] state=tensor([[-0.0299, -0.3898,  0.1238,  0.8071]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0377, -0.1965,  0.1399,  0.5557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 16 ] state=tensor([[-0.0377, -0.1965,  0.1399,  0.5557]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0416, -0.3933,  0.1510,  0.8890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 17 ] state=tensor([[-0.0416, -0.3933,  0.1510,  0.8890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0495, -0.2005,  0.1688,  0.6474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 18 ] state=tensor([[-0.0495, -0.2005,  0.1688,  0.6474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0535, -0.3975,  0.1818,  0.9881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 19 ] state=tensor([[-0.0535, -0.3975,  0.1818,  0.9881]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0615, -0.2053,  0.2015,  0.7576]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 92 ][ timestamp 20 ] state=tensor([[-0.0615, -0.2053,  0.2015,  0.7576]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 92: Exploration_rate=0.05. Score=20.\n",
      "[ episode 93 ] state=tensor([[-0.0426,  0.0043, -0.0172, -0.0098]])\n",
      "[ episode 93 ][ timestamp 1 ] state=tensor([[-0.0426,  0.0043, -0.0172, -0.0098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0425, -0.1906, -0.0174,  0.2774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 2 ] state=tensor([[-0.0425, -0.1906, -0.0174,  0.2774]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0463, -0.3854, -0.0118,  0.5646]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 3 ] state=tensor([[-0.0463, -0.3854, -0.0118,  0.5646]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0540, -0.1901, -0.0005,  0.2682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 4 ] state=tensor([[-0.0540, -0.1901, -0.0005,  0.2682]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0578,  0.0050,  0.0048, -0.0247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 5 ] state=tensor([[-0.0578,  0.0050,  0.0048, -0.0247]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0577,  0.2000,  0.0043, -0.3158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 6 ] state=tensor([[-0.0577,  0.2000,  0.0043, -0.3158]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0537,  0.3951, -0.0020, -0.6071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 7 ] state=tensor([[-0.0537,  0.3951, -0.0020, -0.6071]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0458,  0.5902, -0.0141, -0.9004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 8 ] state=tensor([[-0.0458,  0.5902, -0.0141, -0.9004]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0340,  0.3953, -0.0321, -0.6122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 9 ] state=tensor([[-0.0340,  0.3953, -0.0321, -0.6122]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0261,  0.2007, -0.0444, -0.3298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 10 ] state=tensor([[-0.0261,  0.2007, -0.0444, -0.3298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0221,  0.0062, -0.0510, -0.0515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 11 ] state=tensor([[-0.0221,  0.0062, -0.0510, -0.0515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0220, -0.1882, -0.0520,  0.2247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 12 ] state=tensor([[-0.0220, -0.1882, -0.0520,  0.2247]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0257, -0.3825, -0.0475,  0.5005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 13 ] state=tensor([[-0.0257, -0.3825, -0.0475,  0.5005]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0334, -0.1867, -0.0375,  0.1933]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 14 ] state=tensor([[-0.0334, -0.1867, -0.0375,  0.1933]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0371, -0.3813, -0.0336,  0.4739]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 15 ] state=tensor([[-0.0371, -0.3813, -0.0336,  0.4739]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0447, -0.1857, -0.0242,  0.1708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 16 ] state=tensor([[-0.0447, -0.1857, -0.0242,  0.1708]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0485, -0.3805, -0.0207,  0.4558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 17 ] state=tensor([[-0.0485, -0.3805, -0.0207,  0.4558]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0561, -0.1851, -0.0116,  0.1566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 18 ] state=tensor([[-0.0561, -0.1851, -0.0116,  0.1566]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0598,  0.0102, -0.0085, -0.1397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 19 ] state=tensor([[-0.0598,  0.0102, -0.0085, -0.1397]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0596,  0.2054, -0.0113, -0.4350]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 20 ] state=tensor([[-0.0596,  0.2054, -0.0113, -0.4350]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0555,  0.4007, -0.0200, -0.7313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 21 ] state=tensor([[-0.0555,  0.4007, -0.0200, -0.7313]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0474,  0.2059, -0.0346, -0.4449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 22 ] state=tensor([[-0.0474,  0.2059, -0.0346, -0.4449]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0433,  0.0113, -0.0435, -0.1634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 23 ] state=tensor([[-0.0433,  0.0113, -0.0435, -0.1634]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0431, -0.1832, -0.0468,  0.1153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 24 ] state=tensor([[-0.0431, -0.1832, -0.0468,  0.1153]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0468, -0.3776, -0.0445,  0.3929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 25 ] state=tensor([[-0.0468, -0.3776, -0.0445,  0.3929]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0543, -0.1819, -0.0366,  0.0865]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 26 ] state=tensor([[-0.0543, -0.1819, -0.0366,  0.0865]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0580, -0.3765, -0.0349,  0.3674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 27 ] state=tensor([[-0.0580, -0.3765, -0.0349,  0.3674]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0655, -0.1809, -0.0275,  0.0639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 28 ] state=tensor([[-0.0655, -0.1809, -0.0275,  0.0639]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0691, -0.3756, -0.0263,  0.3478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 29 ] state=tensor([[-0.0691, -0.3756, -0.0263,  0.3478]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0766, -0.1801, -0.0193,  0.0470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 30 ] state=tensor([[-0.0766, -0.1801, -0.0193,  0.0470]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0802, -0.3750, -0.0184,  0.3335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 31 ] state=tensor([[-0.0802, -0.3750, -0.0184,  0.3335]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0877, -0.5698, -0.0117,  0.6203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 32 ] state=tensor([[-0.0877, -0.5698, -0.0117,  0.6203]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0991, -0.3745,  0.0007,  0.3240]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 33 ] state=tensor([[-0.0991, -0.3745,  0.0007,  0.3240]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1066, -0.5697,  0.0072,  0.6169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 34 ] state=tensor([[-0.1066, -0.5697,  0.0072,  0.6169]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1180, -0.3746,  0.0195,  0.3265]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 93 ][ timestamp 35 ] state=tensor([[-0.1180, -0.3746,  0.0195,  0.3265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1255, -0.1798,  0.0261,  0.0400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 36 ] state=tensor([[-0.1255, -0.1798,  0.0261,  0.0400]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1291,  0.0149,  0.0269, -0.2443]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 37 ] state=tensor([[-0.1291,  0.0149,  0.0269, -0.2443]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1288,  0.2097,  0.0220, -0.5284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 38 ] state=tensor([[-0.1288,  0.2097,  0.0220, -0.5284]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1246,  0.4045,  0.0114, -0.8141]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 39 ] state=tensor([[-0.1246,  0.4045,  0.0114, -0.8141]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1165,  0.5994, -0.0049, -1.1031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 40 ] state=tensor([[-0.1165,  0.5994, -0.0049, -1.1031]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1045,  0.4044, -0.0269, -0.8120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 41 ] state=tensor([[-0.1045,  0.4044, -0.0269, -0.8120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0964,  0.5999, -0.0432, -1.1130]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 42 ] state=tensor([[-0.0964,  0.5999, -0.0432, -1.1130]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0844,  0.4053, -0.0654, -0.8342]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 43 ] state=tensor([[-0.0844,  0.4053, -0.0654, -0.8342]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0763,  0.2112, -0.0821, -0.5628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 44 ] state=tensor([[-0.0763,  0.2112, -0.0821, -0.5628]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0721,  0.0173, -0.0934, -0.2970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 45 ] state=tensor([[-0.0721,  0.0173, -0.0934, -0.2970]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0718, -0.1764, -0.0993, -0.0352]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 46 ] state=tensor([[-0.0718, -0.1764, -0.0993, -0.0352]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0753, -0.3700, -0.1000,  0.2246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 47 ] state=tensor([[-0.0753, -0.3700, -0.1000,  0.2246]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0827, -0.5635, -0.0955,  0.4841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 48 ] state=tensor([[-0.0827, -0.5635, -0.0955,  0.4841]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0940, -0.3672, -0.0858,  0.1629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 49 ] state=tensor([[-0.0940, -0.3672, -0.0858,  0.1629]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1013, -0.5610, -0.0826,  0.4273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 50 ] state=tensor([[-0.1013, -0.5610, -0.0826,  0.4273]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1125, -0.7549, -0.0740,  0.6929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 51 ] state=tensor([[-0.1125, -0.7549, -0.0740,  0.6929]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1276, -0.5588, -0.0602,  0.3778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 52 ] state=tensor([[-0.1276, -0.5588, -0.0602,  0.3778]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1388, -0.3629, -0.0526,  0.0668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 53 ] state=tensor([[-0.1388, -0.3629, -0.0526,  0.0668]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1461, -0.1670, -0.0513, -0.2420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 54 ] state=tensor([[-0.1461, -0.1670, -0.0513, -0.2420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1494,  0.0288, -0.0561, -0.5504]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 55 ] state=tensor([[-0.1494,  0.0288, -0.0561, -0.5504]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1488, -0.1655, -0.0671, -0.2759]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 56 ] state=tensor([[-0.1488, -0.1655, -0.0671, -0.2759]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1521, -0.3596, -0.0726, -0.0051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 57 ] state=tensor([[-0.1521, -0.3596, -0.0726, -0.0051]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1593, -0.5536, -0.0728,  0.2638]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 58 ] state=tensor([[-0.1593, -0.5536, -0.0728,  0.2638]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1704, -0.3575, -0.0675, -0.0510]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 59 ] state=tensor([[-0.1704, -0.3575, -0.0675, -0.0510]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1775, -0.5516, -0.0685,  0.2197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 60 ] state=tensor([[-0.1775, -0.5516, -0.0685,  0.2197]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1886, -0.3556, -0.0641, -0.0938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 61 ] state=tensor([[-0.1886, -0.3556, -0.0641, -0.0938]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1957, -0.5497, -0.0660,  0.1780]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 62 ] state=tensor([[-0.1957, -0.5497, -0.0660,  0.1780]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2067, -0.3537, -0.0624, -0.1347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 63 ] state=tensor([[-0.2067, -0.3537, -0.0624, -0.1347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2138, -0.5479, -0.0651,  0.1376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 64 ] state=tensor([[-0.2138, -0.5479, -0.0651,  0.1376]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2247, -0.3519, -0.0624, -0.1749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 65 ] state=tensor([[-0.2247, -0.3519, -0.0624, -0.1749]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2318, -0.5461, -0.0659,  0.0975]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 66 ] state=tensor([[-0.2318, -0.5461, -0.0659,  0.0975]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2427, -0.3501, -0.0639, -0.2152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 67 ] state=tensor([[-0.2427, -0.3501, -0.0639, -0.2152]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2497, -0.5443, -0.0682,  0.0566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 68 ] state=tensor([[-0.2497, -0.5443, -0.0682,  0.0566]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2606, -0.3482, -0.0671, -0.2568]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 69 ] state=tensor([[-0.2606, -0.3482, -0.0671, -0.2568]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2675, -0.5423, -0.0722,  0.0140]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 70 ] state=tensor([[-0.2675, -0.5423, -0.0722,  0.0140]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2784, -0.3462, -0.0719, -0.3005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 71 ] state=tensor([[-0.2784, -0.3462, -0.0719, -0.3005]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2853, -0.1502, -0.0779, -0.6150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 72 ] state=tensor([[-0.2853, -0.1502, -0.0779, -0.6150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2883,  0.0459, -0.0902, -0.9312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 73 ] state=tensor([[-0.2883,  0.0459, -0.0902, -0.9312]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2874, -0.1479, -0.1089, -0.6682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 74 ] state=tensor([[-0.2874, -0.1479, -0.1089, -0.6682]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2903,  0.0486, -0.1222, -0.9930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 75 ] state=tensor([[-0.2903,  0.0486, -0.1222, -0.9930]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2894, -0.1447, -0.1421, -0.7411]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 76 ] state=tensor([[-0.2894, -0.1447, -0.1421, -0.7411]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2923, -0.3376, -0.1569, -0.4963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 77 ] state=tensor([[-0.2923, -0.3376, -0.1569, -0.4963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2990, -0.5302, -0.1668, -0.2569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 78 ] state=tensor([[-0.2990, -0.5302, -0.1668, -0.2569]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3096, -0.3331, -0.1720, -0.5972]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 79 ] state=tensor([[-0.3096, -0.3331, -0.1720, -0.5972]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3163, -0.5255, -0.1839, -0.3633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 80 ] state=tensor([[-0.3163, -0.5255, -0.1839, -0.3633]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3268, -0.3283, -0.1912, -0.7078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 81 ] state=tensor([[-0.3268, -0.3283, -0.1912, -0.7078]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3334, -0.5203, -0.2053, -0.4809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 93 ][ timestamp 82 ] state=tensor([[-0.3334, -0.5203, -0.2053, -0.4809]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 93: Exploration_rate=0.05. Score=82.\n",
      "[ episode 94 ] state=tensor([[ 0.0041, -0.0354, -0.0476,  0.0111]])\n",
      "[ episode 94 ][ timestamp 1 ] state=tensor([[ 0.0041, -0.0354, -0.0476,  0.0111]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0034, -0.2299, -0.0473,  0.2884]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 2 ] state=tensor([[ 0.0034, -0.2299, -0.0473,  0.2884]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0012, -0.4243, -0.0416,  0.5658]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 3 ] state=tensor([[-0.0012, -0.4243, -0.0416,  0.5658]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0097, -0.2286, -0.0303,  0.2603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 4 ] state=tensor([[-0.0097, -0.2286, -0.0303,  0.2603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0142, -0.4233, -0.0251,  0.5433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 5 ] state=tensor([[-0.0142, -0.4233, -0.0251,  0.5433]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0227, -0.2278, -0.0142,  0.2428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 6 ] state=tensor([[-0.0227, -0.2278, -0.0142,  0.2428]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0273, -0.0325, -0.0093, -0.0543]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 7 ] state=tensor([[-0.0273, -0.0325, -0.0093, -0.0543]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0279, -0.2275, -0.0104,  0.2354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 8 ] state=tensor([[-0.0279, -0.2275, -0.0104,  0.2354]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0324, -0.4224, -0.0057,  0.5248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 9 ] state=tensor([[-0.0324, -0.4224, -0.0057,  0.5248]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0409, -0.2272,  0.0048,  0.2303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 10 ] state=tensor([[-0.0409, -0.2272,  0.0048,  0.2303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0454, -0.4224,  0.0094,  0.5245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 11 ] state=tensor([[-0.0454, -0.4224,  0.0094,  0.5245]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0539, -0.2274,  0.0199,  0.2348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 12 ] state=tensor([[-0.0539, -0.2274,  0.0199,  0.2348]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0584, -0.4228,  0.0246,  0.5337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 13 ] state=tensor([[-0.0584, -0.4228,  0.0246,  0.5337]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0669, -0.2281,  0.0353,  0.2488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 14 ] state=tensor([[-0.0669, -0.2281,  0.0353,  0.2488]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0715, -0.4237,  0.0402,  0.5524]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 15 ] state=tensor([[-0.0715, -0.4237,  0.0402,  0.5524]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0799, -0.2291,  0.0513,  0.2727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 16 ] state=tensor([[-0.0799, -0.2291,  0.0513,  0.2727]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0845, -0.4250,  0.0567,  0.5811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 17 ] state=tensor([[-0.0845, -0.4250,  0.0567,  0.5811]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0930, -0.2307,  0.0684,  0.3068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 18 ] state=tensor([[-0.0930, -0.2307,  0.0684,  0.3068]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0976, -0.4267,  0.0745,  0.6202]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 19 ] state=tensor([[-0.0976, -0.4267,  0.0745,  0.6202]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1062, -0.2327,  0.0869,  0.3519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 20 ] state=tensor([[-0.1062, -0.2327,  0.0869,  0.3519]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1108, -0.0389,  0.0939,  0.0878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 21 ] state=tensor([[-0.1108, -0.0389,  0.0939,  0.0878]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1116,  0.1548,  0.0957, -0.1738]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 22 ] state=tensor([[-0.1116,  0.1548,  0.0957, -0.1738]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1085,  0.3484,  0.0922, -0.4348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 23 ] state=tensor([[-0.1085,  0.3484,  0.0922, -0.4348]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1015,  0.5421,  0.0835, -0.6971]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 94 ][ timestamp 24 ] state=tensor([[-0.1015,  0.5421,  0.0835, -0.6971]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0907,  0.7360,  0.0696, -0.9623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 25 ] state=tensor([[-0.0907,  0.7360,  0.0696, -0.9623]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0760,  0.9301,  0.0503, -1.2324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 26 ] state=tensor([[-0.0760,  0.9301,  0.0503, -1.2324]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0574,  1.1245,  0.0257, -1.5089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 27 ] state=tensor([[-0.0574,  1.1245,  0.0257, -1.5089]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0349,  1.3193, -0.0045, -1.7934]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 28 ] state=tensor([[-0.0349,  1.3193, -0.0045, -1.7934]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0085,  1.5145, -0.0404, -2.0875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 29 ] state=tensor([[-0.0085,  1.5145, -0.0404, -2.0875]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0218,  1.3198, -0.0821, -1.8076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 30 ] state=tensor([[ 0.0218,  1.3198, -0.0821, -1.8076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0482,  1.1257, -0.1183, -1.5415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 31 ] state=tensor([[ 0.0482,  1.1257, -0.1183, -1.5415]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0707,  0.9322, -0.1491, -1.2879]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 32 ] state=tensor([[ 0.0707,  0.9322, -0.1491, -1.2879]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0894,  0.7392, -0.1749, -1.0454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 33 ] state=tensor([[ 0.0894,  0.7392, -0.1749, -1.0454]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1041,  0.5468, -0.1958, -0.8123]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 94 ][ timestamp 34 ] state=tensor([[ 0.1041,  0.5468, -0.1958, -0.8123]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 94: Exploration_rate=0.05. Score=34.\n",
      "[ episode 95 ] state=tensor([[ 0.0267, -0.0275, -0.0162, -0.0210]])\n",
      "[ episode 95 ][ timestamp 1 ] state=tensor([[ 0.0267, -0.0275, -0.0162, -0.0210]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0261, -0.2224, -0.0166,  0.2665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 2 ] state=tensor([[ 0.0261, -0.2224, -0.0166,  0.2665]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0217, -0.4173, -0.0112,  0.5539]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 3 ] state=tensor([[ 0.0217, -0.4173, -0.0112,  0.5539]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 1.3305e-02, -2.2203e-01, -1.6462e-04,  2.5772e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 4 ] state=tensor([[ 1.3305e-02, -2.2203e-01, -1.6462e-04,  2.5772e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0089, -0.4171,  0.0050,  0.5503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 5 ] state=tensor([[ 0.0089, -0.4171,  0.0050,  0.5503]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0005, -0.2221,  0.0160,  0.2592]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 6 ] state=tensor([[ 0.0005, -0.2221,  0.0160,  0.2592]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0039, -0.4174,  0.0212,  0.5569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 7 ] state=tensor([[-0.0039, -0.4174,  0.0212,  0.5569]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0123, -0.2226,  0.0323,  0.2710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 8 ] state=tensor([[-0.0123, -0.2226,  0.0323,  0.2710]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0167, -0.4182,  0.0377,  0.5737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 9 ] state=tensor([[-0.0167, -0.4182,  0.0377,  0.5737]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0251, -0.2236,  0.0492,  0.2931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 10 ] state=tensor([[-0.0251, -0.2236,  0.0492,  0.2931]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0296, -0.4194,  0.0551,  0.6009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 11 ] state=tensor([[-0.0296, -0.4194,  0.0551,  0.6009]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0379, -0.2251,  0.0671,  0.3261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 12 ] state=tensor([[-0.0379, -0.2251,  0.0671,  0.3261]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0424, -0.4211,  0.0736,  0.6391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 13 ] state=tensor([[-0.0424, -0.4211,  0.0736,  0.6391]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0509, -0.2271,  0.0864,  0.3705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 14 ] state=tensor([[-0.0509, -0.2271,  0.0864,  0.3705]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0554, -0.0333,  0.0938,  0.1063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 15 ] state=tensor([[-0.0554, -0.0333,  0.0938,  0.1063]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0561,  0.1604,  0.0959, -0.1554]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 16 ] state=tensor([[-0.0561,  0.1604,  0.0959, -0.1554]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0529,  0.3540,  0.0928, -0.4163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 17 ] state=tensor([[-0.0529,  0.3540,  0.0928, -0.4163]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0458,  0.5477,  0.0845, -0.6784]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 18 ] state=tensor([[-0.0458,  0.5477,  0.0845, -0.6784]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0348,  0.7415,  0.0709, -0.9433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 19 ] state=tensor([[-0.0348,  0.7415,  0.0709, -0.9433]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0200,  0.9356,  0.0521, -1.2129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 20 ] state=tensor([[-0.0200,  0.9356,  0.0521, -1.2129]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2925e-03,  1.1301e+00,  2.7811e-02, -1.4888e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 21 ] state=tensor([[-1.2925e-03,  1.1301e+00,  2.7811e-02, -1.4888e+00]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0213,  1.3248, -0.0020, -1.7727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 22 ] state=tensor([[ 0.0213,  1.3248, -0.0020, -1.7727]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0478,  1.5200, -0.0374, -2.0660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 23 ] state=tensor([[ 0.0478,  1.5200, -0.0374, -2.0660]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0782,  1.3252, -0.0787, -1.7851]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 24 ] state=tensor([[ 0.0782,  1.3252, -0.0787, -1.7851]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1047,  1.1311, -0.1144, -1.5179]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 25 ] state=tensor([[ 0.1047,  1.1311, -0.1144, -1.5179]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1273,  0.9375, -0.1448, -1.2630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 26 ] state=tensor([[ 0.1273,  0.9375, -0.1448, -1.2630]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1461,  0.7445, -0.1701, -1.0189]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 27 ] state=tensor([[ 0.1461,  0.7445, -0.1701, -1.0189]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1610,  0.5520, -0.1904, -0.7841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 28 ] state=tensor([[ 0.1610,  0.5520, -0.1904, -0.7841]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1720,  0.3600, -0.2061, -0.5569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 95 ][ timestamp 29 ] state=tensor([[ 0.1720,  0.3600, -0.2061, -0.5569]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 95: Exploration_rate=0.05. Score=29.\n",
      "[ episode 96 ] state=tensor([[-0.0027, -0.0485, -0.0220, -0.0301]])\n",
      "[ episode 96 ][ timestamp 1 ] state=tensor([[-0.0027, -0.0485, -0.0220, -0.0301]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0036, -0.2433, -0.0226,  0.2556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 2 ] state=tensor([[-0.0036, -0.2433, -0.0226,  0.2556]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0085, -0.4381, -0.0175,  0.5410]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 3 ] state=tensor([[-0.0085, -0.4381, -0.0175,  0.5410]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0173, -0.2428, -0.0067,  0.2429]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 4 ] state=tensor([[-0.0173, -0.2428, -0.0067,  0.2429]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0221, -0.4378, -0.0018,  0.5335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 5 ] state=tensor([[-0.0221, -0.4378, -0.0018,  0.5335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0309, -0.2426,  0.0089,  0.2402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 6 ] state=tensor([[-0.0309, -0.2426,  0.0089,  0.2402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0357, -0.4379,  0.0137,  0.5357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 7 ] state=tensor([[-0.0357, -0.4379,  0.0137,  0.5357]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0445, -0.2430,  0.0244,  0.2473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 8 ] state=tensor([[-0.0445, -0.2430,  0.0244,  0.2473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0494, -0.4384,  0.0293,  0.5476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 9 ] state=tensor([[-0.0494, -0.4384,  0.0293,  0.5476]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0581, -0.2437,  0.0403,  0.2643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 10 ] state=tensor([[-0.0581, -0.2437,  0.0403,  0.2643]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0630, -0.4394,  0.0456,  0.5694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 11 ] state=tensor([[-0.0630, -0.4394,  0.0456,  0.5694]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0718, -0.6351,  0.0570,  0.8761]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 12 ] state=tensor([[-0.0718, -0.6351,  0.0570,  0.8761]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0845, -0.4408,  0.0745,  0.6019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 13 ] state=tensor([[-0.0845, -0.4408,  0.0745,  0.6019]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0933, -0.2468,  0.0865,  0.3335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 14 ] state=tensor([[-0.0933, -0.2468,  0.0865,  0.3335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0982, -0.0530,  0.0932,  0.0693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 15 ] state=tensor([[-0.0982, -0.0530,  0.0932,  0.0693]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0993,  0.1406,  0.0946, -0.1926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 16 ] state=tensor([[-0.0993,  0.1406,  0.0946, -0.1926]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0965,  0.3343,  0.0907, -0.4540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 17 ] state=tensor([[-0.0965,  0.3343,  0.0907, -0.4540]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0898,  0.5280,  0.0816, -0.7167]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 18 ] state=tensor([[-0.0898,  0.5280,  0.0816, -0.7167]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0792,  0.7219,  0.0673, -0.9827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 19 ] state=tensor([[-0.0792,  0.7219,  0.0673, -0.9827]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0648,  0.5260,  0.0476, -0.6696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 20 ] state=tensor([[-0.0648,  0.5260,  0.0476, -0.6696]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0543,  0.7204,  0.0343, -0.9469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 21 ] state=tensor([[-0.0543,  0.7204,  0.0343, -0.9469]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0399,  0.9150,  0.0153, -1.2287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 22 ] state=tensor([[-0.0399,  0.9150,  0.0153, -1.2287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0216,  1.1100, -0.0093, -1.5165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 23 ] state=tensor([[-0.0216,  1.1100, -0.0093, -1.5165]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 6.2103e-04,  1.3052e+00, -3.9586e-02, -1.8121e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 24 ] state=tensor([[ 6.2103e-04,  1.3052e+00, -3.9586e-02, -1.8121e+00]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0267,  1.1105, -0.0758, -1.5319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 25 ] state=tensor([[ 0.0267,  1.1105, -0.0758, -1.5319]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0489,  0.9164, -0.1065, -1.2638]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 96 ][ timestamp 26 ] state=tensor([[ 0.0489,  0.9164, -0.1065, -1.2638]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0673,  0.7228, -0.1317, -1.0063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 27 ] state=tensor([[ 0.0673,  0.7228, -0.1317, -1.0063]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0817,  0.5297, -0.1519, -0.7577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 28 ] state=tensor([[ 0.0817,  0.5297, -0.1519, -0.7577]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0923,  0.3369, -0.1670, -0.5164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 29 ] state=tensor([[ 0.0923,  0.3369, -0.1670, -0.5164]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0991,  0.1445, -0.1774, -0.2807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 30 ] state=tensor([[ 0.0991,  0.1445, -0.1774, -0.2807]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1019, -0.0477, -0.1830, -0.0488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 31 ] state=tensor([[ 0.1019, -0.0477, -0.1830, -0.0488]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1010,  0.1495, -0.1839, -0.3931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 32 ] state=tensor([[ 0.1010,  0.1495, -0.1839, -0.3931]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1040, -0.0426, -0.1918, -0.1636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 33 ] state=tensor([[ 0.1040, -0.0426, -0.1918, -0.1636]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1031, -0.2345, -0.1951,  0.0630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 34 ] state=tensor([[ 0.1031, -0.2345, -0.1951,  0.0630]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0984, -0.4264, -0.1938,  0.2883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 35 ] state=tensor([[ 0.0984, -0.4264, -0.1938,  0.2883]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0899, -0.6183, -0.1881,  0.5142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 36 ] state=tensor([[ 0.0899, -0.6183, -0.1881,  0.5142]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0775, -0.4211, -0.1778,  0.1686]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 37 ] state=tensor([[ 0.0775, -0.4211, -0.1778,  0.1686]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0691, -0.6133, -0.1744,  0.4004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 38 ] state=tensor([[ 0.0691, -0.6133, -0.1744,  0.4004]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0569, -0.4162, -0.1664,  0.0582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 39 ] state=tensor([[ 0.0569, -0.4162, -0.1664,  0.0582]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0485, -0.6086, -0.1652,  0.2941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 40 ] state=tensor([[ 0.0485, -0.6086, -0.1652,  0.2941]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0364, -0.4115, -0.1593, -0.0458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 41 ] state=tensor([[ 0.0364, -0.4115, -0.1593, -0.0458]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0281, -0.6041, -0.1603,  0.1927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 42 ] state=tensor([[ 0.0281, -0.6041, -0.1603,  0.1927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0160, -0.4070, -0.1564, -0.1460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 43 ] state=tensor([[ 0.0160, -0.4070, -0.1564, -0.1460]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0079, -0.5996, -0.1593,  0.0936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 44 ] state=tensor([[ 0.0079, -0.5996, -0.1593,  0.0936]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0041, -0.4026, -0.1575, -0.2449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 45 ] state=tensor([[-0.0041, -0.4026, -0.1575, -0.2449]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0121, -0.5952, -0.1624, -0.0057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 46 ] state=tensor([[-0.0121, -0.5952, -0.1624, -0.0057]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0240, -0.7876, -0.1625,  0.2317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 47 ] state=tensor([[-0.0240, -0.7876, -0.1625,  0.2317]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0398, -0.5906, -0.1578, -0.1075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 48 ] state=tensor([[-0.0398, -0.5906, -0.1578, -0.1075]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0516, -0.3936, -0.1600, -0.4455]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 49 ] state=tensor([[-0.0516, -0.3936, -0.1600, -0.4455]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0595, -0.5862, -0.1689, -0.2072]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 50 ] state=tensor([[-0.0595, -0.5862, -0.1689, -0.2072]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0712, -0.3891, -0.1730, -0.5481]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 51 ] state=tensor([[-0.0712, -0.3891, -0.1730, -0.5481]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0790, -0.5814, -0.1840, -0.3145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 52 ] state=tensor([[-0.0790, -0.5814, -0.1840, -0.3145]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0906, -0.3842, -0.1903, -0.6591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 53 ] state=tensor([[-0.0906, -0.3842, -0.1903, -0.6591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0983, -0.5762, -0.2035, -0.4319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 96 ][ timestamp 54 ] state=tensor([[-0.0983, -0.5762, -0.2035, -0.4319]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 96: Exploration_rate=0.05. Score=54.\n",
      "[ episode 97 ] state=tensor([[ 0.0201, -0.0101,  0.0277, -0.0116]])\n",
      "[ episode 97 ][ timestamp 1 ] state=tensor([[ 0.0201, -0.0101,  0.0277, -0.0116]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0199, -0.2056,  0.0274,  0.2896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 2 ] state=tensor([[ 0.0199, -0.2056,  0.0274,  0.2896]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0158, -0.4011,  0.0332,  0.5909]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 3 ] state=tensor([[ 0.0158, -0.4011,  0.0332,  0.5909]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0077, -0.2064,  0.0450,  0.3088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 4 ] state=tensor([[ 0.0077, -0.2064,  0.0450,  0.3088]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0036, -0.4022,  0.0512,  0.6153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 5 ] state=tensor([[ 0.0036, -0.4022,  0.0512,  0.6153]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0044, -0.2078,  0.0635,  0.3392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 6 ] state=tensor([[-0.0044, -0.2078,  0.0635,  0.3392]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0086, -0.4038,  0.0703,  0.6512]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 7 ] state=tensor([[-0.0086, -0.4038,  0.0703,  0.6512]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0167, -0.2097,  0.0833,  0.3815]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 8 ] state=tensor([[-0.0167, -0.2097,  0.0833,  0.3815]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0209, -0.4059,  0.0910,  0.6992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 9 ] state=tensor([[-0.0209, -0.4059,  0.0910,  0.6992]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0290, -0.2121,  0.1049,  0.4365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 10 ] state=tensor([[-0.0290, -0.2121,  0.1049,  0.4365]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0332, -0.0187,  0.1137,  0.1787]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 11 ] state=tensor([[-0.0332, -0.0187,  0.1137,  0.1787]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0336,  0.1747,  0.1172, -0.0761]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 12 ] state=tensor([[-0.0336,  0.1747,  0.1172, -0.0761]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0301,  0.3679,  0.1157, -0.3296]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 13 ] state=tensor([[-0.0301,  0.3679,  0.1157, -0.3296]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0227,  0.5612,  0.1091, -0.5837]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 14 ] state=tensor([[-0.0227,  0.5612,  0.1091, -0.5837]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0115,  0.7547,  0.0975, -0.8401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 15 ] state=tensor([[-0.0115,  0.7547,  0.0975, -0.8401]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0036,  0.9483,  0.0807, -1.1006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 16 ] state=tensor([[ 0.0036,  0.9483,  0.0807, -1.1006]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0226,  1.1423,  0.0586, -1.3669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 17 ] state=tensor([[ 0.0226,  1.1423,  0.0586, -1.3669]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0454,  1.3367,  0.0313, -1.6407]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 18 ] state=tensor([[ 0.0454,  1.3367,  0.0313, -1.6407]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 7.2132e-02,  1.5314e+00, -1.5131e-03, -1.9235e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 19 ] state=tensor([[ 7.2132e-02,  1.5314e+00, -1.5131e-03, -1.9235e+00]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1028,  1.3363, -0.0400, -1.6313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 20 ] state=tensor([[ 0.1028,  1.3363, -0.0400, -1.6313]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1295,  1.1417, -0.0726, -1.3513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 21 ] state=tensor([[ 0.1295,  1.1417, -0.0726, -1.3513]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1523,  0.9475, -0.0996, -1.0822]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 22 ] state=tensor([[ 0.1523,  0.9475, -0.0996, -1.0822]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1713,  0.7538, -0.1213, -0.8224]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 23 ] state=tensor([[ 0.1713,  0.7538, -0.1213, -0.8224]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1863,  0.5606, -0.1377, -0.5702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 24 ] state=tensor([[ 0.1863,  0.5606, -0.1377, -0.5702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1976,  0.3676, -0.1491, -0.3238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 25 ] state=tensor([[ 0.1976,  0.3676, -0.1491, -0.3238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2049,  0.1749, -0.1556, -0.0816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 26 ] state=tensor([[ 0.2049,  0.1749, -0.1556, -0.0816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2084, -0.0177, -0.1572,  0.1582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 27 ] state=tensor([[ 0.2084, -0.0177, -0.1572,  0.1582]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2081, -0.2102, -0.1541,  0.3974]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 28 ] state=tensor([[ 0.2081, -0.2102, -0.1541,  0.3974]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2039, -0.4029, -0.1461,  0.6378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 29 ] state=tensor([[ 0.2039, -0.4029, -0.1461,  0.6378]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1958, -0.5957, -0.1334,  0.8812]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 30 ] state=tensor([[ 0.1958, -0.5957, -0.1334,  0.8812]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1839, -0.3990, -0.1157,  0.5497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 31 ] state=tensor([[ 0.1839, -0.3990, -0.1157,  0.5497]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1759, -0.5924, -0.1048,  0.8038]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 32 ] state=tensor([[ 0.1759, -0.5924, -0.1048,  0.8038]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1641, -0.3960, -0.0887,  0.4801]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 33 ] state=tensor([[ 0.1641, -0.3960, -0.0887,  0.4801]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1561, -0.1997, -0.0791,  0.1608]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 34 ] state=tensor([[ 0.1561, -0.1997, -0.0791,  0.1608]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1521, -0.3936, -0.0759,  0.4276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 35 ] state=tensor([[ 0.1521, -0.3936, -0.0759,  0.4276]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1443, -0.5876, -0.0673,  0.6954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 36 ] state=tensor([[ 0.1443, -0.5876, -0.0673,  0.6954]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1325, -0.3916, -0.0534,  0.3823]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 97 ][ timestamp 37 ] state=tensor([[ 0.1325, -0.3916, -0.0534,  0.3823]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1247, -0.5859, -0.0458,  0.6577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 38 ] state=tensor([[ 0.1247, -0.5859, -0.0458,  0.6577]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1130, -0.7804, -0.0326,  0.9356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 39 ] state=tensor([[ 0.1130, -0.7804, -0.0326,  0.9356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0974, -0.5848, -0.0139,  0.6329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 40 ] state=tensor([[ 0.0974, -0.5848, -0.0139,  0.6329]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0857, -0.3895, -0.0012,  0.3358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 41 ] state=tensor([[ 0.0857, -0.3895, -0.0012,  0.3358]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0779, -0.5846,  0.0055,  0.6281]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 42 ] state=tensor([[ 0.0779, -0.5846,  0.0055,  0.6281]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0662, -0.3896,  0.0181,  0.3372]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 43 ] state=tensor([[ 0.0662, -0.3896,  0.0181,  0.3372]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0584, -0.5850,  0.0248,  0.6355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 44 ] state=tensor([[ 0.0584, -0.5850,  0.0248,  0.6355]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0467, -0.3902,  0.0375,  0.3507]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 45 ] state=tensor([[ 0.0467, -0.3902,  0.0375,  0.3507]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0389, -0.5858,  0.0445,  0.6550]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 46 ] state=tensor([[ 0.0389, -0.5858,  0.0445,  0.6550]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0272, -0.3914,  0.0576,  0.3767]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 47 ] state=tensor([[ 0.0272, -0.3914,  0.0576,  0.3767]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0193, -0.5872,  0.0652,  0.6870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 48 ] state=tensor([[ 0.0193, -0.5872,  0.0652,  0.6870]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0076, -0.3931,  0.0789,  0.4155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 49 ] state=tensor([[ 0.0076, -0.3931,  0.0789,  0.4155]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.7147e-04, -5.8923e-01,  8.7202e-02,  7.3195e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 50 ] state=tensor([[-2.7147e-04, -5.8923e-01,  8.7202e-02,  7.3195e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0121, -0.3954,  0.1018,  0.4679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 51 ] state=tensor([[-0.0121, -0.3954,  0.1018,  0.4679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0200, -0.2019,  0.1112,  0.2090]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 52 ] state=tensor([[-0.0200, -0.2019,  0.1112,  0.2090]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0240, -0.3984,  0.1154,  0.5346]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 53 ] state=tensor([[-0.0240, -0.3984,  0.1154,  0.5346]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0320, -0.5949,  0.1261,  0.8613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 54 ] state=tensor([[-0.0320, -0.5949,  0.1261,  0.8613]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0439, -0.4017,  0.1433,  0.6108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 55 ] state=tensor([[-0.0439, -0.4017,  0.1433,  0.6108]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0519, -0.2089,  0.1555,  0.3664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 56 ] state=tensor([[-0.0519, -0.2089,  0.1555,  0.3664]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0561, -0.4058,  0.1628,  0.7038]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 57 ] state=tensor([[-0.0561, -0.4058,  0.1628,  0.7038]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0642, -0.2133,  0.1769,  0.4665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 58 ] state=tensor([[-0.0642, -0.2133,  0.1769,  0.4665]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0685, -0.0210,  0.1862,  0.2344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 59 ] state=tensor([[-0.0685, -0.0210,  0.1862,  0.2344]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0689,  0.1710,  0.1909,  0.0057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 60 ] state=tensor([[-0.0689,  0.1710,  0.1909,  0.0057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0655,  0.3629,  0.1911, -0.2211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 61 ] state=tensor([[-0.0655,  0.3629,  0.1911, -0.2211]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0582,  0.5549,  0.1866, -0.4480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 62 ] state=tensor([[-0.0582,  0.5549,  0.1866, -0.4480]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0471,  0.7469,  0.1777, -0.6765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 63 ] state=tensor([[-0.0471,  0.7469,  0.1777, -0.6765]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0322,  0.9392,  0.1641, -0.9084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 64 ] state=tensor([[-0.0322,  0.9392,  0.1641, -0.9084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0134,  1.1318,  0.1460, -1.1454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 65 ] state=tensor([[-0.0134,  1.1318,  0.1460, -1.1454]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0093,  1.3247,  0.1231, -1.3889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 66 ] state=tensor([[ 0.0093,  1.3247,  0.1231, -1.3889]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0357,  1.5181,  0.0953, -1.6408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 67 ] state=tensor([[ 0.0357,  1.5181,  0.0953, -1.6408]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0661,  1.7120,  0.0625, -1.9023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 68 ] state=tensor([[ 0.0661,  1.7120,  0.0625, -1.9023]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1003,  1.9064,  0.0244, -2.1750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 69 ] state=tensor([[ 0.1003,  1.9064,  0.0244, -2.1750]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1385,  1.7110, -0.0191, -1.8748]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 70 ] state=tensor([[ 0.1385,  1.7110, -0.0191, -1.8748]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1727,  1.5161, -0.0566, -1.5881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 71 ] state=tensor([[ 0.1727,  1.5161, -0.0566, -1.5881]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2030,  1.3217, -0.0883, -1.3136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 72 ] state=tensor([[ 0.2030,  1.3217, -0.0883, -1.3136]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2295,  1.1278, -0.1146, -1.0498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 73 ] state=tensor([[ 0.2295,  1.1278, -0.1146, -1.0498]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2520,  0.9344, -0.1356, -0.7952]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 74 ] state=tensor([[ 0.2520,  0.9344, -0.1356, -0.7952]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2707,  0.7414, -0.1515, -0.5481]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 75 ] state=tensor([[ 0.2707,  0.7414, -0.1515, -0.5481]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2855,  0.5487, -0.1625, -0.3067]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 76 ] state=tensor([[ 0.2855,  0.5487, -0.1625, -0.3067]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2965,  0.3562, -0.1686, -0.0694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 77 ] state=tensor([[ 0.2965,  0.3562, -0.1686, -0.0694]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.3036,  0.1638, -0.1700,  0.1657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 78 ] state=tensor([[ 0.3036,  0.1638, -0.1700,  0.1657]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.3069, -0.0285, -0.1667,  0.4003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 79 ] state=tensor([[ 0.3069, -0.0285, -0.1667,  0.4003]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.3063, -0.2209, -0.1587,  0.6362]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 80 ] state=tensor([[ 0.3063, -0.2209, -0.1587,  0.6362]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.3019, -0.4135, -0.1460,  0.8750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 81 ] state=tensor([[ 0.3019, -0.4135, -0.1460,  0.8750]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.2936, -0.2167, -0.1285,  0.5402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 82 ] state=tensor([[ 0.2936, -0.2167, -0.1285,  0.5402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2893, -0.4098, -0.1176,  0.7898]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 83 ] state=tensor([[ 0.2893, -0.4098, -0.1176,  0.7898]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.2811, -0.2133, -0.1018,  0.4626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 84 ] state=tensor([[ 0.2811, -0.2133, -0.1018,  0.4626]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2768, -0.4069, -0.0926,  0.7215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 85 ] state=tensor([[ 0.2768, -0.4069, -0.0926,  0.7215]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.2687, -0.2106, -0.0782,  0.4012]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 86 ] state=tensor([[ 0.2687, -0.2106, -0.0782,  0.4012]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2645, -0.4045, -0.0701,  0.6682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 87 ] state=tensor([[ 0.2645, -0.4045, -0.0701,  0.6682]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.2564, -0.2085, -0.0568,  0.3543]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 88 ] state=tensor([[ 0.2564, -0.2085, -0.0568,  0.3543]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2522, -0.4028, -0.0497,  0.6285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 89 ] state=tensor([[ 0.2522, -0.4028, -0.0497,  0.6285]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2442, -0.5972, -0.0371,  0.9052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 90 ] state=tensor([[ 0.2442, -0.5972, -0.0371,  0.9052]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.2322, -0.4016, -0.0190,  0.6011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 91 ] state=tensor([[ 0.2322, -0.4016, -0.0190,  0.6011]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2242, -0.5964, -0.0070,  0.8877]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 92 ] state=tensor([[ 0.2242, -0.5964, -0.0070,  0.8877]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.2123, -0.4012,  0.0108,  0.5928]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 93 ] state=tensor([[ 0.2123, -0.4012,  0.0108,  0.5928]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.2043, -0.5965,  0.0226,  0.8889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 94 ] state=tensor([[ 0.2043, -0.5965,  0.0226,  0.8889]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1923, -0.4017,  0.0404,  0.6034]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 95 ] state=tensor([[ 0.1923, -0.4017,  0.0404,  0.6034]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1843, -0.5973,  0.0525,  0.9085]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 96 ] state=tensor([[ 0.1843, -0.5973,  0.0525,  0.9085]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1723, -0.7931,  0.0706,  1.2172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 97 ] state=tensor([[ 0.1723, -0.7931,  0.0706,  1.2172]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1565, -0.5990,  0.0950,  0.9475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 98 ] state=tensor([[ 0.1565, -0.5990,  0.0950,  0.9475]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1445, -0.4052,  0.1139,  0.6861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 99 ] state=tensor([[ 0.1445, -0.4052,  0.1139,  0.6861]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1364, -0.2119,  0.1276,  0.4313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 100 ] state=tensor([[ 0.1364, -0.2119,  0.1276,  0.4313]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1322, -0.4085,  0.1363,  0.7613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 101 ] state=tensor([[ 0.1322, -0.4085,  0.1363,  0.7613]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1240, -0.6053,  0.1515,  1.0936]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 97 ][ timestamp 102 ] state=tensor([[ 0.1240, -0.6053,  0.1515,  1.0936]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1119, -0.4124,  0.1734,  0.8520]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 103 ] state=tensor([[ 0.1119, -0.4124,  0.1734,  0.8520]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1036, -0.6094,  0.1904,  1.1938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 97 ][ timestamp 104 ] state=tensor([[ 0.1036, -0.6094,  0.1904,  1.1938]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 97: Exploration_rate=0.05. Score=104.\n",
      "[ episode 98 ] state=tensor([[0.0404, 0.0324, 0.0393, 0.0007]])\n",
      "[ episode 98 ][ timestamp 1 ] state=tensor([[0.0404, 0.0324, 0.0393, 0.0007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0411, -0.1633,  0.0393,  0.3056]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 2 ] state=tensor([[ 0.0411, -0.1633,  0.0393,  0.3056]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0378, -0.3589,  0.0454,  0.6104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 3 ] state=tensor([[ 0.0378, -0.3589,  0.0454,  0.6104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0306, -0.1645,  0.0576,  0.3323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 4 ] state=tensor([[ 0.0306, -0.1645,  0.0576,  0.3323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[0.0273, 0.0298, 0.0643, 0.0584]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 5 ] state=tensor([[0.0273, 0.0298, 0.0643, 0.0584]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0279,  0.2239,  0.0654, -0.2134]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 6 ] state=tensor([[ 0.0279,  0.2239,  0.0654, -0.2134]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0324,  0.4180,  0.0612, -0.4847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 7 ] state=tensor([[ 0.0324,  0.4180,  0.0612, -0.4847]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0408,  0.6122,  0.0515, -0.7575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 8 ] state=tensor([[ 0.0408,  0.6122,  0.0515, -0.7575]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0530,  0.8066,  0.0363, -1.0335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 9 ] state=tensor([[ 0.0530,  0.8066,  0.0363, -1.0335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0691,  1.0012,  0.0157, -1.3146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 10 ] state=tensor([[ 0.0691,  1.0012,  0.0157, -1.3146]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0892,  0.8059, -0.0106, -1.0170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 11 ] state=tensor([[ 0.0892,  0.8059, -0.0106, -1.0170]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1053,  0.6109, -0.0310, -0.7277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 12 ] state=tensor([[ 0.1053,  0.6109, -0.0310, -0.7277]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1175,  0.4163, -0.0455, -0.4449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 13 ] state=tensor([[ 0.1175,  0.4163, -0.0455, -0.4449]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1258,  0.2218, -0.0544, -0.1669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 14 ] state=tensor([[ 0.1258,  0.2218, -0.0544, -0.1669]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1303,  0.0275, -0.0578,  0.1081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 15 ] state=tensor([[ 0.1303,  0.0275, -0.0578,  0.1081]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1308, -0.1667, -0.0556,  0.3820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 16 ] state=tensor([[ 0.1308, -0.1667, -0.0556,  0.3820]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1275, -0.3610, -0.0480,  0.6566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 17 ] state=tensor([[ 0.1275, -0.3610, -0.0480,  0.6566]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1203, -0.1653, -0.0348,  0.3493]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 18 ] state=tensor([[ 0.1203, -0.1653, -0.0348,  0.3493]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1170, -0.3599, -0.0278,  0.6308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 19 ] state=tensor([[ 0.1170, -0.3599, -0.0278,  0.6308]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.1098, -0.1644, -0.0152,  0.3294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 20 ] state=tensor([[ 0.1098, -0.1644, -0.0152,  0.3294]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1065, -0.3593, -0.0086,  0.6173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 21 ] state=tensor([[ 0.1065, -0.3593, -0.0086,  0.6173]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0993, -0.1640,  0.0037,  0.3219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 22 ] state=tensor([[ 0.0993, -0.1640,  0.0037,  0.3219]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0960, -0.3592,  0.0102,  0.6157]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 23 ] state=tensor([[ 0.0960, -0.3592,  0.0102,  0.6157]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0888, -0.1642,  0.0225,  0.3263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 24 ] state=tensor([[ 0.0888, -0.1642,  0.0225,  0.3263]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0855, -0.3597,  0.0290,  0.6260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 25 ] state=tensor([[ 0.0855, -0.3597,  0.0290,  0.6260]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0783, -0.1650,  0.0415,  0.3425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 26 ] state=tensor([[ 0.0783, -0.1650,  0.0415,  0.3425]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[0.0750, 0.0295, 0.0484, 0.0632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 27 ] state=tensor([[0.0750, 0.0295, 0.0484, 0.0632]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0756,  0.2239,  0.0496, -0.2138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 28 ] state=tensor([[ 0.0756,  0.2239,  0.0496, -0.2138]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[0.0801, 0.0281, 0.0453, 0.0941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 29 ] state=tensor([[0.0801, 0.0281, 0.0453, 0.0941]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0807,  0.2226,  0.0472, -0.1839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 30 ] state=tensor([[ 0.0807,  0.2226,  0.0472, -0.1839]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[0.0851, 0.0268, 0.0436, 0.1233]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 31 ] state=tensor([[0.0851, 0.0268, 0.0436, 0.1233]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0857,  0.2213,  0.0460, -0.1554]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 32 ] state=tensor([[ 0.0857,  0.2213,  0.0460, -0.1554]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[0.0901, 0.0255, 0.0429, 0.1515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 33 ] state=tensor([[0.0901, 0.0255, 0.0429, 0.1515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0906, -0.1702,  0.0459,  0.4574]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 34 ] state=tensor([[ 0.0906, -0.1702,  0.0459,  0.4574]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0872, -0.3659,  0.0551,  0.7642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 35 ] state=tensor([[ 0.0872, -0.3659,  0.0551,  0.7642]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0799, -0.1716,  0.0704,  0.4893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 36 ] state=tensor([[ 0.0799, -0.1716,  0.0704,  0.4893]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0764, -0.3676,  0.0802,  0.8033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 37 ] state=tensor([[ 0.0764, -0.3676,  0.0802,  0.8033]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0691, -0.1737,  0.0962,  0.5369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 38 ] state=tensor([[ 0.0691, -0.1737,  0.0962,  0.5369]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0656, -0.3700,  0.1070,  0.8583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 39 ] state=tensor([[ 0.0656, -0.3700,  0.1070,  0.8583]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0582, -0.1765,  0.1241,  0.6011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 40 ] state=tensor([[ 0.0582, -0.1765,  0.1241,  0.6011]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0547, -0.3731,  0.1362,  0.9301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 41 ] state=tensor([[ 0.0547, -0.3731,  0.1362,  0.9301]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0472, -0.1801,  0.1548,  0.6832]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 42 ] state=tensor([[ 0.0472, -0.1801,  0.1548,  0.6832]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0436, -0.3770,  0.1684,  1.0203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 43 ] state=tensor([[ 0.0436, -0.3770,  0.1684,  1.0203]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0361, -0.1844,  0.1888,  0.7849]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 44 ] state=tensor([[ 0.0361, -0.1844,  0.1888,  0.7849]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0324, -0.3816,  0.2045,  1.1305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 98 ][ timestamp 45 ] state=tensor([[ 0.0324, -0.3816,  0.2045,  1.1305]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 98: Exploration_rate=0.05. Score=45.\n",
      "[ episode 99 ] state=tensor([[-0.0068, -0.0163,  0.0374,  0.0257]])\n",
      "[ episode 99 ][ timestamp 1 ] state=tensor([[-0.0068, -0.0163,  0.0374,  0.0257]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0071, -0.2120,  0.0379,  0.3299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 2 ] state=tensor([[-0.0071, -0.2120,  0.0379,  0.3299]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0114, -0.0174,  0.0445,  0.0494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 3 ] state=tensor([[-0.0114, -0.0174,  0.0445,  0.0494]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0117,  0.1771,  0.0455, -0.2289]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 4 ] state=tensor([[-0.0117,  0.1771,  0.0455, -0.2289]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0082, -0.0187,  0.0409,  0.0778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 5 ] state=tensor([[-0.0082, -0.0187,  0.0409,  0.0778]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0086, -0.2144,  0.0425,  0.3831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 6 ] state=tensor([[-0.0086, -0.2144,  0.0425,  0.3831]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0128, -0.4101,  0.0502,  0.6889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 7 ] state=tensor([[-0.0128, -0.4101,  0.0502,  0.6889]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0210, -0.2157,  0.0639,  0.4124]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 99 ][ timestamp 8 ] state=tensor([[-0.0210, -0.2157,  0.0639,  0.4124]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0254, -0.4116,  0.0722,  0.7246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 9 ] state=tensor([[-0.0254, -0.4116,  0.0722,  0.7246]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0336, -0.2176,  0.0867,  0.4555]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 10 ] state=tensor([[-0.0336, -0.2176,  0.0867,  0.4555]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0379, -0.4138,  0.0958,  0.7742]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 11 ] state=tensor([[-0.0379, -0.4138,  0.0958,  0.7742]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0462, -0.2201,  0.1113,  0.5131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 12 ] state=tensor([[-0.0462, -0.2201,  0.1113,  0.5131]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0506, -0.4166,  0.1215,  0.8387]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 13 ] state=tensor([[-0.0506, -0.4166,  0.1215,  0.8387]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0589, -0.2234,  0.1383,  0.5865]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 14 ] state=tensor([[-0.0589, -0.2234,  0.1383,  0.5865]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0634, -0.4201,  0.1500,  0.9194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 15 ] state=tensor([[-0.0634, -0.4201,  0.1500,  0.9194]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0718, -0.2273,  0.1684,  0.6774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 16 ] state=tensor([[-0.0718, -0.2273,  0.1684,  0.6774]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0764, -0.4243,  0.1820,  1.0180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 17 ] state=tensor([[-0.0764, -0.4243,  0.1820,  1.0180]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0849, -0.2320,  0.2023,  0.7875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 99 ][ timestamp 18 ] state=tensor([[-0.0849, -0.2320,  0.2023,  0.7875]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 99: Exploration_rate=0.05. Score=18.\n",
      "[ episode 100 ] state=tensor([[ 0.0251, -0.0440, -0.0358, -0.0015]])\n",
      "[ episode 100 ][ timestamp 1 ] state=tensor([[ 0.0251, -0.0440, -0.0358, -0.0015]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0242, -0.2386, -0.0358,  0.2796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 2 ] state=tensor([[ 0.0242, -0.2386, -0.0358,  0.2796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0195, -0.4332, -0.0302,  0.5608]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 3 ] state=tensor([[ 0.0195, -0.4332, -0.0302,  0.5608]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0108, -0.6279, -0.0190,  0.8438]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 4 ] state=tensor([[ 0.0108, -0.6279, -0.0190,  0.8438]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0017, -0.4325, -0.0021,  0.5452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 5 ] state=tensor([[-0.0017, -0.4325, -0.0021,  0.5452]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0104, -0.2374,  0.0088,  0.2519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 6 ] state=tensor([[-0.0104, -0.2374,  0.0088,  0.2519]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0151, -0.0424,  0.0138, -0.0380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 7 ] state=tensor([[-0.0151, -0.0424,  0.0138, -0.0380]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0160, -0.2377,  0.0130,  0.2590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 8 ] state=tensor([[-0.0160, -0.2377,  0.0130,  0.2590]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0207, -0.4330,  0.0182,  0.5557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 9 ] state=tensor([[-0.0207, -0.4330,  0.0182,  0.5557]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0294, -0.6284,  0.0293,  0.8541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 10 ] state=tensor([[-0.0294, -0.6284,  0.0293,  0.8541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0420, -0.4337,  0.0464,  0.5708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 11 ] state=tensor([[-0.0420, -0.4337,  0.0464,  0.5708]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0507, -0.2392,  0.0578,  0.2931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 12 ] state=tensor([[-0.0507, -0.2392,  0.0578,  0.2931]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0554, -0.0450,  0.0637,  0.0192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 13 ] state=tensor([[-0.0554, -0.0450,  0.0637,  0.0192]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0563,  0.1492,  0.0641, -0.2527]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 14 ] state=tensor([[-0.0563,  0.1492,  0.0641, -0.2527]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0534,  0.3433,  0.0590, -0.5245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 15 ] state=tensor([[-0.0534,  0.3433,  0.0590, -0.5245]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0465,  0.5376,  0.0485, -0.7980]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 16 ] state=tensor([[-0.0465,  0.5376,  0.0485, -0.7980]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0357,  0.7320,  0.0326, -1.0751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 17 ] state=tensor([[-0.0357,  0.7320,  0.0326, -1.0751]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0211,  0.9267,  0.0111, -1.3573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 18 ] state=tensor([[-0.0211,  0.9267,  0.0111, -1.3573]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0026,  0.7314, -0.0161, -1.0612]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 19 ] state=tensor([[-0.0026,  0.7314, -0.0161, -1.0612]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0121,  0.5365, -0.0373, -0.7736]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 20 ] state=tensor([[ 0.0121,  0.5365, -0.0373, -0.7736]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0228,  0.3419, -0.0528, -0.4929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 21 ] state=tensor([[ 0.0228,  0.3419, -0.0528, -0.4929]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0296,  0.5377, -0.0626, -0.8017]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 22 ] state=tensor([[ 0.0296,  0.5377, -0.0626, -0.8017]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0404,  0.3435, -0.0787, -0.5294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 23 ] state=tensor([[ 0.0404,  0.3435, -0.0787, -0.5294]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0473,  0.1496, -0.0892, -0.2625]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 24 ] state=tensor([[ 0.0473,  0.1496, -0.0892, -0.2625]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0503, -0.0441, -0.0945,  0.0008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 25 ] state=tensor([[ 0.0503, -0.0441, -0.0945,  0.0008]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0494, -0.2378, -0.0945,  0.2622]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 26 ] state=tensor([[ 0.0494, -0.2378, -0.0945,  0.2622]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0446, -0.4315, -0.0892,  0.5236]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 27 ] state=tensor([[ 0.0446, -0.4315, -0.0892,  0.5236]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0360, -0.2352, -0.0788,  0.2042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 28 ] state=tensor([[ 0.0360, -0.2352, -0.0788,  0.2042]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0313, -0.4291, -0.0747,  0.4711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 29 ] state=tensor([[ 0.0313, -0.4291, -0.0747,  0.4711]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0227, -0.2330, -0.0653,  0.1558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 30 ] state=tensor([[ 0.0227, -0.2330, -0.0653,  0.1558]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0180, -0.4271, -0.0621,  0.4272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 31 ] state=tensor([[ 0.0180, -0.4271, -0.0621,  0.4272]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0095, -0.6213, -0.0536,  0.6997]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 32 ] state=tensor([[ 0.0095, -0.6213, -0.0536,  0.6997]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0029, -0.4255, -0.0396,  0.3906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 33 ] state=tensor([[-0.0029, -0.4255, -0.0396,  0.3906]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0114, -0.6200, -0.0318,  0.6705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 34 ] state=tensor([[-0.0114, -0.6200, -0.0318,  0.6705]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0238, -0.4245, -0.0184,  0.3680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 35 ] state=tensor([[-0.0238, -0.4245, -0.0184,  0.3680]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0323, -0.6194, -0.0110,  0.6549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 36 ] state=tensor([[-0.0323, -0.6194, -0.0110,  0.6549]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0447, -0.4241,  0.0021,  0.3587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 37 ] state=tensor([[-0.0447, -0.4241,  0.0021,  0.3587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0532, -0.6192,  0.0092,  0.6521]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 38 ] state=tensor([[-0.0532, -0.6192,  0.0092,  0.6521]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0656, -0.4242,  0.0223,  0.3623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 39 ] state=tensor([[-0.0656, -0.4242,  0.0223,  0.3623]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0741, -0.6197,  0.0295,  0.6619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 40 ] state=tensor([[-0.0741, -0.6197,  0.0295,  0.6619]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0865, -0.4250,  0.0428,  0.3787]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 41 ] state=tensor([[-0.0865, -0.4250,  0.0428,  0.3787]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0950, -0.2305,  0.0503,  0.0998]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 42 ] state=tensor([[-0.0950, -0.2305,  0.0503,  0.0998]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0996, -0.0361,  0.0523, -0.1766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 43 ] state=tensor([[-0.0996, -0.0361,  0.0523, -0.1766]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1003,  0.1582,  0.0488, -0.4523]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 44 ] state=tensor([[-0.1003,  0.1582,  0.0488, -0.4523]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0971,  0.3526,  0.0398, -0.7292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 45 ] state=tensor([[-0.0971,  0.3526,  0.0398, -0.7292]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0901,  0.5472,  0.0252, -1.0091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 46 ] state=tensor([[-0.0901,  0.5472,  0.0252, -1.0091]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0791,  0.7419,  0.0050, -1.2938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 47 ] state=tensor([[-0.0791,  0.7419,  0.0050, -1.2938]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0643,  0.5468, -0.0209, -0.9995]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 48 ] state=tensor([[-0.0643,  0.5468, -0.0209, -0.9995]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0534,  0.3519, -0.0409, -0.7135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 49 ] state=tensor([[-0.0534,  0.3519, -0.0409, -0.7135]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0463,  0.1574, -0.0551, -0.4339]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 50 ] state=tensor([[-0.0463,  0.1574, -0.0551, -0.4339]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0432, -0.0369, -0.0638, -0.1591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 51 ] state=tensor([[-0.0432, -0.0369, -0.0638, -0.1591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0439, -0.2311, -0.0670,  0.1128]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 52 ] state=tensor([[-0.0439, -0.2311, -0.0670,  0.1128]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0485, -0.0350, -0.0647, -0.2003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 53 ] state=tensor([[-0.0485, -0.0350, -0.0647, -0.2003]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0492, -0.2292, -0.0687,  0.0713]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 54 ] state=tensor([[-0.0492, -0.2292, -0.0687,  0.0713]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0538, -0.4233, -0.0673,  0.3415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 55 ] state=tensor([[-0.0538, -0.4233, -0.0673,  0.3415]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0623, -0.6174, -0.0605,  0.6122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 56 ] state=tensor([[-0.0623, -0.6174, -0.0605,  0.6122]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0746, -0.4215, -0.0482,  0.3011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 57 ] state=tensor([[-0.0746, -0.4215, -0.0482,  0.3011]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0831, -0.6159, -0.0422,  0.5782]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 58 ] state=tensor([[-0.0831, -0.6159, -0.0422,  0.5782]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0954, -0.4202, -0.0307,  0.2725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 59 ] state=tensor([[-0.0954, -0.4202, -0.0307,  0.2725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1038, -0.2246, -0.0252, -0.0297]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 60 ] state=tensor([[-0.1038, -0.2246, -0.0252, -0.0297]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1083, -0.0291, -0.0258, -0.3302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 61 ] state=tensor([[-0.1083, -0.0291, -0.0258, -0.3302]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1089, -0.2239, -0.0324, -0.0458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 62 ] state=tensor([[-0.1089, -0.2239, -0.0324, -0.0458]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1133, -0.0283, -0.0333, -0.3485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 63 ] state=tensor([[-0.1133, -0.0283, -0.0333, -0.3485]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1139, -0.2230, -0.0403, -0.0665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 64 ] state=tensor([[-0.1139, -0.2230, -0.0403, -0.0665]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1184, -0.0273, -0.0416, -0.3716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 65 ] state=tensor([[-0.1184, -0.0273, -0.0416, -0.3716]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1189, -0.2218, -0.0491, -0.0923]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 66 ] state=tensor([[-0.1189, -0.2218, -0.0491, -0.0923]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1233, -0.0260, -0.0509, -0.4001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 67 ] state=tensor([[-0.1233, -0.0260, -0.0509, -0.4001]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1239, -0.2204, -0.0589, -0.1239]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 68 ] state=tensor([[-0.1239, -0.2204, -0.0589, -0.1239]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1283, -0.4146, -0.0614,  0.1497]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 100 ][ timestamp 69 ] state=tensor([[-0.1283, -0.4146, -0.0614,  0.1497]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1366, -0.2186, -0.0584, -0.1617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 70 ] state=tensor([[-0.1366, -0.2186, -0.0584, -0.1617]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1409, -0.4129, -0.0616,  0.1120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 71 ] state=tensor([[-0.1409, -0.4129, -0.0616,  0.1120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1492, -0.2169, -0.0594, -0.1995]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 72 ] state=tensor([[-0.1492, -0.2169, -0.0594, -0.1995]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1535, -0.4112, -0.0634,  0.0739]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 73 ] state=tensor([[-0.1535, -0.4112, -0.0634,  0.0739]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1618, -0.2152, -0.0619, -0.2381]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 74 ] state=tensor([[-0.1618, -0.2152, -0.0619, -0.2381]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1661, -0.4094, -0.0667,  0.0344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 75 ] state=tensor([[-0.1661, -0.4094, -0.0667,  0.0344]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1742, -0.6035, -0.0660,  0.3053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 76 ] state=tensor([[-0.1742, -0.6035, -0.0660,  0.3053]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1863, -0.4075, -0.0599, -0.0074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 77 ] state=tensor([[-0.1863, -0.4075, -0.0599, -0.0074]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1945, -0.2116, -0.0600, -0.3184]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 78 ] state=tensor([[-0.1945, -0.2116, -0.0600, -0.3184]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1987, -0.4058, -0.0664, -0.0452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 79 ] state=tensor([[-0.1987, -0.4058, -0.0664, -0.0452]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2068, -0.2098, -0.0673, -0.3580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 80 ] state=tensor([[-0.2068, -0.2098, -0.0673, -0.3580]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2110, -0.4039, -0.0744, -0.0873]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 81 ] state=tensor([[-0.2110, -0.4039, -0.0744, -0.0873]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2191, -0.5979, -0.0762,  0.1810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 82 ] state=tensor([[-0.2191, -0.5979, -0.0762,  0.1810]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2310, -0.4017, -0.0726, -0.1347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 83 ] state=tensor([[-0.2310, -0.4017, -0.0726, -0.1347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2391, -0.2056, -0.0753, -0.4494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 84 ] state=tensor([[-0.2391, -0.2056, -0.0753, -0.4494]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2432, -0.3996, -0.0843, -0.1814]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 85 ] state=tensor([[-0.2432, -0.3996, -0.0843, -0.1814]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2512, -0.2034, -0.0879, -0.4994]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 86 ] state=tensor([[-0.2512, -0.2034, -0.0879, -0.4994]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2552, -0.3972, -0.0979, -0.2356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 87 ] state=tensor([[-0.2552, -0.3972, -0.0979, -0.2356]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2632, -0.5908, -0.1026,  0.0246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 88 ] state=tensor([[-0.2632, -0.5908, -0.1026,  0.0246]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2750, -0.3944, -0.1021, -0.2986]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 89 ] state=tensor([[-0.2750, -0.3944, -0.1021, -0.2986]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2829, -0.5879, -0.1081, -0.0397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 90 ] state=tensor([[-0.2829, -0.5879, -0.1081, -0.0397]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2947, -0.3914, -0.1089, -0.3645]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 91 ] state=tensor([[-0.2947, -0.3914, -0.1089, -0.3645]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3025, -0.1949, -0.1161, -0.6894]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 92 ] state=tensor([[-0.3025, -0.1949, -0.1161, -0.6894]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3064, -0.3882, -0.1299, -0.4354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 93 ] state=tensor([[-0.3064, -0.3882, -0.1299, -0.4354]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3141, -0.5813, -0.1386, -0.1864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 94 ] state=tensor([[-0.3141, -0.5813, -0.1386, -0.1864]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3258, -0.7742, -0.1424,  0.0596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 95 ] state=tensor([[-0.3258, -0.7742, -0.1424,  0.0596]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3413, -0.5774, -0.1412, -0.2744]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 96 ] state=tensor([[-0.3413, -0.5774, -0.1412, -0.2744]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3528, -0.3805, -0.1467, -0.6081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 97 ] state=tensor([[-0.3528, -0.3805, -0.1467, -0.6081]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3604, -0.5733, -0.1588, -0.3649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 98 ] state=tensor([[-0.3604, -0.5733, -0.1588, -0.3649]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3719, -0.7659, -0.1661, -0.1263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 99 ] state=tensor([[-0.3719, -0.7659, -0.1661, -0.1263]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3872, -0.5688, -0.1686, -0.4664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 100 ] state=tensor([[-0.3872, -0.5688, -0.1686, -0.4664]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3986, -0.3718, -0.1780, -0.8071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 101 ] state=tensor([[-0.3986, -0.3718, -0.1780, -0.8071]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4060, -0.5641, -0.1941, -0.5753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 102 ] state=tensor([[-0.4060, -0.5641, -0.1941, -0.5753]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4173, -0.3668, -0.2056, -0.9223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 100 ][ timestamp 103 ] state=tensor([[-0.4173, -0.3668, -0.2056, -0.9223]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 100: Exploration_rate=0.05. Score=103.\n",
      "[ episode 101 ] state=tensor([[-0.0498,  0.0020, -0.0160,  0.0266]])\n",
      "[ episode 101 ][ timestamp 1 ] state=tensor([[-0.0498,  0.0020, -0.0160,  0.0266]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0498, -0.1929, -0.0154,  0.3142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 2 ] state=tensor([[-0.0498, -0.1929, -0.0154,  0.3142]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0537,  0.0024, -0.0091,  0.0167]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 3 ] state=tensor([[-0.0537,  0.0024, -0.0091,  0.0167]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0536, -0.1926, -0.0088,  0.3065]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 4 ] state=tensor([[-0.0536, -0.1926, -0.0088,  0.3065]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0575, -0.3876, -0.0027,  0.5964]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 5 ] state=tensor([[-0.0575, -0.3876, -0.0027,  0.5964]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0652, -0.1924,  0.0092,  0.3029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 6 ] state=tensor([[-0.0652, -0.1924,  0.0092,  0.3029]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0691,  0.0026,  0.0153,  0.0131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 7 ] state=tensor([[-0.0691,  0.0026,  0.0153,  0.0131]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0690,  0.1975,  0.0156, -0.2747]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 8 ] state=tensor([[-0.0690,  0.1975,  0.0156, -0.2747]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0651,  0.3924,  0.0101, -0.5624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 9 ] state=tensor([[-0.0651,  0.3924,  0.0101, -0.5624]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0572,  0.5874, -0.0012, -0.8519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 10 ] state=tensor([[-0.0572,  0.5874, -0.0012, -0.8519]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0455,  0.7825, -0.0182, -1.1450]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 11 ] state=tensor([[-0.0455,  0.7825, -0.0182, -1.1450]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0298,  0.5876, -0.0411, -0.8580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 12 ] state=tensor([[-0.0298,  0.5876, -0.0411, -0.8580]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0181,  0.3931, -0.0583, -0.5786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 13 ] state=tensor([[-0.0181,  0.3931, -0.0583, -0.5786]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0102,  0.1988, -0.0698, -0.3048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 14 ] state=tensor([[-0.0102,  0.1988, -0.0698, -0.3048]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0062,  0.0048, -0.0759, -0.0349]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 15 ] state=tensor([[-0.0062,  0.0048, -0.0759, -0.0349]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0061, -0.1892, -0.0766,  0.2329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 16 ] state=tensor([[-0.0061, -0.1892, -0.0766,  0.2329]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0099, -0.3831, -0.0720,  0.5004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 17 ] state=tensor([[-0.0099, -0.3831, -0.0720,  0.5004]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0176, -0.1871, -0.0620,  0.1859]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 18 ] state=tensor([[-0.0176, -0.1871, -0.0620,  0.1859]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0213, -0.3813, -0.0583,  0.4585]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 19 ] state=tensor([[-0.0213, -0.3813, -0.0583,  0.4585]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0289, -0.1854, -0.0491,  0.1480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 20 ] state=tensor([[-0.0289, -0.1854, -0.0491,  0.1480]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0326, -0.3798, -0.0461,  0.4248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 21 ] state=tensor([[-0.0326, -0.3798, -0.0461,  0.4248]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0402, -0.1840, -0.0376,  0.1179]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 22 ] state=tensor([[-0.0402, -0.1840, -0.0376,  0.1179]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0439,  0.0116, -0.0353, -0.1864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 23 ] state=tensor([[-0.0439,  0.0116, -0.0353, -0.1864]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0437, -0.1830, -0.0390,  0.0950]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 24 ] state=tensor([[-0.0437, -0.1830, -0.0390,  0.0950]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0473, -0.3775, -0.0371,  0.3751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 25 ] state=tensor([[-0.0473, -0.3775, -0.0371,  0.3751]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0549, -0.1819, -0.0296,  0.0710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 26 ] state=tensor([[-0.0549, -0.1819, -0.0296,  0.0710]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0585,  0.0137, -0.0282, -0.2309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 27 ] state=tensor([[-0.0585,  0.0137, -0.0282, -0.2309]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0583, -0.1811, -0.0328,  0.0528]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 28 ] state=tensor([[-0.0583, -0.1811, -0.0328,  0.0528]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0619,  0.0145, -0.0317, -0.2501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 29 ] state=tensor([[-0.0619,  0.0145, -0.0317, -0.2501]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0616,  0.2101, -0.0367, -0.5526]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 30 ] state=tensor([[-0.0616,  0.2101, -0.0367, -0.5526]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0574,  0.0155, -0.0478, -0.2717]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 31 ] state=tensor([[-0.0574,  0.0155, -0.0478, -0.2717]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0571, -0.1789, -0.0532,  0.0055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 32 ] state=tensor([[-0.0571, -0.1789, -0.0532,  0.0055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0607,  0.0169, -0.0531, -0.3035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 33 ] state=tensor([[-0.0607,  0.0169, -0.0531, -0.3035]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0603, -0.1774, -0.0592, -0.0280]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 101 ][ timestamp 34 ] state=tensor([[-0.0603, -0.1774, -0.0592, -0.0280]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0639,  0.0185, -0.0597, -0.3388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 35 ] state=tensor([[-0.0639,  0.0185, -0.0597, -0.3388]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0635, -0.1757, -0.0665, -0.0655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 36 ] state=tensor([[-0.0635, -0.1757, -0.0665, -0.0655]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0670, -0.3698, -0.0678,  0.2055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 37 ] state=tensor([[-0.0670, -0.3698, -0.0678,  0.2055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0744, -0.1738, -0.0637, -0.1078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 38 ] state=tensor([[-0.0744, -0.1738, -0.0637, -0.1078]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0779, -0.3679, -0.0659,  0.1641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 39 ] state=tensor([[-0.0779, -0.3679, -0.0659,  0.1641]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0852, -0.1719, -0.0626, -0.1486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 40 ] state=tensor([[-0.0852, -0.1719, -0.0626, -0.1486]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0887, -0.3661, -0.0656,  0.1237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 41 ] state=tensor([[-0.0887, -0.3661, -0.0656,  0.1237]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0960, -0.5602, -0.0631,  0.3950]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 42 ] state=tensor([[-0.0960, -0.5602, -0.0631,  0.3950]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1072, -0.3643, -0.0552,  0.0831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 43 ] state=tensor([[-0.1072, -0.3643, -0.0552,  0.0831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1145, -0.1684, -0.0535, -0.2265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 44 ] state=tensor([[-0.1145, -0.1684, -0.0535, -0.2265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1179,  0.0274, -0.0581, -0.5356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 45 ] state=tensor([[-0.1179,  0.0274, -0.0581, -0.5356]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1173, -0.1668, -0.0688, -0.2617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 46 ] state=tensor([[-0.1173, -0.1668, -0.0688, -0.2617]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1206,  0.0292, -0.0740, -0.5753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 47 ] state=tensor([[-0.1206,  0.0292, -0.0740, -0.5753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1201, -0.1648, -0.0855, -0.3068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 48 ] state=tensor([[-0.1201, -0.1648, -0.0855, -0.3068]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1234, -0.3586, -0.0917, -0.0423]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 49 ] state=tensor([[-0.1234, -0.3586, -0.0917, -0.0423]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1305, -0.5523, -0.0925,  0.2201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 50 ] state=tensor([[-0.1305, -0.5523, -0.0925,  0.2201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1416, -0.3560, -0.0881, -0.1003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 51 ] state=tensor([[-0.1416, -0.3560, -0.0881, -0.1003]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1487, -0.1597, -0.0901, -0.4194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 52 ] state=tensor([[-0.1487, -0.1597, -0.0901, -0.4194]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1519, -0.3535, -0.0985, -0.1564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 53 ] state=tensor([[-0.1519, -0.3535, -0.0985, -0.1564]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1590, -0.5470, -0.1016,  0.1037]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 54 ] state=tensor([[-0.1590, -0.5470, -0.1016,  0.1037]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1699, -0.3506, -0.0995, -0.2193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 55 ] state=tensor([[-0.1699, -0.3506, -0.0995, -0.2193]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1769, -0.1542, -0.1039, -0.5416]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 56 ] state=tensor([[-0.1769, -0.1542, -0.1039, -0.5416]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1800, -0.3477, -0.1148, -0.2834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 57 ] state=tensor([[-0.1800, -0.3477, -0.1148, -0.2834]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1870, -0.5411, -0.1204, -0.0290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 58 ] state=tensor([[-0.1870, -0.5411, -0.1204, -0.0290]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1978, -0.7343, -0.1210,  0.2234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 59 ] state=tensor([[-0.1978, -0.7343, -0.1210,  0.2234]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2125, -0.9275, -0.1166,  0.4756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 60 ] state=tensor([[-0.2125, -0.9275, -0.1166,  0.4756]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2310, -0.7309, -0.1070,  0.1485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 61 ] state=tensor([[-0.2310, -0.7309, -0.1070,  0.1485]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2456, -0.5344, -0.1041, -0.1759]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 62 ] state=tensor([[-0.2456, -0.5344, -0.1041, -0.1759]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2563, -0.7279, -0.1076,  0.0822]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 63 ] state=tensor([[-0.2563, -0.7279, -0.1076,  0.0822]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2709, -0.5314, -0.1059, -0.2424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 64 ] state=tensor([[-0.2709, -0.5314, -0.1059, -0.2424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2815, -0.3350, -0.1108, -0.5665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 65 ] state=tensor([[-0.2815, -0.3350, -0.1108, -0.5665]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2882, -0.5284, -0.1221, -0.3107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 66 ] state=tensor([[-0.2882, -0.5284, -0.1221, -0.3107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2988, -0.7216, -0.1283, -0.0589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 67 ] state=tensor([[-0.2988, -0.7216, -0.1283, -0.0589]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3132, -0.5249, -0.1295, -0.3891]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 68 ] state=tensor([[-0.3132, -0.5249, -0.1295, -0.3891]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3237, -0.7179, -0.1373, -0.1399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 69 ] state=tensor([[-0.3237, -0.7179, -0.1373, -0.1399]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3381, -0.5211, -0.1401, -0.4726]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 70 ] state=tensor([[-0.3381, -0.5211, -0.1401, -0.4726]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3485, -0.7140, -0.1495, -0.2271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 71 ] state=tensor([[-0.3485, -0.7140, -0.1495, -0.2271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3628, -0.9067, -0.1541,  0.0149]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 72 ] state=tensor([[-0.3628, -0.9067, -0.1541,  0.0149]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3809, -0.7098, -0.1538, -0.3222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 73 ] state=tensor([[-0.3809, -0.7098, -0.1538, -0.3222]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3951, -0.5128, -0.1602, -0.6591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 74 ] state=tensor([[-0.3951, -0.5128, -0.1602, -0.6591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4053, -0.7054, -0.1734, -0.4209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 75 ] state=tensor([[-0.4053, -0.7054, -0.1734, -0.4209]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4195, -0.5083, -0.1818, -0.7628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 76 ] state=tensor([[-0.4195, -0.5083, -0.1818, -0.7628]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4296, -0.3112, -0.1971, -1.1068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 101 ][ timestamp 77 ] state=tensor([[-0.4296, -0.3112, -0.1971, -1.1068]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 101: Exploration_rate=0.05. Score=77.\n",
      "[ episode 102 ] state=tensor([[-0.0393,  0.0174,  0.0129, -0.0250]])\n",
      "[ episode 102 ][ timestamp 1 ] state=tensor([[-0.0393,  0.0174,  0.0129, -0.0250]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0389,  0.2124,  0.0124, -0.3135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 2 ] state=tensor([[-0.0389,  0.2124,  0.0124, -0.3135]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0347,  0.4073,  0.0061, -0.6023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 3 ] state=tensor([[-0.0347,  0.4073,  0.0061, -0.6023]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0265,  0.6023, -0.0059, -0.8930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 4 ] state=tensor([[-0.0265,  0.6023, -0.0059, -0.8930]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0145,  0.4073, -0.0238, -0.6022]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 5 ] state=tensor([[-0.0145,  0.4073, -0.0238, -0.6022]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0063,  0.2125, -0.0358, -0.3171]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 6 ] state=tensor([[-0.0063,  0.2125, -0.0358, -0.3171]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0021,  0.0179, -0.0422, -0.0359]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 7 ] state=tensor([[-0.0021,  0.0179, -0.0422, -0.0359]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0017, -0.1766, -0.0429,  0.2432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 8 ] state=tensor([[-0.0017, -0.1766, -0.0429,  0.2432]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0053,  0.0191, -0.0380, -0.0627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 9 ] state=tensor([[-0.0053,  0.0191, -0.0380, -0.0627]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0049,  0.2148, -0.0393, -0.3672]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 10 ] state=tensor([[-0.0049,  0.2148, -0.0393, -0.3672]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0006,  0.0202, -0.0466, -0.0871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 11 ] state=tensor([[-0.0006,  0.0202, -0.0466, -0.0871]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6695e-04, -1.7418e-01, -4.8355e-02,  1.9050e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 12 ] state=tensor([[-1.6695e-04, -1.7418e-01, -4.8355e-02,  1.9050e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0037,  0.0216, -0.0445, -0.1170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 13 ] state=tensor([[-0.0037,  0.0216, -0.0445, -0.1170]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0032, -0.1729, -0.0469,  0.1613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 14 ] state=tensor([[-0.0032, -0.1729, -0.0469,  0.1613]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0067,  0.0229, -0.0437, -0.1458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 15 ] state=tensor([[-0.0067,  0.0229, -0.0437, -0.1458]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0062, -0.1716, -0.0466,  0.1328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 16 ] state=tensor([[-0.0062, -0.1716, -0.0466,  0.1328]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0096, -0.3660, -0.0439,  0.4104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 17 ] state=tensor([[-0.0096, -0.3660, -0.0439,  0.4104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0170, -0.1703, -0.0357,  0.1042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 18 ] state=tensor([[-0.0170, -0.1703, -0.0357,  0.1042]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0204, -0.3649, -0.0336,  0.3854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 19 ] state=tensor([[-0.0204, -0.3649, -0.0336,  0.3854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0277, -0.1693, -0.0259,  0.0823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 20 ] state=tensor([[-0.0277, -0.1693, -0.0259,  0.0823]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0311, -0.3640, -0.0243,  0.3667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 21 ] state=tensor([[-0.0311, -0.3640, -0.0243,  0.3667]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0383, -0.5588, -0.0169,  0.6516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 22 ] state=tensor([[-0.0383, -0.5588, -0.0169,  0.6516]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0495, -0.3634, -0.0039,  0.3537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 23 ] state=tensor([[-0.0495, -0.3634, -0.0039,  0.3537]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0568, -0.5585,  0.0032,  0.6451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 24 ] state=tensor([[-0.0568, -0.5585,  0.0032,  0.6451]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0680, -0.3634,  0.0161,  0.3534]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 25 ] state=tensor([[-0.0680, -0.3634,  0.0161,  0.3534]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0752, -0.1685,  0.0231,  0.0658]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 26 ] state=tensor([[-0.0752, -0.1685,  0.0231,  0.0658]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0786,  0.0262,  0.0245, -0.2194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 27 ] state=tensor([[-0.0786,  0.0262,  0.0245, -0.2194]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0781,  0.2210,  0.0201, -0.5043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 28 ] state=tensor([[-0.0781,  0.2210,  0.0201, -0.5043]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0736,  0.0256,  0.0100, -0.2054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 29 ] state=tensor([[-0.0736,  0.0256,  0.0100, -0.2054]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0731,  0.2206,  0.0059, -0.4949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 30 ] state=tensor([[-0.0731,  0.2206,  0.0059, -0.4949]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0687,  0.0254, -0.0040, -0.2004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 31 ] state=tensor([[-0.0687,  0.0254, -0.0040, -0.2004]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0682, -0.1697, -0.0080,  0.0910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 32 ] state=tensor([[-0.0682, -0.1697, -0.0080,  0.0910]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0716,  0.0256, -0.0062, -0.2042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 33 ] state=tensor([[-0.0716,  0.0256, -0.0062, -0.2042]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0711,  0.2208, -0.0103, -0.4988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 34 ] state=tensor([[-0.0711,  0.2208, -0.0103, -0.4988]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0667,  0.0258, -0.0203, -0.2094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 35 ] state=tensor([[-0.0667,  0.0258, -0.0203, -0.2094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0662,  0.2212, -0.0245, -0.5084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 36 ] state=tensor([[-0.0662,  0.2212, -0.0245, -0.5084]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0617,  0.0264, -0.0346, -0.2235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 37 ] state=tensor([[-0.0617,  0.0264, -0.0346, -0.2235]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0612, -0.1682, -0.0391,  0.0580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 38 ] state=tensor([[-0.0612, -0.1682, -0.0391,  0.0580]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0646, -0.3627, -0.0379,  0.3381]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 39 ] state=tensor([[-0.0646, -0.3627, -0.0379,  0.3381]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0718, -0.1671, -0.0312,  0.0337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 40 ] state=tensor([[-0.0718, -0.1671, -0.0312,  0.0337]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0752,  0.0285, -0.0305, -0.2686]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 41 ] state=tensor([[-0.0752,  0.0285, -0.0305, -0.2686]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0746, -0.1662, -0.0359,  0.0143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 42 ] state=tensor([[-0.0746, -0.1662, -0.0359,  0.0143]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0779, -0.3608, -0.0356,  0.2954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 43 ] state=tensor([[-0.0779, -0.3608, -0.0356,  0.2954]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0851, -0.1652, -0.0297, -0.0083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 44 ] state=tensor([[-0.0851, -0.1652, -0.0297, -0.0083]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0885, -0.3599, -0.0298,  0.2749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 45 ] state=tensor([[-0.0885, -0.3599, -0.0298,  0.2749]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0956, -0.5546, -0.0243,  0.5580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 46 ] state=tensor([[-0.0956, -0.5546, -0.0243,  0.5580]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1067, -0.7493, -0.0132,  0.8429]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 47 ] state=tensor([[-0.1067, -0.7493, -0.0132,  0.8429]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1217, -0.5540,  0.0037,  0.5461]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 48 ] state=tensor([[-0.1217, -0.5540,  0.0037,  0.5461]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1328, -0.3590,  0.0146,  0.2546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 49 ] state=tensor([[-0.1328, -0.3590,  0.0146,  0.2546]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1400, -0.1640,  0.0197, -0.0334]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 50 ] state=tensor([[-0.1400, -0.1640,  0.0197, -0.0334]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1433,  0.0308,  0.0190, -0.3198]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 102 ][ timestamp 51 ] state=tensor([[-0.1433,  0.0308,  0.0190, -0.3198]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1427,  0.2256,  0.0126, -0.6065]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 52 ] state=tensor([[-0.1427,  0.2256,  0.0126, -0.6065]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3814e-01,  4.2058e-01,  4.9099e-04, -8.9514e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 53 ] state=tensor([[-1.3814e-01,  4.2058e-01,  4.9099e-04, -8.9514e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1297,  0.6157, -0.0174, -1.1877]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 54 ] state=tensor([[-0.1297,  0.6157, -0.0174, -1.1877]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1174,  0.8110, -0.0412, -1.4858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 55 ] state=tensor([[-0.1174,  0.8110, -0.0412, -1.4858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1012,  0.6164, -0.0709, -1.2062]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 56 ] state=tensor([[-0.1012,  0.6164, -0.0709, -1.2062]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0889,  0.4223, -0.0950, -0.9366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 57 ] state=tensor([[-0.0889,  0.4223, -0.0950, -0.9366]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0804,  0.2286, -0.1137, -0.6752]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 58 ] state=tensor([[-0.0804,  0.2286, -0.1137, -0.6752]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0758,  0.0352, -0.1272, -0.4204]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 59 ] state=tensor([[-0.0758,  0.0352, -0.1272, -0.4204]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0751, -0.1579, -0.1356, -0.1703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 60 ] state=tensor([[-0.0751, -0.1579, -0.1356, -0.1703]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0783, -0.3508, -0.1391,  0.0767]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 61 ] state=tensor([[-0.0783, -0.3508, -0.1391,  0.0767]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0853, -0.1540, -0.1375, -0.2565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 62 ] state=tensor([[-0.0853, -0.1540, -0.1375, -0.2565]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0884, -0.3470, -0.1426, -0.0101]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 63 ] state=tensor([[-0.0884, -0.3470, -0.1426, -0.0101]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0953, -0.5398, -0.1429,  0.2344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 64 ] state=tensor([[-0.0953, -0.5398, -0.1429,  0.2344]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1061, -0.7326, -0.1382,  0.4788]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 65 ] state=tensor([[-0.1061, -0.7326, -0.1382,  0.4788]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1208, -0.5358, -0.1286,  0.1460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 66 ] state=tensor([[-0.1208, -0.5358, -0.1286,  0.1460]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1315, -0.3391, -0.1257, -0.1843]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 67 ] state=tensor([[-0.1315, -0.3391, -0.1257, -0.1843]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1383, -0.5322, -0.1294,  0.0662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 68 ] state=tensor([[-0.1383, -0.5322, -0.1294,  0.0662]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1489, -0.3355, -0.1280, -0.2643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 69 ] state=tensor([[-0.1489, -0.3355, -0.1280, -0.2643]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1556, -0.5286, -0.1333, -0.0146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 70 ] state=tensor([[-0.1556, -0.5286, -0.1333, -0.0146]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1662, -0.3318, -0.1336, -0.3462]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 71 ] state=tensor([[-0.1662, -0.3318, -0.1336, -0.3462]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1728, -0.5248, -0.1405, -0.0985]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 72 ] state=tensor([[-0.1728, -0.5248, -0.1405, -0.0985]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1833, -0.7177, -0.1425,  0.1468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 73 ] state=tensor([[-0.1833, -0.7177, -0.1425,  0.1468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1977, -0.5209, -0.1396, -0.1872]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 74 ] state=tensor([[-0.1977, -0.5209, -0.1396, -0.1872]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2081, -0.7137, -0.1433,  0.0584]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 75 ] state=tensor([[-0.2081, -0.7137, -0.1433,  0.0584]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2224, -0.9065, -0.1421,  0.3026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 76 ] state=tensor([[-0.2224, -0.9065, -0.1421,  0.3026]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2405, -1.0994, -0.1361,  0.5473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 77 ] state=tensor([[-0.2405, -1.0994, -0.1361,  0.5473]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2625, -0.9026, -0.1251,  0.2150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 78 ] state=tensor([[-0.2625, -0.9026, -0.1251,  0.2150]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2806, -1.0958, -0.1208,  0.4658]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 79 ] state=tensor([[-0.2806, -1.0958, -0.1208,  0.4658]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3025, -1.2890, -0.1115,  0.7181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 80 ] state=tensor([[-0.3025, -1.2890, -0.1115,  0.7181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3283, -1.0925, -0.0972,  0.3925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 81 ] state=tensor([[-0.3283, -1.0925, -0.0972,  0.3925]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3501, -1.2861, -0.0893,  0.6530]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 82 ] state=tensor([[-0.3501, -1.2861, -0.0893,  0.6530]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3758, -1.0899, -0.0763,  0.3336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 83 ] state=tensor([[-0.3758, -1.0899, -0.0763,  0.3336]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3976, -1.2838, -0.0696,  0.6013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 84 ] state=tensor([[-0.3976, -1.2838, -0.0696,  0.6013]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4233, -1.0878, -0.0576,  0.2875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 85 ] state=tensor([[-0.4233, -1.0878, -0.0576,  0.2875]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4451, -1.2821, -0.0518,  0.5615]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 86 ] state=tensor([[-0.4451, -1.2821, -0.0518,  0.5615]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4707, -1.0863, -0.0406,  0.2529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 87 ] state=tensor([[-0.4707, -1.0863, -0.0406,  0.2529]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4924, -0.8906, -0.0355, -0.0523]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 88 ] state=tensor([[-0.4924, -0.8906, -0.0355, -0.0523]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5102, -0.6950, -0.0366, -0.3559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 89 ] state=tensor([[-0.5102, -0.6950, -0.0366, -0.3559]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5241, -0.4994, -0.0437, -0.6599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 90 ] state=tensor([[-0.5241, -0.4994, -0.0437, -0.6599]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5341, -0.3037, -0.0569, -0.9660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 91 ] state=tensor([[-0.5341, -0.3037, -0.0569, -0.9660]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5402, -0.4980, -0.0762, -0.6918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 92 ] state=tensor([[-0.5402, -0.4980, -0.0762, -0.6918]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5502, -0.3019, -0.0900, -1.0074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 93 ] state=tensor([[-0.5502, -0.3019, -0.0900, -1.0074]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5562, -0.4957, -0.1102, -0.7443]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 94 ] state=tensor([[-0.5562, -0.4957, -0.1102, -0.7443]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5661, -0.6891, -0.1251, -0.4882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 95 ] state=tensor([[-0.5661, -0.6891, -0.1251, -0.4882]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5799, -0.4925, -0.1348, -0.8176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 96 ] state=tensor([[-0.5799, -0.4925, -0.1348, -0.8176]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5897, -0.6855, -0.1512, -0.5702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 97 ] state=tensor([[-0.5897, -0.6855, -0.1512, -0.5702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6034, -0.8782, -0.1626, -0.3287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 98 ] state=tensor([[-0.6034, -0.8782, -0.1626, -0.3287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6210, -0.6812, -0.1692, -0.6679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 99 ] state=tensor([[-0.6210, -0.6812, -0.1692, -0.6679]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6346, -0.8736, -0.1825, -0.4329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 100 ] state=tensor([[-0.6346, -0.8736, -0.1825, -0.4329]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6521, -0.6765, -0.1912, -0.7771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 101 ] state=tensor([[-0.6521, -0.6765, -0.1912, -0.7771]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6656, -0.8685, -0.2067, -0.5501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 102 ][ timestamp 102 ] state=tensor([[-0.6656, -0.8685, -0.2067, -0.5501]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 102: Exploration_rate=0.05. Score=102.\n",
      "[ episode 103 ] state=tensor([[0.0040, 0.0370, 0.0063, 0.0351]])\n",
      "[ episode 103 ][ timestamp 1 ] state=tensor([[0.0040, 0.0370, 0.0063, 0.0351]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0048,  0.2320,  0.0070, -0.2556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 2 ] state=tensor([[ 0.0048,  0.2320,  0.0070, -0.2556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0094,  0.4270,  0.0019, -0.5461]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 3 ] state=tensor([[ 0.0094,  0.4270,  0.0019, -0.5461]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0179,  0.2319, -0.0090, -0.2528]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 4 ] state=tensor([[ 0.0179,  0.2319, -0.0090, -0.2528]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0226,  0.4271, -0.0141, -0.5483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 5 ] state=tensor([[ 0.0226,  0.4271, -0.0141, -0.5483]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0311,  0.2322, -0.0250, -0.2601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 6 ] state=tensor([[ 0.0311,  0.2322, -0.0250, -0.2601]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0358,  0.0375, -0.0302,  0.0246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 7 ] state=tensor([[ 0.0358,  0.0375, -0.0302,  0.0246]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0365,  0.2330, -0.0297, -0.2775]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 8 ] state=tensor([[ 0.0365,  0.2330, -0.0297, -0.2775]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0412,  0.0383, -0.0353,  0.0057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 9 ] state=tensor([[ 0.0412,  0.0383, -0.0353,  0.0057]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0419, -0.1563, -0.0352,  0.2870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 10 ] state=tensor([[ 0.0419, -0.1563, -0.0352,  0.2870]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0388, -0.3509, -0.0294,  0.5684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 11 ] state=tensor([[ 0.0388, -0.3509, -0.0294,  0.5684]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0318, -0.1554, -0.0181,  0.2666]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 12 ] state=tensor([[ 0.0318, -0.1554, -0.0181,  0.2666]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0287, -0.3502, -0.0127,  0.5536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 13 ] state=tensor([[ 0.0287, -0.3502, -0.0127,  0.5536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0217, -0.1549, -0.0017,  0.2569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 14 ] state=tensor([[ 0.0217, -0.1549, -0.0017,  0.2569]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0186,  0.0402,  0.0035, -0.0363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 15 ] state=tensor([[ 0.0186,  0.0402,  0.0035, -0.0363]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0194,  0.2353,  0.0027, -0.3279]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 16 ] state=tensor([[ 0.0194,  0.2353,  0.0027, -0.3279]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0241,  0.4304, -0.0038, -0.6197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 17 ] state=tensor([[ 0.0241,  0.4304, -0.0038, -0.6197]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0327,  0.2353, -0.0162, -0.3282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 18 ] state=tensor([[ 0.0327,  0.2353, -0.0162, -0.3282]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0374,  0.0404, -0.0228, -0.0407]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 19 ] state=tensor([[ 0.0374,  0.0404, -0.0228, -0.0407]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0382,  0.2359, -0.0236, -0.3405]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 20 ] state=tensor([[ 0.0382,  0.2359, -0.0236, -0.3405]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0429,  0.0411, -0.0304, -0.0553]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 103 ][ timestamp 21 ] state=tensor([[ 0.0429,  0.0411, -0.0304, -0.0553]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0438, -0.1536, -0.0315,  0.2276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 22 ] state=tensor([[ 0.0438, -0.1536, -0.0315,  0.2276]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0407, -0.3483, -0.0269,  0.5102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 23 ] state=tensor([[ 0.0407, -0.3483, -0.0269,  0.5102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0337, -0.1528, -0.0167,  0.2091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 24 ] state=tensor([[ 0.0337, -0.1528, -0.0167,  0.2091]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0307,  0.0426, -0.0126, -0.0888]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 25 ] state=tensor([[ 0.0307,  0.0426, -0.0126, -0.0888]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0315, -0.1523, -0.0143,  0.1999]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 26 ] state=tensor([[ 0.0315, -0.1523, -0.0143,  0.1999]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0285,  0.0430, -0.0103, -0.0973]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 27 ] state=tensor([[ 0.0285,  0.0430, -0.0103, -0.0973]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0293, -0.1520, -0.0123,  0.1921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 28 ] state=tensor([[ 0.0293, -0.1520, -0.0123,  0.1921]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0263, -0.3469, -0.0084,  0.4809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 29 ] state=tensor([[ 0.0263, -0.3469, -0.0084,  0.4809]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0194, -0.5419,  0.0012,  0.7709]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 30 ] state=tensor([[ 0.0194, -0.5419,  0.0012,  0.7709]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0085, -0.3468,  0.0166,  0.4786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 31 ] state=tensor([[ 0.0085, -0.3468,  0.0166,  0.4786]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0016, -0.1519,  0.0262,  0.1912]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 32 ] state=tensor([[ 0.0016, -0.1519,  0.0262,  0.1912]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0015, -0.3474,  0.0300,  0.4920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 33 ] state=tensor([[-0.0015, -0.3474,  0.0300,  0.4920]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0084, -0.1527,  0.0398,  0.2090]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 34 ] state=tensor([[-0.0084, -0.1527,  0.0398,  0.2090]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0115, -0.3484,  0.0440,  0.5139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 35 ] state=tensor([[-0.0115, -0.3484,  0.0440,  0.5139]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0184, -0.1539,  0.0543,  0.2354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 36 ] state=tensor([[-0.0184, -0.1539,  0.0543,  0.2354]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0215,  0.0404,  0.0590, -0.0396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 37 ] state=tensor([[-0.0215,  0.0404,  0.0590, -0.0396]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0207,  0.2346,  0.0582, -0.3131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 38 ] state=tensor([[-0.0207,  0.2346,  0.0582, -0.3131]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0160,  0.4288,  0.0519, -0.5869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 39 ] state=tensor([[-0.0160,  0.4288,  0.0519, -0.5869]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0074,  0.6232,  0.0402, -0.8628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 40 ] state=tensor([[-0.0074,  0.6232,  0.0402, -0.8628]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0050,  0.8177,  0.0230, -1.1426]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 41 ] state=tensor([[ 0.0050,  0.8177,  0.0230, -1.1426]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 2.1387e-02,  1.0126e+00,  9.9526e-05, -1.4280e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 42 ] state=tensor([[ 2.1387e-02,  1.0126e+00,  9.9526e-05, -1.4280e+00]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0416,  0.8174, -0.0285, -1.1352]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 43 ] state=tensor([[ 0.0416,  0.8174, -0.0285, -1.1352]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0580,  0.6227, -0.0512, -0.8516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 44 ] state=tensor([[ 0.0580,  0.6227, -0.0512, -0.8516]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0704,  0.4283, -0.0682, -0.5755]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 45 ] state=tensor([[ 0.0704,  0.4283, -0.0682, -0.5755]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0790,  0.2342, -0.0797, -0.3050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 46 ] state=tensor([[ 0.0790,  0.2342, -0.0797, -0.3050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0837,  0.0403, -0.0858, -0.0385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 47 ] state=tensor([[ 0.0837,  0.0403, -0.0858, -0.0385]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0845, -0.1535, -0.0866,  0.2259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 48 ] state=tensor([[ 0.0845, -0.1535, -0.0866,  0.2259]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0814, -0.3473, -0.0821,  0.4901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 49 ] state=tensor([[ 0.0814, -0.3473, -0.0821,  0.4901]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0745, -0.5411, -0.0723,  0.7558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 50 ] state=tensor([[ 0.0745, -0.5411, -0.0723,  0.7558]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0637, -0.3451, -0.0571,  0.4413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 51 ] state=tensor([[ 0.0637, -0.3451, -0.0571,  0.4413]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0568, -0.5394, -0.0483,  0.7155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 52 ] state=tensor([[ 0.0568, -0.5394, -0.0483,  0.7155]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0460, -0.3436, -0.0340,  0.4080]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 53 ] state=tensor([[ 0.0460, -0.3436, -0.0340,  0.4080]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0391, -0.5382, -0.0258,  0.6897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 54 ] state=tensor([[ 0.0391, -0.5382, -0.0258,  0.6897]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0283, -0.3428, -0.0121,  0.3890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 55 ] state=tensor([[ 0.0283, -0.3428, -0.0121,  0.3890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0215, -0.1475, -0.0043,  0.0926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 56 ] state=tensor([[ 0.0215, -0.1475, -0.0043,  0.0926]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0185,  0.0477, -0.0024, -0.2015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 57 ] state=tensor([[ 0.0185,  0.0477, -0.0024, -0.2015]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0195,  0.2429, -0.0064, -0.4949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 58 ] state=tensor([[ 0.0195,  0.2429, -0.0064, -0.4949]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0243,  0.0478, -0.0163, -0.2043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 59 ] state=tensor([[ 0.0243,  0.0478, -0.0163, -0.2043]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0253, -0.1471, -0.0204,  0.0832]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 60 ] state=tensor([[ 0.0253, -0.1471, -0.0204,  0.0832]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0224, -0.3419, -0.0188,  0.3694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 61 ] state=tensor([[ 0.0224, -0.3419, -0.0188,  0.3694]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0155, -0.1465, -0.0114,  0.0709]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 62 ] state=tensor([[ 0.0155, -0.1465, -0.0114,  0.0709]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0126,  0.0488, -0.0100, -0.2254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 63 ] state=tensor([[ 0.0126,  0.0488, -0.0100, -0.2254]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0136, -0.1462, -0.0145,  0.0641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 64 ] state=tensor([[ 0.0136, -0.1462, -0.0145,  0.0641]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0106, -0.3411, -0.0132,  0.3522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 65 ] state=tensor([[ 0.0106, -0.3411, -0.0132,  0.3522]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0038, -0.1458, -0.0061,  0.0554]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 66 ] state=tensor([[ 0.0038, -0.1458, -0.0061,  0.0554]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0009, -0.3408, -0.0050,  0.3461]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 67 ] state=tensor([[ 0.0009, -0.3408, -0.0050,  0.3461]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0059, -0.1456,  0.0019,  0.0519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 68 ] state=tensor([[-0.0059, -0.1456,  0.0019,  0.0519]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0088, -0.3408,  0.0029,  0.3451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 69 ] state=tensor([[-0.0088, -0.3408,  0.0029,  0.3451]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0156, -0.5359,  0.0098,  0.6388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 70 ] state=tensor([[-0.0156, -0.5359,  0.0098,  0.6388]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0264, -0.7312,  0.0226,  0.9345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 71 ] state=tensor([[-0.0264, -0.7312,  0.0226,  0.9345]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0410, -0.5364,  0.0413,  0.6490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 72 ] state=tensor([[-0.0410, -0.5364,  0.0413,  0.6490]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0517, -0.3419,  0.0543,  0.3696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 73 ] state=tensor([[-0.0517, -0.3419,  0.0543,  0.3696]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0586, -0.5377,  0.0617,  0.6789]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 74 ] state=tensor([[-0.0586, -0.5377,  0.0617,  0.6789]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0693, -0.3435,  0.0752,  0.4063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 75 ] state=tensor([[-0.0693, -0.3435,  0.0752,  0.4063]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0762, -0.1495,  0.0834,  0.1382]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 76 ] state=tensor([[-0.0762, -0.1495,  0.0834,  0.1382]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0792, -0.3457,  0.0861,  0.4560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 77 ] state=tensor([[-0.0792, -0.3457,  0.0861,  0.4560]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0861, -0.1519,  0.0953,  0.1917]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 103 ][ timestamp 78 ] state=tensor([[-0.0861, -0.1519,  0.0953,  0.1917]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0891,  0.0417,  0.0991, -0.0695]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 79 ] state=tensor([[-0.0891,  0.0417,  0.0991, -0.0695]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0883, -0.1547,  0.0977,  0.2527]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 80 ] state=tensor([[-0.0883, -0.1547,  0.0977,  0.2527]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0914,  0.0389,  0.1028, -0.0076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 81 ] state=tensor([[-0.0914,  0.0389,  0.1028, -0.0076]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0906,  0.2324,  0.1026, -0.2662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 82 ] state=tensor([[-0.0906,  0.2324,  0.1026, -0.2662]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0860,  0.4259,  0.0973, -0.5249]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 83 ] state=tensor([[-0.0860,  0.4259,  0.0973, -0.5249]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0774,  0.6196,  0.0868, -0.7854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 84 ] state=tensor([[-0.0774,  0.6196,  0.0868, -0.7854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0650,  0.8134,  0.0711, -1.0495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 85 ] state=tensor([[-0.0650,  0.8134,  0.0711, -1.0495]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0488,  1.0075,  0.0501, -1.3191]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 86 ] state=tensor([[-0.0488,  1.0075,  0.0501, -1.3191]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0286,  1.2020,  0.0237, -1.5957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 87 ] state=tensor([[-0.0286,  1.2020,  0.0237, -1.5957]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0046,  1.3968, -0.0082, -1.8809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 88 ] state=tensor([[-0.0046,  1.3968, -0.0082, -1.8809]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0234,  1.2018, -0.0458, -1.5908]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 89 ] state=tensor([[ 0.0234,  1.2018, -0.0458, -1.5908]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0474,  1.0072, -0.0777, -1.3127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 90 ] state=tensor([[ 0.0474,  1.0072, -0.0777, -1.3127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0675,  0.8132, -0.1039, -1.0453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 91 ] state=tensor([[ 0.0675,  0.8132, -0.1039, -1.0453]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0838,  0.6196, -0.1248, -0.7870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 92 ] state=tensor([[ 0.0838,  0.6196, -0.1248, -0.7870]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0962,  0.4264, -0.1406, -0.5360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 93 ] state=tensor([[ 0.0962,  0.4264, -0.1406, -0.5360]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1047,  0.2335, -0.1513, -0.2907]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 94 ] state=tensor([[ 0.1047,  0.2335, -0.1513, -0.2907]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1094,  0.0408, -0.1571, -0.0493]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 95 ] state=tensor([[ 0.1094,  0.0408, -0.1571, -0.0493]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1102, -0.1518, -0.1581,  0.1900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 96 ] state=tensor([[ 0.1102, -0.1518, -0.1581,  0.1900]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1072, -0.3443, -0.1543,  0.4289]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 97 ] state=tensor([[ 0.1072, -0.3443, -0.1543,  0.4289]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1003, -0.5370, -0.1457,  0.6693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 98 ] state=tensor([[ 0.1003, -0.5370, -0.1457,  0.6693]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0895, -0.3402, -0.1323,  0.3345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 99 ] state=tensor([[ 0.0895, -0.3402, -0.1323,  0.3345]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0827, -0.5332, -0.1256,  0.5827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 100 ] state=tensor([[ 0.0827, -0.5332, -0.1256,  0.5827]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0721, -0.3365, -0.1140,  0.2532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 101 ] state=tensor([[ 0.0721, -0.3365, -0.1140,  0.2532]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0653, -0.5299, -0.1089,  0.5079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 102 ] state=tensor([[ 0.0653, -0.5299, -0.1089,  0.5079]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0547, -0.3334, -0.0987,  0.1830]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 103 ] state=tensor([[ 0.0547, -0.3334, -0.0987,  0.1830]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0481, -0.5270, -0.0951,  0.4430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 104 ] state=tensor([[ 0.0481, -0.5270, -0.0951,  0.4430]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0375, -0.7206, -0.0862,  0.7042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 105 ] state=tensor([[ 0.0375, -0.7206, -0.0862,  0.7042]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0231, -0.5244, -0.0721,  0.3857]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 106 ] state=tensor([[ 0.0231, -0.5244, -0.0721,  0.3857]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0126, -0.7184, -0.0644,  0.6548]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 107 ] state=tensor([[ 0.0126, -0.7184, -0.0644,  0.6548]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0017, -0.9126, -0.0513,  0.9265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 108 ] state=tensor([[-0.0017, -0.9126, -0.0513,  0.9265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0200, -0.7168, -0.0328,  0.6181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 109 ] state=tensor([[-0.0200, -0.7168, -0.0328,  0.6181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0343, -0.5213, -0.0204,  0.3153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 110 ] state=tensor([[-0.0343, -0.5213, -0.0204,  0.3153]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0447, -0.3259, -0.0141,  0.0162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 111 ] state=tensor([[-0.0447, -0.3259, -0.0141,  0.0162]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0513, -0.5208, -0.0138,  0.3044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 112 ] state=tensor([[-0.0513, -0.5208, -0.0138,  0.3044]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0617, -0.3255, -0.0077,  0.0074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 113 ] state=tensor([[-0.0617, -0.3255, -0.0077,  0.0074]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0682, -0.5205, -0.0076,  0.2977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 114 ] state=tensor([[-0.0682, -0.5205, -0.0076,  0.2977]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0786, -0.7155, -0.0016,  0.5880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 115 ] state=tensor([[-0.0786, -0.7155, -0.0016,  0.5880]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0929, -0.5203,  0.0101,  0.2948]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 116 ] state=tensor([[-0.0929, -0.5203,  0.0101,  0.2948]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1033, -0.7156,  0.0160,  0.5906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 117 ] state=tensor([[-0.1033, -0.7156,  0.0160,  0.5906]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1176, -0.5207,  0.0279,  0.3030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 118 ] state=tensor([[-0.1176, -0.5207,  0.0279,  0.3030]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1280, -0.7162,  0.0339,  0.6044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 119 ] state=tensor([[-0.1280, -0.7162,  0.0339,  0.6044]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1424, -0.5216,  0.0460,  0.3226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 120 ] state=tensor([[-0.1424, -0.5216,  0.0460,  0.3226]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1528, -0.7173,  0.0524,  0.6294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 121 ] state=tensor([[-0.1528, -0.7173,  0.0524,  0.6294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1671, -0.5230,  0.0650,  0.3537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 122 ] state=tensor([[-0.1671, -0.5230,  0.0650,  0.3537]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1776, -0.3288,  0.0721,  0.0822]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 123 ] state=tensor([[-0.1776, -0.3288,  0.0721,  0.0822]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1842, -0.5249,  0.0738,  0.3967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 124 ] state=tensor([[-0.1842, -0.5249,  0.0738,  0.3967]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1947, -0.3309,  0.0817,  0.1282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 125 ] state=tensor([[-0.1947, -0.3309,  0.0817,  0.1282]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2013, -0.1371,  0.0843, -0.1377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 126 ] state=tensor([[-0.2013, -0.1371,  0.0843, -0.1377]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2040,  0.0568,  0.0815, -0.4026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 127 ] state=tensor([[-0.2040,  0.0568,  0.0815, -0.4026]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2029,  0.2506,  0.0734, -0.6685]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 128 ] state=tensor([[-0.2029,  0.2506,  0.0734, -0.6685]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1979,  0.4447,  0.0601, -0.9372]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 129 ] state=tensor([[-0.1979,  0.4447,  0.0601, -0.9372]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1890,  0.6389,  0.0413, -1.2104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 130 ] state=tensor([[-0.1890,  0.6389,  0.0413, -1.2104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1762,  0.8335,  0.0171, -1.4899]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 131 ] state=tensor([[-0.1762,  0.8335,  0.0171, -1.4899]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1595,  1.0284, -0.0127, -1.7772]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 132 ] state=tensor([[-0.1595,  1.0284, -0.0127, -1.7772]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1390,  0.8334, -0.0482, -1.4885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 133 ] state=tensor([[-0.1390,  0.8334, -0.0482, -1.4885]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1223,  0.6389, -0.0780, -1.2112]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 134 ] state=tensor([[-0.1223,  0.6389, -0.0780, -1.2112]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1095,  0.4449, -0.1022, -0.9439]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 135 ] state=tensor([[-0.1095,  0.4449, -0.1022, -0.9439]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1006,  0.2513, -0.1211, -0.6850]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 136 ] state=tensor([[-0.1006,  0.2513, -0.1211, -0.6850]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0956,  0.0580, -0.1348, -0.4328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 137 ] state=tensor([[-0.0956,  0.0580, -0.1348, -0.4328]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0944, -0.1349, -0.1434, -0.1855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 138 ] state=tensor([[-0.0944, -0.1349, -0.1434, -0.1855]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0971, -0.3278, -0.1472,  0.0587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 139 ] state=tensor([[-0.0971, -0.3278, -0.1472,  0.0587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1037, -0.5205, -0.1460,  0.3016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 140 ] state=tensor([[-0.1037, -0.5205, -0.1460,  0.3016]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1141, -0.7133, -0.1399,  0.5449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 141 ] state=tensor([[-0.1141, -0.7133, -0.1399,  0.5449]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1284, -0.5165, -0.1290,  0.2116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 142 ] state=tensor([[-0.1284, -0.5165, -0.1290,  0.2116]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1387, -0.3198, -0.1248, -0.1188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 143 ] state=tensor([[-0.1387, -0.3198, -0.1248, -0.1188]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1451, -0.5129, -0.1272,  0.1320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 144 ] state=tensor([[-0.1451, -0.5129, -0.1272,  0.1320]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1554, -0.7060, -0.1246,  0.3821]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 145 ] state=tensor([[-0.1554, -0.7060, -0.1246,  0.3821]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1695, -0.8992, -0.1169,  0.6330]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 146 ] state=tensor([[-0.1695, -0.8992, -0.1169,  0.6330]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1875, -0.7026, -0.1042,  0.3059]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 147 ] state=tensor([[-0.1875, -0.7026, -0.1042,  0.3059]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2015, -0.5062, -0.0981, -0.0177]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 148 ] state=tensor([[-0.2015, -0.5062, -0.0981, -0.0177]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2116, -0.6998, -0.0985,  0.2424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 149 ] state=tensor([[-0.2116, -0.6998, -0.0985,  0.2424]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2256, -0.8933, -0.0936,  0.5025]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 103 ][ timestamp 150 ] state=tensor([[-0.2256, -0.8933, -0.0936,  0.5025]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2435, -0.6970, -0.0836,  0.1818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 151 ] state=tensor([[-0.2435, -0.6970, -0.0836,  0.1818]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2574, -0.8909, -0.0800,  0.4470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 152 ] state=tensor([[-0.2574, -0.8909, -0.0800,  0.4470]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2753, -0.6947, -0.0710,  0.1303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 153 ] state=tensor([[-0.2753, -0.6947, -0.0710,  0.1303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2892, -0.8888, -0.0684,  0.3997]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 154 ] state=tensor([[-0.2892, -0.8888, -0.0684,  0.3997]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3069, -1.0828, -0.0604,  0.6701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 155 ] state=tensor([[-0.3069, -1.0828, -0.0604,  0.6701]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3286, -0.8869, -0.0470,  0.3590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 156 ] state=tensor([[-0.3286, -0.8869, -0.0470,  0.3590]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3463, -0.6912, -0.0398,  0.0519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 157 ] state=tensor([[-0.3463, -0.6912, -0.0398,  0.0519]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3601, -0.8857, -0.0388,  0.3317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 158 ] state=tensor([[-0.3601, -0.8857, -0.0388,  0.3317]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3779, -0.6901, -0.0322,  0.0271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 159 ] state=tensor([[-0.3779, -0.6901, -0.0322,  0.0271]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3917, -0.4945, -0.0316, -0.2756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 160 ] state=tensor([[-0.3917, -0.4945, -0.0316, -0.2756]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4016, -0.6891, -0.0371,  0.0070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 161 ] state=tensor([[-0.4016, -0.6891, -0.0371,  0.0070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4153, -0.4935, -0.0370, -0.2972]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 162 ] state=tensor([[-0.4153, -0.4935, -0.0370, -0.2972]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4252, -0.6881, -0.0429, -0.0164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 163 ] state=tensor([[-0.4252, -0.6881, -0.0429, -0.0164]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4390, -0.4924, -0.0433, -0.3223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 164 ] state=tensor([[-0.4390, -0.4924, -0.0433, -0.3223]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4488, -0.6869, -0.0497, -0.0436]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 165 ] state=tensor([[-0.4488, -0.6869, -0.0497, -0.0436]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4626, -0.4911, -0.0506, -0.3515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 166 ] state=tensor([[-0.4626, -0.4911, -0.0506, -0.3515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4724, -0.6854, -0.0576, -0.0752]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 167 ] state=tensor([[-0.4724, -0.6854, -0.0576, -0.0752]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4861, -0.4895, -0.0591, -0.3855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 168 ] state=tensor([[-0.4861, -0.4895, -0.0591, -0.3855]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4959, -0.6838, -0.0668, -0.1120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 169 ] state=tensor([[-0.4959, -0.6838, -0.0668, -0.1120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5095, -0.4877, -0.0691, -0.4250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 170 ] state=tensor([[-0.5095, -0.4877, -0.0691, -0.4250]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5193, -0.6818, -0.0776, -0.1549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 171 ] state=tensor([[-0.5193, -0.6818, -0.0776, -0.1549]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5329, -0.4857, -0.0807, -0.4710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 172 ] state=tensor([[-0.5329, -0.4857, -0.0807, -0.4710]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5427, -0.6796, -0.0901, -0.2048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 173 ] state=tensor([[-0.5427, -0.6796, -0.0901, -0.2048]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5562, -0.8733, -0.0942,  0.0582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 174 ] state=tensor([[-0.5562, -0.8733, -0.0942,  0.0582]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5737, -0.6770, -0.0930, -0.2627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 175 ] state=tensor([[-0.5737, -0.6770, -0.0930, -0.2627]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-5.8725e-01, -8.7065e-01, -9.8265e-02, -7.0486e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 176 ] state=tensor([[-5.8725e-01, -8.7065e-01, -9.8265e-02, -7.0486e-04]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6047, -0.6743, -0.0983, -0.3227]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 177 ] state=tensor([[-0.6047, -0.6743, -0.0983, -0.3227]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6181, -0.4779, -0.1047, -0.6447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 178 ] state=tensor([[-0.6181, -0.4779, -0.1047, -0.6447]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6277, -0.6714, -0.1176, -0.3867]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 179 ] state=tensor([[-0.6277, -0.6714, -0.1176, -0.3867]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6411, -0.8647, -0.1254, -0.1333]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 180 ] state=tensor([[-0.6411, -0.8647, -0.1254, -0.1333]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6584, -0.6680, -0.1280, -0.4628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 181 ] state=tensor([[-0.6584, -0.6680, -0.1280, -0.4628]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6718, -0.8611, -0.1373, -0.2130]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 182 ] state=tensor([[-0.6718, -0.8611, -0.1373, -0.2130]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6890, -0.6643, -0.1415, -0.5457]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 183 ] state=tensor([[-0.6890, -0.6643, -0.1415, -0.5457]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7023, -0.8572, -0.1525, -0.3007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 184 ] state=tensor([[-0.7023, -0.8572, -0.1525, -0.3007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7194, -1.0499, -0.1585, -0.0598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 185 ] state=tensor([[-0.7194, -1.0499, -0.1585, -0.0598]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7404, -0.8529, -0.1597, -0.3979]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 186 ] state=tensor([[-0.7404, -0.8529, -0.1597, -0.3979]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7575, -1.0454, -0.1676, -0.1596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 187 ] state=tensor([[-0.7575, -1.0454, -0.1676, -0.1596]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7784, -1.2378, -0.1708,  0.0759]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 188 ] state=tensor([[-0.7784, -1.2378, -0.1708,  0.0759]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8032, -1.0407, -0.1693, -0.2654]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 189 ] state=tensor([[-0.8032, -1.0407, -0.1693, -0.2654]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8240, -1.2330, -0.1746, -0.0306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 190 ] state=tensor([[-0.8240, -1.2330, -0.1746, -0.0306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8486, -1.0359, -0.1752, -0.3728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 191 ] state=tensor([[-0.8486, -1.0359, -0.1752, -0.3728]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8694, -1.2281, -0.1827, -0.1401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 192 ] state=tensor([[-0.8694, -1.2281, -0.1827, -0.1401]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8939, -1.0309, -0.1855, -0.4844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 193 ] state=tensor([[-0.8939, -1.0309, -0.1855, -0.4844]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9145, -0.8337, -0.1952, -0.8293]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 103 ][ timestamp 194 ] state=tensor([[-0.9145, -0.8337, -0.1952, -0.8293]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 103: Exploration_rate=0.05. Score=194.\n",
      "[ episode 104 ] state=tensor([[ 0.0417,  0.0133, -0.0197,  0.0350]])\n",
      "[ episode 104 ][ timestamp 1 ] state=tensor([[ 0.0417,  0.0133, -0.0197,  0.0350]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0419, -0.1815, -0.0190,  0.3214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 2 ] state=tensor([[ 0.0419, -0.1815, -0.0190,  0.3214]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0383,  0.0139, -0.0125,  0.0228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 3 ] state=tensor([[ 0.0383,  0.0139, -0.0125,  0.0228]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0386,  0.2092, -0.0121, -0.2738]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 4 ] state=tensor([[ 0.0386,  0.2092, -0.0121, -0.2738]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0428,  0.0142, -0.0176,  0.0150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 5 ] state=tensor([[ 0.0428,  0.0142, -0.0176,  0.0150]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0430, -0.1806, -0.0173,  0.3021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 6 ] state=tensor([[ 0.0430, -0.1806, -0.0173,  0.3021]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0394, -0.3755, -0.0112,  0.5893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 7 ] state=tensor([[ 0.0394, -0.3755, -0.0112,  0.5893]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0319, -0.1802,  0.0006,  0.2931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 8 ] state=tensor([[ 0.0319, -0.1802,  0.0006,  0.2931]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0283, -0.3753,  0.0064,  0.5860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 9 ] state=tensor([[ 0.0283, -0.3753,  0.0064,  0.5860]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0208, -0.1803,  0.0182,  0.2953]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 10 ] state=tensor([[ 0.0208, -0.1803,  0.0182,  0.2953]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[0.0172, 0.0145, 0.0241, 0.0084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 11 ] state=tensor([[0.0172, 0.0145, 0.0241, 0.0084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0175,  0.2093,  0.0242, -0.2766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 12 ] state=tensor([[ 0.0175,  0.2093,  0.0242, -0.2766]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[0.0217, 0.0139, 0.0187, 0.0237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 13 ] state=tensor([[0.0217, 0.0139, 0.0187, 0.0237]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0220,  0.2087,  0.0192, -0.2631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 14 ] state=tensor([[ 0.0220,  0.2087,  0.0192, -0.2631]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[0.0261, 0.0133, 0.0139, 0.0356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 15 ] state=tensor([[0.0261, 0.0133, 0.0139, 0.0356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0264,  0.2082,  0.0146, -0.2527]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 16 ] state=tensor([[ 0.0264,  0.2082,  0.0146, -0.2527]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0306,  0.4031,  0.0096, -0.5407]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 17 ] state=tensor([[ 0.0306,  0.4031,  0.0096, -0.5407]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0386,  0.2079, -0.0012, -0.2450]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 18 ] state=tensor([[ 0.0386,  0.2079, -0.0012, -0.2450]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0428,  0.0128, -0.0061,  0.0473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 19 ] state=tensor([[ 0.0428,  0.0128, -0.0061,  0.0473]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0430,  0.2080, -0.0052, -0.2473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 20 ] state=tensor([[ 0.0430,  0.2080, -0.0052, -0.2473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0472,  0.0130, -0.0102,  0.0437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 21 ] state=tensor([[ 0.0472,  0.0130, -0.0102,  0.0437]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0475,  0.2082, -0.0093, -0.2522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 22 ] state=tensor([[ 0.0475,  0.2082, -0.0093, -0.2522]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0516,  0.4035, -0.0143, -0.5478]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 104 ][ timestamp 23 ] state=tensor([[ 0.0516,  0.4035, -0.0143, -0.5478]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0597,  0.5988, -0.0253, -0.8449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 24 ] state=tensor([[ 0.0597,  0.5988, -0.0253, -0.8449]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0717,  0.4040, -0.0422, -0.5603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 25 ] state=tensor([[ 0.0717,  0.4040, -0.0422, -0.5603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0798,  0.2095, -0.0534, -0.2812]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 26 ] state=tensor([[ 0.0798,  0.2095, -0.0534, -0.2812]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0839,  0.0152, -0.0590, -0.0058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 27 ] state=tensor([[ 0.0839,  0.0152, -0.0590, -0.0058]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0842, -0.1790, -0.0591,  0.2677]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 28 ] state=tensor([[ 0.0842, -0.1790, -0.0591,  0.2677]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0807, -0.3733, -0.0538,  0.5411]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 29 ] state=tensor([[ 0.0807, -0.3733, -0.0538,  0.5411]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0732, -0.1774, -0.0429,  0.2320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 30 ] state=tensor([[ 0.0732, -0.1774, -0.0429,  0.2320]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0697, -0.3719, -0.0383,  0.5108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 31 ] state=tensor([[ 0.0697, -0.3719, -0.0383,  0.5108]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0622, -0.1763, -0.0281,  0.2063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 32 ] state=tensor([[ 0.0622, -0.1763, -0.0281,  0.2063]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0587, -0.3710, -0.0240,  0.4900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 33 ] state=tensor([[ 0.0587, -0.3710, -0.0240,  0.4900]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0513, -0.5658, -0.0142,  0.7751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 34 ] state=tensor([[ 0.0513, -0.5658, -0.0142,  0.7751]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0400, -0.3704,  0.0013,  0.4780]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 35 ] state=tensor([[ 0.0400, -0.3704,  0.0013,  0.4780]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0325, -0.5656,  0.0109,  0.7711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 36 ] state=tensor([[ 0.0325, -0.5656,  0.0109,  0.7711]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0212, -0.3706,  0.0263,  0.4818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 37 ] state=tensor([[ 0.0212, -0.3706,  0.0263,  0.4818]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0138, -0.1759,  0.0360,  0.1976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 38 ] state=tensor([[ 0.0138, -0.1759,  0.0360,  0.1976]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0103,  0.0187,  0.0399, -0.0836]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 39 ] state=tensor([[ 0.0103,  0.0187,  0.0399, -0.0836]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0107,  0.2132,  0.0382, -0.3634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 40 ] state=tensor([[ 0.0107,  0.2132,  0.0382, -0.3634]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0149,  0.4078,  0.0310, -0.6438]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 41 ] state=tensor([[ 0.0149,  0.4078,  0.0310, -0.6438]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0231,  0.6025,  0.0181, -0.9265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 42 ] state=tensor([[ 0.0231,  0.6025,  0.0181, -0.9265]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 3.5150e-02,  4.0712e-01, -4.3487e-04, -6.2823e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 43 ] state=tensor([[ 3.5150e-02,  4.0712e-01, -4.3487e-04, -6.2823e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0433,  0.2120, -0.0130, -0.3357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 44 ] state=tensor([[ 0.0433,  0.2120, -0.0130, -0.3357]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0475,  0.0171, -0.0197, -0.0471]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 45 ] state=tensor([[ 0.0475,  0.0171, -0.0197, -0.0471]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0479,  0.2125, -0.0207, -0.3460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 46 ] state=tensor([[ 0.0479,  0.2125, -0.0207, -0.3460]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0521,  0.0176, -0.0276, -0.0599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 47 ] state=tensor([[ 0.0521,  0.0176, -0.0276, -0.0599]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0525, -0.1771, -0.0288,  0.2240]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 48 ] state=tensor([[ 0.0525, -0.1771, -0.0288,  0.2240]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0489, -0.3718, -0.0243,  0.5075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 49 ] state=tensor([[ 0.0489, -0.3718, -0.0243,  0.5075]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0415, -0.1763, -0.0141,  0.2072]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 50 ] state=tensor([[ 0.0415, -0.1763, -0.0141,  0.2072]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0380,  0.0190, -0.0100, -0.0899]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 51 ] state=tensor([[ 0.0380,  0.0190, -0.0100, -0.0899]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0384, -0.1760, -0.0118,  0.1996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 52 ] state=tensor([[ 0.0384, -0.1760, -0.0118,  0.1996]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0348,  0.0193, -0.0078, -0.0968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 53 ] state=tensor([[ 0.0348,  0.0193, -0.0078, -0.0968]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0352, -0.1757, -0.0097,  0.1935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 54 ] state=tensor([[ 0.0352, -0.1757, -0.0097,  0.1935]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0317,  0.0196, -0.0059, -0.1023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 55 ] state=tensor([[ 0.0317,  0.0196, -0.0059, -0.1023]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0321,  0.2148, -0.0079, -0.3968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 56 ] state=tensor([[ 0.0321,  0.2148, -0.0079, -0.3968]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0364,  0.0198, -0.0159, -0.1066]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 57 ] state=tensor([[ 0.0364,  0.0198, -0.0159, -0.1066]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0368, -0.1751, -0.0180,  0.1810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 58 ] state=tensor([[ 0.0368, -0.1751, -0.0180,  0.1810]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0333, -0.3700, -0.0144,  0.4680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 59 ] state=tensor([[ 0.0333, -0.3700, -0.0144,  0.4680]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0259, -0.5649, -0.0050,  0.7561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 60 ] state=tensor([[ 0.0259, -0.5649, -0.0050,  0.7561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0146, -0.3697,  0.0101,  0.4618]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 61 ] state=tensor([[ 0.0146, -0.3697,  0.0101,  0.4618]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0072, -0.1747,  0.0194,  0.1723]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 62 ] state=tensor([[ 0.0072, -0.1747,  0.0194,  0.1723]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0037,  0.0201,  0.0228, -0.1142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 63 ] state=tensor([[ 0.0037,  0.0201,  0.0228, -0.1142]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0041,  0.2149,  0.0205, -0.3996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 64 ] state=tensor([[ 0.0041,  0.2149,  0.0205, -0.3996]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0084,  0.0195,  0.0125, -0.1005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 65 ] state=tensor([[ 0.0084,  0.0195,  0.0125, -0.1005]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0088, -0.1758,  0.0105,  0.1961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 66 ] state=tensor([[ 0.0088, -0.1758,  0.0105,  0.1961]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0053, -0.3711,  0.0144,  0.4921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 67 ] state=tensor([[ 0.0053, -0.3711,  0.0144,  0.4921]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0021, -0.1762,  0.0243,  0.2040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 68 ] state=tensor([[-0.0021, -0.1762,  0.0243,  0.2040]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0057, -0.3716,  0.0284,  0.5042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 69 ] state=tensor([[-0.0057, -0.3716,  0.0284,  0.5042]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0131, -0.1769,  0.0384,  0.2206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 70 ] state=tensor([[-0.0131, -0.1769,  0.0384,  0.2206]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0166,  0.0176,  0.0429, -0.0597]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 71 ] state=tensor([[-0.0166,  0.0176,  0.0429, -0.0597]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0163,  0.2121,  0.0417, -0.3385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 72 ] state=tensor([[-0.0163,  0.2121,  0.0417, -0.3385]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0120,  0.4066,  0.0349, -0.6178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 73 ] state=tensor([[-0.0120,  0.4066,  0.0349, -0.6178]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0039,  0.6012,  0.0225, -0.8993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 74 ] state=tensor([[-0.0039,  0.6012,  0.0225, -0.8993]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0081,  0.4058,  0.0045, -0.5996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 75 ] state=tensor([[ 0.0081,  0.4058,  0.0045, -0.5996]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0162,  0.6009, -0.0074, -0.8909]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 76 ] state=tensor([[ 0.0162,  0.6009, -0.0074, -0.8909]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0282,  0.4059, -0.0253, -0.6005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 77 ] state=tensor([[ 0.0282,  0.4059, -0.0253, -0.6005]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0364,  0.2111, -0.0373, -0.3159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 78 ] state=tensor([[ 0.0364,  0.2111, -0.0373, -0.3159]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0406,  0.0165, -0.0436, -0.0352]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 104 ][ timestamp 79 ] state=tensor([[ 0.0406,  0.0165, -0.0436, -0.0352]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0409, -0.1779, -0.0443,  0.2434]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 80 ] state=tensor([[ 0.0409, -0.1779, -0.0443,  0.2434]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0374,  0.0178, -0.0394, -0.0629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 81 ] state=tensor([[ 0.0374,  0.0178, -0.0394, -0.0629]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0377, -0.1768, -0.0407,  0.2171]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 82 ] state=tensor([[ 0.0377, -0.1768, -0.0407,  0.2171]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0342, -0.3713, -0.0363,  0.4967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 83 ] state=tensor([[ 0.0342, -0.3713, -0.0363,  0.4967]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0267, -0.1757, -0.0264,  0.1927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 84 ] state=tensor([[ 0.0267, -0.1757, -0.0264,  0.1927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0232,  0.0198, -0.0226, -0.1081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 85 ] state=tensor([[ 0.0232,  0.0198, -0.0226, -0.1081]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0236,  0.2153, -0.0247, -0.4079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 86 ] state=tensor([[ 0.0236,  0.2153, -0.0247, -0.4079]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0279,  0.0205, -0.0329, -0.1231]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 87 ] state=tensor([[ 0.0279,  0.0205, -0.0329, -0.1231]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0283, -0.1741, -0.0353,  0.1591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 88 ] state=tensor([[ 0.0283, -0.1741, -0.0353,  0.1591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0249, -0.3687, -0.0322,  0.4404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 89 ] state=tensor([[ 0.0249, -0.3687, -0.0322,  0.4404]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0175, -0.1732, -0.0233,  0.1378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 90 ] state=tensor([[ 0.0175, -0.1732, -0.0233,  0.1378]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0140,  0.0223, -0.0206, -0.1622]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 91 ] state=tensor([[ 0.0140,  0.0223, -0.0206, -0.1622]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0145, -0.1725, -0.0238,  0.1239]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 92 ] state=tensor([[ 0.0145, -0.1725, -0.0238,  0.1239]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0110, -0.3673, -0.0214,  0.4090]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 93 ] state=tensor([[ 0.0110, -0.3673, -0.0214,  0.4090]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0037, -0.5621, -0.0132,  0.6949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 94 ] state=tensor([[ 0.0037, -0.5621, -0.0132,  0.6949]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0076, -0.3668,  0.0007,  0.3981]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 95 ] state=tensor([[-0.0076, -0.3668,  0.0007,  0.3981]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0149, -0.1717,  0.0087,  0.1056]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 96 ] state=tensor([[-0.0149, -0.1717,  0.0087,  0.1056]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0183, -0.3670,  0.0108,  0.4010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 97 ] state=tensor([[-0.0183, -0.3670,  0.0108,  0.4010]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0257, -0.1720,  0.0188,  0.1117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 98 ] state=tensor([[-0.0257, -0.1720,  0.0188,  0.1117]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0291, -0.3674,  0.0210,  0.4103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 99 ] state=tensor([[-0.0291, -0.3674,  0.0210,  0.4103]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0365, -0.5628,  0.0293,  0.7096]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 100 ] state=tensor([[-0.0365, -0.5628,  0.0293,  0.7096]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0477, -0.3681,  0.0434,  0.4262]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 101 ] state=tensor([[-0.0477, -0.3681,  0.0434,  0.4262]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0551, -0.5638,  0.0520,  0.7323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 102 ] state=tensor([[-0.0551, -0.5638,  0.0520,  0.7323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0664, -0.3694,  0.0666,  0.4564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 103 ] state=tensor([[-0.0664, -0.3694,  0.0666,  0.4564]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0737, -0.1753,  0.0757,  0.1854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 104 ] state=tensor([[-0.0737, -0.1753,  0.0757,  0.1854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0773,  0.0187,  0.0795, -0.0824]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 105 ] state=tensor([[-0.0773,  0.0187,  0.0795, -0.0824]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0769,  0.2126,  0.0778, -0.3490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 106 ] state=tensor([[-0.0769,  0.2126,  0.0778, -0.3490]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0726,  0.4065,  0.0708, -0.6162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 107 ] state=tensor([[-0.0726,  0.4065,  0.0708, -0.6162]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0645,  0.6006,  0.0585, -0.8858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 108 ] state=tensor([[-0.0645,  0.6006,  0.0585, -0.8858]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0525,  0.7948,  0.0408, -1.1595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 109 ] state=tensor([[-0.0525,  0.7948,  0.0408, -1.1595]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0366,  0.9894,  0.0176, -1.4391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 110 ] state=tensor([[-0.0366,  0.9894,  0.0176, -1.4391]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0168,  0.7941, -0.0112, -1.1410]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 111 ] state=tensor([[-0.0168,  0.7941, -0.0112, -1.1410]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0009,  0.5991, -0.0340, -0.8518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 112 ] state=tensor([[-0.0009,  0.5991, -0.0340, -0.8518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0111,  0.4045, -0.0510, -0.5700]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 113 ] state=tensor([[ 0.0111,  0.4045, -0.0510, -0.5700]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0191,  0.2101, -0.0624, -0.2939]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 114 ] state=tensor([[ 0.0191,  0.2101, -0.0624, -0.2939]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0234,  0.0159, -0.0683, -0.0215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 115 ] state=tensor([[ 0.0234,  0.0159, -0.0683, -0.0215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0237, -0.1782, -0.0688,  0.2489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 116 ] state=tensor([[ 0.0237, -0.1782, -0.0688,  0.2489]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0201,  0.0179, -0.0638, -0.0647]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 117 ] state=tensor([[ 0.0201,  0.0179, -0.0638, -0.0647]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0205,  0.2138, -0.0651, -0.3768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 118 ] state=tensor([[ 0.0205,  0.2138, -0.0651, -0.3768]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0247,  0.0197, -0.0726, -0.1053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 119 ] state=tensor([[ 0.0247,  0.0197, -0.0726, -0.1053]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0251,  0.2158, -0.0747, -0.4200]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 104 ][ timestamp 120 ] state=tensor([[ 0.0251,  0.2158, -0.0747, -0.4200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0294,  0.0218, -0.0831, -0.1518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 121 ] state=tensor([[ 0.0294,  0.0218, -0.0831, -0.1518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0299, -0.1721, -0.0861,  0.1136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 122 ] state=tensor([[ 0.0299, -0.1721, -0.0861,  0.1136]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0264, -0.3658, -0.0839,  0.3779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 123 ] state=tensor([[ 0.0264, -0.3658, -0.0839,  0.3779]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0191, -0.1696, -0.0763,  0.0600]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 124 ] state=tensor([[ 0.0191, -0.1696, -0.0763,  0.0600]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0157, -0.3636, -0.0751,  0.3277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 125 ] state=tensor([[ 0.0157, -0.3636, -0.0751,  0.3277]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0085, -0.1675, -0.0686,  0.0123]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 126 ] state=tensor([[ 0.0085, -0.1675, -0.0686,  0.0123]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0051, -0.3616, -0.0683,  0.2825]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 127 ] state=tensor([[ 0.0051, -0.3616, -0.0683,  0.2825]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0021, -0.1655, -0.0627, -0.0309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 128 ] state=tensor([[-0.0021, -0.1655, -0.0627, -0.0309]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0054, -0.3597, -0.0633,  0.2414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 129 ] state=tensor([[-0.0054, -0.3597, -0.0633,  0.2414]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0126, -0.1637, -0.0585, -0.0706]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 130 ] state=tensor([[-0.0126, -0.1637, -0.0585, -0.0706]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0159, -0.3580, -0.0599,  0.2031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 131 ] state=tensor([[-0.0159, -0.3580, -0.0599,  0.2031]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0231, -0.5522, -0.0558,  0.4763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 132 ] state=tensor([[-0.0231, -0.5522, -0.0558,  0.4763]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0341, -0.3563, -0.0463,  0.1666]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 133 ] state=tensor([[-0.0341, -0.3563, -0.0463,  0.1666]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0412, -0.1606, -0.0429, -0.1403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 134 ] state=tensor([[-0.0412, -0.1606, -0.0429, -0.1403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0444, -0.3551, -0.0458,  0.1385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 135 ] state=tensor([[-0.0444, -0.3551, -0.0458,  0.1385]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0515, -0.1593, -0.0430, -0.1682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 136 ] state=tensor([[-0.0515, -0.1593, -0.0430, -0.1682]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0547, -0.3538, -0.0463,  0.1106]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 137 ] state=tensor([[-0.0547, -0.3538, -0.0463,  0.1106]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0618, -0.5482, -0.0441,  0.3883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 138 ] state=tensor([[-0.0618, -0.5482, -0.0441,  0.3883]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0728, -0.3525, -0.0364,  0.0820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 139 ] state=tensor([[-0.0728, -0.3525, -0.0364,  0.0820]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0798, -0.1569, -0.0347, -0.2219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 140 ] state=tensor([[-0.0798, -0.1569, -0.0347, -0.2219]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0830,  0.0387, -0.0392, -0.5253]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 141 ] state=tensor([[-0.0830,  0.0387, -0.0392, -0.5253]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0822, -0.1558, -0.0497, -0.2453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 142 ] state=tensor([[-0.0822, -0.1558, -0.0497, -0.2453]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0853, -0.3502, -0.0546,  0.0313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 143 ] state=tensor([[-0.0853, -0.3502, -0.0546,  0.0313]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0923, -0.1543, -0.0540, -0.2780]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 144 ] state=tensor([[-0.0923, -0.1543, -0.0540, -0.2780]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0954, -0.3486, -0.0595, -0.0029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 145 ] state=tensor([[-0.0954, -0.3486, -0.0595, -0.0029]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1024, -0.5429, -0.0596,  0.2705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 146 ] state=tensor([[-0.1024, -0.5429, -0.0596,  0.2705]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1132, -0.7371, -0.0542,  0.5438]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 147 ] state=tensor([[-0.1132, -0.7371, -0.0542,  0.5438]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1280, -0.5413, -0.0433,  0.2345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 148 ] state=tensor([[-0.1280, -0.5413, -0.0433,  0.2345]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1388, -0.3455, -0.0386, -0.0715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 149 ] state=tensor([[-0.1388, -0.3455, -0.0386, -0.0715]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1457, -0.5401, -0.0400,  0.2088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 150 ] state=tensor([[-0.1457, -0.5401, -0.0400,  0.2088]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1565, -0.3444, -0.0358, -0.0962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 151 ] state=tensor([[-0.1565, -0.3444, -0.0358, -0.0962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1634, -0.5390, -0.0378,  0.1849]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 152 ] state=tensor([[-0.1634, -0.5390, -0.0378,  0.1849]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1742, -0.3434, -0.0341, -0.1194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 153 ] state=tensor([[-0.1742, -0.3434, -0.0341, -0.1194]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1810, -0.5380, -0.0365,  0.1623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 154 ] state=tensor([[-0.1810, -0.5380, -0.0365,  0.1623]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1918, -0.3424, -0.0332, -0.1417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 155 ] state=tensor([[-0.1918, -0.3424, -0.0332, -0.1417]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1986, -0.5370, -0.0361,  0.1404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 156 ] state=tensor([[-0.1986, -0.5370, -0.0361,  0.1404]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2094, -0.3414, -0.0332, -0.1635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 157 ] state=tensor([[-0.2094, -0.3414, -0.0332, -0.1635]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2162, -0.5360, -0.0365,  0.1185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 158 ] state=tensor([[-0.2162, -0.5360, -0.0365,  0.1185]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2269, -0.7306, -0.0341,  0.3995]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 159 ] state=tensor([[-0.2269, -0.7306, -0.0341,  0.3995]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2415, -0.5350, -0.0262,  0.0962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 160 ] state=tensor([[-0.2415, -0.5350, -0.0262,  0.0962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2522, -0.7297, -0.0242,  0.3806]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 161 ] state=tensor([[-0.2522, -0.7297, -0.0242,  0.3806]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2668, -0.5343, -0.0166,  0.0803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 162 ] state=tensor([[-0.2668, -0.5343, -0.0166,  0.0803]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2775, -0.3389, -0.0150, -0.2175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 163 ] state=tensor([[-0.2775, -0.3389, -0.0150, -0.2175]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2843, -0.5338, -0.0194,  0.0704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 164 ] state=tensor([[-0.2843, -0.5338, -0.0194,  0.0704]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2950, -0.3384, -0.0180, -0.2284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 165 ] state=tensor([[-0.2950, -0.3384, -0.0180, -0.2284]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3017, -0.5333, -0.0225,  0.0586]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 166 ] state=tensor([[-0.3017, -0.5333, -0.0225,  0.0586]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3124, -0.7281, -0.0213,  0.3441]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 167 ] state=tensor([[-0.3124, -0.7281, -0.0213,  0.3441]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3270, -0.5327, -0.0145,  0.0448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 168 ] state=tensor([[-0.3270, -0.5327, -0.0145,  0.0448]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3376, -0.3373, -0.0136, -0.2525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 169 ] state=tensor([[-0.3376, -0.3373, -0.0136, -0.2525]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3444, -0.5323, -0.0186,  0.0359]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 170 ] state=tensor([[-0.3444, -0.5323, -0.0186,  0.0359]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3550, -0.3369, -0.0179, -0.2626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 171 ] state=tensor([[-0.3550, -0.3369, -0.0179, -0.2626]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3617, -0.5317, -0.0232,  0.0244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 172 ] state=tensor([[-0.3617, -0.5317, -0.0232,  0.0244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3724, -0.3363, -0.0227, -0.2755]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 173 ] state=tensor([[-0.3724, -0.3363, -0.0227, -0.2755]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3791, -0.5311, -0.0282,  0.0099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 174 ] state=tensor([[-0.3791, -0.5311, -0.0282,  0.0099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3897, -0.3356, -0.0280, -0.2915]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 175 ] state=tensor([[-0.3897, -0.3356, -0.0280, -0.2915]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3964, -0.1401, -0.0338, -0.5929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 176 ] state=tensor([[-0.3964, -0.1401, -0.0338, -0.5929]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3992, -0.3347, -0.0457, -0.3110]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 177 ] state=tensor([[-0.3992, -0.3347, -0.0457, -0.3110]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4059, -0.1389, -0.0519, -0.6178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 178 ] state=tensor([[-0.4059, -0.1389, -0.0519, -0.6178]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4087, -0.3333, -0.0642, -0.3419]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 179 ] state=tensor([[-0.4087, -0.3333, -0.0642, -0.3419]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4154, -0.1373, -0.0711, -0.6541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 180 ] state=tensor([[-0.4154, -0.1373, -0.0711, -0.6541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4181,  0.0587, -0.0842, -0.9683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 181 ] state=tensor([[-0.4181,  0.0587, -0.0842, -0.9683]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4170, -0.1352, -0.1035, -0.7032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 182 ] state=tensor([[-0.4170, -0.1352, -0.1035, -0.7032]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4197, -0.3287, -0.1176, -0.4448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 183 ] state=tensor([[-0.4197, -0.3287, -0.1176, -0.4448]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4262, -0.5220, -0.1265, -0.1914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 184 ] state=tensor([[-0.4262, -0.5220, -0.1265, -0.1914]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4367, -0.7151, -0.1303,  0.0589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 185 ] state=tensor([[-0.4367, -0.7151, -0.1303,  0.0589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4510, -0.9082, -0.1291,  0.3078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 186 ] state=tensor([[-0.4510, -0.9082, -0.1291,  0.3078]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4691, -0.7115, -0.1230, -0.0227]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 104 ][ timestamp 187 ] state=tensor([[-0.4691, -0.7115, -0.1230, -0.0227]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4834, -0.5148, -0.1234, -0.3515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 188 ] state=tensor([[-0.4834, -0.5148, -0.1234, -0.3515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4937, -0.7080, -0.1305, -0.1001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 189 ] state=tensor([[-0.4937, -0.7080, -0.1305, -0.1001]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5078, -0.5113, -0.1325, -0.4310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 190 ] state=tensor([[-0.5078, -0.5113, -0.1325, -0.4310]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5181, -0.7043, -0.1411, -0.1828]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 191 ] state=tensor([[-0.5181, -0.7043, -0.1411, -0.1828]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5321, -0.8971, -0.1447,  0.0623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 192 ] state=tensor([[-0.5321, -0.8971, -0.1447,  0.0623]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5501, -0.7003, -0.1435, -0.2723]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 193 ] state=tensor([[-0.5501, -0.7003, -0.1435, -0.2723]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5641, -0.8931, -0.1489, -0.0281]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 194 ] state=tensor([[-0.5641, -0.8931, -0.1489, -0.0281]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5819, -0.6962, -0.1495, -0.3639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 195 ] state=tensor([[-0.5819, -0.6962, -0.1495, -0.3639]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5959, -0.4993, -0.1568, -0.6997]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 196 ] state=tensor([[-0.5959, -0.4993, -0.1568, -0.6997]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6059, -0.6919, -0.1708, -0.4602]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 197 ] state=tensor([[-0.6059, -0.6919, -0.1708, -0.4602]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6197, -0.8843, -0.1800, -0.2258]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 198 ] state=tensor([[-0.6197, -0.8843, -0.1800, -0.2258]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6374, -0.6871, -0.1845, -0.5694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 199 ] state=tensor([[-0.6374, -0.6871, -0.1845, -0.5694]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6511, -0.8792, -0.1959, -0.3401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 200 ] state=tensor([[-0.6511, -0.8792, -0.1959, -0.3401]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6687, -1.0711, -0.2027, -0.1150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 201 ] state=tensor([[-0.6687, -1.0711, -0.2027, -0.1150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6901, -0.8737, -0.2050, -0.4642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 104 ][ timestamp 202 ] state=tensor([[-0.6901, -0.8737, -0.2050, -0.4642]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 104: Exploration_rate=0.05. Score=202.\n",
      "[ episode 105 ] state=tensor([[-0.0468, -0.0038,  0.0459, -0.0468]])\n",
      "[ episode 105 ][ timestamp 1 ] state=tensor([[-0.0468, -0.0038,  0.0459, -0.0468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0469,  0.1907,  0.0450, -0.3246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 2 ] state=tensor([[-0.0469,  0.1907,  0.0450, -0.3246]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0431,  0.3851,  0.0385, -0.6028]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 3 ] state=tensor([[-0.0431,  0.3851,  0.0385, -0.6028]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0354,  0.5797,  0.0265, -0.8831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 4 ] state=tensor([[-0.0354,  0.5797,  0.0265, -0.8831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0238,  0.7744,  0.0088, -1.1673]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 5 ] state=tensor([[-0.0238,  0.7744,  0.0088, -1.1673]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0083,  0.5792, -0.0145, -0.8719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 6 ] state=tensor([[-0.0083,  0.5792, -0.0145, -0.8719]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0033,  0.3843, -0.0320, -0.5838]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 7 ] state=tensor([[ 0.0033,  0.3843, -0.0320, -0.5838]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0110,  0.1896, -0.0437, -0.3014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 8 ] state=tensor([[ 0.0110,  0.1896, -0.0437, -0.3014]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0148, -0.0049, -0.0497, -0.0228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 9 ] state=tensor([[ 0.0148, -0.0049, -0.0497, -0.0228]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0147, -0.1992, -0.0501,  0.2538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 10 ] state=tensor([[ 0.0147, -0.1992, -0.0501,  0.2538]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0107, -0.3936, -0.0451,  0.5302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 11 ] state=tensor([[ 0.0107, -0.3936, -0.0451,  0.5302]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0028, -0.1979, -0.0345,  0.2237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 12 ] state=tensor([[ 0.0028, -0.1979, -0.0345,  0.2237]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0012, -0.0023, -0.0300, -0.0796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 13 ] state=tensor([[-0.0012, -0.0023, -0.0300, -0.0796]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0012,  0.1933, -0.0316, -0.3816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 14 ] state=tensor([[-0.0012,  0.1933, -0.0316, -0.3816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0027, -0.0014, -0.0392, -0.0991]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 15 ] state=tensor([[ 0.0027, -0.0014, -0.0392, -0.0991]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0026, -0.1959, -0.0412,  0.1810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 16 ] state=tensor([[ 0.0026, -0.1959, -0.0412,  0.1810]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0013, -0.0003, -0.0376, -0.1244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 17 ] state=tensor([[-0.0013, -0.0003, -0.0376, -0.1244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0013,  0.1954, -0.0401, -0.4287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 18 ] state=tensor([[-0.0013,  0.1954, -0.0401, -0.4287]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0026,  0.0009, -0.0486, -0.1489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 19 ] state=tensor([[ 0.0026,  0.0009, -0.0486, -0.1489]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0026, -0.1935, -0.0516,  0.1280]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 20 ] state=tensor([[ 0.0026, -0.1935, -0.0516,  0.1280]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0012, -0.3879, -0.0491,  0.4040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 21 ] state=tensor([[-0.0012, -0.3879, -0.0491,  0.4040]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0090, -0.1921, -0.0410,  0.0963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 22 ] state=tensor([[-0.0090, -0.1921, -0.0410,  0.0963]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0128,  0.0036, -0.0391, -0.2091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 23 ] state=tensor([[-0.0128,  0.0036, -0.0391, -0.2091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0128, -0.1910, -0.0432,  0.0710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 24 ] state=tensor([[-0.0128, -0.1910, -0.0432,  0.0710]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0166, -0.3854, -0.0418,  0.3498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 25 ] state=tensor([[-0.0166, -0.3854, -0.0418,  0.3498]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0243, -0.1897, -0.0348,  0.0442]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 26 ] state=tensor([[-0.0243, -0.1897, -0.0348,  0.0442]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0281,  0.0059, -0.0339, -0.2592]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 27 ] state=tensor([[-0.0281,  0.0059, -0.0339, -0.2592]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0280, -0.1888, -0.0391,  0.0225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 28 ] state=tensor([[-0.0280, -0.1888, -0.0391,  0.0225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0317, -0.3833, -0.0387,  0.3026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 29 ] state=tensor([[-0.0317, -0.3833, -0.0387,  0.3026]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0394, -0.1877, -0.0326, -0.0020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 30 ] state=tensor([[-0.0394, -0.1877, -0.0326, -0.0020]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0432,  0.0079, -0.0327, -0.3048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 31 ] state=tensor([[-0.0432,  0.0079, -0.0327, -0.3048]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0430, -0.1867, -0.0387, -0.0226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 32 ] state=tensor([[-0.0430, -0.1867, -0.0387, -0.0226]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0467,  0.0089, -0.0392, -0.3272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 33 ] state=tensor([[-0.0467,  0.0089, -0.0392, -0.3272]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0466, -0.1856, -0.0457, -0.0472]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 34 ] state=tensor([[-0.0466, -0.1856, -0.0457, -0.0472]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0503, -0.3800, -0.0467,  0.2307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 35 ] state=tensor([[-0.0503, -0.3800, -0.0467,  0.2307]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0579, -0.1843, -0.0421, -0.0763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 36 ] state=tensor([[-0.0579, -0.1843, -0.0421, -0.0763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0616, -0.3788, -0.0436,  0.2028]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 37 ] state=tensor([[-0.0616, -0.3788, -0.0436,  0.2028]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0691, -0.1831, -0.0395, -0.1033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 38 ] state=tensor([[-0.0691, -0.1831, -0.0395, -0.1033]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0728, -0.3776, -0.0416,  0.1767]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 39 ] state=tensor([[-0.0728, -0.3776, -0.0416,  0.1767]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0804, -0.1819, -0.0381, -0.1288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 40 ] state=tensor([[-0.0804, -0.1819, -0.0381, -0.1288]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0840,  0.0137, -0.0407, -0.4333]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 41 ] state=tensor([[-0.0840,  0.0137, -0.0407, -0.4333]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0837, -0.1808, -0.0493, -0.1537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 42 ] state=tensor([[-0.0837, -0.1808, -0.0493, -0.1537]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0873, -0.3752, -0.0524,  0.1230]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 43 ] state=tensor([[-0.0873, -0.3752, -0.0524,  0.1230]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0948, -0.1793, -0.0499, -0.1857]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 44 ] state=tensor([[-0.0948, -0.1793, -0.0499, -0.1857]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0984, -0.3737, -0.0536,  0.0908]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 45 ] state=tensor([[-0.0984, -0.3737, -0.0536,  0.0908]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1059, -0.5680, -0.0518,  0.3661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 46 ] state=tensor([[-0.1059, -0.5680, -0.0518,  0.3661]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1173, -0.7624, -0.0445,  0.6420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 47 ] state=tensor([[-0.1173, -0.7624, -0.0445,  0.6420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1325, -0.5667, -0.0317,  0.3356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 48 ] state=tensor([[-0.1325, -0.5667, -0.0317,  0.3356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1438, -0.3711, -0.0250,  0.0331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 49 ] state=tensor([[-0.1438, -0.3711, -0.0250,  0.0331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1513, -0.5659, -0.0243,  0.3178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 50 ] state=tensor([[-0.1513, -0.5659, -0.0243,  0.3178]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1626, -0.3704, -0.0179,  0.0176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 51 ] state=tensor([[-0.1626, -0.3704, -0.0179,  0.0176]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1700, -0.5653, -0.0176,  0.3046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 52 ] state=tensor([[-0.1700, -0.5653, -0.0176,  0.3046]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1813, -0.3699, -0.0115,  0.0064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 53 ] state=tensor([[-0.1813, -0.3699, -0.0115,  0.0064]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1887, -0.5648, -0.0114,  0.2954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 54 ] state=tensor([[-0.1887, -0.5648, -0.0114,  0.2954]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2000, -0.3696, -0.0055, -0.0008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 55 ] state=tensor([[-0.2000, -0.3696, -0.0055, -0.0008]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2074, -0.1744, -0.0055, -0.2952]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 56 ] state=tensor([[-0.2074, -0.1744, -0.0055, -0.2952]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2109, -0.3694, -0.0114, -0.0043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 57 ] state=tensor([[-0.2109, -0.3694, -0.0114, -0.0043]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2183, -0.5644, -0.0115,  0.2848]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 105 ][ timestamp 58 ] state=tensor([[-0.2183, -0.5644, -0.0115,  0.2848]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2295, -0.3691, -0.0058, -0.0115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 59 ] state=tensor([[-0.2295, -0.3691, -0.0058, -0.0115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2369, -0.5641, -0.0060,  0.2794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 60 ] state=tensor([[-0.2369, -0.5641, -0.0060,  0.2794]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2482, -0.3689, -0.0004, -0.0152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 61 ] state=tensor([[-0.2482, -0.3689, -0.0004, -0.0152]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2556, -0.5640, -0.0007,  0.2774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 62 ] state=tensor([[-0.2556, -0.5640, -0.0007,  0.2774]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2669, -0.3689,  0.0048, -0.0155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 63 ] state=tensor([[-0.2669, -0.3689,  0.0048, -0.0155]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2742, -0.1738,  0.0045, -0.3067]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 64 ] state=tensor([[-0.2742, -0.1738,  0.0045, -0.3067]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2777, -0.3690, -0.0016, -0.0126]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 65 ] state=tensor([[-0.2777, -0.3690, -0.0016, -0.0126]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2851, -0.1739, -0.0019, -0.3057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 66 ] state=tensor([[-0.2851, -0.1739, -0.0019, -0.3057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2886,  0.0213, -0.0080, -0.5990]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 67 ] state=tensor([[-0.2886,  0.0213, -0.0080, -0.5990]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2881, -0.1737, -0.0199, -0.3089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 68 ] state=tensor([[-0.2881, -0.1737, -0.0199, -0.3089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2916, -0.3686, -0.0261, -0.0225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 69 ] state=tensor([[-0.2916, -0.3686, -0.0261, -0.0225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2990, -0.5633, -0.0266,  0.2618]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 70 ] state=tensor([[-0.2990, -0.5633, -0.0266,  0.2618]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3103, -0.3678, -0.0213, -0.0391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 71 ] state=tensor([[-0.3103, -0.3678, -0.0213, -0.0391]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3176, -0.5626, -0.0221,  0.2467]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 72 ] state=tensor([[-0.3176, -0.5626, -0.0221,  0.2467]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3289, -0.3672, -0.0172, -0.0529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 73 ] state=tensor([[-0.3289, -0.3672, -0.0172, -0.0529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3362, -0.5621, -0.0182,  0.2344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 74 ] state=tensor([[-0.3362, -0.5621, -0.0182,  0.2344]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3475, -0.3667, -0.0136, -0.0640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 75 ] state=tensor([[-0.3475, -0.3667, -0.0136, -0.0640]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3548, -0.1714, -0.0148, -0.3610]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 76 ] state=tensor([[-0.3548, -0.1714, -0.0148, -0.3610]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3582, -0.3663, -0.0221, -0.0730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 77 ] state=tensor([[-0.3582, -0.3663, -0.0221, -0.0730]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3655, -0.1709, -0.0235, -0.3725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 78 ] state=tensor([[-0.3655, -0.1709, -0.0235, -0.3725]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3690, -0.3656, -0.0310, -0.0874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 79 ] state=tensor([[-0.3690, -0.3656, -0.0310, -0.0874]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3763, -0.1701, -0.0327, -0.3897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 80 ] state=tensor([[-0.3763, -0.1701, -0.0327, -0.3897]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3797,  0.0255, -0.0405, -0.6925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 81 ] state=tensor([[-0.3797,  0.0255, -0.0405, -0.6925]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3792, -0.1691, -0.0544, -0.4128]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 82 ] state=tensor([[-0.3792, -0.1691, -0.0544, -0.4128]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3825, -0.3634, -0.0626, -0.1378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 83 ] state=tensor([[-0.3825, -0.3634, -0.0626, -0.1378]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3898, -0.5575, -0.0654,  0.1345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 84 ] state=tensor([[-0.3898, -0.5575, -0.0654,  0.1345]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4010, -0.3615, -0.0627, -0.1780]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 85 ] state=tensor([[-0.4010, -0.3615, -0.0627, -0.1780]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4082, -0.5557, -0.0662,  0.0942]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 86 ] state=tensor([[-0.4082, -0.5557, -0.0662,  0.0942]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4193, -0.3597, -0.0644, -0.2186]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 87 ] state=tensor([[-0.4193, -0.3597, -0.0644, -0.2186]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4265, -0.5539, -0.0687,  0.0531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 88 ] state=tensor([[-0.4265, -0.5539, -0.0687,  0.0531]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4376, -0.3578, -0.0677, -0.2604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 89 ] state=tensor([[-0.4376, -0.3578, -0.0677, -0.2604]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4447, -0.5519, -0.0729,  0.0102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 90 ] state=tensor([[-0.4447, -0.5519, -0.0729,  0.0102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4558, -0.3558, -0.0727, -0.3046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 91 ] state=tensor([[-0.4558, -0.3558, -0.0727, -0.3046]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4629, -0.5498, -0.0788, -0.0357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 92 ] state=tensor([[-0.4629, -0.5498, -0.0788, -0.0357]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4739, -0.3537, -0.0795, -0.3521]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 93 ] state=tensor([[-0.4739, -0.3537, -0.0795, -0.3521]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4810, -0.1575, -0.0865, -0.6688]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 94 ] state=tensor([[-0.4810, -0.1575, -0.0865, -0.6688]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4841, -0.3513, -0.0999, -0.4046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 95 ] state=tensor([[-0.4841, -0.3513, -0.0999, -0.4046]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4911, -0.5449, -0.1080, -0.1450]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 96 ] state=tensor([[-0.4911, -0.5449, -0.1080, -0.1450]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5020, -0.7383, -0.1109,  0.1118]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 97 ] state=tensor([[-0.5020, -0.7383, -0.1109,  0.1118]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5168, -0.5418, -0.1086, -0.2137]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 98 ] state=tensor([[-0.5168, -0.5418, -0.1086, -0.2137]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5276, -0.7352, -0.1129,  0.0428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 99 ] state=tensor([[-0.5276, -0.7352, -0.1129,  0.0428]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5423, -0.5387, -0.1121, -0.2832]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 100 ] state=tensor([[-0.5423, -0.5387, -0.1121, -0.2832]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5531, -0.7320, -0.1177, -0.0279]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 101 ] state=tensor([[-0.5531, -0.7320, -0.1177, -0.0279]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5678, -0.5355, -0.1183, -0.3553]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 102 ] state=tensor([[-0.5678, -0.5355, -0.1183, -0.3553]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5785, -0.7287, -0.1254, -0.1021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 103 ] state=tensor([[-0.5785, -0.7287, -0.1254, -0.1021]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5930, -0.5320, -0.1274, -0.4316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 104 ] state=tensor([[-0.5930, -0.5320, -0.1274, -0.4316]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6037, -0.7251, -0.1361, -0.1816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 105 ] state=tensor([[-0.6037, -0.7251, -0.1361, -0.1816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6182, -0.9181, -0.1397,  0.0652]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 106 ] state=tensor([[-0.6182, -0.9181, -0.1397,  0.0652]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6365, -0.7213, -0.1384, -0.2681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 107 ] state=tensor([[-0.6365, -0.7213, -0.1384, -0.2681]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6510, -0.5245, -0.1438, -0.6010]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 105 ][ timestamp 108 ] state=tensor([[-0.6510, -0.5245, -0.1438, -0.6010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6615, -0.7173, -0.1558, -0.3568]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 109 ] state=tensor([[-0.6615, -0.7173, -0.1558, -0.3568]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6758, -0.9099, -0.1629, -0.1170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 110 ] state=tensor([[-0.6758, -0.9099, -0.1629, -0.1170]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6940, -1.1024, -0.1653,  0.1201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 111 ] state=tensor([[-0.6940, -1.1024, -0.1653,  0.1201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7161, -0.9053, -0.1629, -0.2198]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 112 ] state=tensor([[-0.7161, -0.9053, -0.1629, -0.2198]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7342, -1.0978, -0.1673,  0.0174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 113 ] state=tensor([[-0.7342, -1.0978, -0.1673,  0.0174]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7561, -0.9007, -0.1669, -0.3230]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 114 ] state=tensor([[-0.7561, -0.9007, -0.1669, -0.3230]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7741, -1.0931, -0.1734, -0.0873]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 115 ] state=tensor([[-0.7741, -1.0931, -0.1734, -0.0873]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7960, -0.8960, -0.1751, -0.4292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 116 ] state=tensor([[-0.7960, -0.8960, -0.1751, -0.4292]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8139, -1.0882, -0.1837, -0.1965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 117 ] state=tensor([[-0.8139, -1.0882, -0.1837, -0.1965]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8357, -1.2803, -0.1876,  0.0331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 118 ] state=tensor([[-0.8357, -1.2803, -0.1876,  0.0331]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8613, -1.0831, -0.1870, -0.3124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 119 ] state=tensor([[-0.8613, -1.0831, -0.1870, -0.3124]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8829, -1.2751, -0.1932, -0.0840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 120 ] state=tensor([[-0.8829, -1.2751, -0.1932, -0.0840]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9085, -1.0778, -0.1949, -0.4309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 121 ] state=tensor([[-0.9085, -1.0778, -0.1949, -0.4309]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9300, -0.8806, -0.2035, -0.7781]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 105 ][ timestamp 122 ] state=tensor([[-0.9300, -0.8806, -0.2035, -0.7781]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 105: Exploration_rate=0.05. Score=122.\n",
      "[ episode 106 ] state=tensor([[ 0.0072, -0.0007, -0.0006,  0.0273]])\n",
      "[ episode 106 ][ timestamp 1 ] state=tensor([[ 0.0072, -0.0007, -0.0006,  0.0273]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 7.2222e-03, -1.9581e-01, -9.8306e-05,  3.1979e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 2 ] state=tensor([[ 7.2222e-03, -1.9581e-01, -9.8306e-05,  3.1979e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0033, -0.0007,  0.0063,  0.0271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 3 ] state=tensor([[ 0.0033, -0.0007,  0.0063,  0.0271]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0033,  0.1943,  0.0068, -0.2636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 4 ] state=tensor([[ 0.0033,  0.1943,  0.0068, -0.2636]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0072,  0.3894,  0.0016, -0.5541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 5 ] state=tensor([[ 0.0072,  0.3894,  0.0016, -0.5541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0150,  0.5845, -0.0095, -0.8463]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 6 ] state=tensor([[ 0.0150,  0.5845, -0.0095, -0.8463]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0267,  0.7797, -0.0264, -1.1420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 7 ] state=tensor([[ 0.0267,  0.7797, -0.0264, -1.1420]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0423,  0.5850, -0.0493, -0.8577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 8 ] state=tensor([[ 0.0423,  0.5850, -0.0493, -0.8577]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0539,  0.3905, -0.0664, -0.5809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 9 ] state=tensor([[ 0.0539,  0.3905, -0.0664, -0.5809]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0618,  0.1964, -0.0781, -0.3099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 10 ] state=tensor([[ 0.0618,  0.1964, -0.0781, -0.3099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0657,  0.3925, -0.0843, -0.6261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 11 ] state=tensor([[ 0.0657,  0.3925, -0.0843, -0.6261]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0735,  0.1987, -0.0968, -0.3611]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 12 ] state=tensor([[ 0.0735,  0.1987, -0.0968, -0.3611]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0775,  0.0051, -0.1040, -0.1005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 13 ] state=tensor([[ 0.0775,  0.0051, -0.1040, -0.1005]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0776, -0.1884, -0.1060,  0.1577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 14 ] state=tensor([[ 0.0776, -0.1884, -0.1060,  0.1577]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0738, -0.3819, -0.1029,  0.4151]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 15 ] state=tensor([[ 0.0738, -0.3819, -0.1029,  0.4151]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0662, -0.5754, -0.0946,  0.6737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 16 ] state=tensor([[ 0.0662, -0.5754, -0.0946,  0.6737]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0547, -0.3791, -0.0811,  0.3528]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 17 ] state=tensor([[ 0.0547, -0.3791, -0.0811,  0.3528]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0471, -0.1829, -0.0740,  0.0357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 18 ] state=tensor([[ 0.0471, -0.1829, -0.0740,  0.0357]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0435,  0.0132, -0.0733, -0.2794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 19 ] state=tensor([[ 0.0435,  0.0132, -0.0733, -0.2794]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0437, -0.1808, -0.0789, -0.0107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 20 ] state=tensor([[ 0.0437, -0.1808, -0.0789, -0.0107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0401, -0.3747, -0.0791,  0.2561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 21 ] state=tensor([[ 0.0401, -0.3747, -0.0791,  0.2561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0326, -0.5686, -0.0740,  0.5228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 22 ] state=tensor([[ 0.0326, -0.5686, -0.0740,  0.5228]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0212, -0.3726, -0.0635,  0.2078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 23 ] state=tensor([[ 0.0212, -0.3726, -0.0635,  0.2078]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0138, -0.1766, -0.0594, -0.1043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 24 ] state=tensor([[ 0.0138, -0.1766, -0.0594, -0.1043]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0103, -0.3708, -0.0615,  0.1691]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 106 ][ timestamp 25 ] state=tensor([[ 0.0103, -0.3708, -0.0615,  0.1691]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0028, -0.1749, -0.0581, -0.1423]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 26 ] state=tensor([[ 0.0028, -0.1749, -0.0581, -0.1423]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0007, -0.3691, -0.0609,  0.1315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 27 ] state=tensor([[-0.0007, -0.3691, -0.0609,  0.1315]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0080, -0.1732, -0.0583, -0.1798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 28 ] state=tensor([[-0.0080, -0.1732, -0.0583, -0.1798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0115, -0.3674, -0.0619,  0.0940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 29 ] state=tensor([[-0.0115, -0.3674, -0.0619,  0.0940]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0189, -0.1715, -0.0600, -0.2176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 30 ] state=tensor([[-0.0189, -0.1715, -0.0600, -0.2176]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0223, -0.3657, -0.0644,  0.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 31 ] state=tensor([[-0.0223, -0.3657, -0.0644,  0.0556]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0296, -0.5598, -0.0632,  0.3273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 32 ] state=tensor([[-0.0296, -0.5598, -0.0632,  0.3273]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0408, -0.3639, -0.0567,  0.0154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 33 ] state=tensor([[-0.0408, -0.3639, -0.0567,  0.0154]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0481, -0.5581, -0.0564,  0.2896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 34 ] state=tensor([[-0.0481, -0.5581, -0.0564,  0.2896]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0592, -0.3622, -0.0506, -0.0203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 35 ] state=tensor([[-0.0592, -0.3622, -0.0506, -0.0203]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0665, -0.5566, -0.0510,  0.2560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 36 ] state=tensor([[-0.0665, -0.5566, -0.0510,  0.2560]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0776, -0.3608, -0.0459, -0.0523]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 37 ] state=tensor([[-0.0776, -0.3608, -0.0459, -0.0523]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0848, -0.1650, -0.0469, -0.3591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 38 ] state=tensor([[-0.0848, -0.1650, -0.0469, -0.3591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0881, -0.3595, -0.0541, -0.0816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 39 ] state=tensor([[-0.0881, -0.3595, -0.0541, -0.0816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0953, -0.5538, -0.0557,  0.1935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 40 ] state=tensor([[-0.0953, -0.5538, -0.0557,  0.1935]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1064, -0.7481, -0.0519,  0.4681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 41 ] state=tensor([[-0.1064, -0.7481, -0.0519,  0.4681]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1214, -0.5522, -0.0425,  0.1596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 42 ] state=tensor([[-0.1214, -0.5522, -0.0425,  0.1596]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1324, -0.7467, -0.0393,  0.4385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 43 ] state=tensor([[-0.1324, -0.7467, -0.0393,  0.4385]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1473, -0.5511, -0.0306,  0.1337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 44 ] state=tensor([[-0.1473, -0.5511, -0.0306,  0.1337]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1584, -0.3555, -0.0279, -0.1684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 45 ] state=tensor([[-0.1584, -0.3555, -0.0279, -0.1684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1655, -0.5502, -0.0312,  0.1153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 46 ] state=tensor([[-0.1655, -0.5502, -0.0312,  0.1153]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1765, -0.3547, -0.0289, -0.1871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 47 ] state=tensor([[-0.1765, -0.3547, -0.0289, -0.1871]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1836, -0.5494, -0.0327,  0.0964]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 48 ] state=tensor([[-0.1836, -0.5494, -0.0327,  0.0964]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1945, -0.3538, -0.0308, -0.2065]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 49 ] state=tensor([[-0.1945, -0.3538, -0.0308, -0.2065]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2016, -0.5485, -0.0349,  0.0764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 50 ] state=tensor([[-0.2016, -0.5485, -0.0349,  0.0764]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2126, -0.3529, -0.0334, -0.2271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 51 ] state=tensor([[-0.2126, -0.3529, -0.0334, -0.2271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2197, -0.5475, -0.0379,  0.0549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 52 ] state=tensor([[-0.2197, -0.5475, -0.0379,  0.0549]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2306, -0.3519, -0.0368, -0.2495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 53 ] state=tensor([[-0.2306, -0.3519, -0.0368, -0.2495]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2376, -0.5464, -0.0418,  0.0313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 54 ] state=tensor([[-0.2376, -0.5464, -0.0418,  0.0313]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2486, -0.3507, -0.0412, -0.2743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 55 ] state=tensor([[-0.2486, -0.3507, -0.0412, -0.2743]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2556, -0.1551, -0.0467, -0.5796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 56 ] state=tensor([[-0.2556, -0.1551, -0.0467, -0.5796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2587, -0.3495, -0.0582, -0.3020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 57 ] state=tensor([[-0.2587, -0.3495, -0.0582, -0.3020]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2657, -0.5437, -0.0643, -0.0282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 58 ] state=tensor([[-0.2657, -0.5437, -0.0643, -0.0282]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2765, -0.7379, -0.0648,  0.2435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 59 ] state=tensor([[-0.2765, -0.7379, -0.0648,  0.2435]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2913, -0.5419, -0.0600, -0.0689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 60 ] state=tensor([[-0.2913, -0.5419, -0.0600, -0.0689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3021, -0.7361, -0.0614,  0.2042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 61 ] state=tensor([[-0.3021, -0.7361, -0.0614,  0.2042]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3169, -0.9303, -0.0573,  0.4770]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 62 ] state=tensor([[-0.3169, -0.9303, -0.0573,  0.4770]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3355, -0.7344, -0.0477,  0.1668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 63 ] state=tensor([[-0.3355, -0.7344, -0.0477,  0.1668]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3502, -0.5387, -0.0444, -0.1406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 64 ] state=tensor([[-0.3502, -0.5387, -0.0444, -0.1406]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3609, -0.3429, -0.0472, -0.4469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 65 ] state=tensor([[-0.3609, -0.3429, -0.0472, -0.4469]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3678, -0.5373, -0.0561, -0.1695]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 66 ] state=tensor([[-0.3678, -0.5373, -0.0561, -0.1695]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3785, -0.7316, -0.0595,  0.1050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 67 ] state=tensor([[-0.3785, -0.7316, -0.0595,  0.1050]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3932, -0.5357, -0.0574, -0.2059]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 68 ] state=tensor([[-0.3932, -0.5357, -0.0574, -0.2059]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4039, -0.7300, -0.0616,  0.0681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 69 ] state=tensor([[-0.4039, -0.7300, -0.0616,  0.0681]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4185, -0.5340, -0.0602, -0.2433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 70 ] state=tensor([[-0.4185, -0.5340, -0.0602, -0.2433]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4292, -0.7282, -0.0651,  0.0298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 71 ] state=tensor([[-0.4292, -0.7282, -0.0651,  0.0298]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4437, -0.5322, -0.0645, -0.2827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 72 ] state=tensor([[-0.4437, -0.5322, -0.0645, -0.2827]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4544, -0.7264, -0.0701, -0.0110]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 73 ] state=tensor([[-0.4544, -0.7264, -0.0701, -0.0110]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4689, -0.5303, -0.0703, -0.3250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 74 ] state=tensor([[-0.4689, -0.5303, -0.0703, -0.3250]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4795, -0.3343, -0.0768, -0.6390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 75 ] state=tensor([[-0.4795, -0.3343, -0.0768, -0.6390]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4862, -0.5282, -0.0896, -0.3714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 76 ] state=tensor([[-0.4862, -0.5282, -0.0896, -0.3714]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4968, -0.7220, -0.0970, -0.1083]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 106 ][ timestamp 77 ] state=tensor([[-0.4968, -0.7220, -0.0970, -0.1083]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5112, -0.5256, -0.0992, -0.4299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 78 ] state=tensor([[-0.5112, -0.5256, -0.0992, -0.4299]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5217, -0.7192, -0.1078, -0.1701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 79 ] state=tensor([[-0.5217, -0.7192, -0.1078, -0.1701]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5361, -0.9126, -0.1112,  0.0867]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 80 ] state=tensor([[-0.5361, -0.9126, -0.1112,  0.0867]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5543, -0.7161, -0.1095, -0.2389]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 81 ] state=tensor([[-0.5543, -0.7161, -0.1095, -0.2389]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5687, -0.5196, -0.1143, -0.5640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 82 ] state=tensor([[-0.5687, -0.5196, -0.1143, -0.5640]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5791, -0.7130, -0.1255, -0.3094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 83 ] state=tensor([[-0.5791, -0.7130, -0.1255, -0.3094]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5933, -0.9061, -0.1317, -0.0588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 84 ] state=tensor([[-0.5933, -0.9061, -0.1317, -0.0588]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6114, -1.0991, -0.1329,  0.1896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 85 ] state=tensor([[-0.6114, -1.0991, -0.1329,  0.1896]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6334, -0.9023, -0.1291, -0.1419]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 86 ] state=tensor([[-0.6334, -0.9023, -0.1291, -0.1419]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6515, -1.0954, -0.1319,  0.1075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 87 ] state=tensor([[-0.6515, -1.0954, -0.1319,  0.1075]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6734, -0.8987, -0.1298, -0.2237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 88 ] state=tensor([[-0.6734, -0.8987, -0.1298, -0.2237]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6914, -0.7019, -0.1343, -0.5544]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 89 ] state=tensor([[-0.6914, -0.7019, -0.1343, -0.5544]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7054, -0.8950, -0.1454, -0.3068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 90 ] state=tensor([[-0.7054, -0.8950, -0.1454, -0.3068]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7233, -1.0877, -0.1515, -0.0633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 91 ] state=tensor([[-0.7233, -1.0877, -0.1515, -0.0633]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7450, -0.8908, -0.1528, -0.3997]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 92 ] state=tensor([[-0.7450, -0.8908, -0.1528, -0.3997]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7629, -1.0835, -0.1607, -0.1588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 93 ] state=tensor([[-0.7629, -1.0835, -0.1607, -0.1588]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7845, -1.2760, -0.1639,  0.0792]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 94 ] state=tensor([[-0.7845, -1.2760, -0.1639,  0.0792]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8100, -1.0789, -0.1623, -0.2604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 95 ] state=tensor([[-0.8100, -1.0789, -0.1623, -0.2604]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8316, -1.2714, -0.1675, -0.0230]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 96 ] state=tensor([[-0.8316, -1.2714, -0.1675, -0.0230]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8571, -1.0743, -0.1680, -0.3635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 97 ] state=tensor([[-0.8571, -1.0743, -0.1680, -0.3635]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8785, -1.2667, -0.1753, -0.1282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 98 ] state=tensor([[-0.8785, -1.2667, -0.1753, -0.1282]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9039, -1.0696, -0.1778, -0.4706]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 99 ] state=tensor([[-0.9039, -1.0696, -0.1778, -0.4706]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9253, -1.2618, -0.1873, -0.2388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 100 ] state=tensor([[-0.9253, -1.2618, -0.1873, -0.2388]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9505, -1.4538, -0.1920, -0.0106]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 101 ] state=tensor([[-0.9505, -1.4538, -0.1920, -0.0106]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9796, -1.2565, -0.1922, -0.3572]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 102 ] state=tensor([[-0.9796, -1.2565, -0.1922, -0.3572]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0047, -1.0593, -0.1994, -0.7038]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 106 ][ timestamp 103 ] state=tensor([[-1.0047, -1.0593, -0.1994, -0.7038]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 106: Exploration_rate=0.05. Score=103.\n",
      "[ episode 107 ] state=tensor([[ 0.0262, -0.0330, -0.0339, -0.0013]])\n",
      "[ episode 107 ][ timestamp 1 ] state=tensor([[ 0.0262, -0.0330, -0.0339, -0.0013]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0255, -0.2276, -0.0339,  0.2805]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 2 ] state=tensor([[ 0.0255, -0.2276, -0.0339,  0.2805]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0210, -0.0320, -0.0283, -0.0227]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 3 ] state=tensor([[ 0.0210, -0.0320, -0.0283, -0.0227]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0203,  0.1635, -0.0288, -0.3242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 4 ] state=tensor([[ 0.0203,  0.1635, -0.0288, -0.3242]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0236, -0.0312, -0.0353, -0.0407]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 5 ] state=tensor([[ 0.0236, -0.0312, -0.0353, -0.0407]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0230, -0.2258, -0.0361,  0.2406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 6 ] state=tensor([[ 0.0230, -0.2258, -0.0361,  0.2406]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0185, -0.4204, -0.0313,  0.5217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 7 ] state=tensor([[ 0.0185, -0.4204, -0.0313,  0.5217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0101, -0.2248, -0.0208,  0.2194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 8 ] state=tensor([[ 0.0101, -0.2248, -0.0208,  0.2194]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0056, -0.0294, -0.0165, -0.0798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 9 ] state=tensor([[ 0.0056, -0.0294, -0.0165, -0.0798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0050, -0.2243, -0.0180,  0.2076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 10 ] state=tensor([[ 0.0050, -0.2243, -0.0180,  0.2076]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0005, -0.0289, -0.0139, -0.0907]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 11 ] state=tensor([[ 0.0005, -0.0289, -0.0139, -0.0907]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.7204e-05,  1.6639e-01, -1.5709e-02, -3.8773e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 12 ] state=tensor([[-7.7204e-05,  1.6639e-01, -1.5709e-02, -3.8773e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0033, -0.0285, -0.0235, -0.1000]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 13 ] state=tensor([[ 0.0033, -0.0285, -0.0235, -0.1000]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0027,  0.1669, -0.0255, -0.4000]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 14 ] state=tensor([[ 0.0027,  0.1669, -0.0255, -0.4000]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0060, -0.0278, -0.0335, -0.1155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 15 ] state=tensor([[ 0.0060, -0.0278, -0.0335, -0.1155]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0055, -0.2224, -0.0358,  0.1665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 16 ] state=tensor([[ 0.0055, -0.2224, -0.0358,  0.1665]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0010, -0.0268, -0.0324, -0.1373]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 17 ] state=tensor([[ 0.0010, -0.0268, -0.0324, -0.1373]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0005, -0.2215, -0.0352,  0.1450]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 18 ] state=tensor([[ 0.0005, -0.2215, -0.0352,  0.1450]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0040, -0.4161, -0.0323,  0.4263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 19 ] state=tensor([[-0.0040, -0.4161, -0.0323,  0.4263]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0123, -0.2205, -0.0238,  0.1237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 20 ] state=tensor([[-0.0123, -0.2205, -0.0238,  0.1237]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0167, -0.4153, -0.0213,  0.4088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 21 ] state=tensor([[-0.0167, -0.4153, -0.0213,  0.4088]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0250, -0.2198, -0.0131,  0.1094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 22 ] state=tensor([[-0.0250, -0.2198, -0.0131,  0.1094]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0294, -0.4148, -0.0109,  0.3980]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 23 ] state=tensor([[-0.0294, -0.4148, -0.0109,  0.3980]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0377, -0.2195, -0.0030,  0.1018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 24 ] state=tensor([[-0.0377, -0.2195, -0.0030,  0.1018]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0421, -0.4146, -0.0009,  0.3936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 25 ] state=tensor([[-0.0421, -0.4146, -0.0009,  0.3936]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0504, -0.6097,  0.0069,  0.6860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 26 ] state=tensor([[-0.0504, -0.6097,  0.0069,  0.6860]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0626, -0.4147,  0.0207,  0.3955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 27 ] state=tensor([[-0.0626, -0.4147,  0.0207,  0.3955]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0708, -0.2198,  0.0286,  0.1094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 28 ] state=tensor([[-0.0708, -0.2198,  0.0286,  0.1094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0752, -0.0251,  0.0308, -0.1741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 29 ] state=tensor([[-0.0752, -0.0251,  0.0308, -0.1741]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0757,  0.1695,  0.0273, -0.4570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 30 ] state=tensor([[-0.0757,  0.1695,  0.0273, -0.4570]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0724,  0.3642,  0.0181, -0.7409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 31 ] state=tensor([[-0.0724,  0.3642,  0.0181, -0.7409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0651,  0.5591,  0.0033, -1.0279]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 32 ] state=tensor([[-0.0651,  0.5591,  0.0033, -1.0279]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0539,  0.3640, -0.0172, -0.7341]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 33 ] state=tensor([[-0.0539,  0.3640, -0.0172, -0.7341]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0466,  0.1691, -0.0319, -0.4469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 34 ] state=tensor([[-0.0466,  0.1691, -0.0319, -0.4469]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0432, -0.0256, -0.0409, -0.1645]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 35 ] state=tensor([[-0.0432, -0.0256, -0.0409, -0.1645]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0437, -0.2201, -0.0442,  0.1150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 36 ] state=tensor([[-0.0437, -0.2201, -0.0442,  0.1150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0481, -0.0244, -0.0419, -0.1912]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 37 ] state=tensor([[-0.0481, -0.0244, -0.0419, -0.1912]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0486, -0.2189, -0.0457,  0.0880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 38 ] state=tensor([[-0.0486, -0.2189, -0.0457,  0.0880]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0530, -0.4133, -0.0439,  0.3659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 39 ] state=tensor([[-0.0530, -0.4133, -0.0439,  0.3659]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0613, -0.6078, -0.0366,  0.6444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 40 ] state=tensor([[-0.0613, -0.6078, -0.0366,  0.6444]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0734, -0.8024, -0.0237,  0.9253]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 107 ][ timestamp 41 ] state=tensor([[-0.0734, -0.8024, -0.0237,  0.9253]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0895, -0.6069, -0.0052,  0.6253]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 42 ] state=tensor([[-0.0895, -0.6069, -0.0052,  0.6253]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1016, -0.4117,  0.0073,  0.3310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 43 ] state=tensor([[-0.1016, -0.4117,  0.0073,  0.3310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1099, -0.2167,  0.0139,  0.0406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 44 ] state=tensor([[-0.1099, -0.2167,  0.0139,  0.0406]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1142, -0.0218,  0.0147, -0.2477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 45 ] state=tensor([[-0.1142, -0.0218,  0.0147, -0.2477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1146,  0.1731,  0.0098, -0.5357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 46 ] state=tensor([[-0.1146,  0.1731,  0.0098, -0.5357]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1112, -0.0222, -0.0009, -0.2399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 47 ] state=tensor([[-0.1112, -0.0222, -0.0009, -0.2399]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1116,  0.1730, -0.0057, -0.5329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 48 ] state=tensor([[-0.1116,  0.1730, -0.0057, -0.5329]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1081, -0.0221, -0.0164, -0.2420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 49 ] state=tensor([[-0.1081, -0.0221, -0.0164, -0.2420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1086,  0.1733, -0.0212, -0.5398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 50 ] state=tensor([[-0.1086,  0.1733, -0.0212, -0.5398]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1051, -0.0215, -0.0320, -0.2539]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 51 ] state=tensor([[-0.1051, -0.0215, -0.0320, -0.2539]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1056, -0.2162, -0.0371,  0.0285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 52 ] state=tensor([[-0.1056, -0.2162, -0.0371,  0.0285]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1099, -0.0205, -0.0365, -0.2756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 53 ] state=tensor([[-0.1099, -0.0205, -0.0365, -0.2756]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1103,  0.1751, -0.0420, -0.5796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 54 ] state=tensor([[-0.1103,  0.1751, -0.0420, -0.5796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1068, -0.0194, -0.0536, -0.3005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 55 ] state=tensor([[-0.1068, -0.0194, -0.0536, -0.3005]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1072, -0.2137, -0.0596, -0.0252]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 56 ] state=tensor([[-0.1072, -0.2137, -0.0596, -0.0252]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1114, -0.4080, -0.0602,  0.2481]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 57 ] state=tensor([[-0.1114, -0.4080, -0.0602,  0.2481]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1196, -0.2120, -0.0552, -0.0629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 58 ] state=tensor([[-0.1196, -0.2120, -0.0552, -0.0629]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1238, -0.4063, -0.0565,  0.2118]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 59 ] state=tensor([[-0.1238, -0.4063, -0.0565,  0.2118]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1320, -0.6006, -0.0522,  0.4862]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 60 ] state=tensor([[-0.1320, -0.6006, -0.0522,  0.4862]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1440, -0.4048, -0.0425,  0.1775]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 61 ] state=tensor([[-0.1440, -0.4048, -0.0425,  0.1775]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1521, -0.2091, -0.0389, -0.1283]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 62 ] state=tensor([[-0.1521, -0.2091, -0.0389, -0.1283]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1563, -0.0134, -0.0415, -0.4330]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 63 ] state=tensor([[-0.1563, -0.0134, -0.0415, -0.4330]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1565, -0.2079, -0.0502, -0.1537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 64 ] state=tensor([[-0.1565, -0.2079, -0.0502, -0.1537]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1607, -0.4023, -0.0532,  0.1228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 65 ] state=tensor([[-0.1607, -0.4023, -0.0532,  0.1228]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1687, -0.2065, -0.0508, -0.1862]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 66 ] state=tensor([[-0.1687, -0.2065, -0.0508, -0.1862]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1729, -0.4008, -0.0545,  0.0900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 67 ] state=tensor([[-0.1729, -0.4008, -0.0545,  0.0900]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1809, -0.2050, -0.0527, -0.2193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 68 ] state=tensor([[-0.1809, -0.2050, -0.0527, -0.2193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1850, -0.3993, -0.0571,  0.0563]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 69 ] state=tensor([[-0.1850, -0.3993, -0.0571,  0.0563]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1930, -0.5935, -0.0560,  0.3304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 70 ] state=tensor([[-0.1930, -0.5935, -0.0560,  0.3304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2048, -0.7878, -0.0494,  0.6049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 71 ] state=tensor([[-0.2048, -0.7878, -0.0494,  0.6049]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2206, -0.5921, -0.0373,  0.2971]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 72 ] state=tensor([[-0.2206, -0.5921, -0.0373,  0.2971]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2324, -0.3964, -0.0313, -0.0071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 73 ] state=tensor([[-0.2324, -0.3964, -0.0313, -0.0071]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2404, -0.5911, -0.0315,  0.2756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 74 ] state=tensor([[-0.2404, -0.5911, -0.0315,  0.2756]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2522, -0.3955, -0.0259, -0.0269]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 75 ] state=tensor([[-0.2522, -0.3955, -0.0259, -0.0269]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2601, -0.2000, -0.0265, -0.3276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 76 ] state=tensor([[-0.2601, -0.2000, -0.0265, -0.3276]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2641, -0.0046, -0.0330, -0.6285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 77 ] state=tensor([[-0.2641, -0.0046, -0.0330, -0.6285]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2642, -0.1992, -0.0456, -0.3464]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 78 ] state=tensor([[-0.2642, -0.1992, -0.0456, -0.3464]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2682, -0.0035, -0.0525, -0.6531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 79 ] state=tensor([[-0.2682, -0.0035, -0.0525, -0.6531]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2682, -0.1978, -0.0656, -0.3775]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 80 ] state=tensor([[-0.2682, -0.1978, -0.0656, -0.3775]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2722, -0.3919, -0.0731, -0.1062]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 81 ] state=tensor([[-0.2722, -0.3919, -0.0731, -0.1062]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2800, -0.5859, -0.0753,  0.1626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 82 ] state=tensor([[-0.2800, -0.5859, -0.0753,  0.1626]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2918, -0.3898, -0.0720, -0.1529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 83 ] state=tensor([[-0.2918, -0.3898, -0.0720, -0.1529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2996, -0.5839, -0.0751,  0.1163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 84 ] state=tensor([[-0.2996, -0.5839, -0.0751,  0.1163]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3112, -0.7778, -0.0728,  0.3843]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 85 ] state=tensor([[-0.3112, -0.7778, -0.0728,  0.3843]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3268, -0.5817, -0.0651,  0.0696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 86 ] state=tensor([[-0.3268, -0.5817, -0.0651,  0.0696]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3384, -0.7759, -0.0637,  0.3411]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 87 ] state=tensor([[-0.3384, -0.7759, -0.0637,  0.3411]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3539, -0.5799, -0.0568,  0.0290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 88 ] state=tensor([[-0.3539, -0.5799, -0.0568,  0.0290]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3655, -0.3840, -0.0563, -0.2810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 89 ] state=tensor([[-0.3655, -0.3840, -0.0563, -0.2810]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3732, -0.5783, -0.0619, -0.0066]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 90 ] state=tensor([[-0.3732, -0.5783, -0.0619, -0.0066]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3848, -0.3823, -0.0620, -0.3182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 91 ] state=tensor([[-0.3848, -0.3823, -0.0620, -0.3182]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3924, -0.5765, -0.0684, -0.0457]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 92 ] state=tensor([[-0.3924, -0.5765, -0.0684, -0.0457]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4040, -0.7706, -0.0693,  0.2247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 93 ] state=tensor([[-0.4040, -0.7706, -0.0693,  0.2247]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4194, -0.5746, -0.0648, -0.0890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 94 ] state=tensor([[-0.4194, -0.5746, -0.0648, -0.0890]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4309, -0.7687, -0.0666,  0.1825]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 95 ] state=tensor([[-0.4309, -0.7687, -0.0666,  0.1825]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4462, -0.5727, -0.0629, -0.1304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 96 ] state=tensor([[-0.4462, -0.5727, -0.0629, -0.1304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4577, -0.7669, -0.0655,  0.1418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 97 ] state=tensor([[-0.4577, -0.7669, -0.0655,  0.1418]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4730, -0.5709, -0.0627, -0.1708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 98 ] state=tensor([[-0.4730, -0.5709, -0.0627, -0.1708]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4844, -0.7650, -0.0661,  0.1014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 99 ] state=tensor([[-0.4844, -0.7650, -0.0661,  0.1014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4997, -0.5690, -0.0641, -0.2114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 100 ] state=tensor([[-0.4997, -0.5690, -0.0641, -0.2114]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5111, -0.3731, -0.0683, -0.5235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 101 ] state=tensor([[-0.5111, -0.3731, -0.0683, -0.5235]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5186, -0.5672, -0.0788, -0.2531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 102 ] state=tensor([[-0.5186, -0.5672, -0.0788, -0.2531]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5299, -0.7611, -0.0839,  0.0137]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 103 ] state=tensor([[-0.5299, -0.7611, -0.0839,  0.0137]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5452, -0.5648, -0.0836, -0.3042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 104 ] state=tensor([[-0.5452, -0.5648, -0.0836, -0.3042]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5564, -0.7587, -0.0897, -0.0390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 105 ] state=tensor([[-0.5564, -0.7587, -0.0897, -0.0390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5716, -0.5624, -0.0904, -0.3586]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 106 ] state=tensor([[-0.5716, -0.5624, -0.0904, -0.3586]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5829, -0.7561, -0.0976, -0.0958]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 107 ] state=tensor([[-0.5829, -0.7561, -0.0976, -0.0958]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5980, -0.5598, -0.0995, -0.4176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 108 ] state=tensor([[-0.5980, -0.5598, -0.0995, -0.4176]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6092, -0.7533, -0.1079, -0.1579]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 109 ] state=tensor([[-0.6092, -0.7533, -0.1079, -0.1579]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6243, -0.9468, -0.1110,  0.0989]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 110 ] state=tensor([[-0.6243, -0.9468, -0.1110,  0.0989]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6432, -0.7502, -0.1091, -0.2266]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 107 ][ timestamp 111 ] state=tensor([[-0.6432, -0.7502, -0.1091, -0.2266]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6582, -0.9436, -0.1136,  0.0298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 112 ] state=tensor([[-0.6582, -0.9436, -0.1136,  0.0298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6771, -1.1370, -0.1130,  0.2846]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 113 ] state=tensor([[-0.6771, -1.1370, -0.1130,  0.2846]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6998, -0.9404, -0.1073, -0.0415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 114 ] state=tensor([[-0.6998, -0.9404, -0.1073, -0.0415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7186, -0.7439, -0.1081, -0.3660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 115 ] state=tensor([[-0.7186, -0.7439, -0.1081, -0.3660]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7335, -0.9374, -0.1155, -0.1093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 116 ] state=tensor([[-0.7335, -0.9374, -0.1155, -0.1093]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7522, -0.7408, -0.1176, -0.4361]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 117 ] state=tensor([[-0.7522, -0.7408, -0.1176, -0.4361]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7671, -0.9341, -0.1264, -0.1827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 118 ] state=tensor([[-0.7671, -0.9341, -0.1264, -0.1827]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7857, -1.1272, -0.1300,  0.0676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 119 ] state=tensor([[-0.7857, -1.1272, -0.1300,  0.0676]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8083, -0.9305, -0.1287, -0.2631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 120 ] state=tensor([[-0.8083, -0.9305, -0.1287, -0.2631]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8269, -1.1235, -0.1339, -0.0136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 121 ] state=tensor([[-0.8269, -1.1235, -0.1339, -0.0136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8494, -0.9268, -0.1342, -0.3453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 122 ] state=tensor([[-0.8494, -0.9268, -0.1342, -0.3453]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8679, -1.1198, -0.1411, -0.0978]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 123 ] state=tensor([[-0.8679, -1.1198, -0.1411, -0.0978]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8903, -0.9229, -0.1431, -0.4315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 124 ] state=tensor([[-0.8903, -0.9229, -0.1431, -0.4315]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9088, -1.1158, -0.1517, -0.1871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 125 ] state=tensor([[-0.9088, -1.1158, -0.1517, -0.1871]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9311, -0.9188, -0.1554, -0.5235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 126 ] state=tensor([[-0.9311, -0.9188, -0.1554, -0.5235]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9494, -1.1115, -0.1659, -0.2835]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 127 ] state=tensor([[-0.9494, -1.1115, -0.1659, -0.2835]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9717, -1.3039, -0.1716, -0.0474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 128 ] state=tensor([[-0.9717, -1.3039, -0.1716, -0.0474]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9978, -1.1068, -0.1725, -0.3890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 129 ] state=tensor([[-0.9978, -1.1068, -0.1725, -0.3890]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0199, -1.2991, -0.1803, -0.1553]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 130 ] state=tensor([[-1.0199, -1.2991, -0.1803, -0.1553]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0459, -1.1019, -0.1834, -0.4990]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 131 ] state=tensor([[-1.0459, -1.1019, -0.1834, -0.4990]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0679, -1.2940, -0.1934, -0.2692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 132 ] state=tensor([[-1.0679, -1.2940, -0.1934, -0.2692]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0938, -1.0967, -0.1988, -0.6161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 107 ][ timestamp 133 ] state=tensor([[-1.0938, -1.0967, -0.1988, -0.6161]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 107: Exploration_rate=0.05. Score=133.\n",
      "[ episode 108 ] state=tensor([[-0.0026, -0.0261, -0.0111, -0.0356]])\n",
      "[ episode 108 ][ timestamp 1 ] state=tensor([[-0.0026, -0.0261, -0.0111, -0.0356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0031,  0.1691, -0.0118, -0.3317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 2 ] state=tensor([[-0.0031,  0.1691, -0.0118, -0.3317]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0003, -0.0258, -0.0184, -0.0428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 3 ] state=tensor([[ 0.0003, -0.0258, -0.0184, -0.0428]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.5735e-04,  1.6957e-01, -1.9260e-02, -3.4119e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 4 ] state=tensor([[-2.5735e-04,  1.6957e-01, -1.9260e-02, -3.4119e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0031, -0.0253, -0.0261, -0.0546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 5 ] state=tensor([[ 0.0031, -0.0253, -0.0261, -0.0546]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0026,  0.1702, -0.0272, -0.3554]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 6 ] state=tensor([[ 0.0026,  0.1702, -0.0272, -0.3554]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0060, -0.0245, -0.0343, -0.0714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 7 ] state=tensor([[ 0.0060, -0.0245, -0.0343, -0.0714]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0055,  0.1711, -0.0357, -0.3747]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 8 ] state=tensor([[ 0.0055,  0.1711, -0.0357, -0.3747]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0090, -0.0235, -0.0432, -0.0935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 9 ] state=tensor([[ 0.0090, -0.0235, -0.0432, -0.0935]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0085,  0.1722, -0.0451, -0.3995]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 10 ] state=tensor([[ 0.0085,  0.1722, -0.0451, -0.3995]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0119, -0.0223, -0.0531, -0.1214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 11 ] state=tensor([[ 0.0119, -0.0223, -0.0531, -0.1214]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0115,  0.1736, -0.0555, -0.4303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 12 ] state=tensor([[ 0.0115,  0.1736, -0.0555, -0.4303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0150, -0.0207, -0.0641, -0.1557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 13 ] state=tensor([[ 0.0150, -0.0207, -0.0641, -0.1557]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0146, -0.2149, -0.0672,  0.1161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 14 ] state=tensor([[ 0.0146, -0.2149, -0.0672,  0.1161]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0103, -0.0188, -0.0649, -0.1970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 15 ] state=tensor([[ 0.0103, -0.0188, -0.0649, -0.1970]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0099,  0.1771, -0.0688, -0.5094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 16 ] state=tensor([[ 0.0099,  0.1771, -0.0688, -0.5094]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0134, -0.0169, -0.0790, -0.2392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 17 ] state=tensor([[ 0.0134, -0.0169, -0.0790, -0.2392]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0131, -0.2109, -0.0838,  0.0276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 18 ] state=tensor([[ 0.0131, -0.2109, -0.0838,  0.0276]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0089, -0.4047, -0.0833,  0.2927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 19 ] state=tensor([[ 0.0089, -0.4047, -0.0833,  0.2927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0008, -0.2085, -0.0774, -0.0251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 20 ] state=tensor([[ 0.0008, -0.2085, -0.0774, -0.0251]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0034, -0.4024, -0.0779,  0.2422]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 21 ] state=tensor([[-0.0034, -0.4024, -0.0779,  0.2422]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0114, -0.2063, -0.0731, -0.0740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 22 ] state=tensor([[-0.0114, -0.2063, -0.0731, -0.0740]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0156, -0.4003, -0.0745,  0.1948]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 23 ] state=tensor([[-0.0156, -0.4003, -0.0745,  0.1948]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0236, -0.5942, -0.0706,  0.4631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 24 ] state=tensor([[-0.0236, -0.5942, -0.0706,  0.4631]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0355, -0.3982, -0.0614,  0.1490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 25 ] state=tensor([[-0.0355, -0.3982, -0.0614,  0.1490]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0434, -0.5924, -0.0584,  0.4217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 26 ] state=tensor([[-0.0434, -0.5924, -0.0584,  0.4217]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0553, -0.7866, -0.0500,  0.6954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 27 ] state=tensor([[-0.0553, -0.7866, -0.0500,  0.6954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0710, -0.9810, -0.0361,  0.9719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 28 ] state=tensor([[-0.0710, -0.9810, -0.0361,  0.9719]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0906, -0.7854, -0.0166,  0.6682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 29 ] state=tensor([[-0.0906, -0.7854, -0.0166,  0.6682]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1063, -0.5901, -0.0033,  0.3703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 30 ] state=tensor([[-0.1063, -0.5901, -0.0033,  0.3703]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1181, -0.3949,  0.0041,  0.0766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 31 ] state=tensor([[-0.1181, -0.3949,  0.0041,  0.0766]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1260, -0.1999,  0.0057, -0.2148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 32 ] state=tensor([[-0.1260, -0.1999,  0.0057, -0.2148]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1300, -0.3951,  0.0014,  0.0797]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 33 ] state=tensor([[-0.1300, -0.3951,  0.0014,  0.0797]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1379, -0.2000,  0.0030, -0.2126]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 34 ] state=tensor([[-0.1379, -0.2000,  0.0030, -0.2126]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1419, -0.3951, -0.0013,  0.0810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 35 ] state=tensor([[-0.1419, -0.3951, -0.0013,  0.0810]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1498, -0.2000,  0.0003, -0.2120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 36 ] state=tensor([[-0.1498, -0.2000,  0.0003, -0.2120]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1538, -0.3951, -0.0039,  0.0808]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 37 ] state=tensor([[-0.1538, -0.3951, -0.0039,  0.0808]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1617, -0.1999, -0.0023, -0.2132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 38 ] state=tensor([[-0.1617, -0.1999, -0.0023, -0.2132]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1657, -0.3950, -0.0065,  0.0788]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 108 ][ timestamp 39 ] state=tensor([[-0.1657, -0.3950, -0.0065,  0.0788]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1736, -0.1998, -0.0050, -0.2159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 40 ] state=tensor([[-0.1736, -0.1998, -0.0050, -0.2159]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1776, -0.3949, -0.0093,  0.0752]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 41 ] state=tensor([[-0.1776, -0.3949, -0.0093,  0.0752]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1855, -0.1996, -0.0078, -0.2204]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 42 ] state=tensor([[-0.1855, -0.1996, -0.0078, -0.2204]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1895, -0.0044, -0.0122, -0.5155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 43 ] state=tensor([[-0.1895, -0.0044, -0.0122, -0.5155]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1896, -0.1993, -0.0225, -0.2267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 44 ] state=tensor([[-0.1896, -0.1993, -0.0225, -0.2267]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1936, -0.0039, -0.0270, -0.5264]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 45 ] state=tensor([[-0.1936, -0.0039, -0.0270, -0.5264]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1937, -0.1986, -0.0376, -0.2424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 46 ] state=tensor([[-0.1937, -0.1986, -0.0376, -0.2424]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1977, -0.3932, -0.0424,  0.0382]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 47 ] state=tensor([[-0.1977, -0.3932, -0.0424,  0.0382]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2055, -0.1975, -0.0416, -0.2675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 48 ] state=tensor([[-0.2055, -0.1975, -0.0416, -0.2675]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2095, -0.3920, -0.0470,  0.0117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 49 ] state=tensor([[-0.2095, -0.3920, -0.0470,  0.0117]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2173, -0.5864, -0.0468,  0.2892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 50 ] state=tensor([[-0.2173, -0.5864, -0.0468,  0.2892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2290, -0.7808, -0.0410,  0.5668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 51 ] state=tensor([[-0.2290, -0.7808, -0.0410,  0.5668]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2446, -0.5852, -0.0296,  0.2615]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 52 ] state=tensor([[-0.2446, -0.5852, -0.0296,  0.2615]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2564, -0.7798, -0.0244,  0.5447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 53 ] state=tensor([[-0.2564, -0.7798, -0.0244,  0.5447]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2719, -0.5844, -0.0135,  0.2444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 54 ] state=tensor([[-0.2719, -0.5844, -0.0135,  0.2444]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2836, -0.7793, -0.0086,  0.5328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 55 ] state=tensor([[-0.2836, -0.7793, -0.0086,  0.5328]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2992, -0.5841,  0.0020,  0.2374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 56 ] state=tensor([[-0.2992, -0.5841,  0.0020,  0.2374]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3109, -0.3890,  0.0068, -0.0546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 57 ] state=tensor([[-0.3109, -0.3890,  0.0068, -0.0546]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3187, -0.1940,  0.0057, -0.3452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 58 ] state=tensor([[-0.3187, -0.1940,  0.0057, -0.3452]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3226, -0.3892, -0.0012, -0.0507]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 59 ] state=tensor([[-0.3226, -0.3892, -0.0012, -0.0507]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3303, -0.5843, -0.0022,  0.2416]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 60 ] state=tensor([[-0.3303, -0.5843, -0.0022,  0.2416]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3420, -0.3891,  0.0026, -0.0518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 61 ] state=tensor([[-0.3420, -0.3891,  0.0026, -0.0518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3498, -0.5843,  0.0016,  0.2417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 62 ] state=tensor([[-0.3498, -0.5843,  0.0016,  0.2417]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3615, -0.7794,  0.0064,  0.5349]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 63 ] state=tensor([[-0.3615, -0.7794,  0.0064,  0.5349]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3771, -0.5844,  0.0171,  0.2442]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 64 ] state=tensor([[-0.3771, -0.5844,  0.0171,  0.2442]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3888, -0.3895,  0.0220, -0.0430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 65 ] state=tensor([[-0.3888, -0.3895,  0.0220, -0.0430]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3966, -0.1947,  0.0211, -0.3287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 66 ] state=tensor([[-0.3966, -0.1947,  0.0211, -0.3287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.0046e-01,  1.0707e-04,  1.4546e-02, -6.1462e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 67 ] state=tensor([[-4.0046e-01,  1.0707e-04,  1.4546e-02, -6.1462e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4005, -0.1952,  0.0023, -0.3174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 68 ] state=tensor([[-0.4005, -0.1952,  0.0023, -0.3174]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4044, -0.3904, -0.0041, -0.0240]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 69 ] state=tensor([[-0.4044, -0.3904, -0.0041, -0.0240]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4122, -0.1952, -0.0046, -0.3180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 70 ] state=tensor([[-0.4122, -0.1952, -0.0046, -0.3180]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4161, -0.3902, -0.0109, -0.0267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 71 ] state=tensor([[-0.4161, -0.3902, -0.0109, -0.0267]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4239, -0.1950, -0.0115, -0.3229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 72 ] state=tensor([[-0.4239, -0.1950, -0.0115, -0.3229]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.2778e-01,  3.1526e-04, -1.7925e-02, -6.1913e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 73 ] state=tensor([[-4.2778e-01,  3.1526e-04, -1.7925e-02, -6.1913e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4278, -0.1946, -0.0303, -0.3321]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 74 ] state=tensor([[-0.4278, -0.1946, -0.0303, -0.3321]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4317, -0.3892, -0.0370, -0.0492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 75 ] state=tensor([[-0.4317, -0.3892, -0.0370, -0.0492]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4394, -0.5838, -0.0379,  0.2316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 76 ] state=tensor([[-0.4394, -0.5838, -0.0379,  0.2316]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4511, -0.3882, -0.0333, -0.0728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 77 ] state=tensor([[-0.4511, -0.3882, -0.0333, -0.0728]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4589, -0.1926, -0.0348, -0.3758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 78 ] state=tensor([[-0.4589, -0.1926, -0.0348, -0.3758]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4627, -0.3872, -0.0423, -0.0943]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 79 ] state=tensor([[-0.4627, -0.3872, -0.0423, -0.0943]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4705, -0.5817, -0.0442,  0.1848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 80 ] state=tensor([[-0.4705, -0.5817, -0.0442,  0.1848]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4821, -0.3860, -0.0405, -0.1215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 81 ] state=tensor([[-0.4821, -0.3860, -0.0405, -0.1215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4898, -0.5805, -0.0429,  0.1582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 82 ] state=tensor([[-0.4898, -0.5805, -0.0429,  0.1582]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5014, -0.3848, -0.0397, -0.1477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 83 ] state=tensor([[-0.5014, -0.3848, -0.0397, -0.1477]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5091, -0.5793, -0.0427,  0.1322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 84 ] state=tensor([[-0.5091, -0.5793, -0.0427,  0.1322]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5207, -0.7738, -0.0400,  0.4111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 85 ] state=tensor([[-0.5207, -0.7738, -0.0400,  0.4111]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5362, -0.5781, -0.0318,  0.1060]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 86 ] state=tensor([[-0.5362, -0.5781, -0.0318,  0.1060]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5478, -0.3826, -0.0297, -0.1965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 87 ] state=tensor([[-0.5478, -0.3826, -0.0297, -0.1965]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5554, -0.1870, -0.0336, -0.4984]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 88 ] state=tensor([[-0.5554, -0.1870, -0.0336, -0.4984]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5592, -0.3817, -0.0436, -0.2165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 89 ] state=tensor([[-0.5592, -0.3817, -0.0436, -0.2165]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5668, -0.5761, -0.0479,  0.0621]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 90 ] state=tensor([[-0.5668, -0.5761, -0.0479,  0.0621]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5783, -0.3803, -0.0467, -0.2453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 91 ] state=tensor([[-0.5783, -0.3803, -0.0467, -0.2453]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5859, -0.5748, -0.0516,  0.0323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 92 ] state=tensor([[-0.5859, -0.5748, -0.0516,  0.0323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5974, -0.3790, -0.0509, -0.2762]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 93 ] state=tensor([[-0.5974, -0.3790, -0.0509, -0.2762]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-6.0499e-01, -5.7331e-01, -5.6467e-02, -1.2282e-05]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 94 ] state=tensor([[-6.0499e-01, -5.7331e-01, -5.6467e-02, -1.2282e-05]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6165, -0.3774, -0.0565, -0.3100]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 95 ] state=tensor([[-0.6165, -0.3774, -0.0565, -0.3100]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6240, -0.5717, -0.0627, -0.0356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 96 ] state=tensor([[-0.6240, -0.5717, -0.0627, -0.0356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6354, -0.3757, -0.0634, -0.3474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 97 ] state=tensor([[-0.6354, -0.3757, -0.0634, -0.3474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6430, -0.5699, -0.0703, -0.0753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 98 ] state=tensor([[-0.6430, -0.5699, -0.0703, -0.0753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6544, -0.7640, -0.0718,  0.1943]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 99 ] state=tensor([[-0.6544, -0.7640, -0.0718,  0.1943]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6696, -0.5679, -0.0679, -0.1201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 100 ] state=tensor([[-0.6696, -0.5679, -0.0679, -0.1201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6810, -0.3719, -0.0703, -0.4334]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 101 ] state=tensor([[-0.6810, -0.3719, -0.0703, -0.4334]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6884, -0.1758, -0.0790, -0.7474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 102 ] state=tensor([[-0.6884, -0.1758, -0.0790, -0.7474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6919, -0.3698, -0.0940, -0.4806]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 103 ] state=tensor([[-0.6919, -0.3698, -0.0940, -0.4806]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6993, -0.5634, -0.1036, -0.2190]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 108 ][ timestamp 104 ] state=tensor([[-0.6993, -0.5634, -0.1036, -0.2190]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7106, -0.3670, -0.1080, -0.5424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 105 ] state=tensor([[-0.7106, -0.3670, -0.1080, -0.5424]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7179, -0.5604, -0.1188, -0.2856]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 106 ] state=tensor([[-0.7179, -0.5604, -0.1188, -0.2856]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7292, -0.7537, -0.1245, -0.0327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 107 ] state=tensor([[-0.7292, -0.7537, -0.1245, -0.0327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7442, -0.5570, -0.1252, -0.3619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 108 ] state=tensor([[-0.7442, -0.5570, -0.1252, -0.3619]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7554, -0.7502, -0.1324, -0.1111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 109 ] state=tensor([[-0.7554, -0.7502, -0.1324, -0.1111]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7704, -0.5534, -0.1346, -0.4425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 110 ] state=tensor([[-0.7704, -0.5534, -0.1346, -0.4425]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7814, -0.7464, -0.1435, -0.1951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 111 ] state=tensor([[-0.7814, -0.7464, -0.1435, -0.1951]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7964, -0.5496, -0.1474, -0.5294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 112 ] state=tensor([[-0.7964, -0.5496, -0.1474, -0.5294]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8074, -0.7423, -0.1580, -0.2865]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 113 ] state=tensor([[-0.8074, -0.7423, -0.1580, -0.2865]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8222, -0.9349, -0.1637, -0.0475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 114 ] state=tensor([[-0.8222, -0.9349, -0.1637, -0.0475]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8409, -0.7378, -0.1647, -0.3871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 115 ] state=tensor([[-0.8409, -0.7378, -0.1647, -0.3871]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8557, -0.9303, -0.1724, -0.1505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 116 ] state=tensor([[-0.8557, -0.9303, -0.1724, -0.1505]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8743, -0.7332, -0.1754, -0.4922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 117 ] state=tensor([[-0.8743, -0.7332, -0.1754, -0.4922]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8889, -0.9254, -0.1853, -0.2595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 118 ] state=tensor([[-0.8889, -0.9254, -0.1853, -0.2595]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9074, -0.7282, -0.1904, -0.6045]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 119 ] state=tensor([[-0.9074, -0.7282, -0.1904, -0.6045]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9220, -0.9202, -0.2025, -0.3773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 108 ][ timestamp 120 ] state=tensor([[-0.9220, -0.9202, -0.2025, -0.3773]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 108: Exploration_rate=0.05. Score=120.\n",
      "[ episode 109 ] state=tensor([[0.0428, 0.0227, 0.0032, 0.0387]])\n",
      "[ episode 109 ][ timestamp 1 ] state=tensor([[0.0428, 0.0227, 0.0032, 0.0387]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0433,  0.2177,  0.0039, -0.2530]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 2 ] state=tensor([[ 0.0433,  0.2177,  0.0039, -0.2530]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0476,  0.4128, -0.0011, -0.5444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 3 ] state=tensor([[ 0.0476,  0.4128, -0.0011, -0.5444]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0559,  0.6079, -0.0120, -0.8375]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 4 ] state=tensor([[ 0.0559,  0.6079, -0.0120, -0.8375]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0680,  0.4130, -0.0288, -0.5486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 5 ] state=tensor([[ 0.0680,  0.4130, -0.0288, -0.5486]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0763,  0.2183, -0.0397, -0.2651]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 6 ] state=tensor([[ 0.0763,  0.2183, -0.0397, -0.2651]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0807,  0.4139, -0.0450, -0.5701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 7 ] state=tensor([[ 0.0807,  0.4139, -0.0450, -0.5701]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0889,  0.2195, -0.0564, -0.2919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 8 ] state=tensor([[ 0.0889,  0.2195, -0.0564, -0.2919]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0933,  0.0252, -0.0623, -0.0175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 9 ] state=tensor([[ 0.0933,  0.0252, -0.0623, -0.0175]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0938, -0.1690, -0.0626,  0.2549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 10 ] state=tensor([[ 0.0938, -0.1690, -0.0626,  0.2549]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0904,  0.0270, -0.0575, -0.0569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 11 ] state=tensor([[ 0.0904,  0.0270, -0.0575, -0.0569]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0910, -0.1673, -0.0587,  0.2171]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 12 ] state=tensor([[ 0.0910, -0.1673, -0.0587,  0.2171]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0876,  0.0286, -0.0543, -0.0935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 13 ] state=tensor([[ 0.0876,  0.0286, -0.0543, -0.0935]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0882, -0.1657, -0.0562,  0.1816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 14 ] state=tensor([[ 0.0882, -0.1657, -0.0562,  0.1816]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0849,  0.0302, -0.0526, -0.1283]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 15 ] state=tensor([[ 0.0849,  0.0302, -0.0526, -0.1283]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0855, -0.1641, -0.0551,  0.1473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 16 ] state=tensor([[ 0.0855, -0.1641, -0.0551,  0.1473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0822, -0.3584, -0.0522,  0.4221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 17 ] state=tensor([[ 0.0822, -0.3584, -0.0522,  0.4221]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0751, -0.1626, -0.0437,  0.1135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 18 ] state=tensor([[ 0.0751, -0.1626, -0.0437,  0.1135]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0718, -0.3571, -0.0415,  0.3920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 19 ] state=tensor([[ 0.0718, -0.3571, -0.0415,  0.3920]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0647, -0.1614, -0.0336,  0.0866]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 20 ] state=tensor([[ 0.0647, -0.1614, -0.0336,  0.0866]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0614,  0.0342, -0.0319, -0.2165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 21 ] state=tensor([[ 0.0614,  0.0342, -0.0319, -0.2165]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0621, -0.1604, -0.0362,  0.0659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 22 ] state=tensor([[ 0.0621, -0.1604, -0.0362,  0.0659]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0589, -0.3550, -0.0349,  0.3470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 23 ] state=tensor([[ 0.0589, -0.3550, -0.0349,  0.3470]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0518, -0.5496, -0.0280,  0.6284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 24 ] state=tensor([[ 0.0518, -0.5496, -0.0280,  0.6284]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0408, -0.3541, -0.0154,  0.3271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 25 ] state=tensor([[ 0.0408, -0.3541, -0.0154,  0.3271]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0337, -0.1588, -0.0089,  0.0296]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 26 ] state=tensor([[ 0.0337, -0.1588, -0.0089,  0.0296]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0306, -0.3538, -0.0083,  0.3194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 27 ] state=tensor([[ 0.0306, -0.3538, -0.0083,  0.3194]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0235, -0.5488, -0.0019,  0.6095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 28 ] state=tensor([[ 0.0235, -0.5488, -0.0019,  0.6095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0125, -0.7439,  0.0103,  0.9016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 29 ] state=tensor([[ 0.0125, -0.7439,  0.0103,  0.9016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0024, -0.5489,  0.0283,  0.6122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 30 ] state=tensor([[-0.0024, -0.5489,  0.0283,  0.6122]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0133, -0.3542,  0.0406,  0.3286]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 31 ] state=tensor([[-0.0133, -0.3542,  0.0406,  0.3286]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0204, -0.5499,  0.0472,  0.6338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 32 ] state=tensor([[-0.0204, -0.5499,  0.0472,  0.6338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0314, -0.7456,  0.0598,  0.9409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 33 ] state=tensor([[-0.0314, -0.7456,  0.0598,  0.9409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0463, -0.5513,  0.0786,  0.6676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 34 ] state=tensor([[-0.0463, -0.5513,  0.0786,  0.6676]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0574, -0.7475,  0.0920,  0.9840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 35 ] state=tensor([[-0.0574, -0.7475,  0.0920,  0.9840]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0723, -0.9437,  0.1117,  1.3041]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 36 ] state=tensor([[-0.0723, -0.9437,  0.1117,  1.3041]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0912, -0.7501,  0.1378,  1.0483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 37 ] state=tensor([[-0.0912, -0.7501,  0.1378,  1.0483]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1062, -0.5571,  0.1587,  0.8019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 38 ] state=tensor([[-0.1062, -0.5571,  0.1587,  0.8019]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1173, -0.7540,  0.1748,  1.1400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 39 ] state=tensor([[-0.1173, -0.7540,  0.1748,  1.1400]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1324, -0.9509,  0.1976,  1.4820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 109 ][ timestamp 40 ] state=tensor([[-0.1324, -0.9509,  0.1976,  1.4820]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 109: Exploration_rate=0.05. Score=40.\n",
      "[ episode 110 ] state=tensor([[-0.0409, -0.0405, -0.0242, -0.0215]])\n",
      "[ episode 110 ][ timestamp 1 ] state=tensor([[-0.0409, -0.0405, -0.0242, -0.0215]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0417,  0.1549, -0.0246, -0.3217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 2 ] state=tensor([[-0.0417,  0.1549, -0.0246, -0.3217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0386,  0.3504, -0.0310, -0.6220]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 3 ] state=tensor([[-0.0386,  0.3504, -0.0310, -0.6220]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0316,  0.1557, -0.0435, -0.3393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 4 ] state=tensor([[-0.0316,  0.1557, -0.0435, -0.3393]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0285, -0.0388, -0.0503, -0.0606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 5 ] state=tensor([[-0.0285, -0.0388, -0.0503, -0.0606]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0293, -0.2331, -0.0515,  0.2158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 6 ] state=tensor([[-0.0293, -0.2331, -0.0515,  0.2158]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0339, -0.0373, -0.0472, -0.0927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 7 ] state=tensor([[-0.0339, -0.0373, -0.0472, -0.0927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0347,  0.1584, -0.0490, -0.3998]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 8 ] state=tensor([[-0.0347,  0.1584, -0.0490, -0.3998]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0315, -0.0359, -0.0570, -0.1230]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 110 ][ timestamp 9 ] state=tensor([[-0.0315, -0.0359, -0.0570, -0.1230]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0322,  0.1599, -0.0595, -0.4331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 10 ] state=tensor([[-0.0322,  0.1599, -0.0595, -0.4331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0290, -0.0343, -0.0681, -0.1598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 11 ] state=tensor([[-0.0290, -0.0343, -0.0681, -0.1598]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0297, -0.2284, -0.0713,  0.1107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 12 ] state=tensor([[-0.0297, -0.2284, -0.0713,  0.1107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0343, -0.4224, -0.0691,  0.3800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 13 ] state=tensor([[-0.0343, -0.4224, -0.0691,  0.3800]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0427, -0.2264, -0.0615,  0.0664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 14 ] state=tensor([[-0.0427, -0.2264, -0.0615,  0.0664]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0473, -0.0304, -0.0602, -0.2451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 15 ] state=tensor([[-0.0473, -0.0304, -0.0602, -0.2451]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0479,  0.1655, -0.0651, -0.5561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 16 ] state=tensor([[-0.0479,  0.1655, -0.0651, -0.5561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0446, -0.0286, -0.0762, -0.2846]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 17 ] state=tensor([[-0.0446, -0.0286, -0.0762, -0.2846]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0451, -0.2226, -0.0819, -0.0169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 18 ] state=tensor([[-0.0451, -0.2226, -0.0819, -0.0169]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0496, -0.4165, -0.0822,  0.2489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 19 ] state=tensor([[-0.0496, -0.4165, -0.0822,  0.2489]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0579, -0.6103, -0.0773,  0.5145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 20 ] state=tensor([[-0.0579, -0.6103, -0.0773,  0.5145]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0701, -0.8043, -0.0670,  0.7819]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 21 ] state=tensor([[-0.0701, -0.8043, -0.0670,  0.7819]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0862, -0.9984, -0.0513,  1.0528]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 22 ] state=tensor([[-0.0862, -0.9984, -0.0513,  1.0528]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1062, -0.8026, -0.0303,  0.7444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 23 ] state=tensor([[-0.1062, -0.8026, -0.0303,  0.7444]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1222, -0.6071, -0.0154,  0.4424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 24 ] state=tensor([[-0.1222, -0.6071, -0.0154,  0.4424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1344, -0.4118, -0.0065,  0.1449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 25 ] state=tensor([[-0.1344, -0.4118, -0.0065,  0.1449]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1426, -0.2166, -0.0036, -0.1499]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 26 ] state=tensor([[-0.1426, -0.2166, -0.0036, -0.1499]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1469, -0.4116, -0.0066,  0.1417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 27 ] state=tensor([[-0.1469, -0.4116, -0.0066,  0.1417]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1552, -0.6067, -0.0038,  0.4323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 28 ] state=tensor([[-0.1552, -0.6067, -0.0038,  0.4323]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1673, -0.8017,  0.0048,  0.7237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 29 ] state=tensor([[-0.1673, -0.8017,  0.0048,  0.7237]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1833, -0.6067,  0.0193,  0.4326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 30 ] state=tensor([[-0.1833, -0.6067,  0.0193,  0.4326]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1955, -0.4118,  0.0280,  0.1460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 31 ] state=tensor([[-0.1955, -0.4118,  0.0280,  0.1460]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2037, -0.2171,  0.0309, -0.1377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 32 ] state=tensor([[-0.2037, -0.2171,  0.0309, -0.1377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2080, -0.4127,  0.0281,  0.1646]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 33 ] state=tensor([[-0.2080, -0.4127,  0.0281,  0.1646]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2163, -0.6082,  0.0314,  0.4660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 34 ] state=tensor([[-0.2163, -0.6082,  0.0314,  0.4660]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2285, -0.8037,  0.0407,  0.7684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 35 ] state=tensor([[-0.2285, -0.8037,  0.0407,  0.7684]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2445, -0.6092,  0.0561,  0.4888]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 36 ] state=tensor([[-0.2445, -0.6092,  0.0561,  0.4888]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2567, -0.4149,  0.0659,  0.2144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 37 ] state=tensor([[-0.2567, -0.4149,  0.0659,  0.2144]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2650, -0.6109,  0.0702,  0.5271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 38 ] state=tensor([[-0.2650, -0.6109,  0.0702,  0.5271]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2772, -0.4168,  0.0807,  0.2573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 39 ] state=tensor([[-0.2772, -0.4168,  0.0807,  0.2573]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2856, -0.2230,  0.0859, -0.0089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 40 ] state=tensor([[-0.2856, -0.2230,  0.0859, -0.0089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2900, -0.4192,  0.0857,  0.3096]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 41 ] state=tensor([[-0.2900, -0.4192,  0.0857,  0.3096]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2984, -0.2254,  0.0919,  0.0451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 42 ] state=tensor([[-0.2984, -0.2254,  0.0919,  0.0451]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3029, -0.4217,  0.0928,  0.3653]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 43 ] state=tensor([[-0.3029, -0.4217,  0.0928,  0.3653]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3114, -0.2280,  0.1001,  0.1033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 44 ] state=tensor([[-0.3114, -0.2280,  0.1001,  0.1033]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3159, -0.0345,  0.1022, -0.1562]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 45 ] state=tensor([[-0.3159, -0.0345,  0.1022, -0.1562]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3166,  0.1591,  0.0990, -0.4150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 46 ] state=tensor([[-0.3166,  0.1591,  0.0990, -0.4150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3134,  0.3526,  0.0907, -0.6749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 47 ] state=tensor([[-0.3134,  0.3526,  0.0907, -0.6749]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3064,  0.5464,  0.0772, -0.9377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 48 ] state=tensor([[-0.3064,  0.5464,  0.0772, -0.9377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2955,  0.3503,  0.0585, -0.6218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 49 ] state=tensor([[-0.2955,  0.3503,  0.0585, -0.6218]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2884,  0.5446,  0.0460, -0.8955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 50 ] state=tensor([[-0.2884,  0.5446,  0.0460, -0.8955]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2776,  0.7390,  0.0281, -1.1733]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 51 ] state=tensor([[-0.2776,  0.7390,  0.0281, -1.1733]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2628,  0.5436,  0.0047, -0.8720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 52 ] state=tensor([[-0.2628,  0.5436,  0.0047, -0.8720]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2519,  0.7386, -0.0128, -1.1632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 53 ] state=tensor([[-0.2519,  0.7386, -0.0128, -1.1632]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2371,  0.5437, -0.0360, -0.8745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 54 ] state=tensor([[-0.2371,  0.5437, -0.0360, -0.8745]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2263,  0.3491, -0.0535, -0.5934]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 55 ] state=tensor([[-0.2263,  0.3491, -0.0535, -0.5934]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2193,  0.1547, -0.0654, -0.3180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 56 ] state=tensor([[-0.2193,  0.1547, -0.0654, -0.3180]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2162, -0.0394, -0.0718, -0.0467]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 57 ] state=tensor([[-0.2162, -0.0394, -0.0718, -0.0467]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2170, -0.2334, -0.0727,  0.2226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 58 ] state=tensor([[-0.2170, -0.2334, -0.0727,  0.2226]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2216, -0.4274, -0.0682,  0.4914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 59 ] state=tensor([[-0.2216, -0.4274, -0.0682,  0.4914]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2302, -0.2314, -0.0584,  0.1781]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 60 ] state=tensor([[-0.2302, -0.2314, -0.0584,  0.1781]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2348, -0.0355, -0.0548, -0.1325]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 110 ][ timestamp 61 ] state=tensor([[-0.2348, -0.0355, -0.0548, -0.1325]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2355, -0.2298, -0.0575,  0.1424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 62 ] state=tensor([[-0.2355, -0.2298, -0.0575,  0.1424]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2401, -0.4241, -0.0546,  0.4164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 63 ] state=tensor([[-0.2401, -0.4241, -0.0546,  0.4164]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2486, -0.2282, -0.0463,  0.1070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 64 ] state=tensor([[-0.2486, -0.2282, -0.0463,  0.1070]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2532, -0.4226, -0.0442,  0.3848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 65 ] state=tensor([[-0.2532, -0.4226, -0.0442,  0.3848]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2616, -0.2269, -0.0365,  0.0785]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 66 ] state=tensor([[-0.2616, -0.2269, -0.0365,  0.0785]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2662, -0.0313, -0.0349, -0.2255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 67 ] state=tensor([[-0.2662, -0.0313, -0.0349, -0.2255]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2668, -0.2259, -0.0394,  0.0560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 68 ] state=tensor([[-0.2668, -0.2259, -0.0394,  0.0560]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2713, -0.0302, -0.0383, -0.2489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 69 ] state=tensor([[-0.2713, -0.0302, -0.0383, -0.2489]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2719,  0.1654, -0.0433, -0.5534]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 70 ] state=tensor([[-0.2719,  0.1654, -0.0433, -0.5534]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2686, -0.0291, -0.0543, -0.2746]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 71 ] state=tensor([[-0.2686, -0.0291, -0.0543, -0.2746]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2692,  0.1668, -0.0598, -0.5840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 72 ] state=tensor([[-0.2692,  0.1668, -0.0598, -0.5840]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2658, -0.0275, -0.0715, -0.3107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 73 ] state=tensor([[-0.2658, -0.0275, -0.0715, -0.3107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2664, -0.2215, -0.0777, -0.0414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 74 ] state=tensor([[-0.2664, -0.2215, -0.0777, -0.0414]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2708, -0.4154, -0.0786,  0.2258]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 75 ] state=tensor([[-0.2708, -0.4154, -0.0786,  0.2258]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2791, -0.2193, -0.0740, -0.0906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 76 ] state=tensor([[-0.2791, -0.2193, -0.0740, -0.0906]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2835, -0.4133, -0.0759,  0.1778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 77 ] state=tensor([[-0.2835, -0.4133, -0.0759,  0.1778]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2918, -0.2171, -0.0723, -0.1378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 78 ] state=tensor([[-0.2918, -0.2171, -0.0723, -0.1378]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2961, -0.4112, -0.0751,  0.1312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 79 ] state=tensor([[-0.2961, -0.4112, -0.0751,  0.1312]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3043, -0.6051, -0.0724,  0.3993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 80 ] state=tensor([[-0.3043, -0.6051, -0.0724,  0.3993]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3165, -0.4091, -0.0644,  0.0847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 81 ] state=tensor([[-0.3165, -0.4091, -0.0644,  0.0847]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3246, -0.6032, -0.0627,  0.3564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 82 ] state=tensor([[-0.3246, -0.6032, -0.0627,  0.3564]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3367, -0.4072, -0.0556,  0.0446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 83 ] state=tensor([[-0.3367, -0.4072, -0.0556,  0.0446]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3448, -0.6015, -0.0547,  0.3192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 84 ] state=tensor([[-0.3448, -0.6015, -0.0547,  0.3192]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3569, -0.4057, -0.0483,  0.0098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 85 ] state=tensor([[-0.3569, -0.4057, -0.0483,  0.0098]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3650, -0.2099, -0.0481, -0.2977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 86 ] state=tensor([[-0.3650, -0.2099, -0.0481, -0.2977]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3692, -0.4043, -0.0541, -0.0206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 87 ] state=tensor([[-0.3692, -0.4043, -0.0541, -0.0206]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3773, -0.5986, -0.0545,  0.2545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 88 ] state=tensor([[-0.3773, -0.5986, -0.0545,  0.2545]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3892, -0.4027, -0.0494, -0.0549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 89 ] state=tensor([[-0.3892, -0.4027, -0.0494, -0.0549]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3973, -0.2069, -0.0505, -0.3627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 90 ] state=tensor([[-0.3973, -0.2069, -0.0505, -0.3627]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4014, -0.4013, -0.0578, -0.0864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 91 ] state=tensor([[-0.4014, -0.4013, -0.0578, -0.0864]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4095, -0.2054, -0.0595, -0.3967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 92 ] state=tensor([[-0.4095, -0.2054, -0.0595, -0.3967]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4136, -0.3996, -0.0674, -0.1234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 93 ] state=tensor([[-0.4136, -0.3996, -0.0674, -0.1234]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4216, -0.5937, -0.0699,  0.1473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 94 ] state=tensor([[-0.4216, -0.5937, -0.0699,  0.1473]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4334, -0.3977, -0.0670, -0.1666]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 95 ] state=tensor([[-0.4334, -0.3977, -0.0670, -0.1666]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4414, -0.5918, -0.0703,  0.1042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 96 ] state=tensor([[-0.4414, -0.5918, -0.0703,  0.1042]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4532, -0.3957, -0.0682, -0.2098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 97 ] state=tensor([[-0.4532, -0.3957, -0.0682, -0.2098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4611, -0.5898, -0.0724,  0.0606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 98 ] state=tensor([[-0.4611, -0.5898, -0.0724,  0.0606]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4729, -0.3937, -0.0712, -0.2540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 99 ] state=tensor([[-0.4729, -0.3937, -0.0712, -0.2540]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4808, -0.5878, -0.0763,  0.0154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 100 ] state=tensor([[-0.4808, -0.5878, -0.0763,  0.0154]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4926, -0.3916, -0.0760, -0.3003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 101 ] state=tensor([[-0.4926, -0.3916, -0.0760, -0.3003]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5004, -0.5856, -0.0820, -0.0325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 102 ] state=tensor([[-0.5004, -0.5856, -0.0820, -0.0325]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5121, -0.3894, -0.0826, -0.3499]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 103 ] state=tensor([[-0.5121, -0.3894, -0.0826, -0.3499]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5199, -0.5833, -0.0896, -0.0844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 104 ] state=tensor([[-0.5199, -0.5833, -0.0896, -0.0844]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5316, -0.7770, -0.0913,  0.1787]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 105 ] state=tensor([[-0.5316, -0.7770, -0.0913,  0.1787]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5471, -0.5807, -0.0877, -0.1413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 106 ] state=tensor([[-0.5471, -0.5807, -0.0877, -0.1413]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5587, -0.7745, -0.0906,  0.1225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 107 ] state=tensor([[-0.5587, -0.7745, -0.0906,  0.1225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5742, -0.9682, -0.0881,  0.3853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 108 ] state=tensor([[-0.5742, -0.9682, -0.0881,  0.3853]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5936, -0.7719, -0.0804,  0.0662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 109 ] state=tensor([[-0.5936, -0.7719, -0.0804,  0.0662]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6090, -0.5757, -0.0791, -0.2508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 110 ] state=tensor([[-0.6090, -0.5757, -0.0791, -0.2508]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6205, -0.7696, -0.0841,  0.0160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 111 ] state=tensor([[-0.6205, -0.7696, -0.0841,  0.0160]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6359, -0.5734, -0.0838, -0.3020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 112 ] state=tensor([[-0.6359, -0.5734, -0.0838, -0.3020]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6474, -0.7673, -0.0898, -0.0369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 113 ] state=tensor([[-0.6474, -0.7673, -0.0898, -0.0369]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6627, -0.5710, -0.0906, -0.3565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 114 ] state=tensor([[-0.6627, -0.5710, -0.0906, -0.3565]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6742, -0.3747, -0.0977, -0.6763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 115 ] state=tensor([[-0.6742, -0.3747, -0.0977, -0.6763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6816, -0.5683, -0.1112, -0.4159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 116 ] state=tensor([[-0.6816, -0.5683, -0.1112, -0.4159]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6930, -0.7617, -0.1195, -0.1603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 117 ] state=tensor([[-0.6930, -0.7617, -0.1195, -0.1603]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7082, -0.5651, -0.1227, -0.4881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 118 ] state=tensor([[-0.7082, -0.5651, -0.1227, -0.4881]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7195, -0.7583, -0.1325, -0.2365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 119 ] state=tensor([[-0.7195, -0.7583, -0.1325, -0.2365]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7347, -0.9513, -0.1372,  0.0116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 120 ] state=tensor([[-0.7347, -0.9513, -0.1372,  0.0116]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7537, -0.7545, -0.1370, -0.3210]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 110 ][ timestamp 121 ] state=tensor([[-0.7537, -0.7545, -0.1370, -0.3210]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7688, -0.5577, -0.1434, -0.6536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 122 ] state=tensor([[-0.7688, -0.5577, -0.1434, -0.6536]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7800, -0.7506, -0.1565, -0.4093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 123 ] state=tensor([[-0.7800, -0.7506, -0.1565, -0.4093]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7950, -0.9432, -0.1647, -0.1697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 124 ] state=tensor([[-0.7950, -0.9432, -0.1647, -0.1697]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8139, -1.1356, -0.1681,  0.0668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 125 ] state=tensor([[-0.8139, -1.1356, -0.1681,  0.0668]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8366, -1.3280, -0.1667,  0.3021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 126 ] state=tensor([[-0.8366, -1.3280, -0.1667,  0.3021]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8631, -1.1309, -0.1607, -0.0381]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 127 ] state=tensor([[-0.8631, -1.1309, -0.1607, -0.0381]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8857, -0.9339, -0.1614, -0.3769]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 128 ] state=tensor([[-0.8857, -0.9339, -0.1614, -0.3769]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9044, -1.1264, -0.1690, -0.1392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 129 ] state=tensor([[-0.9044, -1.1264, -0.1690, -0.1392]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9270, -1.3188, -0.1718,  0.0958]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 130 ] state=tensor([[-0.9270, -1.3188, -0.1718,  0.0958]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9533, -1.1216, -0.1699, -0.2458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 131 ] state=tensor([[-0.9533, -1.1216, -0.1699, -0.2458]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9758, -1.3140, -0.1748, -0.0111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 132 ] state=tensor([[-0.9758, -1.3140, -0.1748, -0.0111]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0020, -1.1168, -0.1750, -0.3534]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 133 ] state=tensor([[-1.0020, -1.1168, -0.1750, -0.3534]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0244, -0.9197, -0.1821, -0.6958]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 134 ] state=tensor([[-1.0244, -0.9197, -0.1821, -0.6958]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0428, -1.1119, -0.1960, -0.4655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 135 ] state=tensor([[-1.0428, -1.1119, -0.1960, -0.4655]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0650, -1.3038, -0.2053, -0.2404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 110 ][ timestamp 136 ] state=tensor([[-1.0650, -1.3038, -0.2053, -0.2404]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 110: Exploration_rate=0.05. Score=136.\n",
      "[ episode 111 ] state=tensor([[-0.0372, -0.0027,  0.0467, -0.0099]])\n",
      "[ episode 111 ][ timestamp 1 ] state=tensor([[-0.0372, -0.0027,  0.0467, -0.0099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0373,  0.1917,  0.0465, -0.2875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 2 ] state=tensor([[-0.0373,  0.1917,  0.0465, -0.2875]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0334,  0.3862,  0.0408, -0.5652]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 3 ] state=tensor([[-0.0334,  0.3862,  0.0408, -0.5652]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0257,  0.5807,  0.0295, -0.8447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 4 ] state=tensor([[-0.0257,  0.5807,  0.0295, -0.8447]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0141,  0.3852,  0.0126, -0.5429]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 5 ] state=tensor([[-0.0141,  0.3852,  0.0126, -0.5429]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0064,  0.5801,  0.0017, -0.8316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 6 ] state=tensor([[-0.0064,  0.5801,  0.0017, -0.8316]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0052,  0.7752, -0.0149, -1.1238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 7 ] state=tensor([[ 0.0052,  0.7752, -0.0149, -1.1238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0207,  0.5803, -0.0374, -0.8358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 8 ] state=tensor([[ 0.0207,  0.5803, -0.0374, -0.8358]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0323,  0.3857, -0.0541, -0.5551]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 9 ] state=tensor([[ 0.0323,  0.3857, -0.0541, -0.5551]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0400,  0.5816, -0.0652, -0.8643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 10 ] state=tensor([[ 0.0400,  0.5816, -0.0652, -0.8643]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0517,  0.3874, -0.0825, -0.5929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 11 ] state=tensor([[ 0.0517,  0.3874, -0.0825, -0.5929]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0594,  0.1935, -0.0944, -0.3273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 12 ] state=tensor([[ 0.0594,  0.1935, -0.0944, -0.3273]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0633, -0.0002, -0.1009, -0.0658]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 13 ] state=tensor([[ 0.0633, -0.0002, -0.1009, -0.0658]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0633, -0.1937, -0.1022,  0.1935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 14 ] state=tensor([[ 0.0633, -0.1937, -0.1022,  0.1935]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0594, -0.3872, -0.0984,  0.4522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 15 ] state=tensor([[ 0.0594, -0.3872, -0.0984,  0.4522]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0517, -0.1909, -0.0893,  0.1302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 16 ] state=tensor([[ 0.0517, -0.1909, -0.0893,  0.1302]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0478, -0.3846, -0.0867,  0.3935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 17 ] state=tensor([[ 0.0478, -0.3846, -0.0867,  0.3935]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0401, -0.1884, -0.0788,  0.0747]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 18 ] state=tensor([[ 0.0401, -0.1884, -0.0788,  0.0747]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0364, -0.3823, -0.0773,  0.3415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 19 ] state=tensor([[ 0.0364, -0.3823, -0.0773,  0.3415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0287, -0.1861, -0.0705,  0.0255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 20 ] state=tensor([[ 0.0287, -0.1861, -0.0705,  0.0255]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0250, -0.3802, -0.0700,  0.2951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 21 ] state=tensor([[ 0.0250, -0.3802, -0.0700,  0.2951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0174, -0.5742, -0.0641,  0.5650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 22 ] state=tensor([[ 0.0174, -0.5742, -0.0641,  0.5650]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0059, -0.3783, -0.0528,  0.2528]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 23 ] state=tensor([[ 0.0059, -0.3783, -0.0528,  0.2528]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0016, -0.5726, -0.0477,  0.5284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 24 ] state=tensor([[-0.0016, -0.5726, -0.0477,  0.5284]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0131, -0.7670, -0.0372,  0.8056]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 25 ] state=tensor([[-0.0131, -0.7670, -0.0372,  0.8056]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0284, -0.9616, -0.0211,  1.0864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 26 ] state=tensor([[-0.0284, -0.9616, -0.0211,  1.0864]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.7671e-02, -7.6622e-01,  6.6551e-04,  7.8717e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 27 ] state=tensor([[-4.7671e-02, -7.6622e-01,  6.6551e-04,  7.8717e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0630, -0.5711,  0.0164,  0.4947]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 28 ] state=tensor([[-0.0630, -0.5711,  0.0164,  0.4947]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0744, -0.7665,  0.0263,  0.7925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 29 ] state=tensor([[-0.0744, -0.7665,  0.0263,  0.7925]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0897, -0.5717,  0.0422,  0.5082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 30 ] state=tensor([[-0.0897, -0.5717,  0.0422,  0.5082]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1012, -0.3772,  0.0523,  0.2291]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 31 ] state=tensor([[-0.1012, -0.3772,  0.0523,  0.2291]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1087, -0.5730,  0.0569,  0.5378]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 111 ][ timestamp 32 ] state=tensor([[-0.1087, -0.5730,  0.0569,  0.5378]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1202, -0.7689,  0.0677,  0.8479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 33 ] state=tensor([[-0.1202, -0.7689,  0.0677,  0.8479]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1356, -0.5748,  0.0846,  0.5772]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 34 ] state=tensor([[-0.1356, -0.5748,  0.0846,  0.5772]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1471, -0.3809,  0.0962,  0.3123]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 35 ] state=tensor([[-0.1471, -0.3809,  0.0962,  0.3123]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1547, -0.5773,  0.1024,  0.6337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 36 ] state=tensor([[-0.1547, -0.5773,  0.1024,  0.6337]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1662, -0.3837,  0.1151,  0.3750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 37 ] state=tensor([[-0.1662, -0.3837,  0.1151,  0.3750]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1739, -0.1904,  0.1226,  0.1207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 38 ] state=tensor([[-0.1739, -0.1904,  0.1226,  0.1207]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1777,  0.0028,  0.1250, -0.1310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 39 ] state=tensor([[-0.1777,  0.0028,  0.1250, -0.1310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1777,  0.1959,  0.1224, -0.3817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 40 ] state=tensor([[-0.1777,  0.1959,  0.1224, -0.3817]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1737,  0.3891,  0.1147, -0.6335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 41 ] state=tensor([[-0.1737,  0.3891,  0.1147, -0.6335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1660,  0.5824,  0.1021, -0.8879]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 42 ] state=tensor([[-0.1660,  0.5824,  0.1021, -0.8879]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1543,  0.7760,  0.0843, -1.1469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 43 ] state=tensor([[-0.1543,  0.7760,  0.0843, -1.1469]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1388,  0.5799,  0.0614, -0.8290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 44 ] state=tensor([[-0.1388,  0.5799,  0.0614, -0.8290]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1272,  0.7741,  0.0448, -1.1018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 45 ] state=tensor([[-0.1272,  0.7741,  0.0448, -1.1018]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1117,  0.9687,  0.0228, -1.3801]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 46 ] state=tensor([[-0.1117,  0.9687,  0.0228, -1.3801]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0923,  0.7733, -0.0048, -1.0803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 47 ] state=tensor([[-0.0923,  0.7733, -0.0048, -1.0803]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0769,  0.5782, -0.0265, -0.7892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 48 ] state=tensor([[-0.0769,  0.5782, -0.0265, -0.7892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0653,  0.3834, -0.0422, -0.5049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 49 ] state=tensor([[-0.0653,  0.3834, -0.0422, -0.5049]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0576,  0.1889, -0.0523, -0.2259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 50 ] state=tensor([[-0.0576,  0.1889, -0.0523, -0.2259]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0539, -0.0054, -0.0569,  0.0499]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 51 ] state=tensor([[-0.0539, -0.0054, -0.0569,  0.0499]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0540, -0.1997, -0.0559,  0.3241]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 52 ] state=tensor([[-0.0540, -0.1997, -0.0559,  0.3241]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0580, -0.0038, -0.0494,  0.0143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 53 ] state=tensor([[-0.0580, -0.0038, -0.0494,  0.0143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0580,  0.1920, -0.0491, -0.2935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 54 ] state=tensor([[-0.0580,  0.1920, -0.0491, -0.2935]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0542, -0.0024, -0.0550, -0.0167]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 55 ] state=tensor([[-0.0542, -0.0024, -0.0550, -0.0167]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0542,  0.1935, -0.0553, -0.3262]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 56 ] state=tensor([[-0.0542,  0.1935, -0.0553, -0.3262]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0504, -0.0008, -0.0618, -0.0515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 57 ] state=tensor([[-0.0504, -0.0008, -0.0618, -0.0515]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0504,  0.1951, -0.0628, -0.3630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 58 ] state=tensor([[-0.0504,  0.1951, -0.0628, -0.3630]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0465,  0.0010, -0.0701, -0.0908]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 59 ] state=tensor([[-0.0465,  0.0010, -0.0701, -0.0908]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0465,  0.1970, -0.0719, -0.4047]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 60 ] state=tensor([[-0.0465,  0.1970, -0.0719, -0.4047]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0425,  0.0030, -0.0800, -0.1355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 61 ] state=tensor([[-0.0425,  0.0030, -0.0800, -0.1355]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0425, -0.1909, -0.0827,  0.1309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 62 ] state=tensor([[-0.0425, -0.1909, -0.0827,  0.1309]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0463, -0.3847, -0.0801,  0.3963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 63 ] state=tensor([[-0.0463, -0.3847, -0.0801,  0.3963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0540, -0.5786, -0.0722,  0.6627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 64 ] state=tensor([[-0.0540, -0.5786, -0.0722,  0.6627]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0655, -0.3826, -0.0589,  0.3482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 65 ] state=tensor([[-0.0655, -0.3826, -0.0589,  0.3482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0732, -0.1867, -0.0520,  0.0376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 66 ] state=tensor([[-0.0732, -0.1867, -0.0520,  0.0376]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0769,  0.0091, -0.0512, -0.2711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 67 ] state=tensor([[-0.0769,  0.0091, -0.0512, -0.2711]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0767, -0.1852, -0.0566,  0.0050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 68 ] state=tensor([[-0.0767, -0.1852, -0.0566,  0.0050]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0805,  0.0107, -0.0565, -0.3050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 69 ] state=tensor([[-0.0805,  0.0107, -0.0565, -0.3050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0802, -0.1836, -0.0626, -0.0306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 70 ] state=tensor([[-0.0802, -0.1836, -0.0626, -0.0306]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0839, -0.3778, -0.0632,  0.2417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 71 ] state=tensor([[-0.0839, -0.3778, -0.0632,  0.2417]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0915, -0.1818, -0.0584, -0.0703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 72 ] state=tensor([[-0.0915, -0.1818, -0.0584, -0.0703]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0951, -0.3760, -0.0598,  0.2034]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 73 ] state=tensor([[-0.0951, -0.3760, -0.0598,  0.2034]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1026, -0.5703, -0.0557,  0.4766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 74 ] state=tensor([[-0.1026, -0.5703, -0.0557,  0.4766]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1140, -0.3744, -0.0462,  0.1669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 75 ] state=tensor([[-0.1140, -0.3744, -0.0462,  0.1669]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1215, -0.1786, -0.0429, -0.1400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 76 ] state=tensor([[-0.1215, -0.1786, -0.0429, -0.1400]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1251, -0.3731, -0.0457,  0.1389]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 111 ][ timestamp 77 ] state=tensor([[-0.1251, -0.3731, -0.0457,  0.1389]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1326, -0.5676, -0.0429,  0.4168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 78 ] state=tensor([[-0.1326, -0.5676, -0.0429,  0.4168]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1439, -0.7621, -0.0346,  0.6957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 79 ] state=tensor([[-0.1439, -0.7621, -0.0346,  0.6957]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1591, -0.5665, -0.0206,  0.3923]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 80 ] state=tensor([[-0.1591, -0.5665, -0.0206,  0.3923]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1705, -0.7613, -0.0128,  0.6784]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 81 ] state=tensor([[-0.1705, -0.7613, -0.0128,  0.6784]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1857, -0.5660,  0.0008,  0.3817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 82 ] state=tensor([[-0.1857, -0.5660,  0.0008,  0.3817]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1970, -0.7611,  0.0084,  0.6747]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 83 ] state=tensor([[-0.1970, -0.7611,  0.0084,  0.6747]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2122, -0.5661,  0.0219,  0.3846]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 84 ] state=tensor([[-0.2122, -0.5661,  0.0219,  0.3846]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2236, -0.3713,  0.0296,  0.0989]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 85 ] state=tensor([[-0.2236, -0.3713,  0.0296,  0.0989]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2310, -0.5669,  0.0316,  0.4008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 86 ] state=tensor([[-0.2310, -0.5669,  0.0316,  0.4008]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2423, -0.3722,  0.0396,  0.1182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 87 ] state=tensor([[-0.2423, -0.3722,  0.0396,  0.1182]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2498, -0.1777,  0.0419, -0.1617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 88 ] state=tensor([[-0.2498, -0.1777,  0.0419, -0.1617]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2533,  0.0168,  0.0387, -0.4409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 89 ] state=tensor([[-0.2533,  0.0168,  0.0387, -0.4409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2530,  0.2114,  0.0299, -0.7211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 90 ] state=tensor([[-0.2530,  0.2114,  0.0299, -0.7211]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2488,  0.0159,  0.0155, -0.4192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 91 ] state=tensor([[-0.2488,  0.0159,  0.0155, -0.4192]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2484, -0.1795,  0.0071, -0.1216]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 92 ] state=tensor([[-0.2484, -0.1795,  0.0071, -0.1216]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2520,  0.0155,  0.0047, -0.4121]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 93 ] state=tensor([[-0.2520,  0.0155,  0.0047, -0.4121]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2517, -0.1796, -0.0036, -0.1179]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 94 ] state=tensor([[-0.2517, -0.1796, -0.0036, -0.1179]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2553,  0.0155, -0.0059, -0.4117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 95 ] state=tensor([[-0.2553,  0.0155, -0.0059, -0.4117]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2550, -0.1795, -0.0142, -0.1209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 96 ] state=tensor([[-0.2550, -0.1795, -0.0142, -0.1209]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2586, -0.3744, -0.0166,  0.1672]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 97 ] state=tensor([[-0.2586, -0.3744, -0.0166,  0.1672]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2661, -0.1791, -0.0133, -0.1306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 98 ] state=tensor([[-0.2661, -0.1791, -0.0133, -0.1306]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2697, -0.3740, -0.0159,  0.1578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 99 ] state=tensor([[-0.2697, -0.3740, -0.0159,  0.1578]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2771, -0.5689, -0.0127,  0.4455]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 100 ] state=tensor([[-0.2771, -0.5689, -0.0127,  0.4455]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2885, -0.3736, -0.0038,  0.1488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 101 ] state=tensor([[-0.2885, -0.3736, -0.0038,  0.1488]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2960, -0.5687, -0.0008,  0.4403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 102 ] state=tensor([[-0.2960, -0.5687, -0.0008,  0.4403]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3074, -0.3735,  0.0080,  0.1474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 103 ] state=tensor([[-0.3074, -0.3735,  0.0080,  0.1474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3148, -0.5688,  0.0109,  0.4425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 104 ] state=tensor([[-0.3148, -0.5688,  0.0109,  0.4425]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3262, -0.3738,  0.0198,  0.1533]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 105 ] state=tensor([[-0.3262, -0.3738,  0.0198,  0.1533]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3337, -0.1790,  0.0228, -0.1331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 106 ] state=tensor([[-0.3337, -0.1790,  0.0228, -0.1331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3373, -0.3744,  0.0202,  0.1668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 107 ] state=tensor([[-0.3373, -0.3744,  0.0202,  0.1668]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3448, -0.5698,  0.0235,  0.4657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 108 ] state=tensor([[-0.3448, -0.5698,  0.0235,  0.4657]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3562, -0.3750,  0.0328,  0.1806]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 109 ] state=tensor([[-0.3562, -0.3750,  0.0328,  0.1806]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3637, -0.1804,  0.0364, -0.1016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 110 ] state=tensor([[-0.3637, -0.1804,  0.0364, -0.1016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3673,  0.0142,  0.0344, -0.3826]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 111 ] state=tensor([[-0.3673,  0.0142,  0.0344, -0.3826]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3670,  0.2088,  0.0268, -0.6642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 112 ] state=tensor([[-0.3670,  0.2088,  0.0268, -0.6642]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3628,  0.4036,  0.0135, -0.9483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 113 ] state=tensor([[-0.3628,  0.4036,  0.0135, -0.9483]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3547,  0.2083, -0.0055, -0.6514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 114 ] state=tensor([[-0.3547,  0.2083, -0.0055, -0.6514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3506,  0.0132, -0.0185, -0.3605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 115 ] state=tensor([[-0.3506,  0.0132, -0.0185, -0.3605]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3503,  0.2086, -0.0257, -0.6590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 116 ] state=tensor([[-0.3503,  0.2086, -0.0257, -0.6590]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3461,  0.0138, -0.0389, -0.3745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 117 ] state=tensor([[-0.3461,  0.0138, -0.0389, -0.3745]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3459, -0.1807, -0.0464, -0.0943]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 118 ] state=tensor([[-0.3459, -0.1807, -0.0464, -0.0943]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3495, -0.3751, -0.0483,  0.1834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 119 ] state=tensor([[-0.3495, -0.3751, -0.0483,  0.1834]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3570, -0.5695, -0.0446,  0.4604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 120 ] state=tensor([[-0.3570, -0.5695, -0.0446,  0.4604]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3684, -0.3738, -0.0354,  0.1540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 121 ] state=tensor([[-0.3684, -0.3738, -0.0354,  0.1540]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3758, -0.5684, -0.0323,  0.4354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 122 ] state=tensor([[-0.3758, -0.5684, -0.0323,  0.4354]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3872, -0.3729, -0.0236,  0.1327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 123 ] state=tensor([[-0.3872, -0.3729, -0.0236,  0.1327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3947, -0.1774, -0.0210, -0.1674]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 111 ][ timestamp 124 ] state=tensor([[-0.3947, -0.1774, -0.0210, -0.1674]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3982, -0.3722, -0.0243,  0.1186]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 125 ] state=tensor([[-0.3982, -0.3722, -0.0243,  0.1186]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4057, -0.5670, -0.0219,  0.4035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 126 ] state=tensor([[-0.4057, -0.5670, -0.0219,  0.4035]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4170, -0.3716, -0.0139,  0.1040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 127 ] state=tensor([[-0.4170, -0.3716, -0.0139,  0.1040]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4244, -0.5665, -0.0118,  0.3923]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 128 ] state=tensor([[-0.4244, -0.5665, -0.0118,  0.3923]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4358, -0.3712, -0.0039,  0.0959]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 129 ] state=tensor([[-0.4358, -0.3712, -0.0039,  0.0959]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4432, -0.5663, -0.0020,  0.3874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 130 ] state=tensor([[-0.4432, -0.5663, -0.0020,  0.3874]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4545, -0.3711,  0.0057,  0.0940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 131 ] state=tensor([[-0.4545, -0.3711,  0.0057,  0.0940]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4619, -0.5663,  0.0076,  0.3885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 132 ] state=tensor([[-0.4619, -0.5663,  0.0076,  0.3885]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4733, -0.3713,  0.0154,  0.0982]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 133 ] state=tensor([[-0.4733, -0.3713,  0.0154,  0.0982]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4807, -0.1764,  0.0173, -0.1896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 134 ] state=tensor([[-0.4807, -0.1764,  0.0173, -0.1896]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4842, -0.3718,  0.0135,  0.1086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 135 ] state=tensor([[-0.4842, -0.3718,  0.0135,  0.1086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4916, -0.1768,  0.0157, -0.1798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 136 ] state=tensor([[-0.4916, -0.1768,  0.0157, -0.1798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4952, -0.3722,  0.0121,  0.1178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 137 ] state=tensor([[-0.4952, -0.3722,  0.0121,  0.1178]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5026, -0.1772,  0.0145, -0.1711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 138 ] state=tensor([[-0.5026, -0.1772,  0.0145, -0.1711]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5062, -0.3726,  0.0111,  0.1262]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 139 ] state=tensor([[-0.5062, -0.3726,  0.0111,  0.1262]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5136, -0.1776,  0.0136, -0.1630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 140 ] state=tensor([[-0.5136, -0.1776,  0.0136, -0.1630]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5172,  0.0173,  0.0103, -0.4514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 141 ] state=tensor([[-0.5172,  0.0173,  0.0103, -0.4514]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5168,  0.2123,  0.0013, -0.7408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 142 ] state=tensor([[-0.5168,  0.2123,  0.0013, -0.7408]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5126,  0.0172, -0.0135, -0.4477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 143 ] state=tensor([[-0.5126,  0.0172, -0.0135, -0.4477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5122,  0.2125, -0.0225, -0.7446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 144 ] state=tensor([[-0.5122,  0.2125, -0.0225, -0.7446]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5080,  0.0177, -0.0374, -0.4591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 145 ] state=tensor([[-0.5080,  0.0177, -0.0374, -0.4591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5076, -0.1769, -0.0466, -0.1784]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 146 ] state=tensor([[-0.5076, -0.1769, -0.0466, -0.1784]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5112, -0.3713, -0.0501,  0.0992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 147 ] state=tensor([[-0.5112, -0.3713, -0.0501,  0.0992]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5186, -0.5657, -0.0481,  0.3757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 148 ] state=tensor([[-0.5186, -0.5657, -0.0481,  0.3757]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5299, -0.3699, -0.0406,  0.0682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 149 ] state=tensor([[-0.5299, -0.3699, -0.0406,  0.0682]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5373, -0.1743, -0.0393, -0.2370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 150 ] state=tensor([[-0.5373, -0.1743, -0.0393, -0.2370]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5408, -0.3688, -0.0440,  0.0430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 151 ] state=tensor([[-0.5408, -0.3688, -0.0440,  0.0430]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5482, -0.5633, -0.0431,  0.3215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 152 ] state=tensor([[-0.5482, -0.5633, -0.0431,  0.3215]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5594, -0.3675, -0.0367,  0.0156]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 153 ] state=tensor([[-0.5594, -0.3675, -0.0367,  0.0156]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5668, -0.1719, -0.0364, -0.2885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 154 ] state=tensor([[-0.5668, -0.1719, -0.0364, -0.2885]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5702, -0.3665, -0.0422, -0.0075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 155 ] state=tensor([[-0.5702, -0.3665, -0.0422, -0.0075]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5776, -0.1708, -0.0423, -0.3132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 156 ] state=tensor([[-0.5776, -0.1708, -0.0423, -0.3132]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5810, -0.3653, -0.0486, -0.0341]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 157 ] state=tensor([[-0.5810, -0.3653, -0.0486, -0.0341]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5883, -0.5597, -0.0493,  0.2428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 158 ] state=tensor([[-0.5883, -0.5597, -0.0493,  0.2428]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5995, -0.7541, -0.0444,  0.5196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 159 ] state=tensor([[-0.5995, -0.7541, -0.0444,  0.5196]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6145, -0.5584, -0.0340,  0.2133]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 160 ] state=tensor([[-0.6145, -0.5584, -0.0340,  0.2133]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6257, -0.3628, -0.0297, -0.0900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 161 ] state=tensor([[-0.6257, -0.3628, -0.0297, -0.0900]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6330, -0.5574, -0.0315,  0.1932]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 162 ] state=tensor([[-0.6330, -0.5574, -0.0315,  0.1932]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6441, -0.3619, -0.0277, -0.1093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 163 ] state=tensor([[-0.6441, -0.3619, -0.0277, -0.1093]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6514, -0.1664, -0.0299, -0.4106]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 164 ] state=tensor([[-0.6514, -0.1664, -0.0299, -0.4106]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6547, -0.3611, -0.0381, -0.1274]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 165 ] state=tensor([[-0.6547, -0.3611, -0.0381, -0.1274]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6619, -0.1654, -0.0406, -0.4319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 166 ] state=tensor([[-0.6619, -0.1654, -0.0406, -0.4319]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6652, -0.3599, -0.0493, -0.1523]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 167 ] state=tensor([[-0.6652, -0.3599, -0.0493, -0.1523]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6724, -0.5543, -0.0523,  0.1245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 168 ] state=tensor([[-0.6724, -0.5543, -0.0523,  0.1245]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6835, -0.7487, -0.0498,  0.4002]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 169 ] state=tensor([[-0.6835, -0.7487, -0.0498,  0.4002]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6985, -0.5529, -0.0418,  0.0922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 170 ] state=tensor([[-0.6985, -0.5529, -0.0418,  0.0922]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7095, -0.3572, -0.0400, -0.2133]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 171 ] state=tensor([[-0.7095, -0.3572, -0.0400, -0.2133]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7167, -0.5517, -0.0442,  0.0665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 172 ] state=tensor([[-0.7167, -0.5517, -0.0442,  0.0665]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7277, -0.7462, -0.0429,  0.3449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 173 ] state=tensor([[-0.7277, -0.7462, -0.0429,  0.3449]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7426, -0.5505, -0.0360,  0.0390]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 111 ][ timestamp 174 ] state=tensor([[-0.7426, -0.5505, -0.0360,  0.0390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7536, -0.3548, -0.0352, -0.2649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 175 ] state=tensor([[-0.7536, -0.3548, -0.0352, -0.2649]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7607, -0.1592, -0.0405, -0.5684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 176 ] state=tensor([[-0.7607, -0.1592, -0.0405, -0.5684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7639, -0.3538, -0.0519, -0.2888]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 177 ] state=tensor([[-0.7639, -0.3538, -0.0519, -0.2888]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7710, -0.5481, -0.0577, -0.0129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 178 ] state=tensor([[-0.7710, -0.5481, -0.0577, -0.0129]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7820, -0.3522, -0.0579, -0.3232]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 179 ] state=tensor([[-0.7820, -0.3522, -0.0579, -0.3232]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7890, -0.5465, -0.0644, -0.0494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 180 ] state=tensor([[-0.7890, -0.5465, -0.0644, -0.0494]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7999, -0.3505, -0.0654, -0.3616]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 181 ] state=tensor([[-0.7999, -0.3505, -0.0654, -0.3616]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8069, -0.5446, -0.0726, -0.0903]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 182 ] state=tensor([[-0.8069, -0.5446, -0.0726, -0.0903]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8178, -0.3485, -0.0744, -0.4050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 183 ] state=tensor([[-0.8178, -0.3485, -0.0744, -0.4050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8248, -0.5425, -0.0825, -0.1366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 184 ] state=tensor([[-0.8248, -0.5425, -0.0825, -0.1366]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8357, -0.7364, -0.0853,  0.1289]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 185 ] state=tensor([[-0.8357, -0.7364, -0.0853,  0.1289]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8504, -0.9302, -0.0827,  0.3935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 186 ] state=tensor([[-0.8504, -0.9302, -0.0827,  0.3935]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8690, -1.1240, -0.0748,  0.6590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 187 ] state=tensor([[-0.8690, -1.1240, -0.0748,  0.6590]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8915, -0.9280, -0.0616,  0.3438]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 188 ] state=tensor([[-0.8915, -0.9280, -0.0616,  0.3438]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9100, -1.1221, -0.0547,  0.6164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 189 ] state=tensor([[-0.9100, -1.1221, -0.0547,  0.6164]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9325, -1.3165, -0.0424,  0.8914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 190 ] state=tensor([[-0.9325, -1.3165, -0.0424,  0.8914]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9588, -1.5110, -0.0246,  1.1704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 191 ] state=tensor([[-0.9588, -1.5110, -0.0246,  1.1704]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-9.8902e-01, -1.7058e+00, -1.1852e-03,  1.4553e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 192 ] state=tensor([[-9.8902e-01, -1.7058e+00, -1.1852e-03,  1.4553e+00]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0231, -1.9009,  0.0279,  1.7476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 193 ] state=tensor([[-1.0231, -1.9009,  0.0279,  1.7476]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0612, -2.0963,  0.0629,  2.0488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 194 ] state=tensor([[-1.0612, -2.0963,  0.0629,  2.0488]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1031, -2.2920,  0.1038,  2.3603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 195 ] state=tensor([[-1.1031, -2.2920,  0.1038,  2.3603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1489, -2.4879,  0.1511,  2.6830]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 196 ] state=tensor([[-1.1489, -2.4879,  0.1511,  2.6830]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1987, -2.6838,  0.2047,  3.0177]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 111 ][ timestamp 197 ] state=tensor([[-1.1987, -2.6838,  0.2047,  3.0177]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 111: Exploration_rate=0.05. Score=197.\n",
      "[ episode 112 ] state=tensor([[-0.0377,  0.0304,  0.0018,  0.0214]])\n",
      "[ episode 112 ][ timestamp 1 ] state=tensor([[-0.0377,  0.0304,  0.0018,  0.0214]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0371,  0.2255,  0.0022, -0.2707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 2 ] state=tensor([[-0.0371,  0.2255,  0.0022, -0.2707]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0326,  0.4206, -0.0032, -0.5627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 3 ] state=tensor([[-0.0326,  0.4206, -0.0032, -0.5627]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0242,  0.6157, -0.0145, -0.8564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 4 ] state=tensor([[-0.0242,  0.6157, -0.0145, -0.8564]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0119,  0.4208, -0.0316, -0.5683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 5 ] state=tensor([[-0.0119,  0.4208, -0.0316, -0.5683]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0035,  0.2262, -0.0430, -0.2858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 6 ] state=tensor([[-0.0035,  0.2262, -0.0430, -0.2858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0011,  0.0317, -0.0487, -0.0069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 7 ] state=tensor([[ 0.0011,  0.0317, -0.0487, -0.0069]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0017,  0.2275, -0.0488, -0.3146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 8 ] state=tensor([[ 0.0017,  0.2275, -0.0488, -0.3146]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0062,  0.0331, -0.0551, -0.0377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 9 ] state=tensor([[ 0.0062,  0.0331, -0.0551, -0.0377]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0069,  0.2289, -0.0559, -0.3472]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 10 ] state=tensor([[ 0.0069,  0.2289, -0.0559, -0.3472]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0115,  0.0346, -0.0628, -0.0727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 11 ] state=tensor([[ 0.0115,  0.0346, -0.0628, -0.0727]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0122,  0.2306, -0.0643, -0.3845]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 12 ] state=tensor([[ 0.0122,  0.2306, -0.0643, -0.3845]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0168,  0.0365, -0.0719, -0.1127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 13 ] state=tensor([[ 0.0168,  0.0365, -0.0719, -0.1127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0175, -0.1576, -0.0742,  0.1564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 14 ] state=tensor([[ 0.0175, -0.1576, -0.0742,  0.1564]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0144,  0.0385, -0.0711, -0.1587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 15 ] state=tensor([[ 0.0144,  0.0385, -0.0711, -0.1587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0151, -0.1555, -0.0742,  0.1107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 16 ] state=tensor([[ 0.0151, -0.1555, -0.0742,  0.1107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0120, -0.3495, -0.0720,  0.3791]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 17 ] state=tensor([[ 0.0120, -0.3495, -0.0720,  0.3791]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0050, -0.1534, -0.0645,  0.0646]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 18 ] state=tensor([[ 0.0050, -0.1534, -0.0645,  0.0646]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0020,  0.0426, -0.0632, -0.2477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 19 ] state=tensor([[ 0.0020,  0.0426, -0.0632, -0.2477]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0028, -0.1516, -0.0681,  0.0244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 20 ] state=tensor([[ 0.0028, -0.1516, -0.0681,  0.0244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1094e-04, -3.4568e-01, -6.7626e-02,  2.9484e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 21 ] state=tensor([[-2.1094e-04, -3.4568e-01, -6.7626e-02,  2.9484e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0071, -0.5398, -0.0617,  0.5654]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 22 ] state=tensor([[-0.0071, -0.5398, -0.0617,  0.5654]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0179, -0.3438, -0.0504,  0.2540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 23 ] state=tensor([[-0.0179, -0.3438, -0.0504,  0.2540]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0248, -0.5382, -0.0453,  0.5303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 24 ] state=tensor([[-0.0248, -0.5382, -0.0453,  0.5303]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0356, -0.3425, -0.0347,  0.2237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 25 ] state=tensor([[-0.0356, -0.3425, -0.0347,  0.2237]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0424, -0.5371, -0.0303,  0.5052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 26 ] state=tensor([[-0.0424, -0.5371, -0.0303,  0.5052]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0532, -0.3416, -0.0202,  0.2032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 27 ] state=tensor([[-0.0532, -0.3416, -0.0202,  0.2032]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0600, -0.1462, -0.0161, -0.0958]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 28 ] state=tensor([[-0.0600, -0.1462, -0.0161, -0.0958]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0629, -0.3410, -0.0180,  0.1918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 29 ] state=tensor([[-0.0629, -0.3410, -0.0180,  0.1918]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0697, -0.5359, -0.0142,  0.4787]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 30 ] state=tensor([[-0.0697, -0.5359, -0.0142,  0.4787]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0804, -0.3406, -0.0046,  0.1816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 31 ] state=tensor([[-0.0804, -0.3406, -0.0046,  0.1816]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0873, -0.1454, -0.0010, -0.1125]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 112 ][ timestamp 32 ] state=tensor([[-0.0873, -0.1454, -0.0010, -0.1125]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0902, -0.3405, -0.0032,  0.1799]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 33 ] state=tensor([[-0.0902, -0.3405, -0.0032,  0.1799]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0970, -0.1453,  0.0004, -0.1138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 34 ] state=tensor([[-0.0970, -0.1453,  0.0004, -0.1138]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0999, -0.3405, -0.0019,  0.1790]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 35 ] state=tensor([[-0.0999, -0.3405, -0.0019,  0.1790]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1067, -0.5356,  0.0017,  0.4710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 36 ] state=tensor([[-0.1067, -0.5356,  0.0017,  0.4710]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1174, -0.7307,  0.0111,  0.7643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 37 ] state=tensor([[-0.1174, -0.7307,  0.0111,  0.7643]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1320, -0.5357,  0.0264,  0.4751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 38 ] state=tensor([[-0.1320, -0.5357,  0.0264,  0.4751]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1427, -0.7312,  0.0359,  0.7760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 39 ] state=tensor([[-0.1427, -0.7312,  0.0359,  0.7760]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1574, -0.5366,  0.0514,  0.4948]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 40 ] state=tensor([[-0.1574, -0.5366,  0.0514,  0.4948]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1681, -0.7324,  0.0613,  0.8032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 41 ] state=tensor([[-0.1681, -0.7324,  0.0613,  0.8032]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1827, -0.5382,  0.0774,  0.5304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 42 ] state=tensor([[-0.1827, -0.5382,  0.0774,  0.5304]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1935, -0.3442,  0.0880,  0.2631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 43 ] state=tensor([[-0.1935, -0.3442,  0.0880,  0.2631]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2004, -0.1505,  0.0932, -0.0006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 44 ] state=tensor([[-0.2004, -0.1505,  0.0932, -0.0006]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2034,  0.0432,  0.0932, -0.2624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 45 ] state=tensor([[-0.2034,  0.0432,  0.0932, -0.2624]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2025,  0.2369,  0.0880, -0.5243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 46 ] state=tensor([[-0.2025,  0.2369,  0.0880, -0.5243]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1978,  0.4307,  0.0775, -0.7880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 47 ] state=tensor([[-0.1978,  0.4307,  0.0775, -0.7880]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1892,  0.6246,  0.0617, -1.0554]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 48 ] state=tensor([[-0.1892,  0.6246,  0.0617, -1.0554]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1767,  0.8189,  0.0406, -1.3281]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 49 ] state=tensor([[-0.1767,  0.8189,  0.0406, -1.3281]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1603,  1.0135,  0.0141, -1.6078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 50 ] state=tensor([[-0.1603,  1.0135,  0.0141, -1.6078]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1400,  0.8182, -0.0181, -1.3107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 51 ] state=tensor([[-0.1400,  0.8182, -0.0181, -1.3107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1237,  0.6233, -0.0443, -1.0238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 52 ] state=tensor([[-0.1237,  0.6233, -0.0443, -1.0238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1112,  0.4288, -0.0648, -0.7453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 53 ] state=tensor([[-0.1112,  0.4288, -0.0648, -0.7453]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1026,  0.2346, -0.0797, -0.4737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 54 ] state=tensor([[-0.1026,  0.2346, -0.0797, -0.4737]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0979,  0.0407, -0.0892, -0.2071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 55 ] state=tensor([[-0.0979,  0.0407, -0.0892, -0.2071]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0971, -0.1530, -0.0933,  0.0561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 56 ] state=tensor([[-0.0971, -0.1530, -0.0933,  0.0561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1002, -0.3467, -0.0922,  0.3180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 57 ] state=tensor([[-0.1002, -0.3467, -0.0922,  0.3180]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1071, -0.1504, -0.0858, -0.0023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 58 ] state=tensor([[-0.1071, -0.1504, -0.0858, -0.0023]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1101, -0.3442, -0.0859,  0.2621]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 59 ] state=tensor([[-0.1101, -0.3442, -0.0859,  0.2621]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1170, -0.1480, -0.0806, -0.0563]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 60 ] state=tensor([[-0.1170, -0.1480, -0.0806, -0.0563]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1200, -0.3418, -0.0817,  0.2099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 61 ] state=tensor([[-0.1200, -0.3418, -0.0817,  0.2099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1268, -0.1456, -0.0776, -0.1075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 62 ] state=tensor([[-0.1268, -0.1456, -0.0776, -0.1075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1297, -0.3396, -0.0797,  0.1598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 63 ] state=tensor([[-0.1297, -0.3396, -0.0797,  0.1598]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1365, -0.5335, -0.0765,  0.4263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 64 ] state=tensor([[-0.1365, -0.5335, -0.0765,  0.4263]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1472, -0.3373, -0.0680,  0.1105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 65 ] state=tensor([[-0.1472, -0.3373, -0.0680,  0.1105]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1539, -0.5314, -0.0658,  0.3810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 66 ] state=tensor([[-0.1539, -0.5314, -0.0658,  0.3810]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1646, -0.3354, -0.0581,  0.0683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 67 ] state=tensor([[-0.1646, -0.3354, -0.0581,  0.0683]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1713, -0.5297, -0.0568,  0.3421]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 68 ] state=tensor([[-0.1713, -0.5297, -0.0568,  0.3421]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1819, -0.7240, -0.0499,  0.6164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 69 ] state=tensor([[-0.1819, -0.7240, -0.0499,  0.6164]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1963, -0.5282, -0.0376,  0.3084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 70 ] state=tensor([[-0.1963, -0.5282, -0.0376,  0.3084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2069, -0.3325, -0.0314,  0.0041]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 71 ] state=tensor([[-0.2069, -0.3325, -0.0314,  0.0041]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2136, -0.5272, -0.0314,  0.2867]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 72 ] state=tensor([[-0.2136, -0.5272, -0.0314,  0.2867]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2241, -0.7219, -0.0256,  0.5693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 73 ] state=tensor([[-0.2241, -0.7219, -0.0256,  0.5693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2385, -0.9166, -0.0142,  0.8538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 74 ] state=tensor([[-0.2385, -0.9166, -0.0142,  0.8538]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2569, -0.7213,  0.0028,  0.5567]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 75 ] state=tensor([[-0.2569, -0.7213,  0.0028,  0.5567]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2713, -0.9165,  0.0140,  0.8502]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 76 ] state=tensor([[-0.2713, -0.9165,  0.0140,  0.8502]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2896, -0.7215,  0.0310,  0.5620]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 77 ] state=tensor([[-0.2896, -0.7215,  0.0310,  0.5620]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3041, -0.9171,  0.0422,  0.8643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 78 ] state=tensor([[-0.3041, -0.9171,  0.0422,  0.8643]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3224, -0.7225,  0.0595,  0.5852]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 79 ] state=tensor([[-0.3224, -0.7225,  0.0595,  0.5852]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3368, -0.5283,  0.0712,  0.3118]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 80 ] state=tensor([[-0.3368, -0.5283,  0.0712,  0.3118]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3474, -0.7244,  0.0774,  0.6260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 81 ] state=tensor([[-0.3474, -0.7244,  0.0774,  0.6260]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3619, -0.5304,  0.0900,  0.3587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 82 ] state=tensor([[-0.3619, -0.5304,  0.0900,  0.3587]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3725, -0.3367,  0.0971,  0.0957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 83 ] state=tensor([[-0.3725, -0.3367,  0.0971,  0.0957]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3792, -0.1431,  0.0990, -0.1648]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 84 ] state=tensor([[-0.3792, -0.1431,  0.0990, -0.1648]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3821,  0.0505,  0.0957, -0.4247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 85 ] state=tensor([[-0.3821,  0.0505,  0.0957, -0.4247]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3811,  0.2442,  0.0873, -0.6857]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 86 ] state=tensor([[-0.3811,  0.2442,  0.0873, -0.6857]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3762,  0.4380,  0.0735, -0.9497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 87 ] state=tensor([[-0.3762,  0.4380,  0.0735, -0.9497]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3674,  0.6320,  0.0545, -1.2184]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 88 ] state=tensor([[-0.3674,  0.6320,  0.0545, -1.2184]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3548,  0.4362,  0.0302, -0.9091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 89 ] state=tensor([[-0.3548,  0.4362,  0.0302, -0.9091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3461,  0.2407,  0.0120, -0.6071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 90 ] state=tensor([[-0.3461,  0.2407,  0.0120, -0.6071]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.4127e-01,  4.3568e-01, -1.4739e-04, -8.9601e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 91 ] state=tensor([[-3.4127e-01,  4.3568e-01, -1.4739e-04, -8.9601e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3326,  0.2406, -0.0181, -0.6034]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 92 ] state=tensor([[-0.3326,  0.2406, -0.0181, -0.6034]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3277,  0.0457, -0.0301, -0.3164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 93 ] state=tensor([[-0.3277,  0.0457, -0.0301, -0.3164]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3268, -0.1490, -0.0365, -0.0334]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 94 ] state=tensor([[-0.3268, -0.1490, -0.0365, -0.0334]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3298,  0.0466, -0.0371, -0.3374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 95 ] state=tensor([[-0.3298,  0.0466, -0.0371, -0.3374]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3289, -0.1479, -0.0439, -0.0566]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 112 ][ timestamp 96 ] state=tensor([[-0.3289, -0.1479, -0.0439, -0.0566]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3318,  0.0478, -0.0450, -0.3628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 97 ] state=tensor([[-0.3318,  0.0478, -0.0450, -0.3628]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3309, -0.1467, -0.0523, -0.0847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 98 ] state=tensor([[-0.3309, -0.1467, -0.0523, -0.0847]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3338, -0.3410, -0.0540,  0.1911]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 99 ] state=tensor([[-0.3338, -0.3410, -0.0540,  0.1911]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3406, -0.1452, -0.0501, -0.1181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 100 ] state=tensor([[-0.3406, -0.1452, -0.0501, -0.1181]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3435, -0.3395, -0.0525,  0.1583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 101 ] state=tensor([[-0.3435, -0.3395, -0.0525,  0.1583]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3503, -0.5339, -0.0493,  0.4340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 102 ] state=tensor([[-0.3503, -0.5339, -0.0493,  0.4340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3610, -0.7282, -0.0407,  0.7107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 103 ] state=tensor([[-0.3610, -0.7282, -0.0407,  0.7107]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3756, -0.5326, -0.0264,  0.4055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 104 ] state=tensor([[-0.3756, -0.5326, -0.0264,  0.4055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3862, -0.3371, -0.0183,  0.1046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 105 ] state=tensor([[-0.3862, -0.3371, -0.0183,  0.1046]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3930, -0.5320, -0.0162,  0.3915]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 106 ] state=tensor([[-0.3930, -0.5320, -0.0162,  0.3915]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4036, -0.3366, -0.0084,  0.0937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 107 ] state=tensor([[-0.4036, -0.3366, -0.0084,  0.0937]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4103, -0.1414, -0.0065, -0.2016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 108 ] state=tensor([[-0.4103, -0.1414, -0.0065, -0.2016]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4132, -0.3364, -0.0106,  0.0890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 109 ] state=tensor([[-0.4132, -0.3364, -0.0106,  0.0890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4199, -0.1411, -0.0088, -0.2070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 110 ] state=tensor([[-0.4199, -0.1411, -0.0088, -0.2070]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4227, -0.3361, -0.0129,  0.0829]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 111 ] state=tensor([[-0.4227, -0.3361, -0.0129,  0.0829]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4294, -0.5310, -0.0113,  0.3715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 112 ] state=tensor([[-0.4294, -0.5310, -0.0113,  0.3715]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4401, -0.3358, -0.0038,  0.0753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 113 ] state=tensor([[-0.4401, -0.3358, -0.0038,  0.0753]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4468, -0.1406, -0.0023, -0.2186]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 114 ] state=tensor([[-0.4468, -0.1406, -0.0023, -0.2186]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4496,  0.0546, -0.0067, -0.5120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 115 ] state=tensor([[-0.4496,  0.0546, -0.0067, -0.5120]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4485, -0.1405, -0.0169, -0.2215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 116 ] state=tensor([[-0.4485, -0.1405, -0.0169, -0.2215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4513, -0.3353, -0.0214,  0.0658]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 117 ] state=tensor([[-0.4513, -0.3353, -0.0214,  0.0658]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4580, -0.1399, -0.0201, -0.2335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 118 ] state=tensor([[-0.4580, -0.1399, -0.0201, -0.2335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4608,  0.0555, -0.0247, -0.5325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 119 ] state=tensor([[-0.4608,  0.0555, -0.0247, -0.5325]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4597, -0.1393, -0.0354, -0.2477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 120 ] state=tensor([[-0.4597, -0.1393, -0.0354, -0.2477]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4625, -0.3339, -0.0403,  0.0336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 121 ] state=tensor([[-0.4625, -0.3339, -0.0403,  0.0336]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4692, -0.5284, -0.0397,  0.3133]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 122 ] state=tensor([[-0.4692, -0.5284, -0.0397,  0.3133]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4797, -0.7229, -0.0334,  0.5932]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 123 ] state=tensor([[-0.4797, -0.7229, -0.0334,  0.5932]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4942, -0.5274, -0.0215,  0.2902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 124 ] state=tensor([[-0.4942, -0.5274, -0.0215,  0.2902]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5047, -0.3319, -0.0157, -0.0092]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 125 ] state=tensor([[-0.5047, -0.3319, -0.0157, -0.0092]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5114, -0.1366, -0.0159, -0.3068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 126 ] state=tensor([[-0.5114, -0.1366, -0.0159, -0.3068]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5141,  0.0588, -0.0220, -0.6044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 127 ] state=tensor([[-0.5141,  0.0588, -0.0220, -0.6044]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5129, -0.1361, -0.0341, -0.3188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 128 ] state=tensor([[-0.5129, -0.1361, -0.0341, -0.3188]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5156, -0.3307, -0.0405, -0.0371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 129 ] state=tensor([[-0.5156, -0.3307, -0.0405, -0.0371]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5223, -0.5252, -0.0413,  0.2426]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 130 ] state=tensor([[-0.5223, -0.5252, -0.0413,  0.2426]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5328, -0.3295, -0.0364, -0.0628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 131 ] state=tensor([[-0.5328, -0.3295, -0.0364, -0.0628]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5394, -0.5241, -0.0377,  0.2182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 132 ] state=tensor([[-0.5394, -0.5241, -0.0377,  0.2182]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5498, -0.3284, -0.0333, -0.0862]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 133 ] state=tensor([[-0.5498, -0.3284, -0.0333, -0.0862]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5564, -0.5231, -0.0350,  0.1958]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 134 ] state=tensor([[-0.5564, -0.5231, -0.0350,  0.1958]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5669, -0.3275, -0.0311, -0.1077]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 135 ] state=tensor([[-0.5669, -0.3275, -0.0311, -0.1077]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5734, -0.5221, -0.0333,  0.1750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 136 ] state=tensor([[-0.5734, -0.5221, -0.0333,  0.1750]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5839, -0.3266, -0.0298, -0.1280]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 137 ] state=tensor([[-0.5839, -0.3266, -0.0298, -0.1280]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5904, -0.5212, -0.0323,  0.1552]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 138 ] state=tensor([[-0.5904, -0.5212, -0.0323,  0.1552]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6008, -0.7159, -0.0292,  0.4375]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 112 ][ timestamp 139 ] state=tensor([[-0.6008, -0.7159, -0.0292,  0.4375]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6151, -0.5204, -0.0205,  0.1358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 140 ] state=tensor([[-0.6151, -0.5204, -0.0205,  0.1358]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6255, -0.3250, -0.0177, -0.1633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 141 ] state=tensor([[-0.6255, -0.3250, -0.0177, -0.1633]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6320, -0.5198, -0.0210,  0.1237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 142 ] state=tensor([[-0.6320, -0.5198, -0.0210,  0.1237]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6424, -0.3244, -0.0185, -0.1755]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 143 ] state=tensor([[-0.6424, -0.3244, -0.0185, -0.1755]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6489, -0.5193, -0.0220,  0.1113]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 144 ] state=tensor([[-0.6489, -0.5193, -0.0220,  0.1113]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6593, -0.3238, -0.0198, -0.1883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 145 ] state=tensor([[-0.6593, -0.3238, -0.0198, -0.1883]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6658, -0.1284, -0.0236, -0.4872]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 146 ] state=tensor([[-0.6658, -0.1284, -0.0236, -0.4872]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6684, -0.3232, -0.0333, -0.2020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 147 ] state=tensor([[-0.6684, -0.3232, -0.0333, -0.2020]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6748, -0.5178, -0.0374,  0.0800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 148 ] state=tensor([[-0.6748, -0.5178, -0.0374,  0.0800]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6852, -0.3222, -0.0358, -0.2243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 149 ] state=tensor([[-0.6852, -0.3222, -0.0358, -0.2243]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6916, -0.5168, -0.0403,  0.0569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 150 ] state=tensor([[-0.6916, -0.5168, -0.0403,  0.0569]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7020, -0.3211, -0.0391, -0.2482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 151 ] state=tensor([[-0.7020, -0.3211, -0.0391, -0.2482]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7084, -0.5157, -0.0441,  0.0319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 152 ] state=tensor([[-0.7084, -0.5157, -0.0441,  0.0319]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7187, -0.7101, -0.0434,  0.3104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 153 ] state=tensor([[-0.7187, -0.7101, -0.0434,  0.3104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7329, -0.5144, -0.0372,  0.0043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 154 ] state=tensor([[-0.7329, -0.5144, -0.0372,  0.0043]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7432, -0.7090, -0.0371,  0.2850]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 155 ] state=tensor([[-0.7432, -0.7090, -0.0371,  0.2850]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7574, -0.5133, -0.0314, -0.0192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 156 ] state=tensor([[-0.7574, -0.5133, -0.0314, -0.0192]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7676, -0.7080, -0.0318,  0.2634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 157 ] state=tensor([[-0.7676, -0.7080, -0.0318,  0.2634]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7818, -0.5124, -0.0266, -0.0391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 158 ] state=tensor([[-0.7818, -0.5124, -0.0266, -0.0391]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7920, -0.7072, -0.0273,  0.2451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 159 ] state=tensor([[-0.7920, -0.7072, -0.0273,  0.2451]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8062, -0.5117, -0.0224, -0.0561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 160 ] state=tensor([[-0.8062, -0.5117, -0.0224, -0.0561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8164, -0.3162, -0.0236, -0.3558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 161 ] state=tensor([[-0.8164, -0.3162, -0.0236, -0.3558]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8227, -0.5110, -0.0307, -0.0706]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 162 ] state=tensor([[-0.8227, -0.5110, -0.0307, -0.0706]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8330, -0.7057, -0.0321,  0.2122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 163 ] state=tensor([[-0.8330, -0.7057, -0.0321,  0.2122]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8471, -0.5101, -0.0278, -0.0904]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 164 ] state=tensor([[-0.8471, -0.5101, -0.0278, -0.0904]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8573, -0.3146, -0.0297, -0.3917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 165 ] state=tensor([[-0.8573, -0.3146, -0.0297, -0.3917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8636, -0.5093, -0.0375, -0.1086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 166 ] state=tensor([[-0.8636, -0.5093, -0.0375, -0.1086]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8737, -0.7039, -0.0397,  0.1721]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 167 ] state=tensor([[-0.8737, -0.7039, -0.0397,  0.1721]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8878, -0.5082, -0.0362, -0.1329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 168 ] state=tensor([[-0.8878, -0.5082, -0.0362, -0.1329]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8980, -0.3126, -0.0389, -0.4367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 169 ] state=tensor([[-0.8980, -0.3126, -0.0389, -0.4367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9042, -0.5071, -0.0476, -0.1566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 170 ] state=tensor([[-0.9042, -0.5071, -0.0476, -0.1566]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9144, -0.7015, -0.0507,  0.1207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 171 ] state=tensor([[-0.9144, -0.7015, -0.0507,  0.1207]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9284, -0.8959, -0.0483,  0.3970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 172 ] state=tensor([[-0.9284, -0.8959, -0.0483,  0.3970]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9463, -0.7001, -0.0404,  0.0895]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 173 ] state=tensor([[-0.9463, -0.7001, -0.0404,  0.0895]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9603, -0.5044, -0.0386, -0.2157]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 174 ] state=tensor([[-0.9603, -0.5044, -0.0386, -0.2157]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9704, -0.6990, -0.0429,  0.0646]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 175 ] state=tensor([[-0.9704, -0.6990, -0.0429,  0.0646]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9844, -0.5033, -0.0416, -0.2413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 176 ] state=tensor([[-0.9844, -0.5033, -0.0416, -0.2413]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9945, -0.6978, -0.0465,  0.0379]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 177 ] state=tensor([[-0.9945, -0.6978, -0.0465,  0.0379]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0084, -0.5020, -0.0457, -0.2690]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 178 ] state=tensor([[-1.0084, -0.5020, -0.0457, -0.2690]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0185, -0.6965, -0.0511,  0.0089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 179 ] state=tensor([[-1.0185, -0.6965, -0.0511,  0.0089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0324, -0.8908, -0.0509,  0.2850]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 180 ] state=tensor([[-1.0324, -0.8908, -0.0509,  0.2850]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0502, -1.0852, -0.0452,  0.5612]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 181 ] state=tensor([[-1.0502, -1.0852, -0.0452,  0.5612]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0719, -1.2796, -0.0340,  0.8393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 182 ] state=tensor([[-1.0719, -1.2796, -0.0340,  0.8393]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0975, -1.4743, -0.0172,  1.1211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 183 ] state=tensor([[-1.0975, -1.4743, -0.0172,  1.1211]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1270, -1.6692,  0.0052,  1.4084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 184 ] state=tensor([[-1.1270, -1.6692,  0.0052,  1.4084]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1604, -1.8644,  0.0334,  1.7027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 185 ] state=tensor([[-1.1604, -1.8644,  0.0334,  1.7027]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1977, -2.0598,  0.0675,  2.0056]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 186 ] state=tensor([[-1.1977, -2.0598,  0.0675,  2.0056]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2389, -2.2556,  0.1076,  2.3184]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 187 ] state=tensor([[-1.2389, -2.2556,  0.1076,  2.3184]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2840, -2.0616,  0.1539,  2.0606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 188 ] state=tensor([[-1.2840, -2.0616,  0.1539,  2.0606]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3252, -2.2579,  0.1952,  2.3967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 112 ][ timestamp 189 ] state=tensor([[-1.3252, -2.2579,  0.1952,  2.3967]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 112: Exploration_rate=0.05. Score=189.\n",
      "[ episode 113 ] state=tensor([[ 0.0014, -0.0167, -0.0137, -0.0106]])\n",
      "[ episode 113 ][ timestamp 1 ] state=tensor([[ 0.0014, -0.0167, -0.0137, -0.0106]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0011,  0.1786, -0.0139, -0.3076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 2 ] state=tensor([[ 0.0011,  0.1786, -0.0139, -0.3076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0047, -0.0163, -0.0201, -0.0193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 3 ] state=tensor([[ 0.0047, -0.0163, -0.0201, -0.0193]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0044,  0.1791, -0.0204, -0.3183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 4 ] state=tensor([[ 0.0044,  0.1791, -0.0204, -0.3183]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0079, -0.0157, -0.0268, -0.0321]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 5 ] state=tensor([[ 0.0079, -0.0157, -0.0268, -0.0321]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0076,  0.1798, -0.0274, -0.3331]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 113 ][ timestamp 6 ] state=tensor([[ 0.0076,  0.1798, -0.0274, -0.3331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0112, -0.0149, -0.0341, -0.0492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 7 ] state=tensor([[ 0.0112, -0.0149, -0.0341, -0.0492]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0109,  0.1807, -0.0351, -0.3525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 8 ] state=tensor([[ 0.0109,  0.1807, -0.0351, -0.3525]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0145, -0.0139, -0.0421, -0.0711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 9 ] state=tensor([[ 0.0145, -0.0139, -0.0421, -0.0711]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0143, -0.2084, -0.0436,  0.2080]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 10 ] state=tensor([[ 0.0143, -0.2084, -0.0436,  0.2080]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0101, -0.0127, -0.0394, -0.0981]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 11 ] state=tensor([[ 0.0101, -0.0127, -0.0394, -0.0981]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0098,  0.1829, -0.0414, -0.4029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 12 ] state=tensor([[ 0.0098,  0.1829, -0.0414, -0.4029]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0135, -0.0116, -0.0494, -0.1236]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 13 ] state=tensor([[ 0.0135, -0.0116, -0.0494, -0.1236]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0133, -0.2060, -0.0519,  0.1531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 14 ] state=tensor([[ 0.0133, -0.2060, -0.0519,  0.1531]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0091, -0.4003, -0.0488,  0.4290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 15 ] state=tensor([[ 0.0091, -0.4003, -0.0488,  0.4290]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0011, -0.2045, -0.0402,  0.1213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 16 ] state=tensor([[ 0.0011, -0.2045, -0.0402,  0.1213]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0030, -0.0088, -0.0378, -0.1838]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 17 ] state=tensor([[-0.0030, -0.0088, -0.0378, -0.1838]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0031, -0.2034, -0.0415,  0.0968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 18 ] state=tensor([[-0.0031, -0.2034, -0.0415,  0.0968]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0072, -0.3979, -0.0396,  0.3761]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 19 ] state=tensor([[-0.0072, -0.3979, -0.0396,  0.3761]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0152, -0.2022, -0.0320,  0.0712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 20 ] state=tensor([[-0.0152, -0.2022, -0.0320,  0.0712]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0192, -0.0067, -0.0306, -0.2315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 21 ] state=tensor([[-0.0192, -0.0067, -0.0306, -0.2315]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0193,  0.1889, -0.0352, -0.5336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 22 ] state=tensor([[-0.0193,  0.1889, -0.0352, -0.5336]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0156, -0.0057, -0.0459, -0.2523]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 23 ] state=tensor([[-0.0156, -0.0057, -0.0459, -0.2523]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0157, -0.2002, -0.0510,  0.0256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 24 ] state=tensor([[-0.0157, -0.2002, -0.0510,  0.0256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0197, -0.0044, -0.0505, -0.2827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 25 ] state=tensor([[-0.0197, -0.0044, -0.0505, -0.2827]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0198, -0.1987, -0.0561, -0.0064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 26 ] state=tensor([[-0.0198, -0.1987, -0.0561, -0.0064]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0237, -0.3930, -0.0562,  0.2681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 27 ] state=tensor([[-0.0237, -0.3930, -0.0562,  0.2681]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0316, -0.5873, -0.0509,  0.5425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 28 ] state=tensor([[-0.0316, -0.5873, -0.0509,  0.5425]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0434, -0.3915, -0.0400,  0.2343]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 29 ] state=tensor([[-0.0434, -0.3915, -0.0400,  0.2343]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0512, -0.1958, -0.0353, -0.0708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 30 ] state=tensor([[-0.0512, -0.1958, -0.0353, -0.0708]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0551, -0.3904, -0.0368,  0.2105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 31 ] state=tensor([[-0.0551, -0.3904, -0.0368,  0.2105]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0629, -0.1948, -0.0325, -0.0935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 32 ] state=tensor([[-0.0629, -0.1948, -0.0325, -0.0935]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0668, -0.3894, -0.0344,  0.1887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 33 ] state=tensor([[-0.0668, -0.3894, -0.0344,  0.1887]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0746, -0.5840, -0.0306,  0.4704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 34 ] state=tensor([[-0.0746, -0.5840, -0.0306,  0.4704]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0863, -0.3885, -0.0212,  0.1682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 35 ] state=tensor([[-0.0863, -0.3885, -0.0212,  0.1682]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0940, -0.1931, -0.0179, -0.1311]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 36 ] state=tensor([[-0.0940, -0.1931, -0.0179, -0.1311]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0979, -0.3879, -0.0205,  0.1559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 37 ] state=tensor([[-0.0979, -0.3879, -0.0205,  0.1559]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1057, -0.5828, -0.0174,  0.4420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 38 ] state=tensor([[-0.1057, -0.5828, -0.0174,  0.4420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1173, -0.3874, -0.0085,  0.1439]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 39 ] state=tensor([[-0.1173, -0.3874, -0.0085,  0.1439]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1251, -0.5824, -0.0057,  0.4339]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 40 ] state=tensor([[-0.1251, -0.5824, -0.0057,  0.4339]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1367, -0.3872,  0.0030,  0.1394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 41 ] state=tensor([[-0.1367, -0.3872,  0.0030,  0.1394]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1445, -0.5824,  0.0058,  0.4331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 42 ] state=tensor([[-0.1445, -0.5824,  0.0058,  0.4331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1561, -0.7776,  0.0145,  0.7276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 43 ] state=tensor([[-0.1561, -0.7776,  0.0145,  0.7276]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1717, -0.5826,  0.0290,  0.4395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 44 ] state=tensor([[-0.1717, -0.5826,  0.0290,  0.4395]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1833, -0.3879,  0.0378,  0.1561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 45 ] state=tensor([[-0.1833, -0.3879,  0.0378,  0.1561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1911, -0.1934,  0.0409, -0.1244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 46 ] state=tensor([[-0.1911, -0.1934,  0.0409, -0.1244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1949,  0.0011,  0.0385, -0.4039]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 47 ] state=tensor([[-0.1949,  0.0011,  0.0385, -0.4039]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1949, -0.1945,  0.0304, -0.0994]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 48 ] state=tensor([[-0.1949, -0.1945,  0.0304, -0.0994]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9880e-01,  1.5515e-04,  2.8385e-02, -3.8230e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 49 ] state=tensor([[-1.9880e-01,  1.5515e-04,  2.8385e-02, -3.8230e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1988,  0.1949,  0.0207, -0.6659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 50 ] state=tensor([[-0.1988,  0.1949,  0.0207, -0.6659]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1949, -0.0005,  0.0074, -0.3668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 51 ] state=tensor([[-0.1949, -0.0005,  0.0074, -0.3668]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9491e-01,  1.9447e-01,  8.5725e-05, -6.5710e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 52 ] state=tensor([[-1.9491e-01,  1.9447e-01,  8.5725e-05, -6.5710e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1910, -0.0006, -0.0131, -0.3644]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 53 ] state=tensor([[-0.1910, -0.0006, -0.0131, -0.3644]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1910, -0.1956, -0.0203, -0.0758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 54 ] state=tensor([[-0.1910, -0.1956, -0.0203, -0.0758]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9495e-01, -1.7518e-04, -2.1861e-02, -3.7488e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 55 ] state=tensor([[-1.9495e-01, -1.7518e-04, -2.1861e-02, -3.7488e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1950, -0.1950, -0.0294, -0.0892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 56 ] state=tensor([[-0.1950, -0.1950, -0.0294, -0.0892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1989, -0.3897, -0.0311,  0.1941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 57 ] state=tensor([[-0.1989, -0.3897, -0.0311,  0.1941]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2066, -0.1941, -0.0273, -0.1082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 58 ] state=tensor([[-0.2066, -0.1941, -0.0273, -0.1082]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2105, -0.3888, -0.0294,  0.1757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 59 ] state=tensor([[-0.2105, -0.3888, -0.0294,  0.1757]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2183, -0.1933, -0.0259, -0.1261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 60 ] state=tensor([[-0.2183, -0.1933, -0.0259, -0.1261]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2222, -0.3880, -0.0284,  0.1583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 61 ] state=tensor([[-0.2222, -0.3880, -0.0284,  0.1583]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2299, -0.1925, -0.0253, -0.1432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 62 ] state=tensor([[-0.2299, -0.1925, -0.0253, -0.1432]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2338, -0.3873, -0.0281,  0.1414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 63 ] state=tensor([[-0.2338, -0.3873, -0.0281,  0.1414]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2415, -0.5820, -0.0253,  0.4251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 64 ] state=tensor([[-0.2415, -0.5820, -0.0253,  0.4251]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2532, -0.3865, -0.0168,  0.1245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 65 ] state=tensor([[-0.2532, -0.3865, -0.0168,  0.1245]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2609, -0.1912, -0.0143, -0.1734]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 66 ] state=tensor([[-0.2609, -0.1912, -0.0143, -0.1734]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2647, -0.3861, -0.0178,  0.1147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 67 ] state=tensor([[-0.2647, -0.3861, -0.0178,  0.1147]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2724, -0.1907, -0.0155, -0.1835]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 68 ] state=tensor([[-0.2724, -0.1907, -0.0155, -0.1835]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2763, -0.3856, -0.0192,  0.1042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 69 ] state=tensor([[-0.2763, -0.3856, -0.0192,  0.1042]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2840, -0.1902, -0.0171, -0.1944]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 70 ] state=tensor([[-0.2840, -0.1902, -0.0171, -0.1944]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2878, -0.3851, -0.0210,  0.0928]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 71 ] state=tensor([[-0.2878, -0.3851, -0.0210,  0.0928]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2955, -0.1897, -0.0191, -0.2064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 72 ] state=tensor([[-0.2955, -0.1897, -0.0191, -0.2064]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2993, -0.3845, -0.0232,  0.0802]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 113 ][ timestamp 73 ] state=tensor([[-0.2993, -0.3845, -0.0232,  0.0802]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3070, -0.5793, -0.0216,  0.3655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 74 ] state=tensor([[-0.3070, -0.5793, -0.0216,  0.3655]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3185, -0.3839, -0.0143,  0.0660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 75 ] state=tensor([[-0.3185, -0.3839, -0.0143,  0.0660]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3262, -0.1885, -0.0130, -0.2311]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 76 ] state=tensor([[-0.3262, -0.1885, -0.0130, -0.2311]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3300,  0.0068, -0.0176, -0.5279]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 77 ] state=tensor([[-0.3300,  0.0068, -0.0176, -0.5279]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3299, -0.1881, -0.0282, -0.2408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 78 ] state=tensor([[-0.3299, -0.1881, -0.0282, -0.2408]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3336, -0.3828, -0.0330,  0.0429]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 79 ] state=tensor([[-0.3336, -0.3828, -0.0330,  0.0429]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3413, -0.5775, -0.0321,  0.3250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 80 ] state=tensor([[-0.3413, -0.5775, -0.0321,  0.3250]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3528, -0.3819, -0.0256,  0.0223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 81 ] state=tensor([[-0.3528, -0.3819, -0.0256,  0.0223]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3605, -0.5766, -0.0252,  0.3068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 82 ] state=tensor([[-0.3605, -0.5766, -0.0252,  0.3068]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3720, -0.3812, -0.0191,  0.0063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 83 ] state=tensor([[-0.3720, -0.3812, -0.0191,  0.0063]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3796, -0.1858, -0.0189, -0.2923]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 84 ] state=tensor([[-0.3796, -0.1858, -0.0189, -0.2923]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3833, -0.3806, -0.0248, -0.0057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 85 ] state=tensor([[-0.3833, -0.3806, -0.0248, -0.0057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3909, -0.1851, -0.0249, -0.3061]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 86 ] state=tensor([[-0.3909, -0.1851, -0.0249, -0.3061]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3946, -0.3799, -0.0310, -0.0214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 87 ] state=tensor([[-0.3946, -0.3799, -0.0310, -0.0214]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4022, -0.5746, -0.0314,  0.2614]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 88 ] state=tensor([[-0.4022, -0.5746, -0.0314,  0.2614]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4137, -0.7692, -0.0262,  0.5440]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 89 ] state=tensor([[-0.4137, -0.7692, -0.0262,  0.5440]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4291, -0.5738, -0.0153,  0.2432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 90 ] state=tensor([[-0.4291, -0.5738, -0.0153,  0.2432]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4406, -0.7686, -0.0105,  0.5310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 91 ] state=tensor([[-0.4406, -0.7686, -0.0105,  0.5310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.5597e-01, -5.7338e-01,  1.5576e-04,  2.3502e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 92 ] state=tensor([[-4.5597e-01, -5.7338e-01,  1.5576e-04,  2.3502e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4674, -0.3783,  0.0049, -0.0576]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 93 ] state=tensor([[-0.4674, -0.3783,  0.0049, -0.0576]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4750, -0.5735,  0.0037,  0.2366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 94 ] state=tensor([[-0.4750, -0.5735,  0.0037,  0.2366]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4865, -0.3784,  0.0084, -0.0549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 95 ] state=tensor([[-0.4865, -0.3784,  0.0084, -0.0549]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4940, -0.5736,  0.0073,  0.2404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 96 ] state=tensor([[-0.4940, -0.5736,  0.0073,  0.2404]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5055, -0.3786,  0.0121, -0.0499]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 97 ] state=tensor([[-0.5055, -0.3786,  0.0121, -0.0499]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5131, -0.5739,  0.0111,  0.2465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 98 ] state=tensor([[-0.5131, -0.5739,  0.0111,  0.2465]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5246, -0.3789,  0.0161, -0.0426]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 99 ] state=tensor([[-0.5246, -0.3789,  0.0161, -0.0426]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5321, -0.1841,  0.0152, -0.3302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 100 ] state=tensor([[-0.5321, -0.1841,  0.0152, -0.3302]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5358,  0.0108,  0.0086, -0.6180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 101 ] state=tensor([[-0.5358,  0.0108,  0.0086, -0.6180]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5356,  0.2058, -0.0037, -0.9080]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 102 ] state=tensor([[-0.5356,  0.2058, -0.0037, -0.9080]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5315,  0.0108, -0.0219, -0.6165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 103 ] state=tensor([[-0.5315,  0.0108, -0.0219, -0.6165]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5313, -0.1840, -0.0342, -0.3308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 104 ] state=tensor([[-0.5313, -0.1840, -0.0342, -0.3308]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5350, -0.3787, -0.0408, -0.0491]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 105 ] state=tensor([[-0.5350, -0.3787, -0.0408, -0.0491]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5425, -0.5732, -0.0418,  0.2305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 106 ] state=tensor([[-0.5425, -0.5732, -0.0418,  0.2305]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5540, -0.3775, -0.0372, -0.0751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 107 ] state=tensor([[-0.5540, -0.3775, -0.0372, -0.0751]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5615, -0.5720, -0.0387,  0.2056]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 108 ] state=tensor([[-0.5615, -0.5720, -0.0387,  0.2056]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5730, -0.3764, -0.0346, -0.0990]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 109 ] state=tensor([[-0.5730, -0.3764, -0.0346, -0.0990]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5805, -0.5710, -0.0366,  0.1825]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 110 ] state=tensor([[-0.5805, -0.5710, -0.0366,  0.1825]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5919, -0.3754, -0.0329, -0.1215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 111 ] state=tensor([[-0.5919, -0.3754, -0.0329, -0.1215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5994, -0.5700, -0.0354,  0.1607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 112 ] state=tensor([[-0.5994, -0.5700, -0.0354,  0.1607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6108, -0.3744, -0.0321, -0.1430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 113 ] state=tensor([[-0.6108, -0.3744, -0.0321, -0.1430]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6183, -0.5690, -0.0350,  0.1394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 114 ] state=tensor([[-0.6183, -0.5690, -0.0350,  0.1394]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6297, -0.3734, -0.0322, -0.1641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 115 ] state=tensor([[-0.6297, -0.3734, -0.0322, -0.1641]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6372, -0.5681, -0.0355,  0.1182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 116 ] state=tensor([[-0.6372, -0.5681, -0.0355,  0.1182]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6485, -0.3725, -0.0331, -0.1854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 117 ] state=tensor([[-0.6485, -0.3725, -0.0331, -0.1854]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6560, -0.5671, -0.0368,  0.0966]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 118 ] state=tensor([[-0.6560, -0.5671, -0.0368,  0.0966]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6673, -0.3715, -0.0349, -0.2075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 119 ] state=tensor([[-0.6673, -0.3715, -0.0349, -0.2075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6748, -0.5661, -0.0391,  0.0740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 120 ] state=tensor([[-0.6748, -0.5661, -0.0391,  0.0740]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6861, -0.3704, -0.0376, -0.2308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 121 ] state=tensor([[-0.6861, -0.3704, -0.0376, -0.2308]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6935, -0.5650, -0.0422,  0.0498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 122 ] state=tensor([[-0.6935, -0.5650, -0.0422,  0.0498]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7048, -0.3693, -0.0412, -0.2559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 123 ] state=tensor([[-0.7048, -0.3693, -0.0412, -0.2559]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7122, -0.5638, -0.0463,  0.0235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 124 ] state=tensor([[-0.7122, -0.5638, -0.0463,  0.0235]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7234, -0.3680, -0.0459, -0.2834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 125 ] state=tensor([[-0.7234, -0.3680, -0.0459, -0.2834]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7308, -0.5625, -0.0515, -0.0055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 126 ] state=tensor([[-0.7308, -0.5625, -0.0515, -0.0055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7421, -0.3667, -0.0516, -0.3140]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 127 ] state=tensor([[-0.7421, -0.3667, -0.0516, -0.3140]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7494, -0.5610, -0.0579, -0.0380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 128 ] state=tensor([[-0.7494, -0.5610, -0.0579, -0.0380]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7606, -0.7553, -0.0587,  0.2358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 129 ] state=tensor([[-0.7606, -0.7553, -0.0587,  0.2358]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7757, -0.5593, -0.0540, -0.0748]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 130 ] state=tensor([[-0.7757, -0.5593, -0.0540, -0.0748]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7869, -0.7537, -0.0554,  0.2004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 131 ] state=tensor([[-0.7869, -0.7537, -0.0554,  0.2004]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8020, -0.5578, -0.0514, -0.1092]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 132 ] state=tensor([[-0.8020, -0.5578, -0.0514, -0.1092]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8131, -0.7521, -0.0536,  0.1668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 133 ] state=tensor([[-0.8131, -0.7521, -0.0536,  0.1668]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8282, -0.9464, -0.0503,  0.4421]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 134 ] state=tensor([[-0.8282, -0.9464, -0.0503,  0.4421]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8471, -0.7507, -0.0414,  0.1340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 135 ] state=tensor([[-0.8471, -0.7507, -0.0414,  0.1340]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8621, -0.5550, -0.0388, -0.1715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 136 ] state=tensor([[-0.8621, -0.5550, -0.0388, -0.1715]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8732, -0.7495, -0.0422,  0.1087]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 137 ] state=tensor([[-0.8732, -0.7495, -0.0422,  0.1087]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8882, -0.9440, -0.0400,  0.3878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 138 ] state=tensor([[-0.8882, -0.9440, -0.0400,  0.3878]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9071, -1.1385, -0.0323,  0.6676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 139 ] state=tensor([[-0.9071, -1.1385, -0.0323,  0.6676]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9299, -1.3332, -0.0189,  0.9500]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 113 ][ timestamp 140 ] state=tensor([[-0.9299, -1.3332, -0.0189,  0.9500]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-9.5652e-01, -1.5281e+00,  9.0168e-05,  1.2367e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 141 ] state=tensor([[-9.5652e-01, -1.5281e+00,  9.0168e-05,  1.2367e+00]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9871, -1.7232,  0.0248,  1.5294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 142 ] state=tensor([[-0.9871, -1.7232,  0.0248,  1.5294]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0215, -1.9186,  0.0554,  1.8297]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 143 ] state=tensor([[-1.0215, -1.9186,  0.0554,  1.8297]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0599, -2.1143,  0.0920,  2.1391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 144 ] state=tensor([[-1.0599, -2.1143,  0.0920,  2.1391]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1022, -2.3102,  0.1348,  2.4587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 145 ] state=tensor([[-1.1022, -2.3102,  0.1348,  2.4587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1484, -2.5062,  0.1840,  2.7895]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 113 ][ timestamp 146 ] state=tensor([[-1.1484, -2.5062,  0.1840,  2.7895]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 113: Exploration_rate=0.05. Score=146.\n",
      "[ episode 114 ] state=tensor([[-0.0009, -0.0113,  0.0162, -0.0178]])\n",
      "[ episode 114 ][ timestamp 1 ] state=tensor([[-0.0009, -0.0113,  0.0162, -0.0178]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0011,  0.1836,  0.0158, -0.3054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 2 ] state=tensor([[-0.0011,  0.1836,  0.0158, -0.3054]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0026, -0.0117,  0.0097, -0.0078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 3 ] state=tensor([[ 0.0026, -0.0117,  0.0097, -0.0078]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0023,  0.1832,  0.0095, -0.2974]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 4 ] state=tensor([[ 0.0023,  0.1832,  0.0095, -0.2974]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0060,  0.3782,  0.0036, -0.5870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 5 ] state=tensor([[ 0.0060,  0.3782,  0.0036, -0.5870]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0136,  0.5733, -0.0081, -0.8786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 6 ] state=tensor([[ 0.0136,  0.5733, -0.0081, -0.8786]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0250,  0.3783, -0.0257, -0.5885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 7 ] state=tensor([[ 0.0250,  0.3783, -0.0257, -0.5885]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0326,  0.1835, -0.0375, -0.3040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 8 ] state=tensor([[ 0.0326,  0.1835, -0.0375, -0.3040]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0363,  0.3792, -0.0436, -0.6083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 9 ] state=tensor([[ 0.0363,  0.3792, -0.0436, -0.6083]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0439,  0.1847, -0.0557, -0.3296]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 10 ] state=tensor([[ 0.0439,  0.1847, -0.0557, -0.3296]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0476,  0.3806, -0.0623, -0.6393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 11 ] state=tensor([[ 0.0476,  0.3806, -0.0623, -0.6393]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0552,  0.1864, -0.0751, -0.3669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 12 ] state=tensor([[ 0.0552,  0.1864, -0.0751, -0.3669]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0589, -0.0076, -0.0824, -0.0988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 13 ] state=tensor([[ 0.0589, -0.0076, -0.0824, -0.0988]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0587, -0.2015, -0.0844,  0.1668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 14 ] state=tensor([[ 0.0587, -0.2015, -0.0844,  0.1668]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0547, -0.0052, -0.0811, -0.1513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 15 ] state=tensor([[ 0.0547, -0.0052, -0.0811, -0.1513]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0546, -0.1991, -0.0841,  0.1147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 16 ] state=tensor([[ 0.0546, -0.1991, -0.0841,  0.1147]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0506, -0.3929, -0.0818,  0.3797]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 17 ] state=tensor([[ 0.0506, -0.3929, -0.0818,  0.3797]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0428, -0.1968, -0.0742,  0.0624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 18 ] state=tensor([[ 0.0428, -0.1968, -0.0742,  0.0624]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0388, -0.3907, -0.0730,  0.3308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 19 ] state=tensor([[ 0.0388, -0.3907, -0.0730,  0.3308]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0310, -0.1947, -0.0664,  0.0160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 20 ] state=tensor([[ 0.0310, -0.1947, -0.0664,  0.0160]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0271,  0.0013, -0.0660, -0.2969]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 21 ] state=tensor([[ 0.0271,  0.0013, -0.0660, -0.2969]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0271, -0.1928, -0.0720, -0.0257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 22 ] state=tensor([[ 0.0271, -0.1928, -0.0720, -0.0257]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0233, -0.3868, -0.0725,  0.2434]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 23 ] state=tensor([[ 0.0233, -0.3868, -0.0725,  0.2434]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0156, -0.5808, -0.0676,  0.5124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 24 ] state=tensor([[ 0.0156, -0.5808, -0.0676,  0.5124]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0039, -0.3848, -0.0574,  0.1992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 25 ] state=tensor([[ 0.0039, -0.3848, -0.0574,  0.1992]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0038, -0.1889, -0.0534, -0.1110]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 26 ] state=tensor([[-0.0038, -0.1889, -0.0534, -0.1110]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0075, -0.3832, -0.0556,  0.1643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 27 ] state=tensor([[-0.0075, -0.3832, -0.0556,  0.1643]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0152, -0.1874, -0.0523, -0.1454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 28 ] state=tensor([[-0.0152, -0.1874, -0.0523, -0.1454]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0189, -0.3817, -0.0552,  0.1304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 29 ] state=tensor([[-0.0189, -0.3817, -0.0552,  0.1304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0266, -0.5760, -0.0526,  0.4051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 30 ] state=tensor([[-0.0266, -0.5760, -0.0526,  0.4051]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0381, -0.3802, -0.0445,  0.0963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 31 ] state=tensor([[-0.0381, -0.3802, -0.0445,  0.0963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0457, -0.5746, -0.0426,  0.3746]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 32 ] state=tensor([[-0.0457, -0.5746, -0.0426,  0.3746]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0572, -0.7691, -0.0351,  0.6536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 33 ] state=tensor([[-0.0572, -0.7691, -0.0351,  0.6536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0726, -0.5735, -0.0220,  0.3501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 34 ] state=tensor([[-0.0726, -0.5735, -0.0220,  0.3501]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0840, -0.7683, -0.0150,  0.6357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 35 ] state=tensor([[-0.0840, -0.7683, -0.0150,  0.6357]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0994, -0.5730, -0.0023,  0.3384]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 36 ] state=tensor([[-0.0994, -0.5730, -0.0023,  0.3384]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1109, -0.7681,  0.0045,  0.6303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 37 ] state=tensor([[-0.1109, -0.7681,  0.0045,  0.6303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1262, -0.9633,  0.0171,  0.9244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 38 ] state=tensor([[-0.1262, -0.9633,  0.0171,  0.9244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1455, -1.1586,  0.0355,  1.2224]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 39 ] state=tensor([[-0.1455, -1.1586,  0.0355,  1.2224]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1687, -1.3542,  0.0600,  1.5260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 40 ] state=tensor([[-0.1687, -1.3542,  0.0600,  1.5260]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1958, -1.5500,  0.0905,  1.8368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 41 ] state=tensor([[-0.1958, -1.5500,  0.0905,  1.8368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2268, -1.7460,  0.1272,  2.1561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 42 ] state=tensor([[-0.2268, -1.7460,  0.1272,  2.1561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2617, -1.9421,  0.1704,  2.4853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 114 ][ timestamp 43 ] state=tensor([[-0.2617, -1.9421,  0.1704,  2.4853]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Ended! ] Episode 114: Exploration_rate=0.05. Score=43.\n",
      "[ episode 115 ] state=tensor([[-0.0224,  0.0220,  0.0247,  0.0256]])\n",
      "[ episode 115 ][ timestamp 1 ] state=tensor([[-0.0224,  0.0220,  0.0247,  0.0256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0220,  0.2168,  0.0252, -0.2592]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 2 ] state=tensor([[-0.0220,  0.2168,  0.0252, -0.2592]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0176,  0.4115,  0.0200, -0.5438]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 3 ] state=tensor([[-0.0176,  0.4115,  0.0200, -0.5438]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0094,  0.6064,  0.0092, -0.8301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 4 ] state=tensor([[-0.0094,  0.6064,  0.0092, -0.8301]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0027,  0.8014, -0.0074, -1.1199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 5 ] state=tensor([[ 0.0027,  0.8014, -0.0074, -1.1199]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0188,  0.6063, -0.0298, -0.8295]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 6 ] state=tensor([[ 0.0188,  0.6063, -0.0298, -0.8295]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0309,  0.4116, -0.0464, -0.5464]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 7 ] state=tensor([[ 0.0309,  0.4116, -0.0464, -0.5464]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0391,  0.2172, -0.0574, -0.2687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 8 ] state=tensor([[ 0.0391,  0.2172, -0.0574, -0.2687]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0435,  0.0229, -0.0627,  0.0054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 9 ] state=tensor([[ 0.0435,  0.0229, -0.0627,  0.0054]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0439,  0.2189, -0.0626, -0.3064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 10 ] state=tensor([[ 0.0439,  0.2189, -0.0626, -0.3064]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0483,  0.4149, -0.0688, -0.6182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 11 ] state=tensor([[ 0.0483,  0.4149, -0.0688, -0.6182]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0566,  0.2208, -0.0811, -0.3479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 12 ] state=tensor([[ 0.0566,  0.2208, -0.0811, -0.3479]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0610,  0.0269, -0.0881, -0.0819]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 13 ] state=tensor([[ 0.0610,  0.0269, -0.0881, -0.0819]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0615,  0.2232, -0.0897, -0.4010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 14 ] state=tensor([[ 0.0615,  0.2232, -0.0897, -0.4010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0660,  0.0294, -0.0977, -0.1379]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 15 ] state=tensor([[ 0.0660,  0.0294, -0.0977, -0.1379]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0666, -0.1642, -0.1005,  0.1224]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 16 ] state=tensor([[ 0.0666, -0.1642, -0.1005,  0.1224]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0633, -0.3577, -0.0980,  0.3818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 17 ] state=tensor([[ 0.0633, -0.3577, -0.0980,  0.3818]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0562, -0.1614, -0.0904,  0.0598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 18 ] state=tensor([[ 0.0562, -0.1614, -0.0904,  0.0598]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0529, -0.3551, -0.0892,  0.3227]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 19 ] state=tensor([[ 0.0529, -0.3551, -0.0892,  0.3227]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0458, -0.1588, -0.0828,  0.0033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 20 ] state=tensor([[ 0.0458, -0.1588, -0.0828,  0.0033]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0427, -0.3527, -0.0827,  0.2687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 21 ] state=tensor([[ 0.0427, -0.3527, -0.0827,  0.2687]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0356, -0.1565, -0.0773, -0.0488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 22 ] state=tensor([[ 0.0356, -0.1565, -0.0773, -0.0488]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0325, -0.3504, -0.0783,  0.2185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 23 ] state=tensor([[ 0.0325, -0.3504, -0.0783,  0.2185]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0255, -0.5443, -0.0739,  0.4855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 24 ] state=tensor([[ 0.0255, -0.5443, -0.0739,  0.4855]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0146, -0.3482, -0.0642,  0.1704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 25 ] state=tensor([[ 0.0146, -0.3482, -0.0642,  0.1704]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0076, -0.5424, -0.0608,  0.4422]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 26 ] state=tensor([[ 0.0076, -0.5424, -0.0608,  0.4422]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0032, -0.3464, -0.0520,  0.1310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 27 ] state=tensor([[-0.0032, -0.3464, -0.0520,  0.1310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0102, -0.1506, -0.0493, -0.1776]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 28 ] state=tensor([[-0.0102, -0.1506, -0.0493, -0.1776]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0132, -0.3450, -0.0529,  0.0991]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 29 ] state=tensor([[-0.0132, -0.3450, -0.0529,  0.0991]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0201, -0.1492, -0.0509, -0.2098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 30 ] state=tensor([[-0.0201, -0.1492, -0.0509, -0.2098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0231, -0.3435, -0.0551,  0.0664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 31 ] state=tensor([[-0.0231, -0.3435, -0.0551,  0.0664]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0299, -0.5378, -0.0538,  0.3412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 32 ] state=tensor([[-0.0299, -0.5378, -0.0538,  0.3412]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0407, -0.3420, -0.0470,  0.0320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 33 ] state=tensor([[-0.0407, -0.3420, -0.0470,  0.0320]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0475, -0.5364, -0.0463,  0.3095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 34 ] state=tensor([[-0.0475, -0.5364, -0.0463,  0.3095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0583, -0.3406, -0.0401,  0.0026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 35 ] state=tensor([[-0.0583, -0.3406, -0.0401,  0.0026]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0651, -0.5352, -0.0401,  0.2824]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 36 ] state=tensor([[-0.0651, -0.5352, -0.0401,  0.2824]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0758, -0.3395, -0.0344, -0.0227]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 37 ] state=tensor([[-0.0758, -0.3395, -0.0344, -0.0227]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0826, -0.5341, -0.0349,  0.2589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 38 ] state=tensor([[-0.0826, -0.5341, -0.0349,  0.2589]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0932, -0.3385, -0.0297, -0.0446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 39 ] state=tensor([[-0.0932, -0.3385, -0.0297, -0.0446]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1000, -0.5332, -0.0306,  0.2386]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 40 ] state=tensor([[-0.1000, -0.5332, -0.0306,  0.2386]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1107, -0.3376, -0.0258, -0.0636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 41 ] state=tensor([[-0.1107, -0.3376, -0.0258, -0.0636]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1174, -0.1422, -0.0271, -0.3643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 42 ] state=tensor([[-0.1174, -0.1422, -0.0271, -0.3643]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1203, -0.3369, -0.0344, -0.0803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 43 ] state=tensor([[-0.1203, -0.3369, -0.0344, -0.0803]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1270, -0.5315, -0.0360,  0.2014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 44 ] state=tensor([[-0.1270, -0.5315, -0.0360,  0.2014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1376, -0.3359, -0.0320, -0.1025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 45 ] state=tensor([[-0.1376, -0.3359, -0.0320, -0.1025]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1444, -0.5305, -0.0340,  0.1800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 46 ] state=tensor([[-0.1444, -0.5305, -0.0340,  0.1800]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1550, -0.7251, -0.0304,  0.4617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 47 ] state=tensor([[-0.1550, -0.7251, -0.0304,  0.4617]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1695, -0.5296, -0.0212,  0.1596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 48 ] state=tensor([[-0.1695, -0.5296, -0.0212,  0.1596]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1801, -0.7244, -0.0180,  0.4456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 49 ] state=tensor([[-0.1801, -0.7244, -0.0180,  0.4456]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1946, -0.9193, -0.0091,  0.7325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 50 ] state=tensor([[-0.1946, -0.9193, -0.0091,  0.7325]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2129, -0.7240,  0.0056,  0.4370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 51 ] state=tensor([[-0.2129, -0.7240,  0.0056,  0.4370]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2274, -0.9192,  0.0143,  0.7314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 52 ] state=tensor([[-0.2274, -0.9192,  0.0143,  0.7314]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2458, -0.7243,  0.0289,  0.4433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 53 ] state=tensor([[-0.2458, -0.7243,  0.0289,  0.4433]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2603, -0.5296,  0.0378,  0.1599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 54 ] state=tensor([[-0.2603, -0.5296,  0.0378,  0.1599]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2709, -0.7253,  0.0410,  0.4642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 55 ] state=tensor([[-0.2709, -0.7253,  0.0410,  0.4642]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2854, -0.5307,  0.0503,  0.1847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 56 ] state=tensor([[-0.2854, -0.5307,  0.0503,  0.1847]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2960, -0.7265,  0.0540,  0.4928]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 57 ] state=tensor([[-0.2960, -0.7265,  0.0540,  0.4928]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3105, -0.9224,  0.0638,  0.8020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 58 ] state=tensor([[-0.3105, -0.9224,  0.0638,  0.8020]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3290, -0.7282,  0.0799,  0.5301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 59 ] state=tensor([[-0.3290, -0.7282,  0.0799,  0.5301]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3435, -0.5343,  0.0905,  0.2636]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 115 ][ timestamp 60 ] state=tensor([[-0.3435, -0.5343,  0.0905,  0.2636]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3542, -0.3406,  0.0958,  0.0008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 61 ] state=tensor([[-0.3542, -0.3406,  0.0958,  0.0008]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3610, -0.1469,  0.0958, -0.2602]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 62 ] state=tensor([[-0.3610, -0.1469,  0.0958, -0.2602]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3640,  0.0467,  0.0906, -0.5212]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 63 ] state=tensor([[-0.3640,  0.0467,  0.0906, -0.5212]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3630,  0.2404,  0.0801, -0.7840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 64 ] state=tensor([[-0.3630,  0.2404,  0.0801, -0.7840]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3582,  0.4344,  0.0645, -1.0505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 65 ] state=tensor([[-0.3582,  0.4344,  0.0645, -1.0505]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3495,  0.6286,  0.0435, -1.3222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 66 ] state=tensor([[-0.3495,  0.6286,  0.0435, -1.3222]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3370,  0.4329,  0.0170, -1.0163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 67 ] state=tensor([[-0.3370,  0.4329,  0.0170, -1.0163]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3283,  0.2376, -0.0033, -0.7183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 68 ] state=tensor([[-0.3283,  0.2376, -0.0033, -0.7183]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3236,  0.0425, -0.0177, -0.4267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 69 ] state=tensor([[-0.3236,  0.0425, -0.0177, -0.4267]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3227, -0.1523, -0.0262, -0.1396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 70 ] state=tensor([[-0.3227, -0.1523, -0.0262, -0.1396]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3258, -0.3471, -0.0290,  0.1447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 71 ] state=tensor([[-0.3258, -0.3471, -0.0290,  0.1447]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3327, -0.1516, -0.0261, -0.1570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 72 ] state=tensor([[-0.3327, -0.1516, -0.0261, -0.1570]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3357,  0.0439, -0.0293, -0.4578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 73 ] state=tensor([[-0.3357,  0.0439, -0.0293, -0.4578]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3349, -0.1508, -0.0384, -0.1745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 74 ] state=tensor([[-0.3349, -0.1508, -0.0384, -0.1745]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3379, -0.3453, -0.0419,  0.1058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 75 ] state=tensor([[-0.3379, -0.3453, -0.0419,  0.1058]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3448, -0.5398, -0.0398,  0.3850]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 76 ] state=tensor([[-0.3448, -0.5398, -0.0398,  0.3850]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3556, -0.3441, -0.0321,  0.0800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 77 ] state=tensor([[-0.3556, -0.3441, -0.0321,  0.0800]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3625, -0.5388, -0.0305,  0.3624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 78 ] state=tensor([[-0.3625, -0.5388, -0.0305,  0.3624]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3732, -0.3433, -0.0232,  0.0603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 79 ] state=tensor([[-0.3732, -0.3433, -0.0232,  0.0603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3801, -0.5380, -0.0220,  0.3456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 80 ] state=tensor([[-0.3801, -0.5380, -0.0220,  0.3456]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3909, -0.3426, -0.0151,  0.0460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 81 ] state=tensor([[-0.3909, -0.3426, -0.0151,  0.0460]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3977, -0.5375, -0.0142,  0.3339]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 82 ] state=tensor([[-0.3977, -0.5375, -0.0142,  0.3339]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4085, -0.3422, -0.0075,  0.0368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 83 ] state=tensor([[-0.4085, -0.3422, -0.0075,  0.0368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4153, -0.5372, -0.0068,  0.3271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 84 ] state=tensor([[-0.4153, -0.5372, -0.0068,  0.3271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-4.2605e-01, -7.3223e-01, -2.4495e-04,  6.1759e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 85 ] state=tensor([[-4.2605e-01, -7.3223e-01, -2.4495e-04,  6.1759e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4407, -0.5371,  0.0121,  0.3248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 86 ] state=tensor([[-0.4407, -0.5371,  0.0121,  0.3248]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4514, -0.3422,  0.0186,  0.0360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 87 ] state=tensor([[-0.4514, -0.3422,  0.0186,  0.0360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4583, -0.1473,  0.0193, -0.2508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 88 ] state=tensor([[-0.4583, -0.1473,  0.0193, -0.2508]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4612, -0.3427,  0.0143,  0.0480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 89 ] state=tensor([[-0.4612, -0.3427,  0.0143,  0.0480]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4681, -0.1478,  0.0153, -0.2402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 90 ] state=tensor([[-0.4681, -0.1478,  0.0153, -0.2402]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4710,  0.0471,  0.0105, -0.5280]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 91 ] state=tensor([[-0.4710,  0.0471,  0.0105, -0.5280]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.7009e-01,  2.4209e-01, -9.6529e-05, -8.1738e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 92 ] state=tensor([[-4.7009e-01,  2.4209e-01, -9.6529e-05, -8.1738e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4653,  0.0470, -0.0164, -0.5247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 93 ] state=tensor([[-0.4653,  0.0470, -0.0164, -0.5247]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4643, -0.1479, -0.0269, -0.2373]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 94 ] state=tensor([[-0.4643, -0.1479, -0.0269, -0.2373]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4673,  0.0476, -0.0317, -0.5383]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 95 ] state=tensor([[-0.4673,  0.0476, -0.0317, -0.5383]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4663, -0.1471, -0.0425, -0.2558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 96 ] state=tensor([[-0.4663, -0.1471, -0.0425, -0.2558]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4693, -0.3416, -0.0476,  0.0232]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 97 ] state=tensor([[-0.4693, -0.3416, -0.0476,  0.0232]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4761, -0.1458, -0.0471, -0.2841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 98 ] state=tensor([[-0.4761, -0.1458, -0.0471, -0.2841]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4790, -0.3402, -0.0528, -0.0066]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 99 ] state=tensor([[-0.4790, -0.3402, -0.0528, -0.0066]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4858, -0.5345, -0.0529,  0.2689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 100 ] state=tensor([[-0.4858, -0.5345, -0.0529,  0.2689]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4965, -0.3387, -0.0475, -0.0400]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 101 ] state=tensor([[-0.4965, -0.3387, -0.0475, -0.0400]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5033, -0.5331, -0.0483,  0.2374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 102 ] state=tensor([[-0.5033, -0.5331, -0.0483,  0.2374]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5139, -0.3373, -0.0436, -0.0702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 103 ] state=tensor([[-0.5139, -0.3373, -0.0436, -0.0702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5207, -0.5318, -0.0450,  0.2084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 104 ] state=tensor([[-0.5207, -0.5318, -0.0450,  0.2084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5313, -0.3361, -0.0408, -0.0981]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 105 ] state=tensor([[-0.5313, -0.3361, -0.0408, -0.0981]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5380, -0.5306, -0.0428,  0.1814]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 106 ] state=tensor([[-0.5380, -0.5306, -0.0428,  0.1814]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5487, -0.3349, -0.0392, -0.1244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 107 ] state=tensor([[-0.5487, -0.3349, -0.0392, -0.1244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5554, -0.5294, -0.0416,  0.1557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 108 ] state=tensor([[-0.5554, -0.5294, -0.0416,  0.1557]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5659, -0.3337, -0.0385, -0.1499]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 109 ] state=tensor([[-0.5659, -0.3337, -0.0385, -0.1499]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5726, -0.5283, -0.0415,  0.1304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 110 ] state=tensor([[-0.5726, -0.5283, -0.0415,  0.1304]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5832, -0.3326, -0.0389, -0.1751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 111 ] state=tensor([[-0.5832, -0.3326, -0.0389, -0.1751]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5898, -0.5271, -0.0424,  0.1051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 112 ] state=tensor([[-0.5898, -0.5271, -0.0424,  0.1051]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6004, -0.3314, -0.0403, -0.2007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 113 ] state=tensor([[-0.6004, -0.3314, -0.0403, -0.2007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6070, -0.5260, -0.0443,  0.0790]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 114 ] state=tensor([[-0.6070, -0.5260, -0.0443,  0.0790]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6175, -0.7204, -0.0428,  0.3574]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 115 ] state=tensor([[-0.6175, -0.7204, -0.0428,  0.3574]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6319, -0.5247, -0.0356,  0.0515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 116 ] state=tensor([[-0.6319, -0.5247, -0.0356,  0.0515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6424, -0.7193, -0.0346,  0.3328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 117 ] state=tensor([[-0.6424, -0.7193, -0.0346,  0.3328]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6568, -0.5237, -0.0279,  0.0294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 118 ] state=tensor([[-0.6568, -0.5237, -0.0279,  0.0294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6673, -0.3282, -0.0273, -0.2720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 119 ] state=tensor([[-0.6673, -0.3282, -0.0273, -0.2720]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6739, -0.5229, -0.0328,  0.0120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 120 ] state=tensor([[-0.6739, -0.5229, -0.0328,  0.0120]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6843, -0.7176, -0.0325,  0.2941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 121 ] state=tensor([[-0.6843, -0.7176, -0.0325,  0.2941]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6987, -0.5220, -0.0267, -0.0086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 122 ] state=tensor([[-0.6987, -0.5220, -0.0267, -0.0086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7091, -0.3265, -0.0268, -0.3096]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 123 ] state=tensor([[-0.7091, -0.3265, -0.0268, -0.3096]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7156, -0.5212, -0.0330, -0.0255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 124 ] state=tensor([[-0.7156, -0.5212, -0.0330, -0.0255]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7261, -0.7159, -0.0335,  0.2566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 125 ] state=tensor([[-0.7261, -0.7159, -0.0335,  0.2566]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7404, -0.5203, -0.0284, -0.0465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 126 ] state=tensor([[-0.7404, -0.5203, -0.0284, -0.0465]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7508, -0.7150, -0.0293,  0.2371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 127 ] state=tensor([[-0.7508, -0.7150, -0.0293,  0.2371]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7651, -0.5194, -0.0246, -0.0647]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 128 ] state=tensor([[-0.7651, -0.5194, -0.0246, -0.0647]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7755, -0.7142, -0.0259,  0.2202]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 129 ] state=tensor([[-0.7755, -0.7142, -0.0259,  0.2202]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7897, -0.5187, -0.0215, -0.0806]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 130 ] state=tensor([[-0.7897, -0.5187, -0.0215, -0.0806]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8001, -0.3233, -0.0231, -0.3800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 131 ] state=tensor([[-0.8001, -0.3233, -0.0231, -0.3800]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8066, -0.5181, -0.0307, -0.0946]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 132 ] state=tensor([[-0.8066, -0.5181, -0.0307, -0.0946]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8170, -0.7128, -0.0326,  0.1882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 133 ] state=tensor([[-0.8170, -0.7128, -0.0326,  0.1882]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8312, -0.5172, -0.0288, -0.1146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 134 ] state=tensor([[-0.8312, -0.5172, -0.0288, -0.1146]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8416, -0.7119, -0.0311,  0.1689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 135 ] state=tensor([[-0.8416, -0.7119, -0.0311,  0.1689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8558, -0.9065, -0.0277,  0.4516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 136 ] state=tensor([[-0.8558, -0.9065, -0.0277,  0.4516]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8739, -0.7110, -0.0187,  0.1503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 137 ] state=tensor([[-0.8739, -0.7110, -0.0187,  0.1503]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8881, -0.9059, -0.0157,  0.4370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 138 ] state=tensor([[-0.8881, -0.9059, -0.0157,  0.4370]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9063, -0.7106, -0.0069,  0.1394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 139 ] state=tensor([[-0.9063, -0.7106, -0.0069,  0.1394]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9205, -0.9056, -0.0042,  0.4299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 140 ] state=tensor([[-0.9205, -0.9056, -0.0042,  0.4299]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9386, -0.7104,  0.0044,  0.1359]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 141 ] state=tensor([[-0.9386, -0.7104,  0.0044,  0.1359]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9528, -0.9056,  0.0072,  0.4300]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 142 ] state=tensor([[-0.9528, -0.9056,  0.0072,  0.4300]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9709, -1.1008,  0.0158,  0.7249]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 143 ] state=tensor([[-0.9709, -1.1008,  0.0158,  0.7249]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9929, -1.2961,  0.0303,  1.0225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 144 ] state=tensor([[-0.9929, -1.2961,  0.0303,  1.0225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0188, -1.4916,  0.0507,  1.3246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 145 ] state=tensor([[-1.0188, -1.4916,  0.0507,  1.3246]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0487, -1.6874,  0.0772,  1.6327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 146 ] state=tensor([[-1.0487, -1.6874,  0.0772,  1.6327]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0824, -1.8833,  0.1099,  1.9484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 147 ] state=tensor([[-1.0824, -1.8833,  0.1099,  1.9484]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1201, -2.0794,  0.1488,  2.2730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 148 ] state=tensor([[-1.1201, -2.0794,  0.1488,  2.2730]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1617, -2.2756,  0.1943,  2.6076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 115 ][ timestamp 149 ] state=tensor([[-1.1617, -2.2756,  0.1943,  2.6076]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 115: Exploration_rate=0.05. Score=149.\n",
      "[ episode 116 ] state=tensor([[ 0.0240, -0.0286,  0.0051, -0.0430]])\n",
      "[ episode 116 ][ timestamp 1 ] state=tensor([[ 0.0240, -0.0286,  0.0051, -0.0430]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0234,  0.1664,  0.0042, -0.3341]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 2 ] state=tensor([[ 0.0234,  0.1664,  0.0042, -0.3341]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0268,  0.3615, -0.0025, -0.6254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 3 ] state=tensor([[ 0.0268,  0.3615, -0.0025, -0.6254]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0340,  0.1664, -0.0150, -0.3335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 4 ] state=tensor([[ 0.0340,  0.1664, -0.0150, -0.3335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0373,  0.3617, -0.0217, -0.6309]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 116 ][ timestamp 5 ] state=tensor([[ 0.0373,  0.3617, -0.0217, -0.6309]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0446,  0.1669, -0.0343, -0.3451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 6 ] state=tensor([[ 0.0446,  0.1669, -0.0343, -0.3451]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0479,  0.3625, -0.0412, -0.6484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 7 ] state=tensor([[ 0.0479,  0.3625, -0.0412, -0.6484]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0552,  0.1680, -0.0542, -0.3690]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 8 ] state=tensor([[ 0.0552,  0.1680, -0.0542, -0.3690]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0585, -0.0263, -0.0615, -0.0939]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 9 ] state=tensor([[ 0.0585, -0.0263, -0.0615, -0.0939]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0580,  0.1696, -0.0634, -0.4053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 10 ] state=tensor([[ 0.0580,  0.1696, -0.0634, -0.4053]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0614, -0.0246, -0.0715, -0.1333]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 11 ] state=tensor([[ 0.0614, -0.0246, -0.0715, -0.1333]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0609, -0.2186, -0.0742,  0.1360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 12 ] state=tensor([[ 0.0609, -0.2186, -0.0742,  0.1360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0565, -0.0225, -0.0715, -0.1791]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 13 ] state=tensor([[ 0.0565, -0.0225, -0.0715, -0.1791]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0561, -0.2165, -0.0750,  0.0902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 14 ] state=tensor([[ 0.0561, -0.2165, -0.0750,  0.0902]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0517, -0.0204, -0.0732, -0.2252]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 15 ] state=tensor([[ 0.0517, -0.0204, -0.0732, -0.2252]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0513, -0.2144, -0.0777,  0.0435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 16 ] state=tensor([[ 0.0513, -0.2144, -0.0777,  0.0435]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0470, -0.4083, -0.0769,  0.3107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 17 ] state=tensor([[ 0.0470, -0.4083, -0.0769,  0.3107]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0389, -0.2122, -0.0707, -0.0052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 18 ] state=tensor([[ 0.0389, -0.2122, -0.0707, -0.0052]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0346, -0.4062, -0.0708,  0.2644]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 19 ] state=tensor([[ 0.0346, -0.4062, -0.0708,  0.2644]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0265, -0.6003, -0.0655,  0.5339]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 20 ] state=tensor([[ 0.0265, -0.6003, -0.0655,  0.5339]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0145, -0.4043, -0.0548,  0.2214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 21 ] state=tensor([[ 0.0145, -0.4043, -0.0548,  0.2214]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0064, -0.5986, -0.0504,  0.4963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 22 ] state=tensor([[ 0.0064, -0.5986, -0.0504,  0.4963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0056, -0.7930, -0.0404,  0.7727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 23 ] state=tensor([[-0.0056, -0.7930, -0.0404,  0.7727]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0214, -0.9875, -0.0250,  1.0523]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 24 ] state=tensor([[-0.0214, -0.9875, -0.0250,  1.0523]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0412, -0.7921, -0.0039,  0.7519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 25 ] state=tensor([[-0.0412, -0.7921, -0.0039,  0.7519]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0570, -0.5969,  0.0111,  0.4580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 26 ] state=tensor([[-0.0570, -0.5969,  0.0111,  0.4580]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0690, -0.7922,  0.0203,  0.7542]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 27 ] state=tensor([[-0.0690, -0.7922,  0.0203,  0.7542]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0848, -0.5973,  0.0353,  0.4679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 28 ] state=tensor([[-0.0848, -0.5973,  0.0353,  0.4679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0967, -0.4027,  0.0447,  0.1866]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 29 ] state=tensor([[-0.0967, -0.4027,  0.0447,  0.1866]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1048, -0.5985,  0.0484,  0.4930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 30 ] state=tensor([[-0.1048, -0.5985,  0.0484,  0.4930]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1168, -0.4041,  0.0583,  0.2160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 31 ] state=tensor([[-0.1168, -0.4041,  0.0583,  0.2160]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1248, -0.2098,  0.0626, -0.0578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 32 ] state=tensor([[-0.1248, -0.2098,  0.0626, -0.0578]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1290, -0.0157,  0.0615, -0.3300]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 33 ] state=tensor([[-0.1290, -0.0157,  0.0615, -0.3300]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1294,  0.1785,  0.0549, -0.6027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 34 ] state=tensor([[-0.1294,  0.1785,  0.0549, -0.6027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1258,  0.3729,  0.0428, -0.8777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 35 ] state=tensor([[-0.1258,  0.3729,  0.0428, -0.8777]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1183,  0.5674,  0.0252, -1.1566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 36 ] state=tensor([[-0.1183,  0.5674,  0.0252, -1.1566]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1070,  0.3719,  0.0021, -0.8561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 37 ] state=tensor([[-0.1070,  0.3719,  0.0021, -0.8561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0995,  0.1768, -0.0150, -0.5627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 38 ] state=tensor([[-0.0995,  0.1768, -0.0150, -0.5627]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0960, -0.0181, -0.0263, -0.2748]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 39 ] state=tensor([[-0.0960, -0.0181, -0.0263, -0.2748]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0964,  0.1774, -0.0318, -0.5757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 40 ] state=tensor([[-0.0964,  0.1774, -0.0318, -0.5757]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0928, -0.0173, -0.0433, -0.2932]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 41 ] state=tensor([[-0.0928, -0.0173, -0.0433, -0.2932]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0932, -0.2118, -0.0491, -0.0144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 42 ] state=tensor([[-0.0932, -0.2118, -0.0491, -0.0144]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0974, -0.4062, -0.0494,  0.2623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 43 ] state=tensor([[-0.0974, -0.4062, -0.0494,  0.2623]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1055, -0.6006, -0.0442,  0.5390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 44 ] state=tensor([[-0.1055, -0.6006, -0.0442,  0.5390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1175, -0.4048, -0.0334,  0.2328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 45 ] state=tensor([[-0.1175, -0.4048, -0.0334,  0.2328]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1256, -0.2093, -0.0287, -0.0703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 46 ] state=tensor([[-0.1256, -0.2093, -0.0287, -0.0703]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1298, -0.4040, -0.0301,  0.2132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 47 ] state=tensor([[-0.1298, -0.4040, -0.0301,  0.2132]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1379, -0.5986, -0.0259,  0.4962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 48 ] state=tensor([[-0.1379, -0.5986, -0.0259,  0.4962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1499, -0.7934, -0.0160,  0.7807]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 116 ][ timestamp 49 ] state=tensor([[-0.1499, -0.7934, -0.0160,  0.7807]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6574e-01, -5.9804e-01, -3.4760e-04,  4.8299e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 50 ] state=tensor([[-1.6574e-01, -5.9804e-01, -3.4760e-04,  4.8299e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1777, -0.4029,  0.0093,  0.1902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 51 ] state=tensor([[-0.1777, -0.4029,  0.0093,  0.1902]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1858, -0.5982,  0.0131,  0.4858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 52 ] state=tensor([[-0.1858, -0.5982,  0.0131,  0.4858]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1977, -0.4032,  0.0228,  0.1973]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 53 ] state=tensor([[-0.1977, -0.4032,  0.0228,  0.1973]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2058, -0.2084,  0.0268, -0.0881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 54 ] state=tensor([[-0.2058, -0.2084,  0.0268, -0.0881]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2100, -0.4039,  0.0250,  0.2129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 55 ] state=tensor([[-0.2100, -0.4039,  0.0250,  0.2129]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2180, -0.2092,  0.0293, -0.0718]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 56 ] state=tensor([[-0.2180, -0.2092,  0.0293, -0.0718]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2222, -0.0145,  0.0278, -0.3551]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 57 ] state=tensor([[-0.2222, -0.0145,  0.0278, -0.3551]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2225,  0.1802,  0.0207, -0.6389]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 58 ] state=tensor([[-0.2225,  0.1802,  0.0207, -0.6389]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2189, -0.0152,  0.0080, -0.3397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 59 ] state=tensor([[-0.2189, -0.0152,  0.0080, -0.3397]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2192, -0.2104,  0.0012, -0.0445]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 60 ] state=tensor([[-0.2192, -0.2104,  0.0012, -0.0445]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2342e-01, -1.5316e-02,  2.7294e-04, -3.3686e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 61 ] state=tensor([[-2.2342e-01, -1.5316e-02,  2.7294e-04, -3.3686e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2237, -0.2104, -0.0065, -0.0441]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 62 ] state=tensor([[-0.2237, -0.2104, -0.0065, -0.0441]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2279, -0.4055, -0.0073,  0.2465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 63 ] state=tensor([[-0.2279, -0.4055, -0.0073,  0.2465]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2360, -0.6005, -0.0024,  0.5369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 64 ] state=tensor([[-0.2360, -0.6005, -0.0024,  0.5369]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2481, -0.4053,  0.0083,  0.2435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 65 ] state=tensor([[-0.2481, -0.4053,  0.0083,  0.2435]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2562, -0.2103,  0.0132, -0.0466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 66 ] state=tensor([[-0.2562, -0.2103,  0.0132, -0.0466]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2604, -0.0154,  0.0123, -0.3351]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 67 ] state=tensor([[-0.2604, -0.0154,  0.0123, -0.3351]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2607,  0.1795,  0.0056, -0.6239]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 68 ] state=tensor([[-0.2607,  0.1795,  0.0056, -0.6239]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2571,  0.3746, -0.0069, -0.9148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 69 ] state=tensor([[-0.2571,  0.3746, -0.0069, -0.9148]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2496,  0.1796, -0.0252, -0.6243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 70 ] state=tensor([[-0.2496,  0.1796, -0.0252, -0.6243]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2460, -0.0152, -0.0377, -0.3397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 71 ] state=tensor([[-0.2460, -0.0152, -0.0377, -0.3397]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2463, -0.2098, -0.0445, -0.0591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 72 ] state=tensor([[-0.2463, -0.2098, -0.0445, -0.0591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2505, -0.4042, -0.0457,  0.2192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 73 ] state=tensor([[-0.2505, -0.4042, -0.0457,  0.2192]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2586, -0.5987, -0.0413,  0.4971]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 74 ] state=tensor([[-0.2586, -0.5987, -0.0413,  0.4971]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2706, -0.4030, -0.0313,  0.1917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 75 ] state=tensor([[-0.2706, -0.4030, -0.0313,  0.1917]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2786, -0.2074, -0.0275, -0.1107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 76 ] state=tensor([[-0.2786, -0.2074, -0.0275, -0.1107]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2828, -0.0119, -0.0297, -0.4119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 77 ] state=tensor([[-0.2828, -0.0119, -0.0297, -0.4119]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2830, -0.2066, -0.0380, -0.1287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 78 ] state=tensor([[-0.2830, -0.2066, -0.0380, -0.1287]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2871, -0.4012, -0.0405,  0.1517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 79 ] state=tensor([[-0.2871, -0.4012, -0.0405,  0.1517]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2952, -0.5957, -0.0375,  0.4314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 80 ] state=tensor([[-0.2952, -0.5957, -0.0375,  0.4314]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3071, -0.4001, -0.0289,  0.1271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 81 ] state=tensor([[-0.3071, -0.4001, -0.0289,  0.1271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3151, -0.5947, -0.0263,  0.4105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 82 ] state=tensor([[-0.3151, -0.5947, -0.0263,  0.4105]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3270, -0.7895, -0.0181,  0.6948]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 83 ] state=tensor([[-0.3270, -0.7895, -0.0181,  0.6948]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3428, -0.5941, -0.0042,  0.3965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 84 ] state=tensor([[-0.3428, -0.5941, -0.0042,  0.3965]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3546, -0.3989,  0.0037,  0.1024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 85 ] state=tensor([[-0.3546, -0.3989,  0.0037,  0.1024]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3626, -0.5941,  0.0057,  0.3963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 86 ] state=tensor([[-0.3626, -0.5941,  0.0057,  0.3963]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3745, -0.3991,  0.0137,  0.1054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 87 ] state=tensor([[-0.3745, -0.3991,  0.0137,  0.1054]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3825, -0.5944,  0.0158,  0.4024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 88 ] state=tensor([[-0.3825, -0.5944,  0.0158,  0.4024]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3944, -0.3995,  0.0238,  0.1147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 89 ] state=tensor([[-0.3944, -0.3995,  0.0238,  0.1147]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4024, -0.2047,  0.0261, -0.1704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 90 ] state=tensor([[-0.4024, -0.2047,  0.0261, -0.1704]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4065, -0.0100,  0.0227, -0.4547]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 91 ] state=tensor([[-0.4065, -0.0100,  0.0227, -0.4547]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4067, -0.2054,  0.0136, -0.1549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 92 ] state=tensor([[-0.4067, -0.2054,  0.0136, -0.1549]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4108, -0.4007,  0.0105,  0.1420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 93 ] state=tensor([[-0.4108, -0.4007,  0.0105,  0.1420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4188, -0.2058,  0.0134, -0.1473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 94 ] state=tensor([[-0.4188, -0.2058,  0.0134, -0.1473]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4229, -0.0108,  0.0104, -0.4358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 95 ] state=tensor([[-0.4229, -0.0108,  0.0104, -0.4358]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4231, -0.2061,  0.0017, -0.1398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 96 ] state=tensor([[-0.4231, -0.2061,  0.0017, -0.1398]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4272, -0.4012, -0.0011,  0.1534]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 97 ] state=tensor([[-0.4272, -0.4012, -0.0011,  0.1534]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4353, -0.5964,  0.0020,  0.4457]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 98 ] state=tensor([[-0.4353, -0.5964,  0.0020,  0.4457]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4472, -0.4013,  0.0109,  0.1537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 99 ] state=tensor([[-0.4472, -0.4013,  0.0109,  0.1537]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4552, -0.2063,  0.0140, -0.1355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 100 ] state=tensor([[-0.4552, -0.2063,  0.0140, -0.1355]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4593, -0.0114,  0.0113, -0.4238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 101 ] state=tensor([[-0.4593, -0.0114,  0.0113, -0.4238]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4596,  0.1836,  0.0028, -0.7129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 102 ] state=tensor([[-0.4596,  0.1836,  0.0028, -0.7129]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4559, -0.0116, -0.0115, -0.4193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 103 ] state=tensor([[-0.4559, -0.0116, -0.0115, -0.4193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4561, -0.2065, -0.0199, -0.1303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 104 ] state=tensor([[-0.4561, -0.2065, -0.0199, -0.1303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4603, -0.4014, -0.0225,  0.1560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 105 ] state=tensor([[-0.4603, -0.4014, -0.0225,  0.1560]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4683, -0.5962, -0.0194,  0.4416]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 106 ] state=tensor([[-0.4683, -0.5962, -0.0194,  0.4416]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4802, -0.7910, -0.0105,  0.7281]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 107 ] state=tensor([[-0.4802, -0.7910, -0.0105,  0.7281]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4960, -0.5957,  0.0040,  0.4321]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 108 ] state=tensor([[-0.4960, -0.5957,  0.0040,  0.4321]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5079, -0.4007,  0.0127,  0.1407]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 109 ] state=tensor([[-0.5079, -0.4007,  0.0127,  0.1407]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5160, -0.2057,  0.0155, -0.1480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 110 ] state=tensor([[-0.5160, -0.2057,  0.0155, -0.1480]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5201, -0.4011,  0.0125,  0.1496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 111 ] state=tensor([[-0.5201, -0.4011,  0.0125,  0.1496]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5281, -0.2061,  0.0155, -0.1391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 112 ] state=tensor([[-0.5281, -0.2061,  0.0155, -0.1391]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5322, -0.0112,  0.0127, -0.4269]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 113 ] state=tensor([[-0.5322, -0.0112,  0.0127, -0.4269]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5324,  0.1837,  0.0042, -0.7155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 114 ] state=tensor([[-0.5324,  0.1837,  0.0042, -0.7155]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5288, -0.0115, -0.0101, -0.4215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 115 ] state=tensor([[-0.5288, -0.0115, -0.0101, -0.4215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5290, -0.2065, -0.0185, -0.1320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 116 ] state=tensor([[-0.5290, -0.2065, -0.0185, -0.1320]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5331, -0.0111, -0.0212, -0.4305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 117 ] state=tensor([[-0.5331, -0.0111, -0.0212, -0.4305]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5333, -0.2059, -0.0298, -0.1446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 118 ] state=tensor([[-0.5333, -0.2059, -0.0298, -0.1446]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5375, -0.4006, -0.0327,  0.1386]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 119 ] state=tensor([[-0.5375, -0.4006, -0.0327,  0.1386]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5455, -0.5952, -0.0299,  0.4208]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 120 ] state=tensor([[-0.5455, -0.5952, -0.0299,  0.4208]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5574, -0.3997, -0.0215,  0.1188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 121 ] state=tensor([[-0.5574, -0.3997, -0.0215,  0.1188]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5654, -0.2043, -0.0191, -0.1806]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 122 ] state=tensor([[-0.5654, -0.2043, -0.0191, -0.1806]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5695, -0.3991, -0.0227,  0.1060]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 123 ] state=tensor([[-0.5695, -0.3991, -0.0227,  0.1060]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5774, -0.5939, -0.0206,  0.3914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 124 ] state=tensor([[-0.5774, -0.5939, -0.0206,  0.3914]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5893, -0.3985, -0.0128,  0.0923]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 125 ] state=tensor([[-0.5893, -0.3985, -0.0128,  0.0923]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5973, -0.2032, -0.0109, -0.2044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 126 ] state=tensor([[-0.5973, -0.2032, -0.0109, -0.2044]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6013, -0.3981, -0.0150,  0.0848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 127 ] state=tensor([[-0.6013, -0.3981, -0.0150,  0.0848]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6093, -0.2028, -0.0133, -0.2125]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 128 ] state=tensor([[-0.6093, -0.2028, -0.0133, -0.2125]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6134, -0.0075, -0.0176, -0.5094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 129 ] state=tensor([[-0.6134, -0.0075, -0.0176, -0.5094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6135,  0.1879, -0.0278, -0.8076]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 116 ][ timestamp 130 ] state=tensor([[-0.6135,  0.1879, -0.0278, -0.8076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6098, -0.0069, -0.0439, -0.5237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 131 ] state=tensor([[-0.6098, -0.0069, -0.0439, -0.5237]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6099, -0.2013, -0.0544, -0.2452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 132 ] state=tensor([[-0.6099, -0.2013, -0.0544, -0.2452]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6139, -0.3956, -0.0593,  0.0298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 133 ] state=tensor([[-0.6139, -0.3956, -0.0593,  0.0298]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6218, -0.1997, -0.0587, -0.2810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 134 ] state=tensor([[-0.6218, -0.1997, -0.0587, -0.2810]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6258, -0.3940, -0.0643, -0.0073]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 135 ] state=tensor([[-0.6258, -0.3940, -0.0643, -0.0073]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6337, -0.5881, -0.0645,  0.2644]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 136 ] state=tensor([[-0.6337, -0.5881, -0.0645,  0.2644]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6455, -0.7823, -0.0592,  0.5360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 137 ] state=tensor([[-0.6455, -0.7823, -0.0592,  0.5360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6611, -0.5863, -0.0485,  0.2253]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 138 ] state=tensor([[-0.6611, -0.5863, -0.0485,  0.2253]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6728, -0.3906, -0.0439, -0.0822]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 139 ] state=tensor([[-0.6728, -0.3906, -0.0439, -0.0822]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6807, -0.5850, -0.0456,  0.1963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 140 ] state=tensor([[-0.6807, -0.5850, -0.0456,  0.1963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6924, -0.7795, -0.0417,  0.4742]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 141 ] state=tensor([[-0.6924, -0.7795, -0.0417,  0.4742]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7079, -0.5838, -0.0322,  0.1687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 142 ] state=tensor([[-0.7079, -0.5838, -0.0322,  0.1687]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7196, -0.3882, -0.0288, -0.1340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 143 ] state=tensor([[-0.7196, -0.3882, -0.0288, -0.1340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7274, -0.5829, -0.0315,  0.1495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 144 ] state=tensor([[-0.7274, -0.5829, -0.0315,  0.1495]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7390, -0.3874, -0.0285, -0.1530]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 145 ] state=tensor([[-0.7390, -0.3874, -0.0285, -0.1530]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7468, -0.5821, -0.0316,  0.1306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 146 ] state=tensor([[-0.7468, -0.5821, -0.0316,  0.1306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7584, -0.3865, -0.0289, -0.1719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 147 ] state=tensor([[-0.7584, -0.3865, -0.0289, -0.1719]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7662, -0.5812, -0.0324,  0.1115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 148 ] state=tensor([[-0.7662, -0.5812, -0.0324,  0.1115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7778, -0.7758, -0.0302,  0.3938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 149 ] state=tensor([[-0.7778, -0.7758, -0.0302,  0.3938]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7933, -0.5803, -0.0223,  0.0918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 150 ] state=tensor([[-0.7933, -0.5803, -0.0223,  0.0918]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8049, -0.3849, -0.0204, -0.2078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 151 ] state=tensor([[-0.8049, -0.3849, -0.0204, -0.2078]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8126, -0.5797, -0.0246,  0.0783]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 152 ] state=tensor([[-0.8126, -0.5797, -0.0246,  0.0783]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8242, -0.7745, -0.0230,  0.3632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 153 ] state=tensor([[-0.8242, -0.7745, -0.0230,  0.3632]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8397, -0.9692, -0.0158,  0.6485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 154 ] state=tensor([[-0.8397, -0.9692, -0.0158,  0.6485]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8591, -0.7739, -0.0028,  0.3509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 155 ] state=tensor([[-0.8591, -0.7739, -0.0028,  0.3509]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8746, -0.9690,  0.0042,  0.6427]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 156 ] state=tensor([[-0.8746, -0.9690,  0.0042,  0.6427]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8939, -1.1642,  0.0171,  0.9367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 157 ] state=tensor([[-0.8939, -1.1642,  0.0171,  0.9367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9172, -1.3595,  0.0358,  1.2347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 158 ] state=tensor([[-0.9172, -1.3595,  0.0358,  1.2347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9444, -1.5551,  0.0605,  1.5384]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 159 ] state=tensor([[-0.9444, -1.5551,  0.0605,  1.5384]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9755, -1.7509,  0.0913,  1.8493]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 160 ] state=tensor([[-0.9755, -1.7509,  0.0913,  1.8493]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0105, -1.9469,  0.1283,  2.1689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 161 ] state=tensor([[-1.0105, -1.9469,  0.1283,  2.1689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0495, -2.1430,  0.1716,  2.4983]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 116 ][ timestamp 162 ] state=tensor([[-1.0495, -2.1430,  0.1716,  2.4983]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 116: Exploration_rate=0.05. Score=162.\n",
      "[ episode 117 ] state=tensor([[-0.0037,  0.0479, -0.0240,  0.0086]])\n",
      "[ episode 117 ][ timestamp 1 ] state=tensor([[-0.0037,  0.0479, -0.0240,  0.0086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0028,  0.2433, -0.0238, -0.2916]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 2 ] state=tensor([[-0.0028,  0.2433, -0.0238, -0.2916]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0021,  0.0485, -0.0297, -0.0065]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 3 ] state=tensor([[ 0.0021,  0.0485, -0.0297, -0.0065]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0031,  0.2441, -0.0298, -0.3084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 4 ] state=tensor([[ 0.0031,  0.2441, -0.0298, -0.3084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0080,  0.4396, -0.0360, -0.6103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 5 ] state=tensor([[ 0.0080,  0.4396, -0.0360, -0.6103]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0168,  0.2450, -0.0482, -0.3292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 6 ] state=tensor([[ 0.0168,  0.2450, -0.0482, -0.3292]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0217,  0.0506, -0.0547, -0.0521]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 7 ] state=tensor([[ 0.0217,  0.0506, -0.0547, -0.0521]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0227, -0.1437, -0.0558,  0.2229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 8 ] state=tensor([[ 0.0227, -0.1437, -0.0558,  0.2229]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0198, -0.3380, -0.0513,  0.4974]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 9 ] state=tensor([[ 0.0198, -0.3380, -0.0513,  0.4974]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0130, -0.1422, -0.0414,  0.1890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 10 ] state=tensor([[ 0.0130, -0.1422, -0.0414,  0.1890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0102,  0.0535, -0.0376, -0.1164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 11 ] state=tensor([[ 0.0102,  0.0535, -0.0376, -0.1164]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0113, -0.1410, -0.0399,  0.1642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 12 ] state=tensor([[ 0.0113, -0.1410, -0.0399,  0.1642]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0084, -0.3356, -0.0366,  0.4440]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 13 ] state=tensor([[ 0.0084, -0.3356, -0.0366,  0.4440]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0017, -0.1399, -0.0278,  0.1400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 14 ] state=tensor([[ 0.0017, -0.1399, -0.0278,  0.1400]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0011, -0.3347, -0.0250,  0.4238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 15 ] state=tensor([[-0.0011, -0.3347, -0.0250,  0.4238]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0078, -0.1392, -0.0165,  0.1234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 16 ] state=tensor([[-0.0078, -0.1392, -0.0165,  0.1234]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0105,  0.0562, -0.0140, -0.1745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 17 ] state=tensor([[-0.0105,  0.0562, -0.0140, -0.1745]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0094,  0.2515, -0.0175, -0.4716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 18 ] state=tensor([[-0.0094,  0.2515, -0.0175, -0.4716]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0044,  0.0566, -0.0269, -0.1844]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 117 ][ timestamp 19 ] state=tensor([[-0.0044,  0.0566, -0.0269, -0.1844]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0033,  0.2521, -0.0306, -0.4855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 20 ] state=tensor([[-0.0033,  0.2521, -0.0306, -0.4855]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0018,  0.0574, -0.0403, -0.2026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 21 ] state=tensor([[ 0.0018,  0.0574, -0.0403, -0.2026]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0029, -0.1371, -0.0444,  0.0771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 22 ] state=tensor([[ 0.0029, -0.1371, -0.0444,  0.0771]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 1.8710e-04, -3.3155e-01, -4.2848e-02,  3.5542e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 23 ] state=tensor([[ 1.8710e-04, -3.3155e-01, -4.2848e-02,  3.5542e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0064, -0.1358, -0.0357,  0.0495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 24 ] state=tensor([[-0.0064, -0.1358, -0.0357,  0.0495]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0092, -0.3304, -0.0347,  0.3307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 25 ] state=tensor([[-0.0092, -0.3304, -0.0347,  0.3307]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0158, -0.1348, -0.0281,  0.0273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 26 ] state=tensor([[-0.0158, -0.1348, -0.0281,  0.0273]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0185,  0.0607, -0.0276, -0.2741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 27 ] state=tensor([[-0.0185,  0.0607, -0.0276, -0.2741]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0173, -0.1340, -0.0331,  0.0097]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 28 ] state=tensor([[-0.0173, -0.1340, -0.0331,  0.0097]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0199, -0.3287, -0.0329,  0.2918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 29 ] state=tensor([[-0.0199, -0.3287, -0.0329,  0.2918]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0265, -0.1331, -0.0270, -0.0111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 30 ] state=tensor([[-0.0265, -0.1331, -0.0270, -0.0111]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0292,  0.0624, -0.0273, -0.3122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 31 ] state=tensor([[-0.0292,  0.0624, -0.0273, -0.3122]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0279, -0.1323, -0.0335, -0.0282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 32 ] state=tensor([[-0.0279, -0.1323, -0.0335, -0.0282]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0306, -0.3270, -0.0341,  0.2537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 33 ] state=tensor([[-0.0306, -0.3270, -0.0341,  0.2537]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0371, -0.1314, -0.0290, -0.0495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 34 ] state=tensor([[-0.0371, -0.1314, -0.0290, -0.0495]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0397,  0.0642, -0.0300, -0.3512]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 35 ] state=tensor([[-0.0397,  0.0642, -0.0300, -0.3512]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0385, -0.1305, -0.0370, -0.0681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 36 ] state=tensor([[-0.0385, -0.1305, -0.0370, -0.0681]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0411,  0.0651, -0.0384, -0.3722]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 37 ] state=tensor([[-0.0411,  0.0651, -0.0384, -0.3722]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0398, -0.1294, -0.0458, -0.0919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 38 ] state=tensor([[-0.0398, -0.1294, -0.0458, -0.0919]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0423, -0.3239, -0.0477,  0.1860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 39 ] state=tensor([[-0.0423, -0.3239, -0.0477,  0.1860]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0488, -0.5183, -0.0439,  0.4633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 40 ] state=tensor([[-0.0488, -0.5183, -0.0439,  0.4633]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0592, -0.3226, -0.0347,  0.1571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 41 ] state=tensor([[-0.0592, -0.3226, -0.0347,  0.1571]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0656, -0.1270, -0.0315, -0.1463]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 42 ] state=tensor([[-0.0656, -0.1270, -0.0315, -0.1463]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0682,  0.0686, -0.0345, -0.4488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 43 ] state=tensor([[-0.0682,  0.0686, -0.0345, -0.4488]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0668, -0.1260, -0.0434, -0.1672]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 44 ] state=tensor([[-0.0668, -0.1260, -0.0434, -0.1672]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0693, -0.3205, -0.0468,  0.1115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 45 ] state=tensor([[-0.0693, -0.3205, -0.0468,  0.1115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0757, -0.5149, -0.0445,  0.3891]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 46 ] state=tensor([[-0.0757, -0.5149, -0.0445,  0.3891]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0860, -0.7094, -0.0368,  0.6674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 47 ] state=tensor([[-0.0860, -0.7094, -0.0368,  0.6674]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1002, -0.5138, -0.0234,  0.3634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 48 ] state=tensor([[-0.1002, -0.5138, -0.0234,  0.3634]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1105, -0.7086, -0.0161,  0.6486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 49 ] state=tensor([[-0.1105, -0.7086, -0.0161,  0.6486]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1247, -0.5132, -0.0032,  0.3508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 50 ] state=tensor([[-0.1247, -0.5132, -0.0032,  0.3508]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1349, -0.3180,  0.0038,  0.0572]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 51 ] state=tensor([[-0.1349, -0.3180,  0.0038,  0.0572]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1413, -0.5132,  0.0050,  0.3511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 52 ] state=tensor([[-0.1413, -0.5132,  0.0050,  0.3511]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1516, -0.7084,  0.0120,  0.6453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 53 ] state=tensor([[-0.1516, -0.7084,  0.0120,  0.6453]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1657, -0.5135,  0.0249,  0.3564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 54 ] state=tensor([[-0.1657, -0.5135,  0.0249,  0.3564]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1760, -0.3187,  0.0320,  0.0717]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 55 ] state=tensor([[-0.1760, -0.3187,  0.0320,  0.0717]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1824, -0.1241,  0.0335, -0.2107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 56 ] state=tensor([[-0.1824, -0.1241,  0.0335, -0.2107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1849, -0.3196,  0.0293,  0.0924]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 57 ] state=tensor([[-0.1849, -0.3196,  0.0293,  0.0924]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1913, -0.5152,  0.0311,  0.3941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 58 ] state=tensor([[-0.1913, -0.5152,  0.0311,  0.3941]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2016, -0.3205,  0.0390,  0.1114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 59 ] state=tensor([[-0.2016, -0.3205,  0.0390,  0.1114]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2080, -0.1260,  0.0412, -0.1687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 60 ] state=tensor([[-0.2080, -0.1260,  0.0412, -0.1687]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2105,  0.0685,  0.0378, -0.4481]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 61 ] state=tensor([[-0.2105,  0.0685,  0.0378, -0.4481]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2091,  0.2631,  0.0289, -0.7286]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 62 ] state=tensor([[-0.2091,  0.2631,  0.0289, -0.7286]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2038,  0.0676,  0.0143, -0.4270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 63 ] state=tensor([[-0.2038,  0.0676,  0.0143, -0.4270]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2025, -0.1277,  0.0058, -0.1298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 64 ] state=tensor([[-0.2025, -0.1277,  0.0058, -0.1298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2051, -0.3229,  0.0032,  0.1647]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 65 ] state=tensor([[-0.2051, -0.3229,  0.0032,  0.1647]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2115, -0.5181,  0.0065,  0.4583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 66 ] state=tensor([[-0.2115, -0.5181,  0.0065,  0.4583]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2219, -0.3231,  0.0156,  0.1677]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 67 ] state=tensor([[-0.2219, -0.3231,  0.0156,  0.1677]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2283, -0.1282,  0.0190, -0.1200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 68 ] state=tensor([[-0.2283, -0.1282,  0.0190, -0.1200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2309, -0.3236,  0.0166,  0.1786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 69 ] state=tensor([[-0.2309, -0.3236,  0.0166,  0.1786]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2374, -0.1287,  0.0202, -0.1088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 70 ] state=tensor([[-0.2374, -0.1287,  0.0202, -0.1088]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2399,  0.0662,  0.0180, -0.3951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 71 ] state=tensor([[-0.2399,  0.0662,  0.0180, -0.3951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2386, -0.1292,  0.0101, -0.0968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 72 ] state=tensor([[-0.2386, -0.1292,  0.0101, -0.0968]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2412,  0.0658,  0.0081, -0.3863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 73 ] state=tensor([[-0.2412,  0.0658,  0.0081, -0.3863]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2399, -0.1295,  0.0004, -0.0910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 74 ] state=tensor([[-0.2399, -0.1295,  0.0004, -0.0910]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2425,  0.0656, -0.0014, -0.3836]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 75 ] state=tensor([[-0.2425,  0.0656, -0.0014, -0.3836]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2412, -0.1295, -0.0091, -0.0913]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 76 ] state=tensor([[-0.2412, -0.1295, -0.0091, -0.0913]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2438,  0.0658, -0.0109, -0.3869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 77 ] state=tensor([[-0.2438,  0.0658, -0.0109, -0.3869]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2424, -0.1292, -0.0186, -0.0976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 78 ] state=tensor([[-0.2424, -0.1292, -0.0186, -0.0976]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2450,  0.0662, -0.0206, -0.3961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 79 ] state=tensor([[-0.2450,  0.0662, -0.0206, -0.3961]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2437, -0.1286, -0.0285, -0.1100]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 80 ] state=tensor([[-0.2437, -0.1286, -0.0285, -0.1100]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2463,  0.0669, -0.0307, -0.4115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 81 ] state=tensor([[-0.2463,  0.0669, -0.0307, -0.4115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2449, -0.1278, -0.0389, -0.1287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 82 ] state=tensor([[-0.2449, -0.1278, -0.0389, -0.1287]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2475, -0.3223, -0.0415,  0.1515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 83 ] state=tensor([[-0.2475, -0.3223, -0.0415,  0.1515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2539, -0.5168, -0.0385,  0.4308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 84 ] state=tensor([[-0.2539, -0.5168, -0.0385,  0.4308]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2643, -0.3212, -0.0299,  0.1262]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 85 ] state=tensor([[-0.2643, -0.3212, -0.0299,  0.1262]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2707, -0.1256, -0.0273, -0.1758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 86 ] state=tensor([[-0.2707, -0.1256, -0.0273, -0.1758]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2732, -0.3204, -0.0309,  0.1082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 87 ] state=tensor([[-0.2732, -0.3204, -0.0309,  0.1082]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2796, -0.5150, -0.0287,  0.3910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 88 ] state=tensor([[-0.2796, -0.5150, -0.0287,  0.3910]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2899, -0.7097, -0.0209,  0.6745]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 117 ][ timestamp 89 ] state=tensor([[-0.2899, -0.7097, -0.0209,  0.6745]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3041, -0.5143, -0.0074,  0.3753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 90 ] state=tensor([[-0.3041, -0.5143, -0.0074,  0.3753]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.1440e-01, -3.1910e-01,  1.1609e-04,  8.0270e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 91 ] state=tensor([[-3.1440e-01, -3.1910e-01,  1.1609e-04,  8.0270e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3208, -0.5142,  0.0017,  0.3730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 92 ] state=tensor([[-0.3208, -0.5142,  0.0017,  0.3730]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3311, -0.3191,  0.0092,  0.0808]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 93 ] state=tensor([[-0.3311, -0.3191,  0.0092,  0.0808]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3374, -0.1241,  0.0108, -0.2089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 94 ] state=tensor([[-0.3374, -0.1241,  0.0108, -0.2089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3399, -0.3194,  0.0066,  0.0871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 95 ] state=tensor([[-0.3399, -0.3194,  0.0066,  0.0871]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3463, -0.5146,  0.0084,  0.3819]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 96 ] state=tensor([[-0.3463, -0.5146,  0.0084,  0.3819]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3566, -0.3196,  0.0160,  0.0919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 97 ] state=tensor([[-0.3566, -0.3196,  0.0160,  0.0919]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3630, -0.1247,  0.0178, -0.1957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 98 ] state=tensor([[-0.3630, -0.1247,  0.0178, -0.1957]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3655,  0.0701,  0.0139, -0.4827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 99 ] state=tensor([[-0.3655,  0.0701,  0.0139, -0.4827]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3641, -0.1252,  0.0043, -0.1857]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 100 ] state=tensor([[-0.3641, -0.1252,  0.0043, -0.1857]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3666, -0.3204,  0.0006,  0.1083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 101 ] state=tensor([[-0.3666, -0.3204,  0.0006,  0.1083]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3730, -0.1253,  0.0027, -0.1842]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 102 ] state=tensor([[-0.3730, -0.1253,  0.0027, -0.1842]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3755, -0.3204, -0.0010,  0.1094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 103 ] state=tensor([[-0.3755, -0.3204, -0.0010,  0.1094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3819, -0.1253,  0.0012, -0.1836]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 104 ] state=tensor([[-0.3819, -0.1253,  0.0012, -0.1836]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3844, -0.3204, -0.0024,  0.1095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 105 ] state=tensor([[-0.3844, -0.3204, -0.0024,  0.1095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.9083e-01, -1.2526e-01, -2.5510e-04, -1.8399e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 106 ] state=tensor([[-3.9083e-01, -1.2526e-01, -2.5510e-04, -1.8399e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3933,  0.0699, -0.0039, -0.4768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 107 ] state=tensor([[-0.3933,  0.0699, -0.0039, -0.4768]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3919, -0.1252, -0.0135, -0.1853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 108 ] state=tensor([[-0.3919, -0.1252, -0.0135, -0.1853]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3944,  0.0701, -0.0172, -0.4822]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 109 ] state=tensor([[-0.3944,  0.0701, -0.0172, -0.4822]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3930, -0.1248, -0.0268, -0.1950]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 110 ] state=tensor([[-0.3930, -0.1248, -0.0268, -0.1950]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3955,  0.0707, -0.0307, -0.4960]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 111 ] state=tensor([[-0.3955,  0.0707, -0.0307, -0.4960]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3941, -0.1239, -0.0406, -0.2132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 112 ] state=tensor([[-0.3941, -0.1239, -0.0406, -0.2132]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3966, -0.3185, -0.0449,  0.0664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 113 ] state=tensor([[-0.3966, -0.3185, -0.0449,  0.0664]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4030, -0.1227, -0.0436, -0.2401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 114 ] state=tensor([[-0.4030, -0.1227, -0.0436, -0.2401]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4054, -0.3172, -0.0484,  0.0385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 115 ] state=tensor([[-0.4054, -0.3172, -0.0484,  0.0385]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4118, -0.5116, -0.0476,  0.3156]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 116 ] state=tensor([[-0.4118, -0.5116, -0.0476,  0.3156]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4220, -0.7060, -0.0413,  0.5929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 117 ] state=tensor([[-0.4220, -0.7060, -0.0413,  0.5929]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4361, -0.5103, -0.0294,  0.2875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 118 ] state=tensor([[-0.4361, -0.5103, -0.0294,  0.2875]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4463, -0.3148, -0.0237, -0.0143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 119 ] state=tensor([[-0.4463, -0.3148, -0.0237, -0.0143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4526, -0.1194, -0.0240, -0.3144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 120 ] state=tensor([[-0.4526, -0.1194, -0.0240, -0.3144]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4550,  0.0761, -0.0303, -0.6146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 121 ] state=tensor([[-0.4550,  0.0761, -0.0303, -0.6146]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4535, -0.1186, -0.0426, -0.3316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 122 ] state=tensor([[-0.4535, -0.1186, -0.0426, -0.3316]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4559, -0.3131, -0.0492, -0.0526]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 123 ] state=tensor([[-0.4559, -0.3131, -0.0492, -0.0526]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4621, -0.5075, -0.0502,  0.2242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 124 ] state=tensor([[-0.4621, -0.5075, -0.0502,  0.2242]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4723, -0.7018, -0.0458,  0.5006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 125 ] state=tensor([[-0.4723, -0.7018, -0.0458,  0.5006]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4863, -0.5061, -0.0357,  0.1939]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 126 ] state=tensor([[-0.4863, -0.5061, -0.0357,  0.1939]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4964, -0.7007, -0.0319,  0.4751]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 127 ] state=tensor([[-0.4964, -0.7007, -0.0319,  0.4751]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5104, -0.5051, -0.0224,  0.1725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 128 ] state=tensor([[-0.5104, -0.5051, -0.0224,  0.1725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5205, -0.3097, -0.0189, -0.1271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 129 ] state=tensor([[-0.5205, -0.3097, -0.0189, -0.1271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5267, -0.5045, -0.0215,  0.1595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 130 ] state=tensor([[-0.5267, -0.5045, -0.0215,  0.1595]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5368, -0.3091, -0.0183, -0.1399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 131 ] state=tensor([[-0.5368, -0.3091, -0.0183, -0.1399]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5430, -0.5040, -0.0211,  0.1470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 132 ] state=tensor([[-0.5430, -0.5040, -0.0211,  0.1470]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5531, -0.3086, -0.0181, -0.1522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 133 ] state=tensor([[-0.5531, -0.3086, -0.0181, -0.1522]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5593, -0.5034, -0.0212,  0.1347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 134 ] state=tensor([[-0.5593, -0.5034, -0.0212,  0.1347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5693, -0.3080, -0.0185, -0.1646]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 135 ] state=tensor([[-0.5693, -0.3080, -0.0185, -0.1646]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5755, -0.5028, -0.0218,  0.1222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 136 ] state=tensor([[-0.5755, -0.5028, -0.0218,  0.1222]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5855, -0.6977, -0.0193,  0.4079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 137 ] state=tensor([[-0.5855, -0.6977, -0.0193,  0.4079]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5995, -0.5023, -0.0112,  0.1092]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 138 ] state=tensor([[-0.5995, -0.5023, -0.0112,  0.1092]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6095, -0.6972, -0.0090,  0.3983]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 139 ] state=tensor([[-0.6095, -0.6972, -0.0090,  0.3983]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6235, -0.5020, -0.0010,  0.1028]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 140 ] state=tensor([[-0.6235, -0.5020, -0.0010,  0.1028]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6335, -0.3068,  0.0010, -0.1902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 141 ] state=tensor([[-0.6335, -0.3068,  0.0010, -0.1902]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6397, -0.5020, -0.0028,  0.1029]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 117 ][ timestamp 142 ] state=tensor([[-0.6397, -0.5020, -0.0028,  0.1029]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6497, -0.6971, -0.0007,  0.3947]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 143 ] state=tensor([[-0.6497, -0.6971, -0.0007,  0.3947]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6636, -0.5019,  0.0072,  0.1018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 144 ] state=tensor([[-0.6636, -0.5019,  0.0072,  0.1018]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6737, -0.6971,  0.0092,  0.3967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 145 ] state=tensor([[-0.6737, -0.6971,  0.0092,  0.3967]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6876, -0.5022,  0.0172,  0.1069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 146 ] state=tensor([[-0.6876, -0.5022,  0.0172,  0.1069]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6977, -0.3073,  0.0193, -0.1803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 147 ] state=tensor([[-0.6977, -0.3073,  0.0193, -0.1803]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7038, -0.5027,  0.0157,  0.1184]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 148 ] state=tensor([[-0.7038, -0.5027,  0.0157,  0.1184]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7139, -0.3078,  0.0181, -0.1693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 149 ] state=tensor([[-0.7139, -0.3078,  0.0181, -0.1693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7200, -0.5032,  0.0147,  0.1291]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 150 ] state=tensor([[-0.7200, -0.5032,  0.0147,  0.1291]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7301, -0.6985,  0.0173,  0.4263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 151 ] state=tensor([[-0.7301, -0.6985,  0.0173,  0.4263]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7441, -0.5036,  0.0258,  0.1392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 152 ] state=tensor([[-0.7441, -0.5036,  0.0258,  0.1392]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7541, -0.3089,  0.0286, -0.1453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 153 ] state=tensor([[-0.7541, -0.3089,  0.0286, -0.1453]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7603, -0.1142,  0.0257, -0.4288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 154 ] state=tensor([[-0.7603, -0.1142,  0.0257, -0.4288]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7626, -0.3096,  0.0171, -0.1282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 155 ] state=tensor([[-0.7626, -0.3096,  0.0171, -0.1282]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7688, -0.1148,  0.0145, -0.4154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 156 ] state=tensor([[-0.7688, -0.1148,  0.0145, -0.4154]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7711, -0.3101,  0.0062, -0.1182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 157 ] state=tensor([[-0.7711, -0.3101,  0.0062, -0.1182]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7773, -0.5053,  0.0039,  0.1765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 158 ] state=tensor([[-0.7773, -0.5053,  0.0039,  0.1765]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7874, -0.7005,  0.0074,  0.4704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 159 ] state=tensor([[-0.7874, -0.7005,  0.0074,  0.4704]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8014, -0.5055,  0.0168,  0.1800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 160 ] state=tensor([[-0.8014, -0.5055,  0.0168,  0.1800]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8115, -0.3106,  0.0204, -0.1073]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 161 ] state=tensor([[-0.8115, -0.3106,  0.0204, -0.1073]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8177, -0.1158,  0.0182, -0.3935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 162 ] state=tensor([[-0.8177, -0.1158,  0.0182, -0.3935]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8200, -0.3111,  0.0104, -0.0951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 163 ] state=tensor([[-0.8200, -0.3111,  0.0104, -0.0951]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8263, -0.1162,  0.0085, -0.3845]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 164 ] state=tensor([[-0.8263, -0.1162,  0.0085, -0.3845]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.2858e-01,  7.8828e-02,  7.7851e-04, -6.7453e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 165 ] state=tensor([[-8.2858e-01,  7.8828e-02,  7.7851e-04, -6.7453e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8270, -0.1163, -0.0127, -0.3816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 166 ] state=tensor([[-0.8270, -0.1163, -0.0127, -0.3816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8293, -0.3112, -0.0203, -0.0930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 167 ] state=tensor([[-0.8293, -0.3112, -0.0203, -0.0930]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8356, -0.5061, -0.0222,  0.1932]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 168 ] state=tensor([[-0.8356, -0.5061, -0.0222,  0.1932]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8457, -0.7009, -0.0183,  0.4788]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 169 ] state=tensor([[-0.8457, -0.7009, -0.0183,  0.4788]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8597, -0.8957, -0.0088,  0.7657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 170 ] state=tensor([[-0.8597, -0.8957, -0.0088,  0.7657]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8776, -1.0907,  0.0066,  1.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 171 ] state=tensor([[-0.8776, -1.0907,  0.0066,  1.0556]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8994, -1.2859,  0.0277,  1.3503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 172 ] state=tensor([[-0.8994, -1.2859,  0.0277,  1.3503]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9251, -1.4814,  0.0547,  1.6515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 173 ] state=tensor([[-0.9251, -1.4814,  0.0547,  1.6515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9548, -1.6771,  0.0877,  1.9607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 174 ] state=tensor([[-0.9548, -1.6771,  0.0877,  1.9607]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9883, -1.8730,  0.1269,  2.2793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 175 ] state=tensor([[-0.9883, -1.8730,  0.1269,  2.2793]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0258, -2.0691,  0.1725,  2.6082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 117 ][ timestamp 176 ] state=tensor([[-1.0258, -2.0691,  0.1725,  2.6082]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 117: Exploration_rate=0.05. Score=176.\n",
      "[ episode 118 ] state=tensor([[ 0.0012, -0.0227,  0.0418,  0.0374]])\n",
      "[ episode 118 ][ timestamp 1 ] state=tensor([[ 0.0012, -0.0227,  0.0418,  0.0374]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0007,  0.1718,  0.0426, -0.2418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 2 ] state=tensor([[ 0.0007,  0.1718,  0.0426, -0.2418]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0041,  0.3663,  0.0377, -0.5207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 3 ] state=tensor([[ 0.0041,  0.3663,  0.0377, -0.5207]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0115,  0.5608,  0.0273, -0.8013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 4 ] state=tensor([[ 0.0115,  0.5608,  0.0273, -0.8013]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0227,  0.3653,  0.0113, -0.5002]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 5 ] state=tensor([[ 0.0227,  0.3653,  0.0113, -0.5002]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0300,  0.1701,  0.0013, -0.2040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 6 ] state=tensor([[ 0.0300,  0.1701,  0.0013, -0.2040]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0334,  0.3652, -0.0028, -0.4962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 7 ] state=tensor([[ 0.0334,  0.3652, -0.0028, -0.4962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0407,  0.1701, -0.0127, -0.2044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 8 ] state=tensor([[ 0.0407,  0.1701, -0.0127, -0.2044]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0441,  0.3654, -0.0168, -0.5011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 9 ] state=tensor([[ 0.0441,  0.3654, -0.0168, -0.5011]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0514,  0.1705, -0.0268, -0.2138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 10 ] state=tensor([[ 0.0514,  0.1705, -0.0268, -0.2138]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0548,  0.3660, -0.0311, -0.5148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 11 ] state=tensor([[ 0.0548,  0.3660, -0.0311, -0.5148]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0621,  0.1713, -0.0414, -0.2321]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 12 ] state=tensor([[ 0.0621,  0.1713, -0.0414, -0.2321]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0656,  0.3670, -0.0461, -0.5375]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 13 ] state=tensor([[ 0.0656,  0.3670, -0.0461, -0.5375]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0729,  0.1726, -0.0568, -0.2597]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 14 ] state=tensor([[ 0.0729,  0.1726, -0.0568, -0.2597]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0764, -0.0217, -0.0620,  0.0145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 15 ] state=tensor([[ 0.0764, -0.0217, -0.0620,  0.0145]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0759, -0.2159, -0.0617,  0.2870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 16 ] state=tensor([[ 0.0759, -0.2159, -0.0617,  0.2870]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0716, -0.0199, -0.0560, -0.0245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 17 ] state=tensor([[ 0.0716, -0.0199, -0.0560, -0.0245]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0712, -0.2142, -0.0565,  0.2500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 18 ] state=tensor([[ 0.0712, -0.2142, -0.0565,  0.2500]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0669, -0.4085, -0.0515,  0.5244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 19 ] state=tensor([[ 0.0669, -0.4085, -0.0515,  0.5244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0588, -0.6028, -0.0410,  0.8004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 20 ] state=tensor([[ 0.0588, -0.6028, -0.0410,  0.8004]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0467, -0.4072, -0.0250,  0.4951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 21 ] state=tensor([[ 0.0467, -0.4072, -0.0250,  0.4951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0386, -0.6019, -0.0151,  0.7798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 22 ] state=tensor([[ 0.0386, -0.6019, -0.0151,  0.7798]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0265, -0.4066,  0.0005,  0.4825]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 23 ] state=tensor([[ 0.0265, -0.4066,  0.0005,  0.4825]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0184, -0.6017,  0.0102,  0.7753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 24 ] state=tensor([[ 0.0184, -0.6017,  0.0102,  0.7753]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0063, -0.4068,  0.0257,  0.4859]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 25 ] state=tensor([[ 0.0063, -0.4068,  0.0257,  0.4859]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0018, -0.2120,  0.0354,  0.2014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 26 ] state=tensor([[-0.0018, -0.2120,  0.0354,  0.2014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0060, -0.0174,  0.0394, -0.0799]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 27 ] state=tensor([[-0.0060, -0.0174,  0.0394, -0.0799]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0064,  0.1771,  0.0378, -0.3599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 28 ] state=tensor([[-0.0064,  0.1771,  0.0378, -0.3599]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0028,  0.3717,  0.0306, -0.6404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 29 ] state=tensor([[-0.0028,  0.3717,  0.0306, -0.6404]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0046,  0.5664,  0.0178, -0.9233]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 118 ][ timestamp 30 ] state=tensor([[ 0.0046,  0.5664,  0.0178, -0.9233]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0159,  0.3710, -0.0006, -0.6251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 31 ] state=tensor([[ 0.0159,  0.3710, -0.0006, -0.6251]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0233,  0.1759, -0.0131, -0.3326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 32 ] state=tensor([[ 0.0233,  0.1759, -0.0131, -0.3326]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0269,  0.3712, -0.0198, -0.6294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 33 ] state=tensor([[ 0.0269,  0.3712, -0.0198, -0.6294]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0343,  0.1764, -0.0324, -0.3430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 34 ] state=tensor([[ 0.0343,  0.1764, -0.0324, -0.3430]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0378, -0.0183, -0.0392, -0.0607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 35 ] state=tensor([[ 0.0378, -0.0183, -0.0392, -0.0607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0375,  0.1774, -0.0405, -0.3655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 36 ] state=tensor([[ 0.0375,  0.1774, -0.0405, -0.3655]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0410, -0.0171, -0.0478, -0.0858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 37 ] state=tensor([[ 0.0410, -0.0171, -0.0478, -0.0858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0407, -0.2115, -0.0495,  0.1914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 38 ] state=tensor([[ 0.0407, -0.2115, -0.0495,  0.1914]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0364, -0.4059, -0.0457,  0.4681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 39 ] state=tensor([[ 0.0364, -0.4059, -0.0457,  0.4681]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0283, -0.2102, -0.0363,  0.1614]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 40 ] state=tensor([[ 0.0283, -0.2102, -0.0363,  0.1614]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0241, -0.4048, -0.0331,  0.4424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 41 ] state=tensor([[ 0.0241, -0.4048, -0.0331,  0.4424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0160, -0.2092, -0.0242,  0.1395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 42 ] state=tensor([[ 0.0160, -0.2092, -0.0242,  0.1395]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0118, -0.4040, -0.0214,  0.4244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 43 ] state=tensor([[ 0.0118, -0.4040, -0.0214,  0.4244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0037, -0.2086, -0.0129,  0.1250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 44 ] state=tensor([[ 0.0037, -0.2086, -0.0129,  0.1250]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0004, -0.4035, -0.0104,  0.4136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 45 ] state=tensor([[-0.0004, -0.4035, -0.0104,  0.4136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0085, -0.2082, -0.0022,  0.1177]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 46 ] state=tensor([[-0.0085, -0.2082, -0.0022,  0.1177]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0127, -0.0131,  0.0002, -0.1757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 47 ] state=tensor([[-0.0127, -0.0131,  0.0002, -0.1757]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0129,  0.1821, -0.0033, -0.4683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 48 ] state=tensor([[-0.0129,  0.1821, -0.0033, -0.4683]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0093,  0.3772, -0.0127, -0.7621]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 49 ] state=tensor([[-0.0093,  0.3772, -0.0127, -0.7621]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0017,  0.1823, -0.0279, -0.4734]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 50 ] state=tensor([[-0.0017,  0.1823, -0.0279, -0.4734]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0019, -0.0124, -0.0374, -0.1896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 51 ] state=tensor([[ 0.0019, -0.0124, -0.0374, -0.1896]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0017,  0.1832, -0.0412, -0.4939]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 52 ] state=tensor([[ 0.0017,  0.1832, -0.0412, -0.4939]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0053, -0.0113, -0.0511, -0.2145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 53 ] state=tensor([[ 0.0053, -0.0113, -0.0511, -0.2145]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0051, -0.2057, -0.0554,  0.0617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 54 ] state=tensor([[ 0.0051, -0.2057, -0.0554,  0.0617]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0010, -0.4000, -0.0541,  0.3364]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 55 ] state=tensor([[ 0.0010, -0.4000, -0.0541,  0.3364]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0070, -0.5943, -0.0474,  0.6115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 56 ] state=tensor([[-0.0070, -0.5943, -0.0474,  0.6115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0189, -0.3985, -0.0352,  0.3043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 57 ] state=tensor([[-0.0189, -0.3985, -0.0352,  0.3043]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0269, -0.2029, -0.0291,  0.0007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 58 ] state=tensor([[-0.0269, -0.2029, -0.0291,  0.0007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0309, -0.3976, -0.0291,  0.2841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 59 ] state=tensor([[-0.0309, -0.3976, -0.0291,  0.2841]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0389, -0.2021, -0.0234, -0.0176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 60 ] state=tensor([[-0.0389, -0.2021, -0.0234, -0.0176]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0429, -0.0066, -0.0237, -0.3176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 61 ] state=tensor([[-0.0429, -0.0066, -0.0237, -0.3176]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0431, -0.2014, -0.0301, -0.0325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 62 ] state=tensor([[-0.0431, -0.2014, -0.0301, -0.0325]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0471, -0.3961, -0.0307,  0.2506]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 63 ] state=tensor([[-0.0471, -0.3961, -0.0307,  0.2506]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0550, -0.5908, -0.0257,  0.5334]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 64 ] state=tensor([[-0.0550, -0.5908, -0.0257,  0.5334]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0668, -0.3953, -0.0151,  0.2327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 65 ] state=tensor([[-0.0668, -0.3953, -0.0151,  0.2327]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0747, -0.5902, -0.0104,  0.5206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 66 ] state=tensor([[-0.0747, -0.5902, -0.0104,  0.5206]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.6532e-02, -3.9492e-01,  4.4844e-06,  2.2466e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 67 ] state=tensor([[-8.6532e-02, -3.9492e-01,  4.4844e-06,  2.2466e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0944, -0.5900,  0.0045,  0.5173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 68 ] state=tensor([[-0.0944, -0.5900,  0.0045,  0.5173]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1062, -0.3950,  0.0148,  0.2261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 69 ] state=tensor([[-0.1062, -0.3950,  0.0148,  0.2261]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1141, -0.2001,  0.0194, -0.0619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 70 ] state=tensor([[-0.1141, -0.2001,  0.0194, -0.0619]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1181, -0.0052,  0.0181, -0.3484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 71 ] state=tensor([[-0.1181, -0.0052,  0.0181, -0.3484]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1182,  0.1896,  0.0112, -0.6353]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 72 ] state=tensor([[-0.1182,  0.1896,  0.0112, -0.6353]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1144, -0.0057, -0.0015, -0.3391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 73 ] state=tensor([[-0.1144, -0.0057, -0.0015, -0.3391]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1146, -0.2008, -0.0083, -0.0469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 74 ] state=tensor([[-0.1146, -0.2008, -0.0083, -0.0469]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1186, -0.3958, -0.0093,  0.2431]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 75 ] state=tensor([[-0.1186, -0.3958, -0.0093,  0.2431]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1265, -0.2005, -0.0044, -0.0525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 76 ] state=tensor([[-0.1265, -0.2005, -0.0044, -0.0525]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1305, -0.0053, -0.0055, -0.3465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 77 ] state=tensor([[-0.1305, -0.0053, -0.0055, -0.3465]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1306, -0.2004, -0.0124, -0.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 78 ] state=tensor([[-0.1306, -0.2004, -0.0124, -0.0556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1346, -0.0051, -0.0135, -0.3521]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 79 ] state=tensor([[-0.1346, -0.0051, -0.0135, -0.3521]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1347, -0.2000, -0.0205, -0.0638]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 80 ] state=tensor([[-0.1347, -0.2000, -0.0205, -0.0638]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1387, -0.3948, -0.0218,  0.2224]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 81 ] state=tensor([[-0.1387, -0.3948, -0.0218,  0.2224]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1466, -0.5896, -0.0174,  0.5081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 82 ] state=tensor([[-0.1466, -0.5896, -0.0174,  0.5081]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1584, -0.3943, -0.0072,  0.2100]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 83 ] state=tensor([[-0.1584, -0.3943, -0.0072,  0.2100]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1663, -0.5893, -0.0030,  0.5004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 84 ] state=tensor([[-0.1663, -0.5893, -0.0030,  0.5004]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1781, -0.3941,  0.0070,  0.2068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 85 ] state=tensor([[-0.1781, -0.3941,  0.0070,  0.2068]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1860, -0.1991,  0.0111, -0.0837]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 86 ] state=tensor([[-0.1860, -0.1991,  0.0111, -0.0837]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1899, -0.0041,  0.0095, -0.3728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 87 ] state=tensor([[-0.1899, -0.0041,  0.0095, -0.3728]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1900, -0.1994,  0.0020, -0.0772]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 88 ] state=tensor([[-0.1900, -0.1994,  0.0020, -0.0772]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1940, -0.3945,  0.0005,  0.2161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 89 ] state=tensor([[-0.1940, -0.3945,  0.0005,  0.2161]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2019, -0.1994,  0.0048, -0.0764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 90 ] state=tensor([[-0.2019, -0.1994,  0.0048, -0.0764]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2059, -0.3946,  0.0033,  0.2178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 91 ] state=tensor([[-0.2059, -0.3946,  0.0033,  0.2178]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2138, -0.1995,  0.0076, -0.0739]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 92 ] state=tensor([[-0.2138, -0.1995,  0.0076, -0.0739]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2178, -0.3948,  0.0061,  0.2212]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 93 ] state=tensor([[-0.2178, -0.3948,  0.0061,  0.2212]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2257, -0.5900,  0.0106,  0.5158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 94 ] state=tensor([[-0.2257, -0.5900,  0.0106,  0.5158]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2375, -0.3950,  0.0209,  0.2265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 95 ] state=tensor([[-0.2375, -0.3950,  0.0209,  0.2265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2454, -0.2002,  0.0254, -0.0595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 96 ] state=tensor([[-0.2454, -0.2002,  0.0254, -0.0595]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2494, -0.0054,  0.0242, -0.3441]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 97 ] state=tensor([[-0.2494, -0.0054,  0.0242, -0.3441]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2495,  0.1893,  0.0173, -0.6291]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 98 ] state=tensor([[-0.2495,  0.1893,  0.0173, -0.6291]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2457,  0.3842,  0.0048, -0.9162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 99 ] state=tensor([[-0.2457,  0.3842,  0.0048, -0.9162]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2380,  0.1890, -0.0136, -0.6221]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 118 ][ timestamp 100 ] state=tensor([[-0.2380,  0.1890, -0.0136, -0.6221]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2342, -0.0059, -0.0260, -0.3337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 101 ] state=tensor([[-0.2342, -0.0059, -0.0260, -0.3337]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2343, -0.2007, -0.0327, -0.0493]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 102 ] state=tensor([[-0.2343, -0.2007, -0.0327, -0.0493]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2384, -0.3953, -0.0337,  0.2329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 103 ] state=tensor([[-0.2384, -0.3953, -0.0337,  0.2329]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2463, -0.1997, -0.0290, -0.0702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 104 ] state=tensor([[-0.2463, -0.1997, -0.0290, -0.0702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2503, -0.3944, -0.0304,  0.2132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 105 ] state=tensor([[-0.2503, -0.3944, -0.0304,  0.2132]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2581, -0.5891, -0.0262,  0.4961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 106 ] state=tensor([[-0.2581, -0.5891, -0.0262,  0.4961]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2699, -0.3936, -0.0162,  0.1953]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 107 ] state=tensor([[-0.2699, -0.3936, -0.0162,  0.1953]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2778, -0.1982, -0.0123, -0.1025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 108 ] state=tensor([[-0.2778, -0.1982, -0.0123, -0.1025]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2818, -0.3932, -0.0144,  0.1863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 109 ] state=tensor([[-0.2818, -0.3932, -0.0144,  0.1863]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2896, -0.1979, -0.0107, -0.1109]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 110 ] state=tensor([[-0.2896, -0.1979, -0.0107, -0.1109]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2936, -0.3928, -0.0129,  0.1784]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 111 ] state=tensor([[-0.2936, -0.3928, -0.0129,  0.1784]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3014, -0.5878, -0.0093,  0.4670]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 112 ] state=tensor([[-0.3014, -0.5878, -0.0093,  0.4670]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-3.1319e-01, -7.8275e-01,  4.1064e-05,  7.5676e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 113 ] state=tensor([[-3.1319e-01, -7.8275e-01,  4.1064e-05,  7.5676e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3288, -0.5876,  0.0152,  0.4641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 114 ] state=tensor([[-0.3288, -0.5876,  0.0152,  0.4641]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3406, -0.3927,  0.0245,  0.1762]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 115 ] state=tensor([[-0.3406, -0.3927,  0.0245,  0.1762]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3485, -0.1980,  0.0280, -0.1086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 116 ] state=tensor([[-0.3485, -0.1980,  0.0280, -0.1086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3524, -0.0033,  0.0258, -0.3924]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 117 ] state=tensor([[-0.3524, -0.0033,  0.0258, -0.3924]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3525, -0.1987,  0.0180, -0.0917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 118 ] state=tensor([[-0.3525, -0.1987,  0.0180, -0.0917]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3565, -0.0039,  0.0161, -0.3786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 119 ] state=tensor([[-0.3565, -0.0039,  0.0161, -0.3786]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3565, -0.1992,  0.0086, -0.0809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 120 ] state=tensor([[-0.3565, -0.1992,  0.0086, -0.0809]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3605, -0.0042,  0.0069, -0.3709]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 121 ] state=tensor([[-0.3605, -0.0042,  0.0069, -0.3709]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3606, -0.1994, -0.0005, -0.0760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 122 ] state=tensor([[-0.3606, -0.1994, -0.0005, -0.0760]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3646, -0.3946, -0.0020,  0.2165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 123 ] state=tensor([[-0.3646, -0.3946, -0.0020,  0.2165]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3725, -0.1994,  0.0023, -0.0768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 124 ] state=tensor([[-0.3725, -0.1994,  0.0023, -0.0768]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3765, -0.0043,  0.0008, -0.3687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 125 ] state=tensor([[-0.3765, -0.0043,  0.0008, -0.3687]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3766, -0.1994, -0.0066, -0.0758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 126 ] state=tensor([[-0.3766, -0.1994, -0.0066, -0.0758]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3805, -0.3945, -0.0081,  0.2148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 127 ] state=tensor([[-0.3805, -0.3945, -0.0081,  0.2148]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3884, -0.1992, -0.0038, -0.0804]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 128 ] state=tensor([[-0.3884, -0.1992, -0.0038, -0.0804]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3924, -0.0041, -0.0054, -0.3743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 129 ] state=tensor([[-0.3924, -0.0041, -0.0054, -0.3743]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3925, -0.1991, -0.0129, -0.0833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 130 ] state=tensor([[-0.3925, -0.1991, -0.0129, -0.0833]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3965, -0.3940, -0.0146,  0.2053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 131 ] state=tensor([[-0.3965, -0.3940, -0.0146,  0.2053]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4044, -0.1987, -0.0105, -0.0920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 132 ] state=tensor([[-0.4044, -0.1987, -0.0105, -0.0920]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4083, -0.0034, -0.0123, -0.3879]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 133 ] state=tensor([[-0.4083, -0.0034, -0.0123, -0.3879]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4084, -0.1984, -0.0201, -0.0992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 134 ] state=tensor([[-0.4084, -0.1984, -0.0201, -0.0992]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4124, -0.3932, -0.0220,  0.1871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 135 ] state=tensor([[-0.4124, -0.3932, -0.0220,  0.1871]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4202, -0.1978, -0.0183, -0.1124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 136 ] state=tensor([[-0.4202, -0.1978, -0.0183, -0.1124]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4242, -0.0024, -0.0205, -0.4108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 137 ] state=tensor([[-0.4242, -0.0024, -0.0205, -0.4108]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4242, -0.1972, -0.0288, -0.1247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 138 ] state=tensor([[-0.4242, -0.1972, -0.0288, -0.1247]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4282, -0.3919, -0.0313,  0.1588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 139 ] state=tensor([[-0.4282, -0.3919, -0.0313,  0.1588]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4360, -0.5866, -0.0281,  0.4414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 140 ] state=tensor([[-0.4360, -0.5866, -0.0281,  0.4414]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4478, -0.3911, -0.0192,  0.1400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 141 ] state=tensor([[-0.4478, -0.3911, -0.0192,  0.1400]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4556, -0.5859, -0.0164,  0.4266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 142 ] state=tensor([[-0.4556, -0.5859, -0.0164,  0.4266]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4673, -0.3906, -0.0079,  0.1288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 143 ] state=tensor([[-0.4673, -0.3906, -0.0079,  0.1288]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4751, -0.5856, -0.0053,  0.4189]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 144 ] state=tensor([[-0.4751, -0.5856, -0.0053,  0.4189]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4868, -0.3904,  0.0030,  0.1246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 145 ] state=tensor([[-0.4868, -0.3904,  0.0030,  0.1246]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4946, -0.1953,  0.0055, -0.1671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 146 ] state=tensor([[-0.4946, -0.1953,  0.0055, -0.1671]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.9853e-01, -2.6480e-04,  2.1854e-03, -4.5808e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 147 ] state=tensor([[-4.9853e-01, -2.6480e-04,  2.1854e-03, -4.5808e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4985, -0.1954, -0.0070, -0.1647]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 148 ] state=tensor([[-0.4985, -0.1954, -0.0070, -0.1647]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5024, -0.3904, -0.0103,  0.1258]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 149 ] state=tensor([[-0.5024, -0.3904, -0.0103,  0.1258]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5103, -0.5854, -0.0078,  0.4152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 150 ] state=tensor([[-0.5103, -0.5854, -0.0078,  0.4152]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5220, -0.3902,  0.0005,  0.1201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 151 ] state=tensor([[-0.5220, -0.3902,  0.0005,  0.1201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5298, -0.1951,  0.0030, -0.1724]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 152 ] state=tensor([[-0.5298, -0.1951,  0.0030, -0.1724]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-5.3367e-01, -3.9023e-01, -4.9859e-04,  1.2117e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 153 ] state=tensor([[-5.3367e-01, -3.9023e-01, -4.9859e-04,  1.2117e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5415, -0.5853,  0.0019,  0.4137]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 154 ] state=tensor([[-0.5415, -0.5853,  0.0019,  0.4137]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5532, -0.7805,  0.0102,  0.7070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 155 ] state=tensor([[-0.5532, -0.7805,  0.0102,  0.7070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5688, -0.5855,  0.0243,  0.4175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 156 ] state=tensor([[-0.5688, -0.5855,  0.0243,  0.4175]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5805, -0.3907,  0.0327,  0.1326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 157 ] state=tensor([[-0.5805, -0.3907,  0.0327,  0.1326]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5883, -0.1961,  0.0353, -0.1496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 158 ] state=tensor([[-0.5883, -0.1961,  0.0353, -0.1496]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5922, -0.0015,  0.0324, -0.4309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 159 ] state=tensor([[-0.5922, -0.0015,  0.0324, -0.4309]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5923,  0.1931,  0.0237, -0.7132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 160 ] state=tensor([[-0.5923,  0.1931,  0.0237, -0.7132]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5884,  0.3879,  0.0095, -0.9983]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 161 ] state=tensor([[-0.5884,  0.3879,  0.0095, -0.9983]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5806,  0.1927, -0.0105, -0.7027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 162 ] state=tensor([[-0.5806,  0.1927, -0.0105, -0.7027]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5768, -0.0023, -0.0246, -0.4133]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 163 ] state=tensor([[-0.5768, -0.0023, -0.0246, -0.4133]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5768, -0.1971, -0.0328, -0.1285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 164 ] state=tensor([[-0.5768, -0.1971, -0.0328, -0.1285]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5808, -0.3917, -0.0354,  0.1537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 165 ] state=tensor([[-0.5808, -0.3917, -0.0354,  0.1537]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5886, -0.5863, -0.0323,  0.4350]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 118 ][ timestamp 166 ] state=tensor([[-0.5886, -0.5863, -0.0323,  0.4350]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6003, -0.3907, -0.0236,  0.1323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 167 ] state=tensor([[-0.6003, -0.3907, -0.0236,  0.1323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6082, -0.1953, -0.0210, -0.1678]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 168 ] state=tensor([[-0.6082, -0.1953, -0.0210, -0.1678]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6121, -0.3901, -0.0243,  0.1182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 169 ] state=tensor([[-0.6121, -0.3901, -0.0243,  0.1182]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6199, -0.5849, -0.0220,  0.4032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 170 ] state=tensor([[-0.6199, -0.5849, -0.0220,  0.4032]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6316, -0.3894, -0.0139,  0.1036]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 171 ] state=tensor([[-0.6316, -0.3894, -0.0139,  0.1036]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6394, -0.5844, -0.0118,  0.3919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 172 ] state=tensor([[-0.6394, -0.5844, -0.0118,  0.3919]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6510, -0.3891, -0.0040,  0.0955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 173 ] state=tensor([[-0.6510, -0.3891, -0.0040,  0.0955]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6588, -0.1939, -0.0021, -0.1984]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 174 ] state=tensor([[-0.6588, -0.1939, -0.0021, -0.1984]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6627, -0.3890, -0.0060,  0.0936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 175 ] state=tensor([[-0.6627, -0.3890, -0.0060,  0.0936]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6705, -0.1938, -0.0042, -0.2010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 176 ] state=tensor([[-0.6705, -0.1938, -0.0042, -0.2010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6744, -0.3888, -0.0082,  0.0904]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 177 ] state=tensor([[-0.6744, -0.3888, -0.0082,  0.0904]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6821, -0.5838, -0.0064,  0.3805]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 178 ] state=tensor([[-0.6821, -0.5838, -0.0064,  0.3805]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6938, -0.3886,  0.0012,  0.0858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 179 ] state=tensor([[-0.6938, -0.3886,  0.0012,  0.0858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7016, -0.5838,  0.0029,  0.3788]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 180 ] state=tensor([[-0.7016, -0.5838,  0.0029,  0.3788]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7133, -0.7789,  0.0105,  0.6725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 181 ] state=tensor([[-0.7133, -0.7789,  0.0105,  0.6725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7288, -0.5840,  0.0240,  0.3831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 182 ] state=tensor([[-0.7288, -0.5840,  0.0240,  0.3831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7405, -0.3892,  0.0316,  0.0981]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 183 ] state=tensor([[-0.7405, -0.3892,  0.0316,  0.0981]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7483, -0.1945,  0.0336, -0.1845]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 184 ] state=tensor([[-0.7483, -0.1945,  0.0336, -0.1845]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.5218e-01,  9.8430e-05,  2.9899e-02, -4.6637e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 185 ] state=tensor([[-7.5218e-01,  9.8430e-05,  2.9899e-02, -4.6637e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7522, -0.1954,  0.0206, -0.1644]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 186 ] state=tensor([[-0.7522, -0.1954,  0.0206, -0.1644]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.5609e-01, -6.1142e-04,  1.7284e-02, -4.5054e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 187 ] state=tensor([[-7.5609e-01, -6.1142e-04,  1.7284e-02, -4.5054e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7561, -0.1960,  0.0083, -0.1525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 188 ] state=tensor([[-0.7561, -0.1960,  0.0083, -0.1525]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7600, -0.0010,  0.0052, -0.4425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 189 ] state=tensor([[-0.7600, -0.0010,  0.0052, -0.4425]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7600, -0.1962, -0.0036, -0.1482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 190 ] state=tensor([[-0.7600, -0.1962, -0.0036, -0.1482]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7640, -0.3912, -0.0066,  0.1433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 191 ] state=tensor([[-0.7640, -0.3912, -0.0066,  0.1433]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7718, -0.1960, -0.0037, -0.1514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 192 ] state=tensor([[-0.7718, -0.1960, -0.0037, -0.1514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7757, -0.3911, -0.0068,  0.1401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 193 ] state=tensor([[-0.7757, -0.3911, -0.0068,  0.1401]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7835, -0.1959, -0.0040, -0.1547]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 194 ] state=tensor([[-0.7835, -0.1959, -0.0040, -0.1547]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.8745e-01, -6.9270e-04, -7.0448e-03, -4.4864e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 195 ] state=tensor([[-7.8745e-01, -6.9270e-04, -7.0448e-03, -4.4864e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7875, -0.1957, -0.0160, -0.1582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 196 ] state=tensor([[-0.7875, -0.1957, -0.0160, -0.1582]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7914, -0.3906, -0.0192,  0.1294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 197 ] state=tensor([[-0.7914, -0.3906, -0.0192,  0.1294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7992, -0.1952, -0.0166, -0.1693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 198 ] state=tensor([[-0.7992, -0.1952, -0.0166, -0.1693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8031, -0.3901, -0.0200,  0.1181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 199 ] state=tensor([[-0.8031, -0.3901, -0.0200,  0.1181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8109, -0.1947, -0.0176, -0.1808]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 200 ] state=tensor([[-0.8109, -0.1947, -0.0176, -0.1808]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8148, -0.3896, -0.0212,  0.1063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 201 ] state=tensor([[-0.8148, -0.3896, -0.0212,  0.1063]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8226, -0.1941, -0.0191, -0.1930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 202 ] state=tensor([[-0.8226, -0.1941, -0.0191, -0.1930]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8265, -0.3890, -0.0230,  0.0936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 203 ] state=tensor([[-0.8265, -0.3890, -0.0230,  0.0936]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8342, -0.5838, -0.0211,  0.3789]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 204 ] state=tensor([[-0.8342, -0.5838, -0.0211,  0.3789]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8459, -0.3883, -0.0135,  0.0797]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 205 ] state=tensor([[-0.8459, -0.3883, -0.0135,  0.0797]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8537, -0.5833, -0.0119,  0.3681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 206 ] state=tensor([[-0.8537, -0.5833, -0.0119,  0.3681]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8654, -0.3880, -0.0046,  0.0716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 207 ] state=tensor([[-0.8654, -0.3880, -0.0046,  0.0716]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8731, -0.1928, -0.0031, -0.2225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 208 ] state=tensor([[-0.8731, -0.1928, -0.0031, -0.2225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8770, -0.3879, -0.0076,  0.0692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 209 ] state=tensor([[-0.8770, -0.3879, -0.0076,  0.0692]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8847, -0.5829, -0.0062,  0.3595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 210 ] state=tensor([[-0.8847, -0.5829, -0.0062,  0.3595]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8964, -0.3877,  0.0010,  0.0649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 211 ] state=tensor([[-0.8964, -0.3877,  0.0010,  0.0649]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9041, -0.5828,  0.0023,  0.3579]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 212 ] state=tensor([[-0.9041, -0.5828,  0.0023,  0.3579]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9158, -0.3877,  0.0094,  0.0659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 213 ] state=tensor([[-0.9158, -0.3877,  0.0094,  0.0659]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9235, -0.1927,  0.0108, -0.2238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 214 ] state=tensor([[-0.9235, -0.1927,  0.0108, -0.2238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9274, -0.3880,  0.0063,  0.0723]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 215 ] state=tensor([[-0.9274, -0.3880,  0.0063,  0.0723]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9352, -0.1930,  0.0077, -0.2184]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 216 ] state=tensor([[-0.9352, -0.1930,  0.0077, -0.2184]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9390, -0.3882,  0.0034,  0.0767]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 217 ] state=tensor([[-0.9390, -0.3882,  0.0034,  0.0767]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9468, -0.5834,  0.0049,  0.3704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 218 ] state=tensor([[-0.9468, -0.5834,  0.0049,  0.3704]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9585, -0.3883,  0.0123,  0.0793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 219 ] state=tensor([[-0.9585, -0.3883,  0.0123,  0.0793]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9662, -0.1934,  0.0139, -0.2095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 220 ] state=tensor([[-0.9662, -0.1934,  0.0139, -0.2095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9701, -0.3887,  0.0097,  0.0876]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 221 ] state=tensor([[-0.9701, -0.3887,  0.0097,  0.0876]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9779, -0.1937,  0.0115, -0.2020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 222 ] state=tensor([[-0.9779, -0.1937,  0.0115, -0.2020]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9817, -0.3890,  0.0074,  0.0942]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 223 ] state=tensor([[-0.9817, -0.3890,  0.0074,  0.0942]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9895, -0.1940,  0.0093, -0.1961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 224 ] state=tensor([[-0.9895, -0.1940,  0.0093, -0.1961]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9934, -0.3892,  0.0054,  0.0995]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 225 ] state=tensor([[-0.9934, -0.3892,  0.0054,  0.0995]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0012, -0.1942,  0.0074, -0.1915]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 226 ] state=tensor([[-1.0012, -0.1942,  0.0074, -0.1915]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0051e+00,  8.1086e-04,  3.5412e-03, -4.8182e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 227 ] state=tensor([[-1.0051e+00,  8.1086e-04,  3.5412e-03, -4.8182e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0050, -0.1944, -0.0061, -0.1880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 228 ] state=tensor([[-1.0050, -0.1944, -0.0061, -0.1880]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0089, -0.3894, -0.0099,  0.1027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 229 ] state=tensor([[-1.0089, -0.3894, -0.0099,  0.1027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0167, -0.1941, -0.0078, -0.1930]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 118 ][ timestamp 230 ] state=tensor([[-1.0167, -0.1941, -0.0078, -0.1930]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0206, -0.3891, -0.0117,  0.0972]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 231 ] state=tensor([[-1.0206, -0.3891, -0.0117,  0.0972]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0284, -0.1939, -0.0097, -0.1992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 232 ] state=tensor([[-1.0284, -0.1939, -0.0097, -0.1992]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0323, -0.3888, -0.0137,  0.0904]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 233 ] state=tensor([[-1.0323, -0.3888, -0.0137,  0.0904]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0400, -0.1935, -0.0119, -0.2066]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 234 ] state=tensor([[-1.0400, -0.1935, -0.0119, -0.2066]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0439, -0.3885, -0.0160,  0.0824]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 235 ] state=tensor([[-1.0439, -0.3885, -0.0160,  0.0824]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0517, -0.5834, -0.0144,  0.3699]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 236 ] state=tensor([[-1.0517, -0.5834, -0.0144,  0.3699]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0634, -0.3880, -0.0070,  0.0728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 237 ] state=tensor([[-1.0634, -0.3880, -0.0070,  0.0728]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0711, -0.5831, -0.0055,  0.3632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 238 ] state=tensor([[-1.0711, -0.5831, -0.0055,  0.3632]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0828, -0.7781,  0.0017,  0.6542]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 239 ] state=tensor([[-1.0828, -0.7781,  0.0017,  0.6542]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0983, -0.5830,  0.0148,  0.3620]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 240 ] state=tensor([[-1.0983, -0.5830,  0.0148,  0.3620]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1100, -0.7783,  0.0221,  0.6594]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 241 ] state=tensor([[-1.1100, -0.7783,  0.0221,  0.6594]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1256, -0.9738,  0.0353,  0.9589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 242 ] state=tensor([[-1.1256, -0.9738,  0.0353,  0.9589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1450, -1.1693,  0.0544,  1.2624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 243 ] state=tensor([[-1.1450, -1.1693,  0.0544,  1.2624]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1684, -1.3651,  0.0797,  1.5717]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 244 ] state=tensor([[-1.1684, -1.3651,  0.0797,  1.5717]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1957, -1.5611,  0.1111,  1.8881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 245 ] state=tensor([[-1.1957, -1.5611,  0.1111,  1.8881]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2269, -1.7572,  0.1489,  2.2131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 246 ] state=tensor([[-1.2269, -1.7572,  0.1489,  2.2131]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2621, -1.9534,  0.1931,  2.5478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 118 ][ timestamp 247 ] state=tensor([[-1.2621, -1.9534,  0.1931,  2.5478]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 118: Exploration_rate=0.05. Score=247.\n",
      "[ episode 119 ] state=tensor([[ 0.0490, -0.0167, -0.0264, -0.0244]])\n",
      "[ episode 119 ][ timestamp 1 ] state=tensor([[ 0.0490, -0.0167, -0.0264, -0.0244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0486,  0.1788, -0.0269, -0.3253]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 2 ] state=tensor([[ 0.0486,  0.1788, -0.0269, -0.3253]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0522, -0.0159, -0.0334, -0.0412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 3 ] state=tensor([[ 0.0522, -0.0159, -0.0334, -0.0412]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0519,  0.1797, -0.0342, -0.3443]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 4 ] state=tensor([[ 0.0519,  0.1797, -0.0342, -0.3443]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0555, -0.0149, -0.0411, -0.0625]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 5 ] state=tensor([[ 0.0555, -0.0149, -0.0411, -0.0625]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0552, -0.2095, -0.0423,  0.2169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 6 ] state=tensor([[ 0.0552, -0.2095, -0.0423,  0.2169]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0510, -0.0138, -0.0380, -0.0888]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 7 ] state=tensor([[ 0.0510, -0.0138, -0.0380, -0.0888]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0507, -0.2083, -0.0398,  0.1916]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 8 ] state=tensor([[ 0.0507, -0.2083, -0.0398,  0.1916]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0466, -0.0126, -0.0359, -0.1133]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 9 ] state=tensor([[ 0.0466, -0.0126, -0.0359, -0.1133]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0463, -0.2072, -0.0382,  0.1678]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 10 ] state=tensor([[ 0.0463, -0.2072, -0.0382,  0.1678]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0422, -0.4018, -0.0348,  0.4482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 11 ] state=tensor([[ 0.0422, -0.4018, -0.0348,  0.4482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0341, -0.2062, -0.0259,  0.1448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 12 ] state=tensor([[ 0.0341, -0.2062, -0.0259,  0.1448]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0300, -0.0107, -0.0230, -0.1560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 13 ] state=tensor([[ 0.0300, -0.0107, -0.0230, -0.1560]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0298,  0.1847, -0.0261, -0.4558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 14 ] state=tensor([[ 0.0298,  0.1847, -0.0261, -0.4558]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0335, -0.0100, -0.0352, -0.1715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 15 ] state=tensor([[ 0.0335, -0.0100, -0.0352, -0.1715]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0333, -0.2046, -0.0386,  0.1099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 16 ] state=tensor([[ 0.0333, -0.2046, -0.0386,  0.1099]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0292, -0.3992, -0.0364,  0.3901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 17 ] state=tensor([[ 0.0292, -0.3992, -0.0364,  0.3901]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0212, -0.2035, -0.0286,  0.0862]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 18 ] state=tensor([[ 0.0212, -0.2035, -0.0286,  0.0862]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0171, -0.3982, -0.0269,  0.3697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 19 ] state=tensor([[ 0.0171, -0.3982, -0.0269,  0.3697]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0092, -0.2027, -0.0195,  0.0687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 20 ] state=tensor([[ 0.0092, -0.2027, -0.0195,  0.0687]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0051, -0.3976, -0.0182,  0.3551]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 21 ] state=tensor([[ 0.0051, -0.3976, -0.0182,  0.3551]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0028, -0.2022, -0.0111,  0.0568]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 22 ] state=tensor([[-0.0028, -0.2022, -0.0111,  0.0568]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0069, -0.3972, -0.0099,  0.3459]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 23 ] state=tensor([[-0.0069, -0.3972, -0.0099,  0.3459]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0148, -0.2019, -0.0030,  0.0501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 24 ] state=tensor([[-0.0148, -0.2019, -0.0030,  0.0501]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0189, -0.3970, -0.0020,  0.3419]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 25 ] state=tensor([[-0.0189, -0.3970, -0.0020,  0.3419]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0268, -0.2018,  0.0048,  0.0486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 26 ] state=tensor([[-0.0268, -0.2018,  0.0048,  0.0486]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0308, -0.3970,  0.0058,  0.3428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 27 ] state=tensor([[-0.0308, -0.3970,  0.0058,  0.3428]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0388, -0.2020,  0.0127,  0.0519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 28 ] state=tensor([[-0.0388, -0.2020,  0.0127,  0.0519]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0428, -0.0070,  0.0137, -0.2367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 29 ] state=tensor([[-0.0428, -0.0070,  0.0137, -0.2367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0430, -0.2024,  0.0090,  0.0602]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 30 ] state=tensor([[-0.0430, -0.2024,  0.0090,  0.0602]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0470, -0.0074,  0.0102, -0.2296]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 31 ] state=tensor([[-0.0470, -0.0074,  0.0102, -0.2296]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0472,  0.1876,  0.0056, -0.5190]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 32 ] state=tensor([[-0.0472,  0.1876,  0.0056, -0.5190]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0434, -0.0076, -0.0048, -0.2246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 33 ] state=tensor([[-0.0434, -0.0076, -0.0048, -0.2246]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0436, -0.2026, -0.0093,  0.0666]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 34 ] state=tensor([[-0.0436, -0.2026, -0.0093,  0.0666]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0476, -0.0074, -0.0080, -0.2290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 35 ] state=tensor([[-0.0476, -0.0074, -0.0080, -0.2290]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0478,  0.1878, -0.0125, -0.5242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 36 ] state=tensor([[-0.0478,  0.1878, -0.0125, -0.5242]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0440, -0.0071, -0.0230, -0.2355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 37 ] state=tensor([[-0.0440, -0.0071, -0.0230, -0.2355]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0441,  0.1883, -0.0277, -0.5354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 38 ] state=tensor([[-0.0441,  0.1883, -0.0277, -0.5354]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0404, -0.0064, -0.0384, -0.2515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 39 ] state=tensor([[-0.0404, -0.0064, -0.0384, -0.2515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0405, -0.2009, -0.0435,  0.0288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 40 ] state=tensor([[-0.0405, -0.2009, -0.0435,  0.0288]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0445, -0.0052, -0.0429, -0.2773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 41 ] state=tensor([[-0.0445, -0.0052, -0.0429, -0.2773]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0446, -0.1997, -0.0484,  0.0016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 42 ] state=tensor([[-0.0446, -0.1997, -0.0484,  0.0016]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0486, -0.3941, -0.0484,  0.2786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 43 ] state=tensor([[-0.0486, -0.3941, -0.0484,  0.2786]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0565, -0.5885, -0.0428,  0.5556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 44 ] state=tensor([[-0.0565, -0.5885, -0.0428,  0.5556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0683, -0.3928, -0.0317,  0.2497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 45 ] state=tensor([[-0.0683, -0.3928, -0.0317,  0.2497]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0761, -0.1972, -0.0267, -0.0528]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 46 ] state=tensor([[-0.0761, -0.1972, -0.0267, -0.0528]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0801, -0.3920, -0.0278,  0.2313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 47 ] state=tensor([[-0.0801, -0.3920, -0.0278,  0.2313]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0879, -0.5867, -0.0232,  0.5151]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 48 ] state=tensor([[-0.0879, -0.5867, -0.0232,  0.5151]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0996, -0.3912, -0.0129,  0.2153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 49 ] state=tensor([[-0.0996, -0.3912, -0.0129,  0.2153]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1075, -0.1959, -0.0085, -0.0815]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 50 ] state=tensor([[-0.1075, -0.1959, -0.0085, -0.0815]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1114, -0.3909, -0.0102,  0.2085]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 51 ] state=tensor([[-0.1114, -0.3909, -0.0102,  0.2085]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1192, -0.1957, -0.0060, -0.0874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 52 ] state=tensor([[-0.1192, -0.1957, -0.0060, -0.0874]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1231, -0.3907, -0.0078,  0.2034]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 53 ] state=tensor([[-0.1231, -0.3907, -0.0078,  0.2034]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1309, -0.1955, -0.0037, -0.0917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 54 ] state=tensor([[-0.1309, -0.1955, -0.0037, -0.0917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1348, -0.3905, -0.0055,  0.1998]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 55 ] state=tensor([[-0.1348, -0.3905, -0.0055,  0.1998]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1427, -0.5856, -0.0015,  0.4908]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 56 ] state=tensor([[-0.1427, -0.5856, -0.0015,  0.4908]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1544, -0.3904,  0.0083,  0.1976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 57 ] state=tensor([[-0.1544, -0.3904,  0.0083,  0.1976]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1622, -0.1954,  0.0122, -0.0925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 58 ] state=tensor([[-0.1622, -0.1954,  0.0122, -0.0925]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1661, -0.3907,  0.0104,  0.2041]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 59 ] state=tensor([[-0.1661, -0.3907,  0.0104,  0.2041]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1739, -0.5860,  0.0145,  0.5000]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 60 ] state=tensor([[-0.1739, -0.5860,  0.0145,  0.5000]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1856, -0.3911,  0.0245,  0.2119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 61 ] state=tensor([[-0.1856, -0.3911,  0.0245,  0.2119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1934, -0.1963,  0.0287, -0.0729]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 62 ] state=tensor([[-0.1934, -0.1963,  0.0287, -0.0729]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1974, -0.3918,  0.0273,  0.2287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 63 ] state=tensor([[-0.1974, -0.3918,  0.0273,  0.2287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2052, -0.1971,  0.0318, -0.0553]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 64 ] state=tensor([[-0.2052, -0.1971,  0.0318, -0.0553]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2092, -0.0025,  0.0307, -0.3378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 65 ] state=tensor([[-0.2092, -0.0025,  0.0307, -0.3378]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2092, -0.1980,  0.0240, -0.0356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 66 ] state=tensor([[-0.2092, -0.1980,  0.0240, -0.0356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2132, -0.0032,  0.0233, -0.3206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 67 ] state=tensor([[-0.2132, -0.0032,  0.0233, -0.3206]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2132, -0.1987,  0.0168, -0.0207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 68 ] state=tensor([[-0.2132, -0.1987,  0.0168, -0.0207]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2172, -0.0038,  0.0164, -0.3080]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 69 ] state=tensor([[-0.2172, -0.0038,  0.0164, -0.3080]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2173,  0.1911,  0.0103, -0.5955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 70 ] state=tensor([[-0.2173,  0.1911,  0.0103, -0.5955]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2135, -0.0042, -0.0016, -0.2996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 71 ] state=tensor([[-0.2135, -0.0042, -0.0016, -0.2996]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2135, -0.1993, -0.0076, -0.0074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 72 ] state=tensor([[-0.2135, -0.1993, -0.0076, -0.0074]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2175, -0.0041, -0.0078, -0.3025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 73 ] state=tensor([[-0.2175, -0.0041, -0.0078, -0.3025]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2176, -0.1991, -0.0138, -0.0123]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 74 ] state=tensor([[-0.2176, -0.1991, -0.0138, -0.0123]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2216, -0.3940, -0.0141,  0.2760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 75 ] state=tensor([[-0.2216, -0.3940, -0.0141,  0.2760]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2295, -0.1987, -0.0086, -0.0211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 76 ] state=tensor([[-0.2295, -0.1987, -0.0086, -0.0211]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2334, -0.3937, -0.0090,  0.2689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 77 ] state=tensor([[-0.2334, -0.3937, -0.0090,  0.2689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2413, -0.5887, -0.0036,  0.5588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 78 ] state=tensor([[-0.2413, -0.5887, -0.0036,  0.5588]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2531, -0.3935,  0.0076,  0.2649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 79 ] state=tensor([[-0.2531, -0.3935,  0.0076,  0.2649]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2610, -0.1985,  0.0129, -0.0253]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 80 ] state=tensor([[-0.2610, -0.1985,  0.0129, -0.0253]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2649, -0.3938,  0.0124,  0.2714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 81 ] state=tensor([[-0.2649, -0.3938,  0.0124,  0.2714]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2728, -0.1988,  0.0178, -0.0174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 82 ] state=tensor([[-0.2728, -0.1988,  0.0178, -0.0174]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2768, -0.3942,  0.0175,  0.2809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 83 ] state=tensor([[-0.2768, -0.3942,  0.0175,  0.2809]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2847, -0.1993,  0.0231, -0.0063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 84 ] state=tensor([[-0.2847, -0.1993,  0.0231, -0.0063]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2887, -0.3948,  0.0229,  0.2936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 85 ] state=tensor([[-0.2887, -0.3948,  0.0229,  0.2936]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2965, -0.2000,  0.0288,  0.0082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 86 ] state=tensor([[-0.2965, -0.2000,  0.0288,  0.0082]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3005, -0.0053,  0.0290, -0.2752]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 87 ] state=tensor([[-0.3005, -0.0053,  0.0290, -0.2752]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3007,  0.1894,  0.0235, -0.5586]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 88 ] state=tensor([[-0.3007,  0.1894,  0.0235, -0.5586]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2969,  0.3842,  0.0123, -0.8438]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 89 ] state=tensor([[-0.2969,  0.3842,  0.0123, -0.8438]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2892,  0.1889, -0.0046, -0.5473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 90 ] state=tensor([[-0.2892,  0.1889, -0.0046, -0.5473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2854, -0.0062, -0.0155, -0.2560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 91 ] state=tensor([[-0.2854, -0.0062, -0.0155, -0.2560]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2855, -0.2011, -0.0206,  0.0317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 92 ] state=tensor([[-0.2855, -0.2011, -0.0206,  0.0317]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2895, -0.0057, -0.0200, -0.2674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 93 ] state=tensor([[-0.2895, -0.0057, -0.0200, -0.2674]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2897, -0.2005, -0.0254,  0.0189]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 94 ] state=tensor([[-0.2897, -0.2005, -0.0254,  0.0189]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2937, -0.0050, -0.0250, -0.2817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 95 ] state=tensor([[-0.2937, -0.0050, -0.0250, -0.2817]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2938, -0.1998, -0.0306,  0.0030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 96 ] state=tensor([[-0.2938, -0.1998, -0.0306,  0.0030]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2978, -0.3944, -0.0305,  0.2859]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 97 ] state=tensor([[-0.2978, -0.3944, -0.0305,  0.2859]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3057, -0.5891, -0.0248,  0.5688]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 98 ] state=tensor([[-0.3057, -0.5891, -0.0248,  0.5688]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3174, -0.3937, -0.0135,  0.2684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 99 ] state=tensor([[-0.3174, -0.3937, -0.0135,  0.2684]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3253, -0.1983, -0.0081, -0.0285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 100 ] state=tensor([[-0.3253, -0.1983, -0.0081, -0.0285]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3293, -0.0031, -0.0087, -0.3237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 101 ] state=tensor([[-0.3293, -0.0031, -0.0087, -0.3237]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3293, -0.1981, -0.0151, -0.0338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 102 ] state=tensor([[-0.3293, -0.1981, -0.0151, -0.0338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3333, -0.3930, -0.0158,  0.2541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 103 ] state=tensor([[-0.3333, -0.3930, -0.0158,  0.2541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3412, -0.1977, -0.0107, -0.0435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 104 ] state=tensor([[-0.3412, -0.1977, -0.0107, -0.0435]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3451, -0.3926, -0.0116,  0.2457]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 105 ] state=tensor([[-0.3451, -0.3926, -0.0116,  0.2457]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3530, -0.5876, -0.0067,  0.5347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 106 ] state=tensor([[-0.3530, -0.5876, -0.0067,  0.5347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3647, -0.3924,  0.0040,  0.2400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 107 ] state=tensor([[-0.3647, -0.3924,  0.0040,  0.2400]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3726, -0.1973,  0.0088, -0.0515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 108 ] state=tensor([[-0.3726, -0.1973,  0.0088, -0.0515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3765, -0.3925,  0.0078,  0.2440]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 109 ] state=tensor([[-0.3765, -0.3925,  0.0078,  0.2440]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3844, -0.1975,  0.0127, -0.0462]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 110 ] state=tensor([[-0.3844, -0.1975,  0.0127, -0.0462]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3883, -0.3928,  0.0117,  0.2504]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 111 ] state=tensor([[-0.3883, -0.3928,  0.0117,  0.2504]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3962, -0.1979,  0.0167, -0.0385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 112 ] state=tensor([[-0.3962, -0.1979,  0.0167, -0.0385]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4001, -0.0030,  0.0160, -0.3259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 113 ] state=tensor([[-0.4001, -0.0030,  0.0160, -0.3259]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4002,  0.1919,  0.0095, -0.6135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 114 ] state=tensor([[-0.4002,  0.1919,  0.0095, -0.6135]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3964, -0.0034, -0.0028, -0.3178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 115 ] state=tensor([[-0.3964, -0.0034, -0.0028, -0.3178]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3964, -0.1985, -0.0092, -0.0260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 116 ] state=tensor([[-0.3964, -0.1985, -0.0092, -0.0260]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4004, -0.0032, -0.0097, -0.3216]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 117 ] state=tensor([[-0.4004, -0.0032, -0.0097, -0.3216]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4005, -0.1982, -0.0161, -0.0320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 118 ] state=tensor([[-0.4005, -0.1982, -0.0161, -0.0320]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4044, -0.3931, -0.0168,  0.2556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 119 ] state=tensor([[-0.4044, -0.3931, -0.0168,  0.2556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4123, -0.1977, -0.0117, -0.0424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 120 ] state=tensor([[-0.4123, -0.1977, -0.0117, -0.0424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4162, -0.0024, -0.0125, -0.3387]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 121 ] state=tensor([[-0.4162, -0.0024, -0.0125, -0.3387]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4163, -0.1974, -0.0193, -0.0500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 122 ] state=tensor([[-0.4163, -0.1974, -0.0193, -0.0500]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4202, -0.0020, -0.0203, -0.3487]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 123 ] state=tensor([[-0.4202, -0.0020, -0.0203, -0.3487]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4203, -0.1968, -0.0272, -0.0625]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 124 ] state=tensor([[-0.4203, -0.1968, -0.0272, -0.0625]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4242, -0.3915, -0.0285,  0.2215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 125 ] state=tensor([[-0.4242, -0.3915, -0.0285,  0.2215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4320, -0.5862, -0.0241,  0.5051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 126 ] state=tensor([[-0.4320, -0.5862, -0.0241,  0.5051]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4438, -0.7810, -0.0140,  0.7901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 127 ] state=tensor([[-0.4438, -0.7810, -0.0140,  0.7901]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4594, -0.5857,  0.0018,  0.4930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 128 ] state=tensor([[-0.4594, -0.5857,  0.0018,  0.4930]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4711, -0.3906,  0.0117,  0.2009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 129 ] state=tensor([[-0.4711, -0.3906,  0.0117,  0.2009]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4789, -0.1956,  0.0157, -0.0881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 130 ] state=tensor([[-0.4789, -0.1956,  0.0157, -0.0881]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4828, -0.0007,  0.0140, -0.3757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 131 ] state=tensor([[-0.4828, -0.0007,  0.0140, -0.3757]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4828,  0.1942,  0.0064, -0.6640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 132 ] state=tensor([[-0.4828,  0.1942,  0.0064, -0.6640]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4789, -0.0010, -0.0068, -0.3693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 133 ] state=tensor([[-0.4789, -0.0010, -0.0068, -0.3693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4790, -0.1961, -0.0142, -0.0788]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 134 ] state=tensor([[-0.4790, -0.1961, -0.0142, -0.0788]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4829, -0.0007, -0.0158, -0.3759]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 135 ] state=tensor([[-0.4829, -0.0007, -0.0158, -0.3759]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4829, -0.1956, -0.0233, -0.0882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 136 ] state=tensor([[-0.4829, -0.1956, -0.0233, -0.0882]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.8682e-01, -1.8426e-04, -2.5082e-02, -3.8819e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 137 ] state=tensor([[-4.8682e-01, -1.8426e-04, -2.5082e-02, -3.8819e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4868, -0.1949, -0.0328, -0.1035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 138 ] state=tensor([[-0.4868, -0.1949, -0.0328, -0.1035]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4907, -0.3896, -0.0349,  0.1786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 139 ] state=tensor([[-0.4907, -0.3896, -0.0349,  0.1786]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4985, -0.1940, -0.0313, -0.1249]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 140 ] state=tensor([[-0.4985, -0.1940, -0.0313, -0.1249]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5024, -0.3886, -0.0338,  0.1578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 141 ] state=tensor([[-0.5024, -0.3886, -0.0338,  0.1578]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5102, -0.5833, -0.0307,  0.4396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 142 ] state=tensor([[-0.5102, -0.5833, -0.0307,  0.4396]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5218, -0.3877, -0.0219,  0.1374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 143 ] state=tensor([[-0.5218, -0.3877, -0.0219,  0.1374]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5296, -0.1923, -0.0191, -0.1621]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 144 ] state=tensor([[-0.5296, -0.1923, -0.0191, -0.1621]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5334, -0.3871, -0.0224,  0.1245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 145 ] state=tensor([[-0.5334, -0.3871, -0.0224,  0.1245]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5412, -0.5819, -0.0199,  0.4100]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 146 ] state=tensor([[-0.5412, -0.5819, -0.0199,  0.4100]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5528, -0.3865, -0.0117,  0.1111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 147 ] state=tensor([[-0.5528, -0.3865, -0.0117,  0.1111]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5605, -0.5815, -0.0095,  0.4001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 148 ] state=tensor([[-0.5605, -0.5815, -0.0095,  0.4001]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5722, -0.3862, -0.0015,  0.1044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 149 ] state=tensor([[-0.5722, -0.3862, -0.0015,  0.1044]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5799, -0.1911,  0.0006, -0.1887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 150 ] state=tensor([[-0.5799, -0.1911,  0.0006, -0.1887]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5837,  0.0040, -0.0032, -0.4812]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 151 ] state=tensor([[-0.5837,  0.0040, -0.0032, -0.4812]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5836, -0.1910, -0.0128, -0.1895]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 152 ] state=tensor([[-0.5836, -0.1910, -0.0128, -0.1895]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5875, -0.3860, -0.0166,  0.0991]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 153 ] state=tensor([[-0.5875, -0.3860, -0.0166,  0.0991]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5952, -0.5809, -0.0146,  0.3865]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 154 ] state=tensor([[-0.5952, -0.5809, -0.0146,  0.3865]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6068, -0.3855, -0.0069,  0.0893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 155 ] state=tensor([[-0.6068, -0.3855, -0.0069,  0.0893]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6145, -0.5806, -0.0051,  0.3798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 156 ] state=tensor([[-0.6145, -0.5806, -0.0051,  0.3798]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6261, -0.3854,  0.0025,  0.0855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 157 ] state=tensor([[-0.6261, -0.3854,  0.0025,  0.0855]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6338, -0.1903,  0.0042, -0.2064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 158 ] state=tensor([[-0.6338, -0.1903,  0.0042, -0.2064]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-6.3763e-01, -3.8546e-01,  9.7840e-05,  8.7608e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 159 ] state=tensor([[-6.3763e-01, -3.8546e-01,  9.7840e-05,  8.7608e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6453, -0.1903,  0.0018, -0.2050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 160 ] state=tensor([[-0.6453, -0.1903,  0.0018, -0.2050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6491, -0.3855, -0.0023,  0.0882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 161 ] state=tensor([[-0.6491, -0.3855, -0.0023,  0.0882]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-6.5685e-01, -1.9033e-01, -4.8645e-04, -2.0517e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 162 ] state=tensor([[-6.5685e-01, -1.9033e-01, -4.8645e-04, -2.0517e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6607, -0.3854, -0.0046,  0.0874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 163 ] state=tensor([[-0.6607, -0.3854, -0.0046,  0.0874]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6684, -0.1903, -0.0028, -0.2068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 164 ] state=tensor([[-0.6684, -0.1903, -0.0028, -0.2068]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6722, -0.3853, -0.0070,  0.0850]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 165 ] state=tensor([[-0.6722, -0.3853, -0.0070,  0.0850]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6799, -0.1901, -0.0053, -0.2099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 166 ] state=tensor([[-0.6799, -0.1901, -0.0053, -0.2099]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6837, -0.3852, -0.0095,  0.0812]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 167 ] state=tensor([[-0.6837, -0.3852, -0.0095,  0.0812]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6914, -0.5801, -0.0079,  0.3708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 168 ] state=tensor([[-0.6914, -0.5801, -0.0079,  0.3708]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.0299e-01, -3.8492e-01, -4.3523e-04,  7.5684e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 169 ] state=tensor([[-7.0299e-01, -3.8492e-01, -4.3523e-04,  7.5684e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7107, -0.1898,  0.0011, -0.2171]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 170 ] state=tensor([[-0.7107, -0.1898,  0.0011, -0.2171]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7145, -0.3849, -0.0033,  0.0759]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 171 ] state=tensor([[-0.7145, -0.3849, -0.0033,  0.0759]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7222, -0.5800, -0.0017,  0.3675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 172 ] state=tensor([[-0.7222, -0.5800, -0.0017,  0.3675]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7338, -0.3849,  0.0056,  0.0743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 173 ] state=tensor([[-0.7338, -0.3849,  0.0056,  0.0743]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7415, -0.5801,  0.0071,  0.3688]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 174 ] state=tensor([[-0.7415, -0.5801,  0.0071,  0.3688]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7531, -0.3850,  0.0145,  0.0783]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 175 ] state=tensor([[-0.7531, -0.3850,  0.0145,  0.0783]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7608, -0.1901,  0.0160, -0.2098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 176 ] state=tensor([[-0.7608, -0.1901,  0.0160, -0.2098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7646, -0.3855,  0.0118,  0.0879]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 177 ] state=tensor([[-0.7646, -0.3855,  0.0118,  0.0879]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7723, -0.1905,  0.0136, -0.2010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 178 ] state=tensor([[-0.7723, -0.1905,  0.0136, -0.2010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7761, -0.3858,  0.0096,  0.0959]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 179 ] state=tensor([[-0.7761, -0.3858,  0.0096,  0.0959]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7838, -0.1909,  0.0115, -0.1937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 180 ] state=tensor([[-0.7838, -0.1909,  0.0115, -0.1937]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7876,  0.0041,  0.0076, -0.4827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 181 ] state=tensor([[-0.7876,  0.0041,  0.0076, -0.4827]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7876, -0.1911, -0.0020, -0.1877]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 182 ] state=tensor([[-0.7876, -0.1911, -0.0020, -0.1877]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7914, -0.3862, -0.0058,  0.1044]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 183 ] state=tensor([[-0.7914, -0.3862, -0.0058,  0.1044]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7991, -0.1910, -0.0037, -0.1901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 184 ] state=tensor([[-0.7991, -0.1910, -0.0037, -0.1901]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8029, -0.3861, -0.0075,  0.1014]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 185 ] state=tensor([[-0.8029, -0.3861, -0.0075,  0.1014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8106, -0.1909, -0.0055, -0.1937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 186 ] state=tensor([[-0.8106, -0.1909, -0.0055, -0.1937]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8145, -0.3859, -0.0093,  0.0973]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 187 ] state=tensor([[-0.8145, -0.3859, -0.0093,  0.0973]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8222, -0.1906, -0.0074, -0.1983]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 188 ] state=tensor([[-0.8222, -0.1906, -0.0074, -0.1983]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8260, -0.3857, -0.0114,  0.0920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 189 ] state=tensor([[-0.8260, -0.3857, -0.0114,  0.0920]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8337, -0.5806, -0.0095,  0.3811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 190 ] state=tensor([[-0.8337, -0.5806, -0.0095,  0.3811]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8453, -0.3854, -0.0019,  0.0854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 191 ] state=tensor([[-0.8453, -0.3854, -0.0019,  0.0854]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-8.5302e-01, -5.8045e-01, -2.0072e-04,  3.7749e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 192 ] state=tensor([[-8.5302e-01, -5.8045e-01, -2.0072e-04,  3.7749e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8646, -0.3853,  0.0073,  0.0847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 193 ] state=tensor([[-0.8646, -0.3853,  0.0073,  0.0847]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8723, -0.1903,  0.0090, -0.2056]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 194 ] state=tensor([[-0.8723, -0.1903,  0.0090, -0.2056]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8761, -0.3856,  0.0049,  0.0899]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 195 ] state=tensor([[-0.8761, -0.3856,  0.0049,  0.0899]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8839, -0.1905,  0.0067, -0.2012]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 196 ] state=tensor([[-0.8839, -0.1905,  0.0067, -0.2012]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8877,  0.0045,  0.0027, -0.4918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 197 ] state=tensor([[-0.8877,  0.0045,  0.0027, -0.4918]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8876, -0.1906, -0.0071, -0.1982]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 198 ] state=tensor([[-0.8876, -0.1906, -0.0071, -0.1982]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8914, -0.3857, -0.0111,  0.0922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 199 ] state=tensor([[-0.8914, -0.3857, -0.0111,  0.0922]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8991, -0.5806, -0.0093,  0.3814]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 200 ] state=tensor([[-0.8991, -0.5806, -0.0093,  0.3814]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9107, -0.3854, -0.0016,  0.0858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 201 ] state=tensor([[-0.9107, -0.3854, -0.0016,  0.0858]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-9.1842e-01, -1.9023e-01,  9.2423e-05, -2.0742e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 202 ] state=tensor([[-9.1842e-01, -1.9023e-01,  9.2423e-05, -2.0742e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9222, -0.3854, -0.0041,  0.0853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 203 ] state=tensor([[-0.9222, -0.3854, -0.0041,  0.0853]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9299, -0.1902, -0.0024, -0.2087]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 204 ] state=tensor([[-0.9299, -0.1902, -0.0024, -0.2087]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9337, -0.3853, -0.0065,  0.0833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 205 ] state=tensor([[-0.9337, -0.3853, -0.0065,  0.0833]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9414, -0.1900, -0.0049, -0.2115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 206 ] state=tensor([[-0.9414, -0.1900, -0.0049, -0.2115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9452,  0.0051, -0.0091, -0.5057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 207 ] state=tensor([[-0.9452,  0.0051, -0.0091, -0.5057]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9451, -0.1898, -0.0192, -0.2159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 208 ] state=tensor([[-0.9451, -0.1898, -0.0192, -0.2159]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9489, -0.3847, -0.0235,  0.0707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 209 ] state=tensor([[-0.9489, -0.3847, -0.0235,  0.0707]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9566, -0.1892, -0.0221, -0.2293]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 210 ] state=tensor([[-0.9566, -0.1892, -0.0221, -0.2293]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9604, -0.3840, -0.0267,  0.0563]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 211 ] state=tensor([[-0.9604, -0.3840, -0.0267,  0.0563]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9681, -0.1885, -0.0256, -0.2447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 212 ] state=tensor([[-0.9681, -0.1885, -0.0256, -0.2447]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9719, -0.3833, -0.0305,  0.0398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 213 ] state=tensor([[-0.9719, -0.3833, -0.0305,  0.0398]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9795, -0.1877, -0.0297, -0.2623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 214 ] state=tensor([[-0.9795, -0.1877, -0.0297, -0.2623]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9833, -0.3824, -0.0349,  0.0209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 215 ] state=tensor([[-0.9833, -0.3824, -0.0349,  0.0209]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9909, -0.5770, -0.0345,  0.3024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 216 ] state=tensor([[-0.9909, -0.5770, -0.0345,  0.3024]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0025e+00, -3.8144e-01, -2.8442e-02, -9.9805e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 217 ] state=tensor([[-1.0025e+00, -3.8144e-01, -2.8442e-02, -9.9805e-04]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0101, -0.5761, -0.0285,  0.2826]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 218 ] state=tensor([[-1.0101, -0.5761, -0.0285,  0.2826]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0216, -0.3806, -0.0228, -0.0189]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 219 ] state=tensor([[-1.0216, -0.3806, -0.0228, -0.0189]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0292, -0.5754, -0.0232,  0.2665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 220 ] state=tensor([[-1.0292, -0.5754, -0.0232,  0.2665]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0407, -0.3800, -0.0179, -0.0335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 221 ] state=tensor([[-1.0407, -0.3800, -0.0179, -0.0335]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0483, -0.5748, -0.0185,  0.2535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 222 ] state=tensor([[-1.0483, -0.5748, -0.0185,  0.2535]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0598, -0.7697, -0.0135,  0.5403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 223 ] state=tensor([[-1.0598, -0.7697, -0.0135,  0.5403]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0752, -0.5744, -0.0027,  0.2434]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 224 ] state=tensor([[-1.0752, -0.5744, -0.0027,  0.2434]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0867, -0.3792,  0.0022, -0.0501]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 225 ] state=tensor([[-1.0867, -0.3792,  0.0022, -0.0501]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0943, -0.5744,  0.0012,  0.2433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 226 ] state=tensor([[-1.0943, -0.5744,  0.0012,  0.2433]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1058, -0.3793,  0.0061, -0.0490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 227 ] state=tensor([[-1.1058, -0.3793,  0.0061, -0.0490]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1134, -0.1842,  0.0051, -0.3398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 228 ] state=tensor([[-1.1134, -0.1842,  0.0051, -0.3398]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1171,  0.0108, -0.0017, -0.6308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 229 ] state=tensor([[-1.1171,  0.0108, -0.0017, -0.6308]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1169, -0.1843, -0.0143, -0.3387]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 230 ] state=tensor([[-1.1169, -0.1843, -0.0143, -0.3387]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1205, -0.3792, -0.0211, -0.0505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 231 ] state=tensor([[-1.1205, -0.3792, -0.0211, -0.0505]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1281, -0.1838, -0.0221, -0.3498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 232 ] state=tensor([[-1.1281, -0.1838, -0.0221, -0.3498]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1318, -0.3786, -0.0291, -0.0642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 233 ] state=tensor([[-1.1318, -0.3786, -0.0291, -0.0642]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1394, -0.1831, -0.0304, -0.3659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 234 ] state=tensor([[-1.1394, -0.1831, -0.0304, -0.3659]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1430, -0.3777, -0.0377, -0.0829]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 235 ] state=tensor([[-1.1430, -0.3777, -0.0377, -0.0829]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1506, -0.5723, -0.0394,  0.1976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 236 ] state=tensor([[-1.1506, -0.5723, -0.0394,  0.1976]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1620, -0.3766, -0.0354, -0.1072]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 237 ] state=tensor([[-1.1620, -0.3766, -0.0354, -0.1072]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1696, -0.5712, -0.0375,  0.1741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 238 ] state=tensor([[-1.1696, -0.5712, -0.0375,  0.1741]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1810, -0.7658, -0.0341,  0.4547]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 239 ] state=tensor([[-1.1810, -0.7658, -0.0341,  0.4547]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1963, -0.5702, -0.0250,  0.1515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 240 ] state=tensor([[-1.1963, -0.5702, -0.0250,  0.1515]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2077, -0.3747, -0.0219, -0.1490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 241 ] state=tensor([[-1.2077, -0.3747, -0.0219, -0.1490]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2152, -0.5695, -0.0249,  0.1367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 242 ] state=tensor([[-1.2152, -0.5695, -0.0249,  0.1367]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2266, -0.3741, -0.0222, -0.1637]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 243 ] state=tensor([[-1.2266, -0.3741, -0.0222, -0.1637]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2341, -0.5689, -0.0255,  0.1219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 244 ] state=tensor([[-1.2341, -0.5689, -0.0255,  0.1219]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2455, -0.7636, -0.0230,  0.4064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 245 ] state=tensor([[-1.2455, -0.7636, -0.0230,  0.4064]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2607, -0.5682, -0.0149,  0.1066]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 246 ] state=tensor([[-1.2607, -0.5682, -0.0149,  0.1066]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2721, -0.3728, -0.0128, -0.1908]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 247 ] state=tensor([[-1.2721, -0.3728, -0.0128, -0.1908]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2795, -0.5678, -0.0166,  0.0979]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 248 ] state=tensor([[-1.2795, -0.5678, -0.0166,  0.0979]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2909, -0.7627, -0.0146,  0.3853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 249 ] state=tensor([[-1.2909, -0.7627, -0.0146,  0.3853]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3062, -0.5673, -0.0069,  0.0880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 250 ] state=tensor([[-1.3062, -0.5673, -0.0069,  0.0880]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3175, -0.7624, -0.0052,  0.3785]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 251 ] state=tensor([[-1.3175, -0.7624, -0.0052,  0.3785]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3327, -0.5672,  0.0024,  0.0842]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 252 ] state=tensor([[-1.3327, -0.5672,  0.0024,  0.0842]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3441, -0.3721,  0.0041, -0.2077]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 253 ] state=tensor([[-1.3441, -0.3721,  0.0041, -0.2077]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3515e+00, -1.7701e-01, -5.2695e-05, -4.9911e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 254 ] state=tensor([[-1.3515e+00, -1.7701e-01, -5.2695e-05, -4.9911e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3551, -0.3721, -0.0100, -0.2064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 255 ] state=tensor([[-1.3551, -0.3721, -0.0100, -0.2064]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3625, -0.5671, -0.0142,  0.0831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 256 ] state=tensor([[-1.3625, -0.5671, -0.0142,  0.0831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3739, -0.3718, -0.0125, -0.2141]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 119 ][ timestamp 257 ] state=tensor([[-1.3739, -0.3718, -0.0125, -0.2141]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3813, -0.5667, -0.0168,  0.0747]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 258 ] state=tensor([[-1.3813, -0.5667, -0.0168,  0.0747]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3926, -0.7616, -0.0153,  0.3620]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 259 ] state=tensor([[-1.3926, -0.7616, -0.0153,  0.3620]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4079, -0.5663, -0.0081,  0.0645]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 260 ] state=tensor([[-1.4079, -0.5663, -0.0081,  0.0645]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4192, -0.3710, -0.0068, -0.2307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 261 ] state=tensor([[-1.4192, -0.3710, -0.0068, -0.2307]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4266, -0.5661, -0.0114,  0.0599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 262 ] state=tensor([[-1.4266, -0.5661, -0.0114,  0.0599]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4379, -0.3708, -0.0102, -0.2364]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 263 ] state=tensor([[-1.4379, -0.3708, -0.0102, -0.2364]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4453, -0.5657, -0.0149,  0.0531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 264 ] state=tensor([[-1.4453, -0.5657, -0.0149,  0.0531]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4567, -0.7607, -0.0138,  0.3410]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 265 ] state=tensor([[-1.4567, -0.7607, -0.0138,  0.3410]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4719, -0.9556, -0.0070,  0.6293]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 266 ] state=tensor([[-1.4719, -0.9556, -0.0070,  0.6293]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4910, -1.1506,  0.0056,  0.9198]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 267 ] state=tensor([[-1.4910, -1.1506,  0.0056,  0.9198]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5140, -1.3458,  0.0240,  1.2142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 268 ] state=tensor([[-1.5140, -1.3458,  0.0240,  1.2142]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5409, -1.5412,  0.0482,  1.5143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 269 ] state=tensor([[-1.5409, -1.5412,  0.0482,  1.5143]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5717, -1.7369,  0.0785,  1.8216]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 270 ] state=tensor([[-1.5717, -1.7369,  0.0785,  1.8216]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6065, -1.9328,  0.1150,  2.1376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 271 ] state=tensor([[-1.6065, -1.9328,  0.1150,  2.1376]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6451, -2.1288,  0.1577,  2.4635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 272 ] state=tensor([[-1.6451, -2.1288,  0.1577,  2.4635]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6877, -1.9354,  0.2070,  2.2231]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 119 ][ timestamp 273 ] state=tensor([[-1.6877, -1.9354,  0.2070,  2.2231]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 119: Exploration_rate=0.05. Score=273.\n",
      "[ episode 120 ] state=tensor([[-0.0305,  0.0033,  0.0448, -0.0459]])\n",
      "[ episode 120 ][ timestamp 1 ] state=tensor([[-0.0305,  0.0033,  0.0448, -0.0459]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0305,  0.1977,  0.0439, -0.3241]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 2 ] state=tensor([[-0.0305,  0.1977,  0.0439, -0.3241]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0265,  0.3922,  0.0374, -0.6027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 3 ] state=tensor([[-0.0265,  0.3922,  0.0374, -0.6027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0187,  0.5868,  0.0254, -0.8833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 4 ] state=tensor([[-0.0187,  0.5868,  0.0254, -0.8833]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0069,  0.3913,  0.0077, -0.5828]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 5 ] state=tensor([[-0.0069,  0.3913,  0.0077, -0.5828]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0009,  0.5863, -0.0039, -0.8730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 6 ] state=tensor([[ 0.0009,  0.5863, -0.0039, -0.8730]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0126,  0.3913, -0.0214, -0.5816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 7 ] state=tensor([[ 0.0126,  0.3913, -0.0214, -0.5816]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0204,  0.5867, -0.0330, -0.8809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 8 ] state=tensor([[ 0.0204,  0.5867, -0.0330, -0.8809]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0322,  0.3920, -0.0506, -0.5988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 9 ] state=tensor([[ 0.0322,  0.3920, -0.0506, -0.5988]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0400,  0.1977, -0.0626, -0.3225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 10 ] state=tensor([[ 0.0400,  0.1977, -0.0626, -0.3225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0440,  0.0035, -0.0691, -0.0502]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 11 ] state=tensor([[ 0.0440,  0.0035, -0.0691, -0.0502]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0440,  0.1995, -0.0701, -0.3638]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 12 ] state=tensor([[ 0.0440,  0.1995, -0.0701, -0.3638]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0480,  0.0055, -0.0773, -0.0940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 13 ] state=tensor([[ 0.0480,  0.0055, -0.0773, -0.0940]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0481, -0.1885, -0.0792,  0.1733]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 14 ] state=tensor([[ 0.0481, -0.1885, -0.0792,  0.1733]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0444, -0.3824, -0.0758,  0.4400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 15 ] state=tensor([[ 0.0444, -0.3824, -0.0758,  0.4400]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0367, -0.1863, -0.0670,  0.1244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 16 ] state=tensor([[ 0.0367, -0.1863, -0.0670,  0.1244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0330, -0.3804, -0.0645,  0.3952]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 17 ] state=tensor([[ 0.0330, -0.3804, -0.0645,  0.3952]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0254, -0.5745, -0.0566,  0.6669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 18 ] state=tensor([[ 0.0254, -0.5745, -0.0566,  0.6669]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0139, -0.3787, -0.0432,  0.3570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 19 ] state=tensor([[ 0.0139, -0.3787, -0.0432,  0.3570]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0063, -0.1829, -0.0361,  0.0510]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 20 ] state=tensor([[ 0.0063, -0.1829, -0.0361,  0.0510]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0027,  0.0127, -0.0351, -0.2529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 21 ] state=tensor([[ 0.0027,  0.0127, -0.0351, -0.2529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0029, -0.1819, -0.0401,  0.0285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 22 ] state=tensor([[ 0.0029, -0.1819, -0.0401,  0.0285]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0007, -0.3765, -0.0396,  0.3083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 23 ] state=tensor([[-0.0007, -0.3765, -0.0396,  0.3083]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0082, -0.5710, -0.0334,  0.5882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 24 ] state=tensor([[-0.0082, -0.5710, -0.0334,  0.5882]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0197, -0.3754, -0.0216,  0.2852]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 25 ] state=tensor([[-0.0197, -0.3754, -0.0216,  0.2852]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0272, -0.1800, -0.0159, -0.0142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 26 ] state=tensor([[-0.0272, -0.1800, -0.0159, -0.0142]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0308, -0.3749, -0.0162,  0.2734]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 27 ] state=tensor([[-0.0308, -0.3749, -0.0162,  0.2734]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0383, -0.5698, -0.0107,  0.5609]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 28 ] state=tensor([[-0.0383, -0.5698, -0.0107,  0.5609]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0497, -0.3745,  0.0005,  0.2649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 29 ] state=tensor([[-0.0497, -0.3745,  0.0005,  0.2649]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0572, -0.1794,  0.0058, -0.0276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 30 ] state=tensor([[-0.0572, -0.1794,  0.0058, -0.0276]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0607,  0.0157,  0.0052, -0.3185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 31 ] state=tensor([[-0.0607,  0.0157,  0.0052, -0.3185]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0604,  0.2107, -0.0011, -0.6095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 32 ] state=tensor([[-0.0604,  0.2107, -0.0011, -0.6095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0562,  0.0156, -0.0133, -0.3172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 33 ] state=tensor([[-0.0562,  0.0156, -0.0133, -0.3172]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0559, -0.1793, -0.0197, -0.0288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 34 ] state=tensor([[-0.0559, -0.1793, -0.0197, -0.0288]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0595, -0.3742, -0.0203,  0.2577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 35 ] state=tensor([[-0.0595, -0.3742, -0.0203,  0.2577]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0670, -0.1788, -0.0151, -0.0413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 36 ] state=tensor([[-0.0670, -0.1788, -0.0151, -0.0413]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0705, -0.3737, -0.0159,  0.2465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 37 ] state=tensor([[-0.0705, -0.3737, -0.0159,  0.2465]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0780, -0.1783, -0.0110, -0.0511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 38 ] state=tensor([[-0.0780, -0.1783, -0.0110, -0.0511]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0816,  0.0170, -0.0120, -0.3473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 39 ] state=tensor([[-0.0816,  0.0170, -0.0120, -0.3473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0812, -0.1780, -0.0190, -0.0584]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 40 ] state=tensor([[-0.0812, -0.1780, -0.0190, -0.0584]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0848, -0.3728, -0.0201,  0.2282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 41 ] state=tensor([[-0.0848, -0.3728, -0.0201,  0.2282]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0923, -0.1774, -0.0156, -0.0707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 42 ] state=tensor([[-0.0923, -0.1774, -0.0156, -0.0707]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0958, -0.3723, -0.0170,  0.2170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 43 ] state=tensor([[-0.0958, -0.3723, -0.0170,  0.2170]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1033, -0.1770, -0.0126, -0.0810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 44 ] state=tensor([[-0.1033, -0.1770, -0.0126, -0.0810]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1068, -0.3719, -0.0143,  0.2077]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 45 ] state=tensor([[-0.1068, -0.3719, -0.0143,  0.2077]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1142, -0.1766, -0.0101, -0.0895]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 46 ] state=tensor([[-0.1142, -0.1766, -0.0101, -0.0895]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1178,  0.0187, -0.0119, -0.3853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 47 ] state=tensor([[-0.1178,  0.0187, -0.0119, -0.3853]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1174, -0.1763, -0.0196, -0.0964]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 48 ] state=tensor([[-0.1174, -0.1763, -0.0196, -0.0964]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1209, -0.3711, -0.0215,  0.1900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 49 ] state=tensor([[-0.1209, -0.3711, -0.0215,  0.1900]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1283, -0.5659, -0.0177,  0.4758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 50 ] state=tensor([[-0.1283, -0.5659, -0.0177,  0.4758]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1397, -0.3705, -0.0082,  0.1776]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 51 ] state=tensor([[-0.1397, -0.3705, -0.0082,  0.1776]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1471, -0.1753, -0.0047, -0.1176]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 120 ][ timestamp 52 ] state=tensor([[-0.1471, -0.1753, -0.0047, -0.1176]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1506, -0.3704, -0.0070,  0.1736]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 53 ] state=tensor([[-0.1506, -0.3704, -0.0070,  0.1736]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1580, -0.1751, -0.0035, -0.1213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 54 ] state=tensor([[-0.1580, -0.1751, -0.0035, -0.1213]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1615, -0.3702, -0.0060,  0.1702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 55 ] state=tensor([[-0.1615, -0.3702, -0.0060,  0.1702]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1689, -0.1750, -0.0026, -0.1243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 56 ] state=tensor([[-0.1689, -0.1750, -0.0026, -0.1243]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1724,  0.0202, -0.0051, -0.4178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 57 ] state=tensor([[-0.1724,  0.0202, -0.0051, -0.4178]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1720, -0.1749, -0.0134, -0.1267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 58 ] state=tensor([[-0.1720, -0.1749, -0.0134, -0.1267]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1755,  0.0204, -0.0159, -0.4236]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 59 ] state=tensor([[-0.1755,  0.0204, -0.0159, -0.4236]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1751, -0.1745, -0.0244, -0.1360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 60 ] state=tensor([[-0.1751, -0.1745, -0.0244, -0.1360]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1786, -0.3692, -0.0271,  0.1489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 61 ] state=tensor([[-0.1786, -0.3692, -0.0271,  0.1489]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1860, -0.5640, -0.0242,  0.4329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 62 ] state=tensor([[-0.1860, -0.5640, -0.0242,  0.4329]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1972, -0.7587, -0.0155,  0.7178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 63 ] state=tensor([[-0.1972, -0.7587, -0.0155,  0.7178]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2124, -0.5634, -0.0011,  0.4203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 64 ] state=tensor([[-0.2124, -0.5634, -0.0011,  0.4203]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2237, -0.3683,  0.0073,  0.1273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 65 ] state=tensor([[-0.2237, -0.3683,  0.0073,  0.1273]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2310, -0.5635,  0.0098,  0.4222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 66 ] state=tensor([[-0.2310, -0.5635,  0.0098,  0.4222]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2423, -0.3685,  0.0183,  0.1327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 67 ] state=tensor([[-0.2423, -0.3685,  0.0183,  0.1327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2497, -0.1737,  0.0209, -0.1542]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 68 ] state=tensor([[-0.2497, -0.1737,  0.0209, -0.1542]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2532,  0.0212,  0.0178, -0.4402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 69 ] state=tensor([[-0.2532,  0.0212,  0.0178, -0.4402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2527, -0.1742,  0.0090, -0.1420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 70 ] state=tensor([[-0.2527, -0.1742,  0.0090, -0.1420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2562,  0.0208,  0.0062, -0.4318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 71 ] state=tensor([[-0.2562,  0.0208,  0.0062, -0.4318]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2558, -0.1744, -0.0025, -0.1372]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 72 ] state=tensor([[-0.2558, -0.1744, -0.0025, -0.1372]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2593, -0.3695, -0.0052,  0.1547]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 73 ] state=tensor([[-0.2593, -0.3695, -0.0052,  0.1547]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2667, -0.1743, -0.0021, -0.1396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 74 ] state=tensor([[-0.2667, -0.1743, -0.0021, -0.1396]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2702, -0.3694, -0.0049,  0.1524]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 75 ] state=tensor([[-0.2702, -0.3694, -0.0049,  0.1524]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2776, -0.5645, -0.0019,  0.4436]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 76 ] state=tensor([[-0.2776, -0.5645, -0.0019,  0.4436]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2888, -0.3693,  0.0070,  0.1503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 77 ] state=tensor([[-0.2888, -0.3693,  0.0070,  0.1503]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2962, -0.5645,  0.0100,  0.4452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 78 ] state=tensor([[-0.2962, -0.5645,  0.0100,  0.4452]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3075, -0.3696,  0.0189,  0.1557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 79 ] state=tensor([[-0.3075, -0.3696,  0.0189,  0.1557]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3149, -0.1747,  0.0220, -0.1310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 80 ] state=tensor([[-0.3149, -0.1747,  0.0220, -0.1310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3184,  0.0201,  0.0194, -0.4166]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 81 ] state=tensor([[-0.3184,  0.0201,  0.0194, -0.4166]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3180, -0.1753,  0.0111, -0.1179]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 82 ] state=tensor([[-0.3180, -0.1753,  0.0111, -0.1179]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3215,  0.0197,  0.0087, -0.4070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 83 ] state=tensor([[-0.3215,  0.0197,  0.0087, -0.4070]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3211, -0.1756,  0.0006, -0.1116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 84 ] state=tensor([[-0.3211, -0.1756,  0.0006, -0.1116]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3246, -0.3707, -0.0016,  0.1813]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 85 ] state=tensor([[-0.3246, -0.3707, -0.0016,  0.1813]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3320, -0.1756,  0.0020, -0.1119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 86 ] state=tensor([[-0.3320, -0.1756,  0.0020, -0.1119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.3555e-01,  1.9527e-02, -2.5268e-04, -4.0400e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 87 ] state=tensor([[-3.3555e-01,  1.9527e-02, -2.5268e-04, -4.0400e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3352, -0.1756, -0.0083, -0.1114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 88 ] state=tensor([[-0.3352, -0.1756, -0.0083, -0.1114]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3387, -0.3706, -0.0106,  0.1786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 89 ] state=tensor([[-0.3387, -0.3706, -0.0106,  0.1786]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3461, -0.1753, -0.0070, -0.1173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 90 ] state=tensor([[-0.3461, -0.1753, -0.0070, -0.1173]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3496, -0.3703, -0.0093,  0.1731]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 91 ] state=tensor([[-0.3496, -0.3703, -0.0093,  0.1731]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3570, -0.1751, -0.0059, -0.1225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 92 ] state=tensor([[-0.3570, -0.1751, -0.0059, -0.1225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3605, -0.3701, -0.0083,  0.1683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 93 ] state=tensor([[-0.3605, -0.3701, -0.0083,  0.1683]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3679, -0.5651, -0.0050,  0.4584]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 94 ] state=tensor([[-0.3679, -0.5651, -0.0050,  0.4584]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3792, -0.7602,  0.0042,  0.7495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 95 ] state=tensor([[-0.3792, -0.7602,  0.0042,  0.7495]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3944, -0.5651,  0.0192,  0.4581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 96 ] state=tensor([[-0.3944, -0.5651,  0.0192,  0.4581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4057, -0.3703,  0.0284,  0.1716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 97 ] state=tensor([[-0.4057, -0.3703,  0.0284,  0.1716]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4131, -0.1756,  0.0318, -0.1120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 98 ] state=tensor([[-0.4131, -0.1756,  0.0318, -0.1120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4166,  0.0191,  0.0296, -0.3945]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 99 ] state=tensor([[-0.4166,  0.0191,  0.0296, -0.3945]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4162,  0.2138,  0.0217, -0.6777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 100 ] state=tensor([[-0.4162,  0.2138,  0.0217, -0.6777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4120,  0.0184,  0.0081, -0.3783]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 101 ] state=tensor([[-0.4120,  0.0184,  0.0081, -0.3783]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.1160e-01,  2.1337e-01,  5.4470e-04, -6.6842e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 102 ] state=tensor([[-4.1160e-01,  2.1337e-01,  5.4470e-04, -6.6842e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4073,  0.0182, -0.0128, -0.3756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 103 ] state=tensor([[-0.4073,  0.0182, -0.0128, -0.3756]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4070, -0.1767, -0.0203, -0.0870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 104 ] state=tensor([[-0.4070, -0.1767, -0.0203, -0.0870]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4105, -0.3715, -0.0221,  0.1992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 105 ] state=tensor([[-0.4105, -0.3715, -0.0221,  0.1992]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4179, -0.5663, -0.0181,  0.4849]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 106 ] state=tensor([[-0.4179, -0.5663, -0.0181,  0.4849]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4293, -0.3710, -0.0084,  0.1866]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 107 ] state=tensor([[-0.4293, -0.3710, -0.0084,  0.1866]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4367, -0.5660, -0.0047,  0.4766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 108 ] state=tensor([[-0.4367, -0.5660, -0.0047,  0.4766]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4480, -0.3708,  0.0049,  0.1824]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 109 ] state=tensor([[-0.4480, -0.3708,  0.0049,  0.1824]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4554, -0.1757,  0.0085, -0.1087]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 110 ] state=tensor([[-0.4554, -0.1757,  0.0085, -0.1087]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4589,  0.0193,  0.0063, -0.3987]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 111 ] state=tensor([[-0.4589,  0.0193,  0.0063, -0.3987]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4585, -0.1759, -0.0016, -0.1040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 112 ] state=tensor([[-0.4585, -0.1759, -0.0016, -0.1040]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4621,  0.0192, -0.0037, -0.3972]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 113 ] state=tensor([[-0.4621,  0.0192, -0.0037, -0.3972]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4617, -0.1758, -0.0117, -0.1057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 114 ] state=tensor([[-0.4617, -0.1758, -0.0117, -0.1057]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4652, -0.3708, -0.0138,  0.1833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 115 ] state=tensor([[-0.4652, -0.3708, -0.0138,  0.1833]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4726, -0.5657, -0.0101,  0.4716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 116 ] state=tensor([[-0.4726, -0.5657, -0.0101,  0.4716]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4839, -0.3705, -0.0007,  0.1757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 117 ] state=tensor([[-0.4839, -0.3705, -0.0007,  0.1757]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4913, -0.5656,  0.0028,  0.4682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 118 ] state=tensor([[-0.4913, -0.5656,  0.0028,  0.4682]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5026, -0.3705,  0.0122,  0.1764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 119 ] state=tensor([[-0.5026, -0.3705,  0.0122,  0.1764]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5101, -0.1755,  0.0157, -0.1124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 120 ] state=tensor([[-0.5101, -0.1755,  0.0157, -0.1124]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5136,  0.0193,  0.0135, -0.4001]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 120 ][ timestamp 121 ] state=tensor([[-0.5136,  0.0193,  0.0135, -0.4001]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5132, -0.1760,  0.0055, -0.1032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 122 ] state=tensor([[-0.5132, -0.1760,  0.0055, -0.1032]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5167,  0.0191,  0.0034, -0.3941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 123 ] state=tensor([[-0.5167,  0.0191,  0.0034, -0.3941]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5163,  0.2142, -0.0045, -0.6857]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 124 ] state=tensor([[-0.5163,  0.2142, -0.0045, -0.6857]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5120,  0.0191, -0.0182, -0.3944]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 125 ] state=tensor([[-0.5120,  0.0191, -0.0182, -0.3944]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5117, -0.1758, -0.0261, -0.1075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 126 ] state=tensor([[-0.5117, -0.1758, -0.0261, -0.1075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5152, -0.3705, -0.0282,  0.1768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 127 ] state=tensor([[-0.5152, -0.3705, -0.0282,  0.1768]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5226, -0.5652, -0.0247,  0.4605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 128 ] state=tensor([[-0.5226, -0.5652, -0.0247,  0.4605]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5339, -0.3698, -0.0155,  0.1601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 129 ] state=tensor([[-0.5339, -0.3698, -0.0155,  0.1601]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5413, -0.5646, -0.0123,  0.4479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 130 ] state=tensor([[-0.5413, -0.5646, -0.0123,  0.4479]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5526, -0.3694, -0.0033,  0.1514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 131 ] state=tensor([[-0.5526, -0.3694, -0.0033,  0.1514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-5.5996e-01, -5.6443e-01, -2.7872e-04,  4.4299e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 132 ] state=tensor([[-5.5996e-01, -5.6443e-01, -2.7872e-04,  4.4299e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5712, -0.7595,  0.0086,  0.7356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 133 ] state=tensor([[-0.5712, -0.7595,  0.0086,  0.7356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5864, -0.5645,  0.0233,  0.4456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 134 ] state=tensor([[-0.5864, -0.5645,  0.0233,  0.4456]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5977, -0.3698,  0.0322,  0.1604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 135 ] state=tensor([[-0.5977, -0.3698,  0.0322,  0.1604]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6051, -0.1751,  0.0354, -0.1220]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 136 ] state=tensor([[-0.6051, -0.1751,  0.0354, -0.1220]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6086,  0.0195,  0.0330, -0.4033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 137 ] state=tensor([[-0.6086,  0.0195,  0.0330, -0.4033]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6082, -0.1761,  0.0249, -0.1004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 138 ] state=tensor([[-0.6082, -0.1761,  0.0249, -0.1004]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6118,  0.0187,  0.0229, -0.3851]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 139 ] state=tensor([[-0.6118,  0.0187,  0.0229, -0.3851]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6114,  0.2135,  0.0152, -0.6705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 140 ] state=tensor([[-0.6114,  0.2135,  0.0152, -0.6705]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6071,  0.0181,  0.0018, -0.3731]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 141 ] state=tensor([[-0.6071,  0.0181,  0.0018, -0.3731]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6068, -0.1770, -0.0057, -0.0798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 142 ] state=tensor([[-0.6068, -0.1770, -0.0057, -0.0798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6103, -0.3721, -0.0073,  0.2111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 143 ] state=tensor([[-0.6103, -0.3721, -0.0073,  0.2111]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6177, -0.1768, -0.0030, -0.0839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 144 ] state=tensor([[-0.6177, -0.1768, -0.0030, -0.0839]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6213,  0.0183, -0.0047, -0.3775]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 145 ] state=tensor([[-0.6213,  0.0183, -0.0047, -0.3775]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6209, -0.1767, -0.0123, -0.0864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 146 ] state=tensor([[-0.6209, -0.1767, -0.0123, -0.0864]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6244, -0.3717, -0.0140,  0.2024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 147 ] state=tensor([[-0.6244, -0.3717, -0.0140,  0.2024]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6319, -0.5666, -0.0100,  0.4907]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 148 ] state=tensor([[-0.6319, -0.5666, -0.0100,  0.4907]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-6.4320e-01, -3.7133e-01, -1.4411e-04,  1.9486e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 149 ] state=tensor([[-6.4320e-01, -3.7133e-01, -1.4411e-04,  1.9486e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6506, -0.5664,  0.0038,  0.4875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 150 ] state=tensor([[-0.6506, -0.5664,  0.0038,  0.4875]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6620, -0.3714,  0.0135,  0.1960]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 151 ] state=tensor([[-0.6620, -0.3714,  0.0135,  0.1960]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6694, -0.1765,  0.0174, -0.0924]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 152 ] state=tensor([[-0.6694, -0.1765,  0.0174, -0.0924]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6729, -0.3718,  0.0156,  0.2057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 153 ] state=tensor([[-0.6729, -0.3718,  0.0156,  0.2057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6804, -0.1769,  0.0197, -0.0820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 154 ] state=tensor([[-0.6804, -0.1769,  0.0197, -0.0820]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6839,  0.0179,  0.0180, -0.3684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 155 ] state=tensor([[-0.6839,  0.0179,  0.0180, -0.3684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6835, -0.1775,  0.0107, -0.0701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 156 ] state=tensor([[-0.6835, -0.1775,  0.0107, -0.0701]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6871,  0.0175,  0.0093, -0.3594]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 157 ] state=tensor([[-0.6871,  0.0175,  0.0093, -0.3594]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6867, -0.1777,  0.0021, -0.0638]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 158 ] state=tensor([[-0.6867, -0.1777,  0.0021, -0.0638]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6903, -0.3729,  0.0008,  0.2296]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 159 ] state=tensor([[-0.6903, -0.3729,  0.0008,  0.2296]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6977, -0.1778,  0.0054, -0.0629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 160 ] state=tensor([[-0.6977, -0.1778,  0.0054, -0.0629]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7013,  0.0173,  0.0041, -0.3538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 161 ] state=tensor([[-0.7013,  0.0173,  0.0041, -0.3538]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7010,  0.2123, -0.0029, -0.6452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 162 ] state=tensor([[-0.7010,  0.2123, -0.0029, -0.6452]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6967,  0.0172, -0.0158, -0.3535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 163 ] state=tensor([[-0.6967,  0.0172, -0.0158, -0.3535]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6964, -0.1777, -0.0229, -0.0658]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 120 ][ timestamp 164 ] state=tensor([[-0.6964, -0.1777, -0.0229, -0.0658]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6999, -0.3724, -0.0242,  0.2196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 165 ] state=tensor([[-0.6999, -0.3724, -0.0242,  0.2196]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7074, -0.1770, -0.0198, -0.0807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 166 ] state=tensor([[-0.7074, -0.1770, -0.0198, -0.0807]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7109,  0.0184, -0.0214, -0.3795]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 167 ] state=tensor([[-0.7109,  0.0184, -0.0214, -0.3795]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7105, -0.1764, -0.0290, -0.0937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 168 ] state=tensor([[-0.7105, -0.1764, -0.0290, -0.0937]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7141, -0.3711, -0.0309,  0.1897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 169 ] state=tensor([[-0.7141, -0.3711, -0.0309,  0.1897]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7215, -0.1755, -0.0271, -0.1126]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 170 ] state=tensor([[-0.7215, -0.1755, -0.0271, -0.1126]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7250, -0.3703, -0.0294,  0.1714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 171 ] state=tensor([[-0.7250, -0.3703, -0.0294,  0.1714]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7324, -0.1747, -0.0259, -0.1304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 172 ] state=tensor([[-0.7324, -0.1747, -0.0259, -0.1304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7359, -0.3695, -0.0285,  0.1540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 173 ] state=tensor([[-0.7359, -0.3695, -0.0285,  0.1540]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7433, -0.1740, -0.0255, -0.1475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 174 ] state=tensor([[-0.7433, -0.1740, -0.0255, -0.1475]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7468, -0.3687, -0.0284,  0.1370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 175 ] state=tensor([[-0.7468, -0.3687, -0.0284,  0.1370]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7541, -0.5634, -0.0257,  0.4206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 176 ] state=tensor([[-0.7541, -0.5634, -0.0257,  0.4206]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7654, -0.3679, -0.0173,  0.1200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 177 ] state=tensor([[-0.7654, -0.3679, -0.0173,  0.1200]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7728, -0.1726, -0.0149, -0.1781]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 178 ] state=tensor([[-0.7728, -0.1726, -0.0149, -0.1781]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7762, -0.3675, -0.0184,  0.1098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 179 ] state=tensor([[-0.7762, -0.3675, -0.0184,  0.1098]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7836, -0.1721, -0.0162, -0.1886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 180 ] state=tensor([[-0.7836, -0.1721, -0.0162, -0.1886]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7870, -0.3670, -0.0200,  0.0989]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 181 ] state=tensor([[-0.7870, -0.3670, -0.0200,  0.0989]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7944, -0.1716, -0.0180, -0.2000]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 182 ] state=tensor([[-0.7944, -0.1716, -0.0180, -0.2000]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7978, -0.3664, -0.0220,  0.0870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 183 ] state=tensor([[-0.7978, -0.3664, -0.0220,  0.0870]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8051, -0.1710, -0.0203, -0.2126]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 184 ] state=tensor([[-0.8051, -0.1710, -0.0203, -0.2126]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8085, -0.3658, -0.0245,  0.0736]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 185 ] state=tensor([[-0.8085, -0.3658, -0.0245,  0.0736]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8158, -0.5606, -0.0231,  0.3585]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 186 ] state=tensor([[-0.8158, -0.5606, -0.0231,  0.3585]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8271, -0.3652, -0.0159,  0.0586]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 187 ] state=tensor([[-0.8271, -0.3652, -0.0159,  0.0586]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8344, -0.1698, -0.0147, -0.2390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 188 ] state=tensor([[-0.8344, -0.1698, -0.0147, -0.2390]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8378, -0.3647, -0.0195,  0.0490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 189 ] state=tensor([[-0.8378, -0.3647, -0.0195,  0.0490]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8451, -0.5596, -0.0185,  0.3354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 190 ] state=tensor([[-0.8451, -0.5596, -0.0185,  0.3354]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8562, -0.3642, -0.0118,  0.0370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 191 ] state=tensor([[-0.8562, -0.3642, -0.0118,  0.0370]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8635, -0.1689, -0.0111, -0.2594]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 192 ] state=tensor([[-0.8635, -0.1689, -0.0111, -0.2594]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8669, -0.3638, -0.0163,  0.0298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 193 ] state=tensor([[-0.8669, -0.3638, -0.0163,  0.0298]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8742, -0.1685, -0.0157, -0.2680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 194 ] state=tensor([[-0.8742, -0.1685, -0.0157, -0.2680]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8776,  0.0268, -0.0210, -0.5656]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 195 ] state=tensor([[-0.8776,  0.0268, -0.0210, -0.5656]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8770, -0.1680, -0.0323, -0.2796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 196 ] state=tensor([[-0.8770, -0.1680, -0.0323, -0.2796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8804, -0.3626, -0.0379,  0.0027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 197 ] state=tensor([[-0.8804, -0.3626, -0.0379,  0.0027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8876, -0.1670, -0.0379, -0.3017]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 198 ] state=tensor([[-0.8876, -0.1670, -0.0379, -0.3017]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8910, -0.3615, -0.0439, -0.0212]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 199 ] state=tensor([[-0.8910, -0.3615, -0.0439, -0.0212]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8982, -0.5560, -0.0443,  0.2573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 200 ] state=tensor([[-0.8982, -0.5560, -0.0443,  0.2573]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9093, -0.3603, -0.0392, -0.0490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 201 ] state=tensor([[-0.9093, -0.3603, -0.0392, -0.0490]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9165, -0.5548, -0.0402,  0.2311]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 202 ] state=tensor([[-0.9165, -0.5548, -0.0402,  0.2311]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9276, -0.3591, -0.0355, -0.0740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 203 ] state=tensor([[-0.9276, -0.3591, -0.0355, -0.0740]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9348, -0.5537, -0.0370,  0.2073]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 204 ] state=tensor([[-0.9348, -0.5537, -0.0370,  0.2073]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9459, -0.3581, -0.0329, -0.0969]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 205 ] state=tensor([[-0.9459, -0.3581, -0.0329, -0.0969]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9530, -0.5527, -0.0348,  0.1853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 206 ] state=tensor([[-0.9530, -0.5527, -0.0348,  0.1853]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9641, -0.7473, -0.0311,  0.4668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 207 ] state=tensor([[-0.9641, -0.7473, -0.0311,  0.4668]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9790, -0.5518, -0.0218,  0.1645]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 208 ] state=tensor([[-0.9790, -0.5518, -0.0218,  0.1645]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9901, -0.3564, -0.0185, -0.1350]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 209 ] state=tensor([[-0.9901, -0.3564, -0.0185, -0.1350]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9972, -0.5512, -0.0212,  0.1518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 210 ] state=tensor([[-0.9972, -0.5512, -0.0212,  0.1518]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0082, -0.3558, -0.0181, -0.1475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 211 ] state=tensor([[-1.0082, -0.3558, -0.0181, -0.1475]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0153, -0.5507, -0.0211,  0.1394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 212 ] state=tensor([[-1.0153, -0.5507, -0.0211,  0.1394]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0264, -0.3552, -0.0183, -0.1599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 213 ] state=tensor([[-1.0264, -0.3552, -0.0183, -0.1599]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0335, -0.5501, -0.0215,  0.1270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 214 ] state=tensor([[-1.0335, -0.5501, -0.0215,  0.1270]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0445, -0.3547, -0.0190, -0.1724]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 215 ] state=tensor([[-1.0445, -0.3547, -0.0190, -0.1724]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0516, -0.5495, -0.0224,  0.1142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 216 ] state=tensor([[-1.0516, -0.5495, -0.0224,  0.1142]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0625, -0.3541, -0.0201, -0.1854]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 120 ][ timestamp 217 ] state=tensor([[-1.0625, -0.3541, -0.0201, -0.1854]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0696, -0.5489, -0.0238,  0.1008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 218 ] state=tensor([[-1.0696, -0.5489, -0.0238,  0.1008]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0806, -0.7437, -0.0218,  0.3859]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 219 ] state=tensor([[-1.0806, -0.7437, -0.0218,  0.3859]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0955, -0.5483, -0.0141,  0.0864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 220 ] state=tensor([[-1.0955, -0.5483, -0.0141,  0.0864]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1064, -0.7432, -0.0124,  0.3746]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 221 ] state=tensor([[-1.1064, -0.7432, -0.0124,  0.3746]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1213, -0.5479, -0.0049,  0.0781]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 222 ] state=tensor([[-1.1213, -0.5479, -0.0049,  0.0781]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1323, -0.7429, -0.0033,  0.3692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 223 ] state=tensor([[-1.1323, -0.7429, -0.0033,  0.3692]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1471, -0.5478,  0.0041,  0.0755]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 224 ] state=tensor([[-1.1471, -0.5478,  0.0041,  0.0755]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1581, -0.3527,  0.0056, -0.2159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 225 ] state=tensor([[-1.1581, -0.3527,  0.0056, -0.2159]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1651, -0.5479,  0.0013,  0.0785]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 226 ] state=tensor([[-1.1651, -0.5479,  0.0013,  0.0785]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1761, -0.3528,  0.0028, -0.2138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 227 ] state=tensor([[-1.1761, -0.3528,  0.0028, -0.2138]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1832, -0.5480, -0.0014,  0.0798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 228 ] state=tensor([[-1.1832, -0.5480, -0.0014,  0.0798]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1941e+00, -3.5282e-01,  1.4870e-04, -2.1333e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 229 ] state=tensor([[-1.1941e+00, -3.5282e-01,  1.4870e-04, -2.1333e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2012, -0.5479, -0.0041,  0.0794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 230 ] state=tensor([[-1.2012, -0.5479, -0.0041,  0.0794]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2121, -0.3528, -0.0025, -0.2146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 231 ] state=tensor([[-1.2121, -0.3528, -0.0025, -0.2146]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2192, -0.5479, -0.0068,  0.0773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 232 ] state=tensor([[-1.2192, -0.5479, -0.0068,  0.0773]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2301, -0.3526, -0.0053, -0.2175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 233 ] state=tensor([[-1.2301, -0.3526, -0.0053, -0.2175]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2372, -0.5477, -0.0096,  0.0735]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 234 ] state=tensor([[-1.2372, -0.5477, -0.0096,  0.0735]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2481, -0.3524, -0.0082, -0.2222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 235 ] state=tensor([[-1.2481, -0.3524, -0.0082, -0.2222]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2552, -0.5474, -0.0126,  0.0679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 236 ] state=tensor([[-1.2552, -0.5474, -0.0126,  0.0679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2661, -0.3521, -0.0112, -0.2287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 237 ] state=tensor([[-1.2661, -0.3521, -0.0112, -0.2287]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2732, -0.5471, -0.0158,  0.0604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 238 ] state=tensor([[-1.2732, -0.5471, -0.0158,  0.0604]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2841, -0.3517, -0.0146, -0.2373]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 239 ] state=tensor([[-1.2841, -0.3517, -0.0146, -0.2373]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2912, -0.5467, -0.0194,  0.0508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 240 ] state=tensor([[-1.2912, -0.5467, -0.0194,  0.0508]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3021, -0.7415, -0.0183,  0.3373]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 241 ] state=tensor([[-1.3021, -0.7415, -0.0183,  0.3373]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3169, -0.5461, -0.0116,  0.0389]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 242 ] state=tensor([[-1.3169, -0.5461, -0.0116,  0.0389]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3278, -0.3508, -0.0108, -0.2574]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 243 ] state=tensor([[-1.3278, -0.3508, -0.0108, -0.2574]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3349, -0.5458, -0.0160,  0.0318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 244 ] state=tensor([[-1.3349, -0.5458, -0.0160,  0.0318]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3458, -0.3504, -0.0153, -0.2659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 245 ] state=tensor([[-1.3458, -0.3504, -0.0153, -0.2659]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3528, -0.5453, -0.0206,  0.0219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 246 ] state=tensor([[-1.3528, -0.5453, -0.0206,  0.0219]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3637, -0.3499, -0.0202, -0.2772]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 247 ] state=tensor([[-1.3637, -0.3499, -0.0202, -0.2772]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3707, -0.5448, -0.0258,  0.0091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 248 ] state=tensor([[-1.3707, -0.5448, -0.0258,  0.0091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3816, -0.7395, -0.0256,  0.2935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 249 ] state=tensor([[-1.3816, -0.7395, -0.0256,  0.2935]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3964, -0.5440, -0.0197, -0.0071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 250 ] state=tensor([[-1.3964, -0.5440, -0.0197, -0.0071]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4073, -0.7389, -0.0198,  0.2793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 251 ] state=tensor([[-1.4073, -0.7389, -0.0198,  0.2793]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4220, -0.5435, -0.0143, -0.0196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 252 ] state=tensor([[-1.4220, -0.5435, -0.0143, -0.0196]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4329, -0.3481, -0.0147, -0.3168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 253 ] state=tensor([[-1.4329, -0.3481, -0.0147, -0.3168]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4399, -0.5430, -0.0210, -0.0287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 254 ] state=tensor([[-1.4399, -0.5430, -0.0210, -0.0287]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4507, -0.7379, -0.0216,  0.2573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 255 ] state=tensor([[-1.4507, -0.7379, -0.0216,  0.2573]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4655, -0.9327, -0.0164,  0.5431]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 256 ] state=tensor([[-1.4655, -0.9327, -0.0164,  0.5431]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4841, -1.1276, -0.0056,  0.8305]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 120 ][ timestamp 257 ] state=tensor([[-1.4841, -1.1276, -0.0056,  0.8305]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5067, -1.3226,  0.0111,  1.1215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 258 ] state=tensor([[-1.5067, -1.3226,  0.0111,  1.1215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5331, -1.5179,  0.0335,  1.4176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 259 ] state=tensor([[-1.5331, -1.5179,  0.0335,  1.4176]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5635, -1.7134,  0.0618,  1.7206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 260 ] state=tensor([[-1.5635, -1.7134,  0.0618,  1.7206]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5978, -1.9092,  0.0962,  2.0318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 261 ] state=tensor([[-1.5978, -1.9092,  0.0962,  2.0318]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6360, -2.1051,  0.1369,  2.3527]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 262 ] state=tensor([[-1.6360, -2.1051,  0.1369,  2.3527]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6781, -1.9115,  0.1839,  2.1050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 120 ][ timestamp 263 ] state=tensor([[-1.6781, -1.9115,  0.1839,  2.1050]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 120: Exploration_rate=0.05. Score=263.\n",
      "[ episode 121 ] state=tensor([[-0.0401, -0.0260, -0.0245,  0.0440]])\n",
      "[ episode 121 ][ timestamp 1 ] state=tensor([[-0.0401, -0.0260, -0.0245,  0.0440]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0407,  0.1695, -0.0236, -0.2563]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 2 ] state=tensor([[-0.0407,  0.1695, -0.0236, -0.2563]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0373,  0.3649, -0.0288, -0.5564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 3 ] state=tensor([[-0.0373,  0.3649, -0.0288, -0.5564]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0300,  0.1702, -0.0399, -0.2729]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 4 ] state=tensor([[-0.0300,  0.1702, -0.0399, -0.2729]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0266,  0.3659, -0.0453, -0.5779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 5 ] state=tensor([[-0.0266,  0.3659, -0.0453, -0.5779]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0193,  0.1714, -0.0569, -0.2998]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 6 ] state=tensor([[-0.0193,  0.1714, -0.0569, -0.2998]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0158, -0.0228, -0.0629, -0.0256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 7 ] state=tensor([[-0.0158, -0.0228, -0.0629, -0.0256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0163,  0.1731, -0.0634, -0.3374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 8 ] state=tensor([[-0.0163,  0.1731, -0.0634, -0.3374]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0128,  0.3691, -0.0702, -0.6494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 9 ] state=tensor([[-0.0128,  0.3691, -0.0702, -0.6494]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0054,  0.1750, -0.0831, -0.3796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 10 ] state=tensor([[-0.0054,  0.1750, -0.0831, -0.3796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0019, -0.0188, -0.0907, -0.1143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 11 ] state=tensor([[-0.0019, -0.0188, -0.0907, -0.1143]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0023, -0.2125, -0.0930,  0.1485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 12 ] state=tensor([[-0.0023, -0.2125, -0.0930,  0.1485]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0066, -0.4062, -0.0900,  0.4104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 13 ] state=tensor([[-0.0066, -0.4062, -0.0900,  0.4104]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0147, -0.6000, -0.0818,  0.6734]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 14 ] state=tensor([[-0.0147, -0.6000, -0.0818,  0.6734]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0267, -0.4038, -0.0684,  0.3561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 15 ] state=tensor([[-0.0267, -0.4038, -0.0684,  0.3561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0348, -0.2078, -0.0612,  0.0427]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 16 ] state=tensor([[-0.0348, -0.2078, -0.0612,  0.0427]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0389, -0.4020, -0.0604,  0.3154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 17 ] state=tensor([[-0.0389, -0.4020, -0.0604,  0.3154]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0470, -0.5962, -0.0541,  0.5885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 18 ] state=tensor([[-0.0470, -0.5962, -0.0541,  0.5885]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0589, -0.4003, -0.0423,  0.2793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 19 ] state=tensor([[-0.0589, -0.4003, -0.0423,  0.2793]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0669, -0.2046, -0.0367, -0.0265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 20 ] state=tensor([[-0.0669, -0.2046, -0.0367, -0.0265]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0710, -0.3992, -0.0373,  0.2544]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 21 ] state=tensor([[-0.0710, -0.3992, -0.0373,  0.2544]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0790, -0.5938, -0.0322,  0.5351]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 22 ] state=tensor([[-0.0790, -0.5938, -0.0322,  0.5351]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0908, -0.3982, -0.0215,  0.2325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 23 ] state=tensor([[-0.0908, -0.3982, -0.0215,  0.2325]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0988, -0.2028, -0.0168, -0.0669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 24 ] state=tensor([[-0.0988, -0.2028, -0.0168, -0.0669]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1029, -0.3977, -0.0182,  0.2204]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 25 ] state=tensor([[-0.1029, -0.3977, -0.0182,  0.2204]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1108, -0.2023, -0.0138, -0.0779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 26 ] state=tensor([[-0.1108, -0.2023, -0.0138, -0.0779]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1149, -0.3972, -0.0153,  0.2104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 27 ] state=tensor([[-0.1149, -0.3972, -0.0153,  0.2104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1228, -0.2019, -0.0111, -0.0871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 28 ] state=tensor([[-0.1228, -0.2019, -0.0111, -0.0871]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1268, -0.3969, -0.0128,  0.2021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 29 ] state=tensor([[-0.1268, -0.3969, -0.0128,  0.2021]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1348, -0.2016, -0.0088, -0.0946]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 30 ] state=tensor([[-0.1348, -0.2016, -0.0088, -0.0946]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1388, -0.3965, -0.0107,  0.1952]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 31 ] state=tensor([[-0.1388, -0.3965, -0.0107,  0.1952]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1467, -0.2013, -0.0068, -0.1008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 32 ] state=tensor([[-0.1467, -0.2013, -0.0068, -0.1008]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1508, -0.3963, -0.0088,  0.1897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 33 ] state=tensor([[-0.1508, -0.3963, -0.0088,  0.1897]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1587, -0.5913, -0.0050,  0.4796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 34 ] state=tensor([[-0.1587, -0.5913, -0.0050,  0.4796]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1705, -0.3961,  0.0046,  0.1854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 35 ] state=tensor([[-0.1705, -0.3961,  0.0046,  0.1854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1784, -0.2010,  0.0083, -0.1059]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 36 ] state=tensor([[-0.1784, -0.2010,  0.0083, -0.1059]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1825, -0.0060,  0.0062, -0.3959]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 37 ] state=tensor([[-0.1825, -0.0060,  0.0062, -0.3959]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1826, -0.2012, -0.0017, -0.1013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 38 ] state=tensor([[-0.1826, -0.2012, -0.0017, -0.1013]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1866, -0.3963, -0.0038,  0.1908]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 121 ][ timestamp 39 ] state=tensor([[-0.1866, -0.3963, -0.0038,  0.1908]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9454e-01, -2.0117e-01,  4.4646e-05, -1.0303e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 40 ] state=tensor([[-1.9454e-01, -2.0117e-01,  4.4646e-05, -1.0303e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1986, -0.3963, -0.0020,  0.1897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 41 ] state=tensor([[-0.1986, -0.3963, -0.0020,  0.1897]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2065, -0.2011,  0.0018, -0.1037]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 42 ] state=tensor([[-0.2065, -0.2011,  0.0018, -0.1037]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1051e-01, -3.9629e-01, -2.9560e-04,  1.8959e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 43 ] state=tensor([[-2.1051e-01, -3.9629e-01, -2.9560e-04,  1.8959e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2184, -0.2012,  0.0035, -0.1032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 44 ] state=tensor([[-0.2184, -0.2012,  0.0035, -0.1032]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2225, -0.3963,  0.0014,  0.1906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 45 ] state=tensor([[-0.2225, -0.3963,  0.0014,  0.1906]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2304, -0.2012,  0.0052, -0.1016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 46 ] state=tensor([[-0.2304, -0.2012,  0.0052, -0.1016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2344, -0.0062,  0.0032, -0.3927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 47 ] state=tensor([[-0.2344, -0.0062,  0.0032, -0.3927]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2345, -0.2014, -0.0046, -0.0990]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 48 ] state=tensor([[-0.2345, -0.2014, -0.0046, -0.0990]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2386, -0.0062, -0.0066, -0.3931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 49 ] state=tensor([[-0.2386, -0.0062, -0.0066, -0.3931]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2387, -0.2012, -0.0145, -0.1025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 50 ] state=tensor([[-0.2387, -0.2012, -0.0145, -0.1025]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2427, -0.0059, -0.0165, -0.3997]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 51 ] state=tensor([[-0.2427, -0.0059, -0.0165, -0.3997]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2428, -0.2008, -0.0245, -0.1123]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 52 ] state=tensor([[-0.2428, -0.2008, -0.0245, -0.1123]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2468, -0.3955, -0.0268,  0.1725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 53 ] state=tensor([[-0.2468, -0.3955, -0.0268,  0.1725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2547, -0.2000, -0.0233, -0.1285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 54 ] state=tensor([[-0.2547, -0.2000, -0.0233, -0.1285]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2587, -0.3948, -0.0259,  0.1568]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 55 ] state=tensor([[-0.2587, -0.3948, -0.0259,  0.1568]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2666, -0.5895, -0.0228,  0.4412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 56 ] state=tensor([[-0.2666, -0.5895, -0.0228,  0.4412]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2784, -0.3941, -0.0139,  0.1414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 57 ] state=tensor([[-0.2784, -0.3941, -0.0139,  0.1414]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2863, -0.1988, -0.0111, -0.1556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 58 ] state=tensor([[-0.2863, -0.1988, -0.0111, -0.1556]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2903, -0.3937, -0.0142,  0.1335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 59 ] state=tensor([[-0.2903, -0.3937, -0.0142,  0.1335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2982, -0.1984, -0.0115, -0.1636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 60 ] state=tensor([[-0.2982, -0.1984, -0.0115, -0.1636]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3021, -0.3934, -0.0148,  0.1254]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 61 ] state=tensor([[-0.3021, -0.3934, -0.0148,  0.1254]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3100, -0.5883, -0.0123,  0.4134]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 62 ] state=tensor([[-0.3100, -0.5883, -0.0123,  0.4134]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3218, -0.3930, -0.0040,  0.1168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 63 ] state=tensor([[-0.3218, -0.3930, -0.0040,  0.1168]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3296, -0.1978, -0.0017, -0.1771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 64 ] state=tensor([[-0.3296, -0.1978, -0.0017, -0.1771]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3336, -0.3929, -0.0053,  0.1150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 65 ] state=tensor([[-0.3336, -0.3929, -0.0053,  0.1150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3414, -0.1977, -0.0030, -0.1793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 66 ] state=tensor([[-0.3414, -0.1977, -0.0030, -0.1793]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3454, -0.3928, -0.0065,  0.1124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 67 ] state=tensor([[-0.3454, -0.3928, -0.0065,  0.1124]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3533, -0.1976, -0.0043, -0.1823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 68 ] state=tensor([[-0.3533, -0.1976, -0.0043, -0.1823]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3572, -0.0024, -0.0079, -0.4763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 69 ] state=tensor([[-0.3572, -0.0024, -0.0079, -0.4763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3573, -0.1974, -0.0175, -0.1862]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 70 ] state=tensor([[-0.3573, -0.1974, -0.0175, -0.1862]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3612, -0.3923, -0.0212,  0.1010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 71 ] state=tensor([[-0.3612, -0.3923, -0.0212,  0.1010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3690, -0.5871, -0.0192,  0.3869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 72 ] state=tensor([[-0.3690, -0.5871, -0.0192,  0.3869]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3808, -0.3917, -0.0114,  0.0882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 73 ] state=tensor([[-0.3808, -0.3917, -0.0114,  0.0882]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3886, -0.5866, -0.0097,  0.3773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 74 ] state=tensor([[-0.3886, -0.5866, -0.0097,  0.3773]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4004, -0.3914, -0.0021,  0.0816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 75 ] state=tensor([[-0.4004, -0.3914, -0.0021,  0.0816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-4.0818e-01, -5.8648e-01, -4.8778e-04,  3.7357e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 76 ] state=tensor([[-4.0818e-01, -5.8648e-01, -4.8778e-04,  3.7357e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4199, -0.3914,  0.0070,  0.0807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 77 ] state=tensor([[-0.4199, -0.3914,  0.0070,  0.0807]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4277, -0.5866,  0.0086,  0.3756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 78 ] state=tensor([[-0.4277, -0.5866,  0.0086,  0.3756]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4395, -0.3916,  0.0161,  0.0857]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 79 ] state=tensor([[-0.4395, -0.3916,  0.0161,  0.0857]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4473, -0.1967,  0.0178, -0.2019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 80 ] state=tensor([[-0.4473, -0.1967,  0.0178, -0.2019]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4512, -0.0018,  0.0138, -0.4889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 81 ] state=tensor([[-0.4512, -0.0018,  0.0138, -0.4889]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4513, -0.1971,  0.0040, -0.1919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 82 ] state=tensor([[-0.4513, -0.1971,  0.0040, -0.1919]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-4.5522e-01, -3.9232e-01,  1.6877e-04,  1.0203e-01]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 121 ][ timestamp 83 ] state=tensor([[-4.5522e-01, -3.9232e-01,  1.6877e-04,  1.0203e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4631, -0.1972,  0.0022, -0.1906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 84 ] state=tensor([[-0.4631, -0.1972,  0.0022, -0.1906]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4670, -0.3924, -0.0016,  0.1028]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 85 ] state=tensor([[-0.4670, -0.3924, -0.0016,  0.1028]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.7485e-01, -1.9721e-01,  4.5275e-04, -1.9041e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 86 ] state=tensor([[-4.7485e-01, -1.9721e-01,  4.5275e-04, -1.9041e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4788, -0.3923, -0.0034,  0.1024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 87 ] state=tensor([[-0.4788, -0.3923, -0.0034,  0.1024]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4866, -0.5874, -0.0013,  0.3940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 88 ] state=tensor([[-0.4866, -0.5874, -0.0013,  0.3940]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4984, -0.3923,  0.0066,  0.1009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 89 ] state=tensor([[-0.4984, -0.3923,  0.0066,  0.1009]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5062, -0.5875,  0.0086,  0.3957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 90 ] state=tensor([[-0.5062, -0.5875,  0.0086,  0.3957]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5180, -0.3925,  0.0165,  0.1057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 91 ] state=tensor([[-0.5180, -0.3925,  0.0165,  0.1057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5258, -0.1976,  0.0186, -0.1817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 92 ] state=tensor([[-0.5258, -0.1976,  0.0186, -0.1817]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5298, -0.3930,  0.0150,  0.1168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 93 ] state=tensor([[-0.5298, -0.3930,  0.0150,  0.1168]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5377, -0.1981,  0.0173, -0.1711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 94 ] state=tensor([[-0.5377, -0.1981,  0.0173, -0.1711]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5416, -0.3935,  0.0139,  0.1270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 95 ] state=tensor([[-0.5416, -0.3935,  0.0139,  0.1270]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5495, -0.5888,  0.0164,  0.4240]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 96 ] state=tensor([[-0.5495, -0.5888,  0.0164,  0.4240]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5613, -0.3939,  0.0249,  0.1366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 97 ] state=tensor([[-0.5613, -0.3939,  0.0249,  0.1366]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5691, -0.5894,  0.0277,  0.4370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 98 ] state=tensor([[-0.5691, -0.5894,  0.0277,  0.4370]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5809, -0.3946,  0.0364,  0.1532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 99 ] state=tensor([[-0.5809, -0.3946,  0.0364,  0.1532]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5888, -0.2001,  0.0395, -0.1278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 100 ] state=tensor([[-0.5888, -0.2001,  0.0395, -0.1278]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5928, -0.0055,  0.0369, -0.4078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 101 ] state=tensor([[-0.5928, -0.0055,  0.0369, -0.4078]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5929, -0.2011,  0.0287, -0.1037]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 102 ] state=tensor([[-0.5929, -0.2011,  0.0287, -0.1037]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5969, -0.0064,  0.0267, -0.3872]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 103 ] state=tensor([[-0.5969, -0.0064,  0.0267, -0.3872]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5971,  0.1883,  0.0189, -0.6714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 104 ] state=tensor([[-0.5971,  0.1883,  0.0189, -0.6714]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5933, -0.0071,  0.0055, -0.3728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 105 ] state=tensor([[-0.5933, -0.0071,  0.0055, -0.3728]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5935, -0.2023, -0.0020, -0.0784]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 106 ] state=tensor([[-0.5935, -0.2023, -0.0020, -0.0784]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5975, -0.3974, -0.0035,  0.2137]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 107 ] state=tensor([[-0.5975, -0.3974, -0.0035,  0.2137]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6054, -0.2022,  0.0007, -0.0801]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 108 ] state=tensor([[-0.6054, -0.2022,  0.0007, -0.0801]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6095, -0.0071, -0.0009, -0.3725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 109 ] state=tensor([[-0.6095, -0.0071, -0.0009, -0.3725]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6096, -0.2022, -0.0083, -0.0801]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 110 ] state=tensor([[-0.6096, -0.2022, -0.0083, -0.0801]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6137, -0.3972, -0.0099,  0.2099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 111 ] state=tensor([[-0.6137, -0.3972, -0.0099,  0.2099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6216, -0.2019, -0.0057, -0.0859]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 112 ] state=tensor([[-0.6216, -0.2019, -0.0057, -0.0859]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6257, -0.3970, -0.0074,  0.2050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 113 ] state=tensor([[-0.6257, -0.3970, -0.0074,  0.2050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6336, -0.5920, -0.0033,  0.4953]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 114 ] state=tensor([[-0.6336, -0.5920, -0.0033,  0.4953]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6454, -0.3968,  0.0066,  0.2016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 115 ] state=tensor([[-0.6454, -0.3968,  0.0066,  0.2016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6534, -0.2018,  0.0106, -0.0890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 116 ] state=tensor([[-0.6534, -0.2018,  0.0106, -0.0890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6574, -0.0068,  0.0088, -0.3783]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 117 ] state=tensor([[-0.6574, -0.0068,  0.0088, -0.3783]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6575, -0.2021,  0.0013, -0.0829]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 118 ] state=tensor([[-0.6575, -0.2021,  0.0013, -0.0829]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-6.6159e-01, -6.9814e-03, -3.9135e-04, -3.7514e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 119 ] state=tensor([[-6.6159e-01, -6.9814e-03, -3.9135e-04, -3.7514e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6617, -0.2021, -0.0079, -0.0826]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 120 ] state=tensor([[-0.6617, -0.2021, -0.0079, -0.0826]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6658, -0.0069, -0.0095, -0.3777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 121 ] state=tensor([[-0.6658, -0.0069, -0.0095, -0.3777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6659, -0.2018, -0.0171, -0.0881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 122 ] state=tensor([[-0.6659, -0.2018, -0.0171, -0.0881]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6699, -0.3967, -0.0189,  0.1992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 123 ] state=tensor([[-0.6699, -0.3967, -0.0189,  0.1992]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6779, -0.5916, -0.0149,  0.4858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 124 ] state=tensor([[-0.6779, -0.5916, -0.0149,  0.4858]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6897, -0.3962, -0.0052,  0.1885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 125 ] state=tensor([[-0.6897, -0.3962, -0.0052,  0.1885]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6976, -0.2010, -0.0014, -0.1058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 126 ] state=tensor([[-0.6976, -0.2010, -0.0014, -0.1058]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7017, -0.3961, -0.0035,  0.1864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 127 ] state=tensor([[-0.7017, -0.3961, -0.0035,  0.1864]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.0958e-01, -2.0097e-01,  2.1971e-04, -1.0736e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 128 ] state=tensor([[-7.0958e-01, -2.0097e-01,  2.1971e-04, -1.0736e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7136, -0.3961, -0.0019,  0.1854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 129 ] state=tensor([[-0.7136, -0.3961, -0.0019,  0.1854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7215, -0.2009,  0.0018, -0.1079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 130 ] state=tensor([[-0.7215, -0.2009,  0.0018, -0.1079]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.2554e-01, -5.8536e-03, -3.7747e-04, -4.0002e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 131 ] state=tensor([[-7.2554e-01, -5.8536e-03, -3.7747e-04, -4.0002e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7257, -0.2010, -0.0084, -0.1075]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 121 ][ timestamp 132 ] state=tensor([[-0.7257, -0.2010, -0.0084, -0.1075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7297, -0.3960, -0.0105,  0.1826]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 133 ] state=tensor([[-0.7297, -0.3960, -0.0105,  0.1826]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7376, -0.2007, -0.0069, -0.1134]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 134 ] state=tensor([[-0.7376, -0.2007, -0.0069, -0.1134]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7416, -0.0055, -0.0091, -0.4083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 135 ] state=tensor([[-0.7416, -0.0055, -0.0091, -0.4083]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7417, -0.2005, -0.0173, -0.1185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 136 ] state=tensor([[-0.7417, -0.2005, -0.0173, -0.1185]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7457, -0.3953, -0.0197,  0.1687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 137 ] state=tensor([[-0.7457, -0.3953, -0.0197,  0.1687]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7536, -0.5902, -0.0163,  0.4551]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 138 ] state=tensor([[-0.7536, -0.5902, -0.0163,  0.4551]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7654, -0.3948, -0.0072,  0.1573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 139 ] state=tensor([[-0.7654, -0.3948, -0.0072,  0.1573]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7733, -0.1996, -0.0041, -0.1376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 140 ] state=tensor([[-0.7733, -0.1996, -0.0041, -0.1376]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7773, -0.3947, -0.0068,  0.1538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 141 ] state=tensor([[-0.7773, -0.3947, -0.0068,  0.1538]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7852, -0.1994, -0.0037, -0.1410]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 142 ] state=tensor([[-0.7852, -0.1994, -0.0037, -0.1410]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7892, -0.3945, -0.0066,  0.1505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 143 ] state=tensor([[-0.7892, -0.3945, -0.0066,  0.1505]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7971, -0.1993, -0.0035, -0.1443]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 144 ] state=tensor([[-0.7971, -0.1993, -0.0035, -0.1443]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8011, -0.3944, -0.0064,  0.1473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 145 ] state=tensor([[-0.8011, -0.3944, -0.0064,  0.1473]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8090, -0.1992, -0.0035, -0.1474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 146 ] state=tensor([[-0.8090, -0.1992, -0.0035, -0.1474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8130, -0.3942, -0.0064,  0.1442]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 147 ] state=tensor([[-0.8130, -0.3942, -0.0064,  0.1442]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8208, -0.1990, -0.0035, -0.1505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 148 ] state=tensor([[-0.8208, -0.1990, -0.0035, -0.1505]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8248, -0.3941, -0.0066,  0.1410]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 149 ] state=tensor([[-0.8248, -0.3941, -0.0066,  0.1410]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8327, -0.1989, -0.0037, -0.1537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 150 ] state=tensor([[-0.8327, -0.1989, -0.0037, -0.1537]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8367, -0.3939, -0.0068,  0.1378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 151 ] state=tensor([[-0.8367, -0.3939, -0.0068,  0.1378]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8446, -0.1987, -0.0041, -0.1570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 152 ] state=tensor([[-0.8446, -0.1987, -0.0041, -0.1570]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8485, -0.3938, -0.0072,  0.1344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 153 ] state=tensor([[-0.8485, -0.3938, -0.0072,  0.1344]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8564, -0.1986, -0.0045, -0.1606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 154 ] state=tensor([[-0.8564, -0.1986, -0.0045, -0.1606]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8604, -0.3936, -0.0077,  0.1307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 155 ] state=tensor([[-0.8604, -0.3936, -0.0077,  0.1307]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8683, -0.5886, -0.0051,  0.4209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 156 ] state=tensor([[-0.8683, -0.5886, -0.0051,  0.4209]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8800, -0.3934,  0.0033,  0.1266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 157 ] state=tensor([[-0.8800, -0.3934,  0.0033,  0.1266]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8879, -0.1984,  0.0058, -0.1650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 158 ] state=tensor([[-0.8879, -0.1984,  0.0058, -0.1650]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8919, -0.0033,  0.0025, -0.4558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 159 ] state=tensor([[-0.8919, -0.0033,  0.0025, -0.4558]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8919, -0.1985, -0.0066, -0.1624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 160 ] state=tensor([[-0.8919, -0.1985, -0.0066, -0.1624]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8959, -0.3935, -0.0098,  0.1282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 161 ] state=tensor([[-0.8959, -0.3935, -0.0098,  0.1282]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9038, -0.1982, -0.0073, -0.1675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 162 ] state=tensor([[-0.9038, -0.1982, -0.0073, -0.1675]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9077, -0.3933, -0.0106,  0.1229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 163 ] state=tensor([[-0.9077, -0.3933, -0.0106,  0.1229]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9156, -0.1980, -0.0081, -0.1731]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 164 ] state=tensor([[-0.9156, -0.1980, -0.0081, -0.1731]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9196, -0.3930, -0.0116,  0.1170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 165 ] state=tensor([[-0.9196, -0.3930, -0.0116,  0.1170]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9274, -0.1977, -0.0093, -0.1794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 166 ] state=tensor([[-0.9274, -0.1977, -0.0093, -0.1794]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9314, -0.3927, -0.0129,  0.1104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 167 ] state=tensor([[-0.9314, -0.3927, -0.0129,  0.1104]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9392, -0.5876, -0.0107,  0.3990]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 168 ] state=tensor([[-0.9392, -0.5876, -0.0107,  0.3990]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9510, -0.3924, -0.0027,  0.1030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 169 ] state=tensor([[-0.9510, -0.3924, -0.0027,  0.1030]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-9.5883e-01, -1.9720e-01, -6.1214e-04, -1.9057e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 170 ] state=tensor([[-9.5883e-01, -1.9720e-01, -6.1214e-04, -1.9057e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9628, -0.3923, -0.0044,  0.1019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 171 ] state=tensor([[-0.9628, -0.3923, -0.0044,  0.1019]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9706, -0.1971, -0.0024, -0.1922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 172 ] state=tensor([[-0.9706, -0.1971, -0.0024, -0.1922]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9746, -0.3922, -0.0062,  0.0998]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 173 ] state=tensor([[-0.9746, -0.3922, -0.0062,  0.0998]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9824, -0.1970, -0.0042, -0.1949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 174 ] state=tensor([[-0.9824, -0.1970, -0.0042, -0.1949]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9863, -0.3921, -0.0081,  0.0965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 175 ] state=tensor([[-0.9863, -0.3921, -0.0081,  0.0965]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9942, -0.1968, -0.0062, -0.1988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 176 ] state=tensor([[-0.9942, -0.1968, -0.0062, -0.1988]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9981, -0.3919, -0.0102,  0.0920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 177 ] state=tensor([[-0.9981, -0.3919, -0.0102,  0.0920]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0060, -0.1966, -0.0083, -0.2039]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 178 ] state=tensor([[-1.0060, -0.1966, -0.0083, -0.2039]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0099, -0.3916, -0.0124,  0.0861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 179 ] state=tensor([[-1.0099, -0.3916, -0.0124,  0.0861]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0177, -0.1963, -0.0107, -0.2104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 180 ] state=tensor([[-1.0177, -0.1963, -0.0107, -0.2104]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0217, -0.3913, -0.0149,  0.0788]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 181 ] state=tensor([[-1.0217, -0.3913, -0.0149,  0.0788]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0295, -0.1959, -0.0133, -0.2185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 182 ] state=tensor([[-1.0295, -0.1959, -0.0133, -0.2185]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0334, -0.3909, -0.0177,  0.0699]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 183 ] state=tensor([[-1.0334, -0.3909, -0.0177,  0.0699]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0412, -0.5857, -0.0163,  0.3570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 184 ] state=tensor([[-1.0412, -0.5857, -0.0163,  0.3570]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0529, -0.3904, -0.0092,  0.0592]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 185 ] state=tensor([[-1.0529, -0.3904, -0.0092,  0.0592]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0607, -0.1951, -0.0080, -0.2363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 186 ] state=tensor([[-1.0607, -0.1951, -0.0080, -0.2363]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0646, -0.3901, -0.0127,  0.0538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 187 ] state=tensor([[-1.0646, -0.3901, -0.0127,  0.0538]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0724, -0.1948, -0.0116, -0.2428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 188 ] state=tensor([[-1.0724, -0.1948, -0.0116, -0.2428]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0763, -0.3898, -0.0165,  0.0462]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 189 ] state=tensor([[-1.0763, -0.3898, -0.0165,  0.0462]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0841, -0.1944, -0.0156, -0.2517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 190 ] state=tensor([[-1.0841, -0.1944, -0.0156, -0.2517]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0880, -0.3893, -0.0206,  0.0361]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 191 ] state=tensor([[-1.0880, -0.3893, -0.0206,  0.0361]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0958, -0.1939, -0.0199, -0.2631]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 121 ][ timestamp 192 ] state=tensor([[-1.0958, -0.1939, -0.0199, -0.2631]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0997, -0.3888, -0.0251,  0.0233]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 193 ] state=tensor([[-1.0997, -0.3888, -0.0251,  0.0233]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1075, -0.5835, -0.0247,  0.3079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 194 ] state=tensor([[-1.1075, -0.5835, -0.0247,  0.3079]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1191, -0.3880, -0.0185,  0.0076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 195 ] state=tensor([[-1.1191, -0.3880, -0.0185,  0.0076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1269, -0.5829, -0.0184,  0.2944]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 196 ] state=tensor([[-1.1269, -0.5829, -0.0184,  0.2944]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1386, -0.3875, -0.0125, -0.0040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 197 ] state=tensor([[-1.1386, -0.3875, -0.0125, -0.0040]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1463, -0.5825, -0.0125,  0.2847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 198 ] state=tensor([[-1.1463, -0.5825, -0.0125,  0.2847]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1579, -0.3872, -0.0069, -0.0119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 199 ] state=tensor([[-1.1579, -0.3872, -0.0069, -0.0119]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1657, -0.5822, -0.0071,  0.2786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 200 ] state=tensor([[-1.1657, -0.5822, -0.0071,  0.2786]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1773, -0.3870, -0.0015, -0.0163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 201 ] state=tensor([[-1.1773, -0.3870, -0.0015, -0.0163]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1851, -0.5821, -0.0018,  0.2759]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 202 ] state=tensor([[-1.1851, -0.5821, -0.0018,  0.2759]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1967, -0.3869,  0.0037, -0.0174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 203 ] state=tensor([[-1.1967, -0.3869,  0.0037, -0.0174]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2045, -0.1918,  0.0033, -0.3089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 204 ] state=tensor([[-1.2045, -0.1918,  0.0033, -0.3089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2083, -0.3870, -0.0029, -0.0152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 205 ] state=tensor([[-1.2083, -0.3870, -0.0029, -0.0152]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2160, -0.1918, -0.0032, -0.3088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 206 ] state=tensor([[-1.2160, -0.1918, -0.0032, -0.3088]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2199, -0.3869, -0.0093, -0.0171]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 207 ] state=tensor([[-1.2199, -0.3869, -0.0093, -0.0171]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2276, -0.1917, -0.0097, -0.3127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 208 ] state=tensor([[-1.2276, -0.1917, -0.0097, -0.3127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2314, -0.3867, -0.0159, -0.0231]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 209 ] state=tensor([[-1.2314, -0.3867, -0.0159, -0.0231]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2392, -0.1913, -0.0164, -0.3207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 210 ] state=tensor([[-1.2392, -0.1913, -0.0164, -0.3207]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2430, -0.3862, -0.0228, -0.0333]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 211 ] state=tensor([[-1.2430, -0.3862, -0.0228, -0.0333]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2507, -0.5810, -0.0235,  0.2522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 212 ] state=tensor([[-1.2507, -0.5810, -0.0235,  0.2522]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2623, -0.3855, -0.0184, -0.0478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 213 ] state=tensor([[-1.2623, -0.3855, -0.0184, -0.0478]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2701, -0.5804, -0.0194,  0.2390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 214 ] state=tensor([[-1.2701, -0.5804, -0.0194,  0.2390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2817, -0.3850, -0.0146, -0.0598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 215 ] state=tensor([[-1.2817, -0.3850, -0.0146, -0.0598]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2894, -0.5799, -0.0158,  0.2283]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 216 ] state=tensor([[-1.2894, -0.5799, -0.0158,  0.2283]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3010, -0.3846, -0.0112, -0.0693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 217 ] state=tensor([[-1.3010, -0.3846, -0.0112, -0.0693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3086, -0.5795, -0.0126,  0.2198]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 218 ] state=tensor([[-1.3086, -0.5795, -0.0126,  0.2198]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3202, -0.3842, -0.0082, -0.0769]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 219 ] state=tensor([[-1.3202, -0.3842, -0.0082, -0.0769]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3279, -0.5792, -0.0098,  0.2132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 220 ] state=tensor([[-1.3279, -0.5792, -0.0098,  0.2132]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3395, -0.3840, -0.0055, -0.0825]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 221 ] state=tensor([[-1.3395, -0.3840, -0.0055, -0.0825]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3472, -0.5790, -0.0071,  0.2084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 222 ] state=tensor([[-1.3472, -0.5790, -0.0071,  0.2084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3588, -0.3838, -0.0030, -0.0865]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 223 ] state=tensor([[-1.3588, -0.3838, -0.0030, -0.0865]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3664, -0.5789, -0.0047,  0.2052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 224 ] state=tensor([[-1.3664, -0.5789, -0.0047,  0.2052]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3780e+00, -3.8366e-01, -5.9996e-04, -8.8925e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 225 ] state=tensor([[-1.3780e+00, -3.8366e-01, -5.9996e-04, -8.8925e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3857, -0.5788, -0.0024,  0.2036]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 226 ] state=tensor([[-1.3857, -0.5788, -0.0024,  0.2036]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3973, -0.3836,  0.0017, -0.0899]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 227 ] state=tensor([[-1.3973, -0.3836,  0.0017, -0.0899]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4049e+00, -5.7877e-01, -1.0438e-04,  2.0335e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 228 ] state=tensor([[-1.4049e+00, -5.7877e-01, -1.0438e-04,  2.0335e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4165, -0.3836,  0.0040, -0.0894]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 229 ] state=tensor([[-1.4165, -0.3836,  0.0040, -0.0894]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4242, -0.5788,  0.0022,  0.2046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 230 ] state=tensor([[-1.4242, -0.5788,  0.0022,  0.2046]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4358, -0.3837,  0.0063, -0.0874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 231 ] state=tensor([[-1.4358, -0.3837,  0.0063, -0.0874]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4434, -0.5789,  0.0045,  0.2072]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 232 ] state=tensor([[-1.4434, -0.5789,  0.0045,  0.2072]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4550, -0.3839,  0.0087, -0.0840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 233 ] state=tensor([[-1.4550, -0.3839,  0.0087, -0.0840]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4627, -0.5791,  0.0070,  0.2114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 234 ] state=tensor([[-1.4627, -0.5791,  0.0070,  0.2114]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4743, -0.3841,  0.0112, -0.0791]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 235 ] state=tensor([[-1.4743, -0.3841,  0.0112, -0.0791]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4820, -0.1892,  0.0096, -0.3682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 236 ] state=tensor([[-1.4820, -0.1892,  0.0096, -0.3682]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4857, -0.3844,  0.0023, -0.0725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 237 ] state=tensor([[-1.4857, -0.3844,  0.0023, -0.0725]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4934e+00, -5.7956e-01,  8.1273e-04,  2.2088e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 238 ] state=tensor([[-1.4934e+00, -5.7956e-01,  8.1273e-04,  2.2088e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5050, -0.3845,  0.0052, -0.0716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 239 ] state=tensor([[-1.5050, -0.3845,  0.0052, -0.0716]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5127, -0.5796,  0.0038,  0.2228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 240 ] state=tensor([[-1.5127, -0.5796,  0.0038,  0.2228]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5243, -0.3846,  0.0083, -0.0687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 241 ] state=tensor([[-1.5243, -0.3846,  0.0083, -0.0687]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5320, -0.5798,  0.0069,  0.2266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 242 ] state=tensor([[-1.5320, -0.5798,  0.0069,  0.2266]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5436, -0.3848,  0.0114, -0.0639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 243 ] state=tensor([[-1.5436, -0.3848,  0.0114, -0.0639]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5513, -0.5801,  0.0101,  0.2323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 244 ] state=tensor([[-1.5513, -0.5801,  0.0101,  0.2323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5629, -0.3851,  0.0148, -0.0571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 245 ] state=tensor([[-1.5629, -0.3851,  0.0148, -0.0571]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5706, -0.5804,  0.0136,  0.2402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 246 ] state=tensor([[-1.5706, -0.5804,  0.0136,  0.2402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5822, -0.7758,  0.0184,  0.5371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 247 ] state=tensor([[-1.5822, -0.7758,  0.0184,  0.5371]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5977, -0.9711,  0.0292,  0.8356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 248 ] state=tensor([[-1.5977, -0.9711,  0.0292,  0.8356]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6171, -1.1666,  0.0459,  1.1373]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 249 ] state=tensor([[-1.6171, -1.1666,  0.0459,  1.1373]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6405, -1.3623,  0.0686,  1.4440]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 250 ] state=tensor([[-1.6405, -1.3623,  0.0686,  1.4440]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6677, -1.5582,  0.0975,  1.7573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 251 ] state=tensor([[-1.6677, -1.5582,  0.0975,  1.7573]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6989, -1.3643,  0.1327,  1.4965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 252 ] state=tensor([[-1.6989, -1.3643,  0.1327,  1.4965]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7262, -1.1710,  0.1626,  1.2480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 253 ] state=tensor([[-1.7262, -1.1710,  0.1626,  1.2480]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7496, -1.3678,  0.1876,  1.5869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 121 ][ timestamp 254 ] state=tensor([[-1.7496, -1.3678,  0.1876,  1.5869]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 121: Exploration_rate=0.05. Score=254.\n",
      "[ episode 122 ] state=tensor([[ 0.0019, -0.0457,  0.0499,  0.0048]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 1 ] state=tensor([[ 0.0019, -0.0457,  0.0499,  0.0048]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0009,  0.1487,  0.0500, -0.2717]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 2 ] state=tensor([[ 0.0009,  0.1487,  0.0500, -0.2717]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0039,  0.3431,  0.0446, -0.5482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 3 ] state=tensor([[ 0.0039,  0.3431,  0.0446, -0.5482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0108,  0.5376,  0.0336, -0.8265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 4 ] state=tensor([[ 0.0108,  0.5376,  0.0336, -0.8265]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0215,  0.3420,  0.0171, -0.5235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 5 ] state=tensor([[ 0.0215,  0.3420,  0.0171, -0.5235]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0284,  0.1466,  0.0066, -0.2255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 6 ] state=tensor([[ 0.0284,  0.1466,  0.0066, -0.2255]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0313,  0.3417,  0.0021, -0.5161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 7 ] state=tensor([[ 0.0313,  0.3417,  0.0021, -0.5161]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0381,  0.5367, -0.0082, -0.8081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 8 ] state=tensor([[ 0.0381,  0.5367, -0.0082, -0.8081]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0489,  0.3417, -0.0244, -0.5180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 9 ] state=tensor([[ 0.0489,  0.3417, -0.0244, -0.5180]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0557,  0.1470, -0.0348, -0.2331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 10 ] state=tensor([[ 0.0557,  0.1470, -0.0348, -0.2331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0586, -0.0476, -0.0394,  0.0484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 11 ] state=tensor([[ 0.0586, -0.0476, -0.0394,  0.0484]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0577,  0.1480, -0.0385, -0.2565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 12 ] state=tensor([[ 0.0577,  0.1480, -0.0385, -0.2565]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0606,  0.3437, -0.0436, -0.5610]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 13 ] state=tensor([[ 0.0606,  0.3437, -0.0436, -0.5610]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0675,  0.1492, -0.0548, -0.2824]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 14 ] state=tensor([[ 0.0675,  0.1492, -0.0548, -0.2824]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0705,  0.3451, -0.0605, -0.5918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 15 ] state=tensor([[ 0.0705,  0.3451, -0.0605, -0.5918]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0774,  0.1508, -0.0723, -0.3188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 16 ] state=tensor([[ 0.0774,  0.1508, -0.0723, -0.3188]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0804, -0.0432, -0.0787, -0.0498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 17 ] state=tensor([[ 0.0804, -0.0432, -0.0787, -0.0498]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0796,  0.1530, -0.0797, -0.3662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 18 ] state=tensor([[ 0.0796,  0.1530, -0.0797, -0.3662]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0826, -0.0409, -0.0870, -0.0996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 19 ] state=tensor([[ 0.0826, -0.0409, -0.0870, -0.0996]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0818, -0.2347, -0.0890,  0.1644]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 20 ] state=tensor([[ 0.0818, -0.2347, -0.0890,  0.1644]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0771, -0.4285, -0.0857,  0.4277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 21 ] state=tensor([[ 0.0771, -0.4285, -0.0857,  0.4277]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0685, -0.2322, -0.0771,  0.1093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 22 ] state=tensor([[ 0.0685, -0.2322, -0.0771,  0.1093]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0639, -0.4262, -0.0750,  0.3767]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 23 ] state=tensor([[ 0.0639, -0.4262, -0.0750,  0.3767]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0554, -0.6202, -0.0674,  0.6448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 24 ] state=tensor([[ 0.0554, -0.6202, -0.0674,  0.6448]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0430, -0.8143, -0.0545,  0.9155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 25 ] state=tensor([[ 0.0430, -0.8143, -0.0545,  0.9155]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0267, -1.0086, -0.0362,  1.1906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 26 ] state=tensor([[ 0.0267, -1.0086, -0.0362,  1.1906]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0065, -0.8130, -0.0124,  0.8868]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 27 ] state=tensor([[ 0.0065, -0.8130, -0.0124,  0.8868]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0098, -0.6178,  0.0053,  0.5902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 28 ] state=tensor([[-0.0098, -0.6178,  0.0053,  0.5902]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0221, -0.4227,  0.0171,  0.2992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 29 ] state=tensor([[-0.0221, -0.4227,  0.0171,  0.2992]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0306, -0.2278,  0.0231,  0.0120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 30 ] state=tensor([[-0.0306, -0.2278,  0.0231,  0.0120]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0351, -0.4233,  0.0234,  0.3119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 31 ] state=tensor([[-0.0351, -0.4233,  0.0234,  0.3119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0436, -0.2285,  0.0296,  0.0267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 32 ] state=tensor([[-0.0436, -0.2285,  0.0296,  0.0267]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0482, -0.0338,  0.0301, -0.2565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 33 ] state=tensor([[-0.0482, -0.0338,  0.0301, -0.2565]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0488,  0.1609,  0.0250, -0.5395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 34 ] state=tensor([[-0.0488,  0.1609,  0.0250, -0.5395]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0456, -0.0346,  0.0142, -0.2391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 35 ] state=tensor([[-0.0456, -0.0346,  0.0142, -0.2391]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0463,  0.1603,  0.0094, -0.5273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 36 ] state=tensor([[-0.0463,  0.1603,  0.0094, -0.5273]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0431, -0.0349, -0.0011, -0.2316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 37 ] state=tensor([[-0.0431, -0.0349, -0.0011, -0.2316]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0438, -0.2300, -0.0057,  0.0607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 38 ] state=tensor([[-0.0438, -0.2300, -0.0057,  0.0607]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0484, -0.4251, -0.0045,  0.3516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 39 ] state=tensor([[-0.0484, -0.4251, -0.0045,  0.3516]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0569, -0.2299,  0.0025,  0.0575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 40 ] state=tensor([[-0.0569, -0.2299,  0.0025,  0.0575]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0615, -0.4251,  0.0037,  0.3509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 41 ] state=tensor([[-0.0615, -0.4251,  0.0037,  0.3509]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0700, -0.2300,  0.0107,  0.0594]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 42 ] state=tensor([[-0.0700, -0.2300,  0.0107,  0.0594]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0746, -0.0350,  0.0119, -0.2299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 43 ] state=tensor([[-0.0746, -0.0350,  0.0119, -0.2299]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0753,  0.1599,  0.0073, -0.5188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 44 ] state=tensor([[-0.0753,  0.1599,  0.0073, -0.5188]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0721, -0.0353, -0.0031, -0.2238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 45 ] state=tensor([[-0.0721, -0.0353, -0.0031, -0.2238]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0728,  0.1599, -0.0076, -0.5175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 46 ] state=tensor([[-0.0728,  0.1599, -0.0076, -0.5175]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0696, -0.0351, -0.0179, -0.2272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 47 ] state=tensor([[-0.0696, -0.0351, -0.0179, -0.2272]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0703, -0.2300, -0.0225,  0.0598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 48 ] state=tensor([[-0.0703, -0.2300, -0.0225,  0.0598]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0749, -0.0346, -0.0213, -0.2399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 49 ] state=tensor([[-0.0749, -0.0346, -0.0213, -0.2399]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0756,  0.1609, -0.0261, -0.5393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 50 ] state=tensor([[-0.0756,  0.1609, -0.0261, -0.5393]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0724, -0.0339, -0.0369, -0.2549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 51 ] state=tensor([[-0.0724, -0.0339, -0.0369, -0.2549]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0731, -0.2285, -0.0420,  0.0259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 52 ] state=tensor([[-0.0731, -0.2285, -0.0420,  0.0259]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0776, -0.4230, -0.0415,  0.3051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 53 ] state=tensor([[-0.0776, -0.4230, -0.0415,  0.3051]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0861, -0.6175, -0.0353,  0.5844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 54 ] state=tensor([[-0.0861, -0.6175, -0.0353,  0.5844]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0984, -0.8121, -0.0237,  0.8657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 55 ] state=tensor([[-0.0984, -0.8121, -0.0237,  0.8657]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1147, -0.6166, -0.0063,  0.5657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 56 ] state=tensor([[-0.1147, -0.6166, -0.0063,  0.5657]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1270, -0.4214,  0.0050,  0.2710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 57 ] state=tensor([[-0.1270, -0.4214,  0.0050,  0.2710]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1355, -0.2264,  0.0104, -0.0201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 58 ] state=tensor([[-0.1355, -0.2264,  0.0104, -0.0201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1400, -0.0314,  0.0100, -0.3095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 59 ] state=tensor([[-0.1400, -0.0314,  0.0100, -0.3095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1406,  0.1636,  0.0038, -0.5990]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 60 ] state=tensor([[-0.1406,  0.1636,  0.0038, -0.5990]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1373, -0.0316, -0.0082, -0.3051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 61 ] state=tensor([[-0.1373, -0.0316, -0.0082, -0.3051]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1380, -0.2266, -0.0143, -0.0150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 62 ] state=tensor([[-0.1380, -0.2266, -0.0143, -0.0150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1425, -0.0313, -0.0146, -0.3122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 63 ] state=tensor([[-0.1425, -0.0313, -0.0146, -0.3122]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1431, -0.2262, -0.0208, -0.0241]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 64 ] state=tensor([[-0.1431, -0.2262, -0.0208, -0.0241]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1477, -0.4210, -0.0213,  0.2619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 65 ] state=tensor([[-0.1477, -0.4210, -0.0213,  0.2619]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1561, -0.2256, -0.0161, -0.0374]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 66 ] state=tensor([[-0.1561, -0.2256, -0.0161, -0.0374]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1606, -0.0303, -0.0168, -0.3351]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 67 ] state=tensor([[-0.1606, -0.0303, -0.0168, -0.3351]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1612, -0.2251, -0.0235, -0.0478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 68 ] state=tensor([[-0.1612, -0.2251, -0.0235, -0.0478]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1657, -0.4199, -0.0245,  0.2374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 69 ] state=tensor([[-0.1657, -0.4199, -0.0245,  0.2374]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1741, -0.6147, -0.0197,  0.5223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 70 ] state=tensor([[-0.1741, -0.6147, -0.0197,  0.5223]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1864, -0.8095, -0.0093,  0.8087]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 71 ] state=tensor([[-0.1864, -0.8095, -0.0093,  0.8087]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2026, -0.6143,  0.0069,  0.5131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 72 ] state=tensor([[-0.2026, -0.6143,  0.0069,  0.5131]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2149, -0.8095,  0.0172,  0.8079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 73 ] state=tensor([[-0.2149, -0.8095,  0.0172,  0.8079]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2310, -0.6146,  0.0333,  0.5207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 74 ] state=tensor([[-0.2310, -0.6146,  0.0333,  0.5207]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2433, -0.4200,  0.0437,  0.2387]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 75 ] state=tensor([[-0.2433, -0.4200,  0.0437,  0.2387]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2517, -0.2255,  0.0485, -0.0399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 76 ] state=tensor([[-0.2517, -0.2255,  0.0485, -0.0399]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2562, -0.0311,  0.0477, -0.3169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 77 ] state=tensor([[-0.2562, -0.0311,  0.0477, -0.3169]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2569, -0.2269,  0.0414, -0.0095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 78 ] state=tensor([[-0.2569, -0.2269,  0.0414, -0.0095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2614, -0.0324,  0.0412, -0.2889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 79 ] state=tensor([[-0.2614, -0.0324,  0.0412, -0.2889]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2621,  0.1622,  0.0354, -0.5683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 80 ] state=tensor([[-0.2621,  0.1622,  0.0354, -0.5683]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2588,  0.3568,  0.0240, -0.8496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 81 ] state=tensor([[-0.2588,  0.3568,  0.0240, -0.8496]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2517,  0.1613,  0.0070, -0.5495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 82 ] state=tensor([[-0.2517,  0.1613,  0.0070, -0.5495]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2485,  0.3563, -0.0040, -0.8399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 83 ] state=tensor([[-0.2485,  0.3563, -0.0040, -0.8399]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2413,  0.1613, -0.0207, -0.5485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 84 ] state=tensor([[-0.2413,  0.1613, -0.0207, -0.5485]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2381, -0.0336, -0.0317, -0.2624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 85 ] state=tensor([[-0.2381, -0.0336, -0.0317, -0.2624]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2388,  0.1620, -0.0370, -0.5650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 86 ] state=tensor([[-0.2388,  0.1620, -0.0370, -0.5650]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2355, -0.0326, -0.0483, -0.2841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 87 ] state=tensor([[-0.2355, -0.0326, -0.0483, -0.2841]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2362, -0.2270, -0.0540, -0.0071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 88 ] state=tensor([[-0.2362, -0.2270, -0.0540, -0.0071]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2407, -0.4213, -0.0541,  0.2681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 89 ] state=tensor([[-0.2407, -0.4213, -0.0541,  0.2681]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2491, -0.6156, -0.0487,  0.5433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 90 ] state=tensor([[-0.2491, -0.6156, -0.0487,  0.5433]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2615, -0.4198, -0.0379,  0.2356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 91 ] state=tensor([[-0.2615, -0.4198, -0.0379,  0.2356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2699, -0.2242, -0.0332, -0.0687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 92 ] state=tensor([[-0.2699, -0.2242, -0.0332, -0.0687]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2743, -0.4188, -0.0345,  0.2133]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 93 ] state=tensor([[-0.2743, -0.4188, -0.0345,  0.2133]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2827, -0.2232, -0.0303, -0.0901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 94 ] state=tensor([[-0.2827, -0.2232, -0.0303, -0.0901]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2872, -0.0277, -0.0321, -0.3921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 95 ] state=tensor([[-0.2872, -0.0277, -0.0321, -0.3921]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2877, -0.2223, -0.0399, -0.1097]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 96 ] state=tensor([[-0.2877, -0.2223, -0.0399, -0.1097]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2922, -0.4169, -0.0421,  0.1701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 97 ] state=tensor([[-0.2922, -0.4169, -0.0421,  0.1701]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3005, -0.6113, -0.0387,  0.4492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 98 ] state=tensor([[-0.3005, -0.6113, -0.0387,  0.4492]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3127, -0.4157, -0.0297,  0.1446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 99 ] state=tensor([[-0.3127, -0.4157, -0.0297,  0.1446]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3211, -0.2202, -0.0268, -0.1573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 100 ] state=tensor([[-0.3211, -0.2202, -0.0268, -0.1573]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3255, -0.0247, -0.0300, -0.4584]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 101 ] state=tensor([[-0.3255, -0.0247, -0.0300, -0.4584]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3260, -0.2194, -0.0391, -0.1753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 102 ] state=tensor([[-0.3260, -0.2194, -0.0391, -0.1753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3303, -0.4139, -0.0426,  0.1048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 103 ] state=tensor([[-0.3303, -0.4139, -0.0426,  0.1048]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3386, -0.6084, -0.0405,  0.3837]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 104 ] state=tensor([[-0.3386, -0.6084, -0.0405,  0.3837]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3508, -0.8029, -0.0329,  0.6634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 105 ] state=tensor([[-0.3508, -0.8029, -0.0329,  0.6634]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3668, -0.6073, -0.0196,  0.3605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 106 ] state=tensor([[-0.3668, -0.6073, -0.0196,  0.3605]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3790, -0.8022, -0.0124,  0.6470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 107 ] state=tensor([[-0.3790, -0.8022, -0.0124,  0.6470]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.9503e-01, -6.0689e-01,  5.4631e-04,  3.5040e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 108 ] state=tensor([[-3.9503e-01, -6.0689e-01,  5.4631e-04,  3.5040e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4072, -0.4118,  0.0076,  0.0579]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 109 ] state=tensor([[-0.4072, -0.4118,  0.0076,  0.0579]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4154, -0.6070,  0.0087,  0.3529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 110 ] state=tensor([[-0.4154, -0.6070,  0.0087,  0.3529]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4275, -0.4120,  0.0158,  0.0630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 111 ] state=tensor([[-0.4275, -0.4120,  0.0158,  0.0630]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4358, -0.6074,  0.0170,  0.3606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 112 ] state=tensor([[-0.4358, -0.6074,  0.0170,  0.3606]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4479, -0.4125,  0.0242,  0.0734]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 113 ] state=tensor([[-0.4479, -0.4125,  0.0242,  0.0734]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4562, -0.2177,  0.0257, -0.2116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 114 ] state=tensor([[-0.4562, -0.2177,  0.0257, -0.2116]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4605, -0.4132,  0.0215,  0.0891]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 115 ] state=tensor([[-0.4605, -0.4132,  0.0215,  0.0891]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4688, -0.2184,  0.0233, -0.1967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 116 ] state=tensor([[-0.4688, -0.2184,  0.0233, -0.1967]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4732, -0.4138,  0.0193,  0.1032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 117 ] state=tensor([[-0.4732, -0.4138,  0.0193,  0.1032]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4814, -0.2190,  0.0214, -0.1833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 118 ] state=tensor([[-0.4814, -0.2190,  0.0214, -0.1833]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4858, -0.0242,  0.0177, -0.4692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 119 ] state=tensor([[-0.4858, -0.0242,  0.0177, -0.4692]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4863, -0.2195,  0.0083, -0.1709]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 120 ] state=tensor([[-0.4863, -0.2195,  0.0083, -0.1709]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4907, -0.4148,  0.0049,  0.1244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 121 ] state=tensor([[-0.4907, -0.4148,  0.0049,  0.1244]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4990, -0.2197,  0.0074, -0.1668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 122 ] state=tensor([[-0.4990, -0.2197,  0.0074, -0.1668]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5034, -0.0247,  0.0041, -0.4571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 123 ] state=tensor([[-0.5034, -0.0247,  0.0041, -0.4571]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5039, -0.2199, -0.0051, -0.1631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 124 ] state=tensor([[-0.5039, -0.2199, -0.0051, -0.1631]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5083, -0.4150, -0.0083,  0.1279]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 125 ] state=tensor([[-0.5083, -0.4150, -0.0083,  0.1279]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5166, -0.6100, -0.0058,  0.4180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 126 ] state=tensor([[-0.5166, -0.6100, -0.0058,  0.4180]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5288, -0.4147,  0.0026,  0.1235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 127 ] state=tensor([[-0.5288, -0.4147,  0.0026,  0.1235]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5371, -0.2197,  0.0051, -0.1684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 128 ] state=tensor([[-0.5371, -0.2197,  0.0051, -0.1684]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5415, -0.0246,  0.0017, -0.4594]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 129 ] state=tensor([[-0.5415, -0.0246,  0.0017, -0.4594]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5420, -0.2198, -0.0075, -0.1662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 130 ] state=tensor([[-0.5420, -0.2198, -0.0075, -0.1662]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5464, -0.4148, -0.0108,  0.1241]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 131 ] state=tensor([[-0.5464, -0.4148, -0.0108,  0.1241]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5547, -0.2195, -0.0083, -0.1720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 132 ] state=tensor([[-0.5547, -0.2195, -0.0083, -0.1720]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5590, -0.4145, -0.0118,  0.1180]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 133 ] state=tensor([[-0.5590, -0.4145, -0.0118,  0.1180]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5673, -0.6095, -0.0094,  0.4070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 134 ] state=tensor([[-0.5673, -0.6095, -0.0094,  0.4070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5795, -0.4142, -0.0013,  0.1113]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 135 ] state=tensor([[-0.5795, -0.4142, -0.0013,  0.1113]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5878, -0.2191,  0.0009, -0.1817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 136 ] state=tensor([[-0.5878, -0.2191,  0.0009, -0.1817]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5922, -0.4142, -0.0027,  0.1112]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 137 ] state=tensor([[-0.5922, -0.4142, -0.0027,  0.1112]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-6.0047e-01, -2.1903e-01, -4.6041e-04, -1.8228e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 138 ] state=tensor([[-6.0047e-01, -2.1903e-01, -4.6041e-04, -1.8228e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6049, -0.4141, -0.0041,  0.1103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 139 ] state=tensor([[-0.6049, -0.4141, -0.0041,  0.1103]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6131, -0.2190, -0.0019, -0.1837]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 140 ] state=tensor([[-0.6131, -0.2190, -0.0019, -0.1837]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6175, -0.4141, -0.0056,  0.1084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 141 ] state=tensor([[-0.6175, -0.4141, -0.0056,  0.1084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6258, -0.2189, -0.0034, -0.1861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 142 ] state=tensor([[-0.6258, -0.2189, -0.0034, -0.1861]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6302, -0.4139, -0.0071,  0.1055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 143 ] state=tensor([[-0.6302, -0.4139, -0.0071,  0.1055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6385, -0.2187, -0.0050, -0.1894]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 144 ] state=tensor([[-0.6385, -0.2187, -0.0050, -0.1894]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6428, -0.4138, -0.0088,  0.1017]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 145 ] state=tensor([[-0.6428, -0.4138, -0.0088,  0.1017]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6511, -0.2185, -0.0068, -0.1937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 146 ] state=tensor([[-0.6511, -0.2185, -0.0068, -0.1937]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6555, -0.4135, -0.0106,  0.0968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 147 ] state=tensor([[-0.6555, -0.4135, -0.0106,  0.0968]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6637, -0.6085, -0.0087,  0.3861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 148 ] state=tensor([[-0.6637, -0.6085, -0.0087,  0.3861]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6759, -0.4133, -0.0010,  0.0907]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 149 ] state=tensor([[-0.6759, -0.4133, -0.0010,  0.0907]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6842, -0.2181,  0.0008, -0.2023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 150 ] state=tensor([[-0.6842, -0.2181,  0.0008, -0.2023]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6885, -0.4133, -0.0032,  0.0906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 151 ] state=tensor([[-0.6885, -0.4133, -0.0032,  0.0906]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6968, -0.2181, -0.0014, -0.2031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 152 ] state=tensor([[-0.6968, -0.2181, -0.0014, -0.2031]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7012, -0.4132, -0.0055,  0.0892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 153 ] state=tensor([[-0.7012, -0.4132, -0.0055,  0.0892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7094, -0.6082, -0.0037,  0.3801]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 154 ] state=tensor([[-0.7094, -0.6082, -0.0037,  0.3801]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7216, -0.4131,  0.0039,  0.0863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 155 ] state=tensor([[-0.7216, -0.4131,  0.0039,  0.0863]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7299, -0.2180,  0.0056, -0.2052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 156 ] state=tensor([[-0.7299, -0.2180,  0.0056, -0.2052]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7342, -0.0230,  0.0015, -0.4961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 157 ] state=tensor([[-0.7342, -0.0230,  0.0015, -0.4961]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7347, -0.2181, -0.0084, -0.2029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 158 ] state=tensor([[-0.7347, -0.2181, -0.0084, -0.2029]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7390, -0.4131, -0.0124,  0.0871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 159 ] state=tensor([[-0.7390, -0.4131, -0.0124,  0.0871]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7473, -0.2178, -0.0107, -0.2095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 160 ] state=tensor([[-0.7473, -0.2178, -0.0107, -0.2095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7517, -0.4128, -0.0149,  0.0798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 161 ] state=tensor([[-0.7517, -0.4128, -0.0149,  0.0798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7599, -0.6077, -0.0133,  0.3678]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 162 ] state=tensor([[-0.7599, -0.6077, -0.0133,  0.3678]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7721, -0.4124, -0.0059,  0.0709]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 163 ] state=tensor([[-0.7721, -0.4124, -0.0059,  0.0709]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7803, -0.6074, -0.0045,  0.3617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 164 ] state=tensor([[-0.7803, -0.6074, -0.0045,  0.3617]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7925, -0.4122,  0.0027,  0.0676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 165 ] state=tensor([[-0.7925, -0.4122,  0.0027,  0.0676]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8007, -0.2171,  0.0041, -0.2242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 166 ] state=tensor([[-0.8007, -0.2171,  0.0041, -0.2242]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-8.0505e-01, -4.1231e-01, -4.1723e-04,  6.9764e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 167 ] state=tensor([[-8.0505e-01, -4.1231e-01, -4.1723e-04,  6.9764e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8133, -0.2172,  0.0010, -0.2231]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 168 ] state=tensor([[-0.8133, -0.2172,  0.0010, -0.2231]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8176, -0.4123, -0.0035,  0.0699]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 169 ] state=tensor([[-0.8176, -0.4123, -0.0035,  0.0699]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8259, -0.2171, -0.0021, -0.2238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 170 ] state=tensor([[-0.8259, -0.2171, -0.0021, -0.2238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8302, -0.4122, -0.0066,  0.0682]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 171 ] state=tensor([[-0.8302, -0.4122, -0.0066,  0.0682]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8385, -0.6073, -0.0052,  0.3588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 172 ] state=tensor([[-0.8385, -0.6073, -0.0052,  0.3588]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8506, -0.4121,  0.0020,  0.0645]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 173 ] state=tensor([[-0.8506, -0.4121,  0.0020,  0.0645]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8589, -0.2170,  0.0033, -0.2276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 174 ] state=tensor([[-0.8589, -0.2170,  0.0033, -0.2276]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8632, -0.4121, -0.0013,  0.0661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 175 ] state=tensor([[-0.8632, -0.4121, -0.0013,  0.0661]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.7144e-01, -2.1701e-01,  3.9006e-05, -2.2696e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 176 ] state=tensor([[-8.7144e-01, -2.1701e-01,  3.9006e-05, -2.2696e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8758, -0.4121, -0.0045,  0.0657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 177 ] state=tensor([[-0.8758, -0.4121, -0.0045,  0.0657]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8840, -0.2169, -0.0032, -0.2284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 178 ] state=tensor([[-0.8840, -0.2169, -0.0032, -0.2284]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8884, -0.4120, -0.0078,  0.0633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 179 ] state=tensor([[-0.8884, -0.4120, -0.0078,  0.0633]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8966, -0.6070, -0.0065,  0.3535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 180 ] state=tensor([[-0.8966, -0.6070, -0.0065,  0.3535]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-9.0874e-01, -4.1182e-01,  5.8433e-04,  5.8819e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 181 ] state=tensor([[-9.0874e-01, -4.1182e-01,  5.8433e-04,  5.8819e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9170, -0.2167,  0.0018, -0.2337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 182 ] state=tensor([[-0.9170, -0.2167,  0.0018, -0.2337]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9213, -0.4119, -0.0029,  0.0596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 183 ] state=tensor([[-0.9213, -0.4119, -0.0029,  0.0596]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9296, -0.2167, -0.0017, -0.2340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 184 ] state=tensor([[-0.9296, -0.2167, -0.0017, -0.2340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9339, -0.4118, -0.0064,  0.0581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 185 ] state=tensor([[-0.9339, -0.4118, -0.0064,  0.0581]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9421, -0.6068, -0.0052,  0.3488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 186 ] state=tensor([[-0.9421, -0.6068, -0.0052,  0.3488]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9543, -0.4116,  0.0017,  0.0544]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 187 ] state=tensor([[-0.9543, -0.4116,  0.0017,  0.0544]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9625, -0.6068,  0.0028,  0.3477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 188 ] state=tensor([[-0.9625, -0.6068,  0.0028,  0.3477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9746, -0.4117,  0.0098,  0.0559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 189 ] state=tensor([[-0.9746, -0.4117,  0.0098,  0.0559]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9829, -0.2167,  0.0109, -0.2337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 190 ] state=tensor([[-0.9829, -0.2167,  0.0109, -0.2337]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9872, -0.0217,  0.0062, -0.5229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 191 ] state=tensor([[-0.9872, -0.0217,  0.0062, -0.5229]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9876, -0.2169, -0.0042, -0.2283]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 192 ] state=tensor([[-0.9876, -0.2169, -0.0042, -0.2283]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9920, -0.4120, -0.0088,  0.0630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 193 ] state=tensor([[-0.9920, -0.4120, -0.0088,  0.0630]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0002, -0.2168, -0.0075, -0.2324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 194 ] state=tensor([[-1.0002, -0.2168, -0.0075, -0.2324]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0045, -0.4118, -0.0122,  0.0579]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 195 ] state=tensor([[-1.0045, -0.4118, -0.0122,  0.0579]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0128, -0.6067, -0.0110,  0.3467]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 196 ] state=tensor([[-1.0128, -0.6067, -0.0110,  0.3467]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0249, -0.4114, -0.0041,  0.0505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 197 ] state=tensor([[-1.0249, -0.4114, -0.0041,  0.0505]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0331, -0.2163, -0.0031, -0.2434]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 198 ] state=tensor([[-1.0331, -0.2163, -0.0031, -0.2434]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0375, -0.4113, -0.0080,  0.0483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 199 ] state=tensor([[-1.0375, -0.4113, -0.0080,  0.0483]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0457, -0.2161, -0.0070, -0.2469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 200 ] state=tensor([[-1.0457, -0.2161, -0.0070, -0.2469]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0500, -0.4111, -0.0119,  0.0436]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 201 ] state=tensor([[-1.0500, -0.4111, -0.0119,  0.0436]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0582, -0.2158, -0.0111, -0.2529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 202 ] state=tensor([[-1.0582, -0.2158, -0.0111, -0.2529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0626, -0.4108, -0.0161,  0.0363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 203 ] state=tensor([[-1.0626, -0.4108, -0.0161,  0.0363]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0708, -0.2154, -0.0154, -0.2614]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 204 ] state=tensor([[-1.0708, -0.2154, -0.0154, -0.2614]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0751, -0.4103, -0.0206,  0.0264]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 122 ][ timestamp 205 ] state=tensor([[-1.0751, -0.4103, -0.0206,  0.0264]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0833, -0.6052, -0.0201,  0.3125]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 206 ] state=tensor([[-1.0833, -0.6052, -0.0201,  0.3125]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0954, -0.4098, -0.0138,  0.0135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 207 ] state=tensor([[-1.0954, -0.4098, -0.0138,  0.0135]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1036, -0.2144, -0.0136, -0.2835]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 208 ] state=tensor([[-1.1036, -0.2144, -0.0136, -0.2835]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1079, -0.4094, -0.0192,  0.0049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 209 ] state=tensor([[-1.1079, -0.4094, -0.0192,  0.0049]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1161, -0.6042, -0.0191,  0.2914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 210 ] state=tensor([[-1.1161, -0.6042, -0.0191,  0.2914]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1281, -0.4088, -0.0133, -0.0072]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 211 ] state=tensor([[-1.1281, -0.4088, -0.0133, -0.0072]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1363, -0.2135, -0.0135, -0.3041]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 212 ] state=tensor([[-1.1363, -0.2135, -0.0135, -0.3041]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1406, -0.4084, -0.0195, -0.0157]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 213 ] state=tensor([[-1.1406, -0.4084, -0.0195, -0.0157]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1488, -0.6033, -0.0199,  0.2708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 214 ] state=tensor([[-1.1488, -0.6033, -0.0199,  0.2708]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1608, -0.4079, -0.0144, -0.0281]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 215 ] state=tensor([[-1.1608, -0.4079, -0.0144, -0.0281]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1690, -0.2126, -0.0150, -0.3253]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 216 ] state=tensor([[-1.1690, -0.2126, -0.0150, -0.3253]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1732, -0.4075, -0.0215, -0.0374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 217 ] state=tensor([[-1.1732, -0.4075, -0.0215, -0.0374]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1814, -0.2120, -0.0223, -0.3368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 218 ] state=tensor([[-1.1814, -0.2120, -0.0223, -0.3368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1856, -0.4068, -0.0290, -0.0512]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 219 ] state=tensor([[-1.1856, -0.4068, -0.0290, -0.0512]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1938, -0.2113, -0.0300, -0.3529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 220 ] state=tensor([[-1.1938, -0.2113, -0.0300, -0.3529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1980, -0.4060, -0.0371, -0.0698]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 221 ] state=tensor([[-1.1980, -0.4060, -0.0371, -0.0698]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2061, -0.6006, -0.0385,  0.2109]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 222 ] state=tensor([[-1.2061, -0.6006, -0.0385,  0.2109]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2181, -0.4049, -0.0343, -0.0936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 223 ] state=tensor([[-1.2181, -0.4049, -0.0343, -0.0936]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2262, -0.5995, -0.0361,  0.1880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 224 ] state=tensor([[-1.2262, -0.5995, -0.0361,  0.1880]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2382, -0.4039, -0.0324, -0.1158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 225 ] state=tensor([[-1.2382, -0.4039, -0.0324, -0.1158]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2463, -0.5985, -0.0347,  0.1665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 226 ] state=tensor([[-1.2463, -0.5985, -0.0347,  0.1665]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2583, -0.4029, -0.0314, -0.1369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 227 ] state=tensor([[-1.2583, -0.4029, -0.0314, -0.1369]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2663, -0.2074, -0.0341, -0.4394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 228 ] state=tensor([[-1.2663, -0.2074, -0.0341, -0.4394]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2705, -0.4020, -0.0429, -0.1576]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 229 ] state=tensor([[-1.2705, -0.4020, -0.0429, -0.1576]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2785, -0.5965, -0.0460,  0.1212]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 230 ] state=tensor([[-1.2785, -0.5965, -0.0460,  0.1212]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2904, -0.4007, -0.0436, -0.1856]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 231 ] state=tensor([[-1.2904, -0.4007, -0.0436, -0.1856]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2984, -0.5952, -0.0473,  0.0930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 232 ] state=tensor([[-1.2984, -0.5952, -0.0473,  0.0930]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3103, -0.7896, -0.0455,  0.3704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 233 ] state=tensor([[-1.3103, -0.7896, -0.0455,  0.3704]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3261, -0.5939, -0.0381,  0.0637]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 234 ] state=tensor([[-1.3261, -0.5939, -0.0381,  0.0637]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3380, -0.3982, -0.0368, -0.2407]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 235 ] state=tensor([[-1.3380, -0.3982, -0.0368, -0.2407]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3460, -0.5928, -0.0416,  0.0401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 236 ] state=tensor([[-1.3460, -0.5928, -0.0416,  0.0401]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3578, -0.3971, -0.0408, -0.2654]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 237 ] state=tensor([[-1.3578, -0.3971, -0.0408, -0.2654]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3658, -0.5916, -0.0461,  0.0142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 238 ] state=tensor([[-1.3658, -0.5916, -0.0461,  0.0142]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3776, -0.7861, -0.0458,  0.2920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 239 ] state=tensor([[-1.3776, -0.7861, -0.0458,  0.2920]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3933, -0.5903, -0.0400, -0.0148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 240 ] state=tensor([[-1.3933, -0.5903, -0.0400, -0.0148]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4051, -0.7849, -0.0403,  0.2650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 241 ] state=tensor([[-1.4051, -0.7849, -0.0403,  0.2650]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4208, -0.5892, -0.0350, -0.0401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 242 ] state=tensor([[-1.4208, -0.5892, -0.0350, -0.0401]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4326, -0.7838, -0.0358,  0.2413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 243 ] state=tensor([[-1.4326, -0.7838, -0.0358,  0.2413]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4483, -0.5882, -0.0309, -0.0624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 244 ] state=tensor([[-1.4483, -0.5882, -0.0309, -0.0624]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4601, -0.7828, -0.0322,  0.2204]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 245 ] state=tensor([[-1.4601, -0.7828, -0.0322,  0.2204]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4757, -0.5873, -0.0278, -0.0823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 246 ] state=tensor([[-1.4757, -0.5873, -0.0278, -0.0823]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4875, -0.7820, -0.0294,  0.2015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 247 ] state=tensor([[-1.4875, -0.7820, -0.0294,  0.2015]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5031, -0.5865, -0.0254, -0.1003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 248 ] state=tensor([[-1.5031, -0.5865, -0.0254, -0.1003]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5148, -0.3910, -0.0274, -0.4009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 249 ] state=tensor([[-1.5148, -0.3910, -0.0274, -0.4009]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5227, -0.5857, -0.0354, -0.1170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 250 ] state=tensor([[-1.5227, -0.5857, -0.0354, -0.1170]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5344, -0.7803, -0.0378,  0.1643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 251 ] state=tensor([[-1.5344, -0.7803, -0.0378,  0.1643]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5500, -0.5847, -0.0345, -0.1401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 252 ] state=tensor([[-1.5500, -0.5847, -0.0345, -0.1401]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5617, -0.7793, -0.0373,  0.1415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 253 ] state=tensor([[-1.5617, -0.7793, -0.0373,  0.1415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5773, -0.5836, -0.0345, -0.1627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 254 ] state=tensor([[-1.5773, -0.5836, -0.0345, -0.1627]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5889, -0.7782, -0.0377,  0.1189]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 255 ] state=tensor([[-1.5889, -0.7782, -0.0377,  0.1189]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6045, -0.9728, -0.0353,  0.3995]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 256 ] state=tensor([[-1.6045, -0.9728, -0.0353,  0.3995]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6239, -1.1674, -0.0273,  0.6808]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 257 ] state=tensor([[-1.6239, -1.1674, -0.0273,  0.6808]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6473, -0.9719, -0.0137,  0.3797]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 258 ] state=tensor([[-1.6473, -0.9719, -0.0137,  0.3797]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6667, -0.7766, -0.0061,  0.0827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 259 ] state=tensor([[-1.6667, -0.7766, -0.0061,  0.0827]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6823, -0.5814, -0.0045, -0.2119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 260 ] state=tensor([[-1.6823, -0.5814, -0.0045, -0.2119]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6939, -0.7765, -0.0087,  0.0793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 261 ] state=tensor([[-1.6939, -0.7765, -0.0087,  0.0793]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7094, -0.9715, -0.0071,  0.3693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 262 ] state=tensor([[-1.7094, -0.9715, -0.0071,  0.3693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7289e+00, -1.1665e+00,  2.5550e-04,  6.5969e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 263 ] state=tensor([[-1.7289e+00, -1.1665e+00,  2.5550e-04,  6.5969e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7522, -1.3616,  0.0134,  0.9525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 264 ] state=tensor([[-1.7522, -1.3616,  0.0134,  0.9525]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7794, -1.5569,  0.0325,  1.2493]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 265 ] state=tensor([[-1.7794, -1.5569,  0.0325,  1.2493]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8106, -1.3622,  0.0575,  0.9670]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 266 ] state=tensor([[-1.8106, -1.3622,  0.0575,  0.9670]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8378, -1.5581,  0.0768,  1.2772]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 267 ] state=tensor([[-1.8378, -1.5581,  0.0768,  1.2772]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8690, -1.3640,  0.1024,  1.0095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 268 ] state=tensor([[-1.8690, -1.3640,  0.1024,  1.0095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8962, -1.5603,  0.1226,  1.3325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 269 ] state=tensor([[-1.8962, -1.5603,  0.1226,  1.3325]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9274, -1.7568,  0.1492,  1.6609]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 270 ] state=tensor([[-1.9274, -1.7568,  0.1492,  1.6609]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9626, -1.5636,  0.1824,  1.4182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 122 ][ timestamp 271 ] state=tensor([[-1.9626, -1.5636,  0.1824,  1.4182]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 122: Exploration_rate=0.05. Score=271.\n",
      "[ episode 123 ] state=tensor([[ 0.0338,  0.0282,  0.0214, -0.0481]])\n",
      "[ episode 123 ][ timestamp 1 ] state=tensor([[ 0.0338,  0.0282,  0.0214, -0.0481]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0344,  0.2230,  0.0204, -0.3340]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 123 ][ timestamp 2 ] state=tensor([[ 0.0344,  0.2230,  0.0204, -0.3340]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0388,  0.4178,  0.0137, -0.6201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 3 ] state=tensor([[ 0.0388,  0.4178,  0.0137, -0.6201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0472,  0.6128,  0.0013, -0.9085]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 4 ] state=tensor([[ 0.0472,  0.6128,  0.0013, -0.9085]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0595,  0.8079, -0.0168, -1.2007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 5 ] state=tensor([[ 0.0595,  0.8079, -0.0168, -1.2007]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0756,  0.6130, -0.0408, -0.9134]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 6 ] state=tensor([[ 0.0756,  0.6130, -0.0408, -0.9134]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0879,  0.4184, -0.0591, -0.6338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 7 ] state=tensor([[ 0.0879,  0.4184, -0.0591, -0.6338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0962,  0.2242, -0.0718, -0.3603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 8 ] state=tensor([[ 0.0962,  0.2242, -0.0718, -0.3603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1007,  0.0301, -0.0790, -0.0911]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 9 ] state=tensor([[ 0.1007,  0.0301, -0.0790, -0.0911]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.1013, -0.1638, -0.0808,  0.1757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 10 ] state=tensor([[ 0.1013, -0.1638, -0.0808,  0.1757]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0981,  0.0324, -0.0773, -0.1414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 11 ] state=tensor([[ 0.0981,  0.0324, -0.0773, -0.1414]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0987, -0.1615, -0.0801,  0.1260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 12 ] state=tensor([[ 0.0987, -0.1615, -0.0801,  0.1260]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0955,  0.0346, -0.0776, -0.1909]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 13 ] state=tensor([[ 0.0955,  0.0346, -0.0776, -0.1909]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0962, -0.1593, -0.0814,  0.0764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 14 ] state=tensor([[ 0.0962, -0.1593, -0.0814,  0.0764]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0930, -0.3532, -0.0799,  0.3423]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 15 ] state=tensor([[ 0.0930, -0.3532, -0.0799,  0.3423]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0859, -0.5471, -0.0730,  0.6087]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 16 ] state=tensor([[ 0.0859, -0.5471, -0.0730,  0.6087]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0750, -0.3510, -0.0609,  0.2940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 17 ] state=tensor([[ 0.0750, -0.3510, -0.0609,  0.2940]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0680, -0.5452, -0.0550,  0.5669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 18 ] state=tensor([[ 0.0680, -0.5452, -0.0550,  0.5669]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0570, -0.3493, -0.0437,  0.2574]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 19 ] state=tensor([[ 0.0570, -0.3493, -0.0437,  0.2574]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0501, -0.5438, -0.0385,  0.5360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 20 ] state=tensor([[ 0.0501, -0.5438, -0.0385,  0.5360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0392, -0.3482, -0.0278,  0.2314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 21 ] state=tensor([[ 0.0392, -0.3482, -0.0278,  0.2314]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0322, -0.5429, -0.0232,  0.5152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 22 ] state=tensor([[ 0.0322, -0.5429, -0.0232,  0.5152]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0214, -0.7377, -0.0129,  0.8005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 23 ] state=tensor([[ 0.0214, -0.7377, -0.0129,  0.8005]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0066, -0.5424,  0.0032,  0.5038]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 24 ] state=tensor([[ 0.0066, -0.5424,  0.0032,  0.5038]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0042, -0.3473,  0.0132,  0.2121]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 25 ] state=tensor([[-0.0042, -0.3473,  0.0132,  0.2121]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0112, -0.1524,  0.0175, -0.0764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 26 ] state=tensor([[-0.0112, -0.1524,  0.0175, -0.0764]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0142,  0.0425,  0.0159, -0.3635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 27 ] state=tensor([[-0.0142,  0.0425,  0.0159, -0.3635]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0134,  0.2374,  0.0087, -0.6511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 28 ] state=tensor([[-0.0134,  0.2374,  0.0087, -0.6511]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0086,  0.0421, -0.0043, -0.3557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 29 ] state=tensor([[-0.0086,  0.0421, -0.0043, -0.3557]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0078,  0.2373, -0.0115, -0.6498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 30 ] state=tensor([[-0.0078,  0.2373, -0.0115, -0.6498]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0030,  0.0424, -0.0245, -0.3607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 31 ] state=tensor([[-0.0030,  0.0424, -0.0245, -0.3607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0022,  0.2378, -0.0317, -0.6610]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 32 ] state=tensor([[-0.0022,  0.2378, -0.0317, -0.6610]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0026,  0.0432, -0.0449, -0.3785]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 33 ] state=tensor([[ 0.0026,  0.0432, -0.0449, -0.3785]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0034, -0.1513, -0.0525, -0.1003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 34 ] state=tensor([[ 0.0034, -0.1513, -0.0525, -0.1003]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0004, -0.3456, -0.0545,  0.1754]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 35 ] state=tensor([[ 0.0004, -0.3456, -0.0545,  0.1754]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0065, -0.5399, -0.0510,  0.4504]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 36 ] state=tensor([[-0.0065, -0.5399, -0.0510,  0.4504]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0173, -0.7343, -0.0419,  0.7266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 37 ] state=tensor([[-0.0173, -0.7343, -0.0419,  0.7266]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0320, -0.5386, -0.0274,  0.4210]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 38 ] state=tensor([[-0.0320, -0.5386, -0.0274,  0.4210]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0428, -0.3431, -0.0190,  0.1198]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 39 ] state=tensor([[-0.0428, -0.3431, -0.0190,  0.1198]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0496, -0.5380, -0.0166,  0.4065]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 40 ] state=tensor([[-0.0496, -0.5380, -0.0166,  0.4065]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0604, -0.3426, -0.0085,  0.1086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 41 ] state=tensor([[-0.0604, -0.3426, -0.0085,  0.1086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0672, -0.1474, -0.0063, -0.1867]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 42 ] state=tensor([[-0.0672, -0.1474, -0.0063, -0.1867]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0702, -0.3424, -0.0100,  0.1040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 43 ] state=tensor([[-0.0702, -0.3424, -0.0100,  0.1040]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0770, -0.1471, -0.0080, -0.1919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 44 ] state=tensor([[-0.0770, -0.1471, -0.0080, -0.1919]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0800,  0.0481, -0.0118, -0.4871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 45 ] state=tensor([[-0.0800,  0.0481, -0.0118, -0.4871]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0790, -0.1469, -0.0215, -0.1981]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 46 ] state=tensor([[-0.0790, -0.1469, -0.0215, -0.1981]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0820,  0.0486, -0.0255, -0.4975]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 47 ] state=tensor([[-0.0820,  0.0486, -0.0255, -0.4975]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0810, -0.1462, -0.0354, -0.2130]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 48 ] state=tensor([[-0.0810, -0.1462, -0.0354, -0.2130]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0839,  0.0494, -0.0397, -0.5166]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 49 ] state=tensor([[-0.0839,  0.0494, -0.0397, -0.5166]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0829, -0.1451, -0.0500, -0.2367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 50 ] state=tensor([[-0.0829, -0.1451, -0.0500, -0.2367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0858, -0.3395, -0.0548,  0.0398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 51 ] state=tensor([[-0.0858, -0.3395, -0.0548,  0.0398]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0926, -0.5338, -0.0540,  0.3147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 52 ] state=tensor([[-0.0926, -0.5338, -0.0540,  0.3147]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1033, -0.3379, -0.0477,  0.0055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 53 ] state=tensor([[-0.1033, -0.3379, -0.0477,  0.0055]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1100, -0.5323, -0.0476,  0.2828]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 54 ] state=tensor([[-0.1100, -0.5323, -0.0476,  0.2828]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1207, -0.7268, -0.0419,  0.5601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 55 ] state=tensor([[-0.1207, -0.7268, -0.0419,  0.5601]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1352, -0.5311, -0.0307,  0.2545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 56 ] state=tensor([[-0.1352, -0.5311, -0.0307,  0.2545]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1459, -0.3355, -0.0256, -0.0477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 57 ] state=tensor([[-0.1459, -0.3355, -0.0256, -0.0477]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1526, -0.5303, -0.0266,  0.2368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 58 ] state=tensor([[-0.1526, -0.5303, -0.0266,  0.2368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1632, -0.7250, -0.0218,  0.5209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 59 ] state=tensor([[-0.1632, -0.7250, -0.0218,  0.5209]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1777, -0.5296, -0.0114,  0.2215]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 123 ][ timestamp 60 ] state=tensor([[-0.1777, -0.5296, -0.0114,  0.2215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1883, -0.7245, -0.0070,  0.5105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 61 ] state=tensor([[-0.1883, -0.7245, -0.0070,  0.5105]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2028, -0.5293,  0.0032,  0.2156]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 62 ] state=tensor([[-0.2028, -0.5293,  0.0032,  0.2156]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2133, -0.3342,  0.0075, -0.0760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 63 ] state=tensor([[-0.2133, -0.3342,  0.0075, -0.0760]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2200, -0.5295,  0.0060,  0.2190]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 64 ] state=tensor([[-0.2200, -0.5295,  0.0060,  0.2190]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2306, -0.3344,  0.0104, -0.0718]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 65 ] state=tensor([[-0.2306, -0.3344,  0.0104, -0.0718]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2373, -0.5297,  0.0090,  0.2242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 66 ] state=tensor([[-0.2373, -0.5297,  0.0090,  0.2242]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2479, -0.3347,  0.0134, -0.0657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 67 ] state=tensor([[-0.2479, -0.3347,  0.0134, -0.0657]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2546, -0.5300,  0.0121,  0.2312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 68 ] state=tensor([[-0.2546, -0.5300,  0.0121,  0.2312]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2652, -0.7253,  0.0167,  0.5277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 69 ] state=tensor([[-0.2652, -0.7253,  0.0167,  0.5277]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2797, -0.5304,  0.0273,  0.2404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 70 ] state=tensor([[-0.2797, -0.5304,  0.0273,  0.2404]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2903, -0.7259,  0.0321,  0.5415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 71 ] state=tensor([[-0.2903, -0.7259,  0.0321,  0.5415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3048, -0.5313,  0.0429,  0.2591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 72 ] state=tensor([[-0.3048, -0.5313,  0.0429,  0.2591]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3154, -0.3368,  0.0481, -0.0197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 73 ] state=tensor([[-0.3154, -0.3368,  0.0481, -0.0197]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3222, -0.1424,  0.0477, -0.2968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 74 ] state=tensor([[-0.3222, -0.1424,  0.0477, -0.2968]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3250,  0.0520,  0.0418, -0.5741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 75 ] state=tensor([[-0.3250,  0.0520,  0.0418, -0.5741]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3240, -0.1437,  0.0303, -0.2685]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 76 ] state=tensor([[-0.3240, -0.1437,  0.0303, -0.2685]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3269,  0.0510,  0.0249, -0.5515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 77 ] state=tensor([[-0.3269,  0.0510,  0.0249, -0.5515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3258, -0.1445,  0.0139, -0.2511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 78 ] state=tensor([[-0.3258, -0.1445,  0.0139, -0.2511]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3287,  0.0505,  0.0089, -0.5393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 79 ] state=tensor([[-0.3287,  0.0505,  0.0089, -0.5393]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3277, -0.1448, -0.0019, -0.2439]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 80 ] state=tensor([[-0.3277, -0.1448, -0.0019, -0.2439]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3306,  0.0504, -0.0068, -0.5371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 81 ] state=tensor([[-0.3306,  0.0504, -0.0068, -0.5371]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3296,  0.2456, -0.0175, -0.8319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 82 ] state=tensor([[-0.3296,  0.2456, -0.0175, -0.8319]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3247,  0.0507, -0.0342, -0.5448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 83 ] state=tensor([[-0.3247,  0.0507, -0.0342, -0.5448]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3237, -0.1439, -0.0451, -0.2631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 84 ] state=tensor([[-0.3237, -0.1439, -0.0451, -0.2631]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3266, -0.3384, -0.0503,  0.0150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 85 ] state=tensor([[-0.3266, -0.3384, -0.0503,  0.0150]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3333, -0.5327, -0.0500,  0.2914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 86 ] state=tensor([[-0.3333, -0.5327, -0.0500,  0.2914]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3440, -0.7271, -0.0442,  0.5679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 87 ] state=tensor([[-0.3440, -0.7271, -0.0442,  0.5679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3585, -0.5314, -0.0328,  0.2617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 88 ] state=tensor([[-0.3585, -0.5314, -0.0328,  0.2617]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3692, -0.3358, -0.0276, -0.0412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 89 ] state=tensor([[-0.3692, -0.3358, -0.0276, -0.0412]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3759, -0.5305, -0.0284,  0.2427]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 90 ] state=tensor([[-0.3759, -0.5305, -0.0284,  0.2427]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3865, -0.7252, -0.0236,  0.5262]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 91 ] state=tensor([[-0.3865, -0.7252, -0.0236,  0.5262]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4010, -0.5298, -0.0130,  0.2262]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 92 ] state=tensor([[-0.4010, -0.5298, -0.0130,  0.2262]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4116, -0.3345, -0.0085, -0.0705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 93 ] state=tensor([[-0.4116, -0.3345, -0.0085, -0.0705]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4183, -0.5295, -0.0099,  0.2195]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 94 ] state=tensor([[-0.4183, -0.5295, -0.0099,  0.2195]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4289, -0.3342, -0.0055, -0.0763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 95 ] state=tensor([[-0.4289, -0.3342, -0.0055, -0.0763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4355, -0.5293, -0.0071,  0.2146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 96 ] state=tensor([[-0.4355, -0.5293, -0.0071,  0.2146]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4461, -0.3340, -0.0028, -0.0803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 97 ] state=tensor([[-0.4461, -0.3340, -0.0028, -0.0803]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4528, -0.5291, -0.0044,  0.2115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 98 ] state=tensor([[-0.4528, -0.5291, -0.0044,  0.2115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.6340e-01, -3.3394e-01, -1.4783e-04, -8.2571e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 99 ] state=tensor([[-4.6340e-01, -3.3394e-01, -1.4783e-04, -8.2571e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4701, -0.5291, -0.0018,  0.2101]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 100 ] state=tensor([[-0.4701, -0.5291, -0.0018,  0.2101]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4807, -0.3339,  0.0024, -0.0832]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 101 ] state=tensor([[-0.4807, -0.3339,  0.0024, -0.0832]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4873, -0.5291,  0.0007,  0.2103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 102 ] state=tensor([[-0.4873, -0.5291,  0.0007,  0.2103]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4979, -0.3340,  0.0049, -0.0822]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 103 ] state=tensor([[-0.4979, -0.3340,  0.0049, -0.0822]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5046, -0.5292,  0.0033,  0.2120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 104 ] state=tensor([[-0.5046, -0.5292,  0.0033,  0.2120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5152, -0.3341,  0.0075, -0.0796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 105 ] state=tensor([[-0.5152, -0.3341,  0.0075, -0.0796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5219, -0.5293,  0.0059,  0.2155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 106 ] state=tensor([[-0.5219, -0.5293,  0.0059,  0.2155]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5324, -0.3343,  0.0103, -0.0753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 107 ] state=tensor([[-0.5324, -0.3343,  0.0103, -0.0753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5391, -0.5295,  0.0088,  0.2206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 108 ] state=tensor([[-0.5391, -0.5295,  0.0088,  0.2206]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5497, -0.3345,  0.0132, -0.0694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 109 ] state=tensor([[-0.5497, -0.3345,  0.0132, -0.0694]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5564, -0.5299,  0.0118,  0.2275]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 110 ] state=tensor([[-0.5564, -0.5299,  0.0118,  0.2275]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5670, -0.7251,  0.0163,  0.5238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 111 ] state=tensor([[-0.5670, -0.7251,  0.0163,  0.5238]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5815, -0.5303,  0.0268,  0.2363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 112 ] state=tensor([[-0.5815, -0.5303,  0.0268,  0.2363]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5921, -0.3355,  0.0315, -0.0478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 113 ] state=tensor([[-0.5921, -0.3355,  0.0315, -0.0478]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5988, -0.1409,  0.0306, -0.3304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 114 ] state=tensor([[-0.5988, -0.1409,  0.0306, -0.3304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6016, -0.3364,  0.0240, -0.0282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 115 ] state=tensor([[-0.6016, -0.3364,  0.0240, -0.0282]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6084, -0.5319,  0.0234,  0.2720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 116 ] state=tensor([[-0.6084, -0.5319,  0.0234,  0.2720]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6190, -0.3371,  0.0288, -0.0133]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 117 ] state=tensor([[-0.6190, -0.3371,  0.0288, -0.0133]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6258, -0.1424,  0.0286, -0.2967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 118 ] state=tensor([[-0.6258, -0.1424,  0.0286, -0.2967]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6286, -0.3379,  0.0226,  0.0049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 119 ] state=tensor([[-0.6286, -0.3379,  0.0226,  0.0049]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6354, -0.1431,  0.0227, -0.2806]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 120 ] state=tensor([[-0.6354, -0.1431,  0.0227, -0.2806]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6382, -0.3386,  0.0171,  0.0192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 121 ] state=tensor([[-0.6382, -0.3386,  0.0171,  0.0192]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6450, -0.1437,  0.0175, -0.2681]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 122 ] state=tensor([[-0.6450, -0.1437,  0.0175, -0.2681]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6479, -0.3391,  0.0121,  0.0301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 123 ] state=tensor([[-0.6479, -0.3391,  0.0121,  0.0301]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6546, -0.1441,  0.0128, -0.2587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 124 ] state=tensor([[-0.6546, -0.1441,  0.0128, -0.2587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6575, -0.3394,  0.0076,  0.0379]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 125 ] state=tensor([[-0.6575, -0.3394,  0.0076,  0.0379]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6643, -0.1444,  0.0083, -0.2523]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 126 ] state=tensor([[-0.6643, -0.1444,  0.0083, -0.2523]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6672, -0.3396,  0.0033,  0.0430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 127 ] state=tensor([[-0.6672, -0.3396,  0.0033,  0.0430]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6740, -0.5348,  0.0041,  0.3367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 128 ] state=tensor([[-0.6740, -0.5348,  0.0041,  0.3367]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6847, -0.3397,  0.0109,  0.0453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 129 ] state=tensor([[-0.6847, -0.3397,  0.0109,  0.0453]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6915, -0.5350,  0.0118,  0.3414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 130 ] state=tensor([[-0.6915, -0.5350,  0.0118,  0.3414]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7022, -0.3401,  0.0186,  0.0525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 131 ] state=tensor([[-0.7022, -0.3401,  0.0186,  0.0525]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7090, -0.1452,  0.0197, -0.2343]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 132 ] state=tensor([[-0.7090, -0.1452,  0.0197, -0.2343]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7119, -0.3406,  0.0150,  0.0645]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 123 ][ timestamp 133 ] state=tensor([[-0.7119, -0.3406,  0.0150,  0.0645]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7187, -0.1457,  0.0163, -0.2234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 134 ] state=tensor([[-0.7187, -0.1457,  0.0163, -0.2234]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7216, -0.3411,  0.0118,  0.0744]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 135 ] state=tensor([[-0.7216, -0.3411,  0.0118,  0.0744]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7284, -0.1461,  0.0133, -0.2146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 136 ] state=tensor([[-0.7284, -0.1461,  0.0133, -0.2146]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7314, -0.3414,  0.0090,  0.0823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 137 ] state=tensor([[-0.7314, -0.3414,  0.0090,  0.0823]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7382, -0.1464,  0.0106, -0.2075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 138 ] state=tensor([[-0.7382, -0.1464,  0.0106, -0.2075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7411, -0.3417,  0.0065,  0.0885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 139 ] state=tensor([[-0.7411, -0.3417,  0.0065,  0.0885]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7480, -0.1467,  0.0083, -0.2021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 140 ] state=tensor([[-0.7480, -0.1467,  0.0083, -0.2021]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7509,  0.0483,  0.0042, -0.4922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 141 ] state=tensor([[-0.7509,  0.0483,  0.0042, -0.4922]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7499, -0.1468, -0.0056, -0.1982]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 142 ] state=tensor([[-0.7499, -0.1468, -0.0056, -0.1982]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7529, -0.3419, -0.0096,  0.0927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 143 ] state=tensor([[-0.7529, -0.3419, -0.0096,  0.0927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7597, -0.1466, -0.0077, -0.2030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 144 ] state=tensor([[-0.7597, -0.1466, -0.0077, -0.2030]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7626, -0.3416, -0.0118,  0.0872]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 145 ] state=tensor([[-0.7626, -0.3416, -0.0118,  0.0872]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7695, -0.1464, -0.0100, -0.2091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 146 ] state=tensor([[-0.7695, -0.1464, -0.0100, -0.2091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7724, -0.3413, -0.0142,  0.0804]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 147 ] state=tensor([[-0.7724, -0.3413, -0.0142,  0.0804]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7792, -0.5362, -0.0126,  0.3685]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 148 ] state=tensor([[-0.7792, -0.5362, -0.0126,  0.3685]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7899, -0.3409, -0.0053,  0.0719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 149 ] state=tensor([[-0.7899, -0.3409, -0.0053,  0.0719]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7968, -0.1457, -0.0038, -0.2225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 150 ] state=tensor([[-0.7968, -0.1457, -0.0038, -0.2225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7997, -0.3408, -0.0083,  0.0690]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 151 ] state=tensor([[-0.7997, -0.3408, -0.0083,  0.0690]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8065, -0.1456, -0.0069, -0.2263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 152 ] state=tensor([[-0.8065, -0.1456, -0.0069, -0.2263]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8094, -0.3406, -0.0114,  0.0642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 153 ] state=tensor([[-0.8094, -0.3406, -0.0114,  0.0642]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8162, -0.1453, -0.0101, -0.2320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 154 ] state=tensor([[-0.8162, -0.1453, -0.0101, -0.2320]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8191, -0.3403, -0.0148,  0.0575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 155 ] state=tensor([[-0.8191, -0.3403, -0.0148,  0.0575]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8259, -0.5352, -0.0136,  0.3454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 156 ] state=tensor([[-0.8259, -0.5352, -0.0136,  0.3454]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8366, -0.3399, -0.0067,  0.0485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 157 ] state=tensor([[-0.8366, -0.3399, -0.0067,  0.0485]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8434, -0.1447, -0.0057, -0.2463]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 158 ] state=tensor([[-0.8434, -0.1447, -0.0057, -0.2463]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8463, -0.3397, -0.0107,  0.0446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 159 ] state=tensor([[-0.8463, -0.3397, -0.0107,  0.0446]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8531, -0.1444, -0.0098, -0.2514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 160 ] state=tensor([[-0.8531, -0.1444, -0.0098, -0.2514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8560, -0.3394, -0.0148,  0.0381]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 161 ] state=tensor([[-0.8560, -0.3394, -0.0148,  0.0381]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8628, -0.1441, -0.0140, -0.2592]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 162 ] state=tensor([[-0.8628, -0.1441, -0.0140, -0.2592]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8657, -0.3390, -0.0192,  0.0290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 163 ] state=tensor([[-0.8657, -0.3390, -0.0192,  0.0290]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8725, -0.1436, -0.0186, -0.2696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 164 ] state=tensor([[-0.8725, -0.1436, -0.0186, -0.2696]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8753, -0.3385, -0.0240,  0.0171]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 165 ] state=tensor([[-0.8753, -0.3385, -0.0240,  0.0171]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8821, -0.1430, -0.0237, -0.2831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 166 ] state=tensor([[-0.8821, -0.1430, -0.0237, -0.2831]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8850, -0.3378, -0.0294,  0.0021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 167 ] state=tensor([[-0.8850, -0.3378, -0.0294,  0.0021]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8917, -0.5325, -0.0293,  0.2853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 168 ] state=tensor([[-0.8917, -0.5325, -0.0293,  0.2853]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9024, -0.3369, -0.0236, -0.0164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 169 ] state=tensor([[-0.9024, -0.3369, -0.0236, -0.0164]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9091, -0.1415, -0.0239, -0.3165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 170 ] state=tensor([[-0.9091, -0.1415, -0.0239, -0.3165]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9119, -0.3363, -0.0303, -0.0314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 171 ] state=tensor([[-0.9119, -0.3363, -0.0303, -0.0314]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9187, -0.5309, -0.0309,  0.2515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 172 ] state=tensor([[-0.9187, -0.5309, -0.0309,  0.2515]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9293, -0.3354, -0.0259, -0.0507]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 173 ] state=tensor([[-0.9293, -0.3354, -0.0259, -0.0507]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9360, -0.1399, -0.0269, -0.3515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 174 ] state=tensor([[-0.9360, -0.1399, -0.0269, -0.3515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9388, -0.3346, -0.0339, -0.0674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 175 ] state=tensor([[-0.9388, -0.3346, -0.0339, -0.0674]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9455, -0.1390, -0.0353, -0.3705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 176 ] state=tensor([[-0.9455, -0.1390, -0.0353, -0.3705]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9483, -0.3336, -0.0427, -0.0892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 177 ] state=tensor([[-0.9483, -0.3336, -0.0427, -0.0892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9549, -0.5281, -0.0444,  0.1897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 178 ] state=tensor([[-0.9549, -0.5281, -0.0444,  0.1897]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9655, -0.3324, -0.0407, -0.1166]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 179 ] state=tensor([[-0.9655, -0.3324, -0.0407, -0.1166]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9721, -0.5269, -0.0430,  0.1630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 180 ] state=tensor([[-0.9721, -0.5269, -0.0430,  0.1630]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9827, -0.3312, -0.0397, -0.1430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 181 ] state=tensor([[-0.9827, -0.3312, -0.0397, -0.1430]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9893, -0.1355, -0.0426, -0.4479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 182 ] state=tensor([[-0.9893, -0.1355, -0.0426, -0.4479]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9920, -0.3300, -0.0515, -0.1689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 183 ] state=tensor([[-0.9920, -0.3300, -0.0515, -0.1689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9986, -0.5244, -0.0549,  0.1070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 184 ] state=tensor([[-0.9986, -0.5244, -0.0549,  0.1070]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0091, -0.7187, -0.0528,  0.3819]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 185 ] state=tensor([[-1.0091, -0.7187, -0.0528,  0.3819]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0235, -0.5228, -0.0451,  0.0731]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 123 ][ timestamp 186 ] state=tensor([[-1.0235, -0.5228, -0.0451,  0.0731]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0339, -0.7173, -0.0437,  0.3512]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 187 ] state=tensor([[-1.0339, -0.7173, -0.0437,  0.3512]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0483, -0.5216, -0.0367,  0.0450]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 188 ] state=tensor([[-1.0483, -0.5216, -0.0367,  0.0450]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0587, -0.7162, -0.0358,  0.3259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 189 ] state=tensor([[-1.0587, -0.7162, -0.0358,  0.3259]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0730, -0.5205, -0.0292,  0.0222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 190 ] state=tensor([[-1.0730, -0.5205, -0.0292,  0.0222]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0834, -0.3250, -0.0288, -0.2796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 191 ] state=tensor([[-1.0834, -0.3250, -0.0288, -0.2796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0899, -0.5197, -0.0344,  0.0039]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 192 ] state=tensor([[-1.0899, -0.5197, -0.0344,  0.0039]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1003, -0.7143, -0.0343,  0.2855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 193 ] state=tensor([[-1.1003, -0.7143, -0.0343,  0.2855]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1146, -0.9089, -0.0286,  0.5672]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 194 ] state=tensor([[-1.1146, -0.9089, -0.0286,  0.5672]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1328, -0.7134, -0.0173,  0.2656]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 195 ] state=tensor([[-1.1328, -0.7134, -0.0173,  0.2656]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1471, -0.5181, -0.0119, -0.0324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 196 ] state=tensor([[-1.1471, -0.5181, -0.0119, -0.0324]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1574, -0.3228, -0.0126, -0.3289]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 197 ] state=tensor([[-1.1574, -0.3228, -0.0126, -0.3289]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1639, -0.5177, -0.0192, -0.0402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 198 ] state=tensor([[-1.1639, -0.5177, -0.0192, -0.0402]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1742, -0.3223, -0.0200, -0.3388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 199 ] state=tensor([[-1.1742, -0.3223, -0.0200, -0.3388]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1807, -0.5172, -0.0267, -0.0525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 200 ] state=tensor([[-1.1807, -0.5172, -0.0267, -0.0525]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1910, -0.7119, -0.0278,  0.2316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 201 ] state=tensor([[-1.1910, -0.7119, -0.0278,  0.2316]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2053, -0.5164, -0.0232, -0.0697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 202 ] state=tensor([[-1.2053, -0.5164, -0.0232, -0.0697]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2156, -0.3209, -0.0246, -0.3696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 203 ] state=tensor([[-1.2156, -0.3209, -0.0246, -0.3696]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2220, -0.5157, -0.0320, -0.0848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 204 ] state=tensor([[-1.2220, -0.5157, -0.0320, -0.0848]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2323, -0.7103, -0.0336,  0.1977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 205 ] state=tensor([[-1.2323, -0.7103, -0.0336,  0.1977]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2465, -0.5148, -0.0297, -0.1055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 206 ] state=tensor([[-1.2465, -0.5148, -0.0297, -0.1055]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2568, -0.7094, -0.0318,  0.1777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 207 ] state=tensor([[-1.2568, -0.7094, -0.0318,  0.1777]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2710, -0.5139, -0.0283, -0.1248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 208 ] state=tensor([[-1.2710, -0.5139, -0.0283, -0.1248]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2813, -0.3184, -0.0307, -0.4263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 209 ] state=tensor([[-1.2813, -0.3184, -0.0307, -0.4263]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2877, -0.5130, -0.0393, -0.1435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 210 ] state=tensor([[-1.2877, -0.5130, -0.0393, -0.1435]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2979, -0.3174, -0.0421, -0.4483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 211 ] state=tensor([[-1.2979, -0.3174, -0.0421, -0.4483]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3043, -0.5119, -0.0511, -0.1692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 212 ] state=tensor([[-1.3043, -0.5119, -0.0511, -0.1692]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3145, -0.7062, -0.0545,  0.1070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 213 ] state=tensor([[-1.3145, -0.7062, -0.0545,  0.1070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3286, -0.5104, -0.0524, -0.2024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 214 ] state=tensor([[-1.3286, -0.5104, -0.0524, -0.2024]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3388, -0.7047, -0.0564,  0.0733]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 215 ] state=tensor([[-1.3388, -0.7047, -0.0564,  0.0733]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3529, -0.8990, -0.0549,  0.3477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 216 ] state=tensor([[-1.3529, -0.8990, -0.0549,  0.3477]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3709, -1.0933, -0.0480,  0.6226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 217 ] state=tensor([[-1.3709, -1.0933, -0.0480,  0.6226]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3928, -0.8975, -0.0355,  0.3152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 218 ] state=tensor([[-1.3928, -0.8975, -0.0355,  0.3152]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4107, -0.7019, -0.0292,  0.0115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 219 ] state=tensor([[-1.4107, -0.7019, -0.0292,  0.0115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4248, -0.5064, -0.0290, -0.2903]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 220 ] state=tensor([[-1.4248, -0.5064, -0.0290, -0.2903]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4349, -0.7011, -0.0348, -0.0069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 221 ] state=tensor([[-1.4349, -0.7011, -0.0348, -0.0069]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4489, -0.8957, -0.0349,  0.2746]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 222 ] state=tensor([[-1.4489, -0.8957, -0.0349,  0.2746]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4668, -0.7001, -0.0294, -0.0289]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 223 ] state=tensor([[-1.4668, -0.7001, -0.0294, -0.0289]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4808, -0.8948, -0.0300,  0.2544]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 224 ] state=tensor([[-1.4808, -0.8948, -0.0300,  0.2544]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4987, -0.6992, -0.0249, -0.0476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 225 ] state=tensor([[-1.4987, -0.6992, -0.0249, -0.0476]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5127, -0.5038, -0.0259, -0.3480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 226 ] state=tensor([[-1.5127, -0.5038, -0.0259, -0.3480]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5228, -0.6985, -0.0328, -0.0636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 227 ] state=tensor([[-1.5228, -0.6985, -0.0328, -0.0636]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5368, -0.8931, -0.0341,  0.2185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 228 ] state=tensor([[-1.5368, -0.8931, -0.0341,  0.2185]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5546, -0.6976, -0.0297, -0.0847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 229 ] state=tensor([[-1.5546, -0.6976, -0.0297, -0.0847]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5686, -0.8922, -0.0314,  0.1984]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 230 ] state=tensor([[-1.5686, -0.8922, -0.0314,  0.1984]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5864, -0.6967, -0.0275, -0.1040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 231 ] state=tensor([[-1.5864, -0.6967, -0.0275, -0.1040]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6004, -0.8914, -0.0296,  0.1799]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 232 ] state=tensor([[-1.6004, -0.8914, -0.0296,  0.1799]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6182, -0.6959, -0.0260, -0.1220]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 233 ] state=tensor([[-1.6182, -0.6959, -0.0260, -0.1220]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6321, -0.5004, -0.0284, -0.4228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 234 ] state=tensor([[-1.6321, -0.5004, -0.0284, -0.4228]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6421, -0.6951, -0.0369, -0.1392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 235 ] state=tensor([[-1.6421, -0.6951, -0.0369, -0.1392]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6560, -0.4995, -0.0396, -0.4432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 236 ] state=tensor([[-1.6560, -0.4995, -0.0396, -0.4432]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6660, -0.6940, -0.0485, -0.1633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 237 ] state=tensor([[-1.6660, -0.6940, -0.0485, -0.1633]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6799, -0.8884, -0.0518,  0.1137]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 238 ] state=tensor([[-1.6799, -0.8884, -0.0518,  0.1137]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6977, -1.0827, -0.0495,  0.3896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 239 ] state=tensor([[-1.6977, -1.0827, -0.0495,  0.3896]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7193, -1.2771, -0.0417,  0.6663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 240 ] state=tensor([[-1.7193, -1.2771, -0.0417,  0.6663]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7448, -1.0814, -0.0284,  0.3608]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 241 ] state=tensor([[-1.7448, -1.0814, -0.0284,  0.3608]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7665, -1.2762, -0.0212,  0.6444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 242 ] state=tensor([[-1.7665, -1.2762, -0.0212,  0.6444]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7920, -1.0807, -0.0083,  0.3451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 243 ] state=tensor([[-1.7920, -1.0807, -0.0083,  0.3451]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8136e+00, -8.8551e-01, -1.3718e-03,  4.9813e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 244 ] state=tensor([[-1.8136e+00, -8.8551e-01, -1.3718e-03,  4.9813e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8313e+00, -6.9037e-01, -3.7559e-04, -2.4330e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 245 ] state=tensor([[-1.8313e+00, -6.9037e-01, -3.7559e-04, -2.4330e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8451, -0.4952, -0.0052, -0.5361]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 246 ] state=tensor([[-1.8451, -0.4952, -0.0052, -0.5361]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8550, -0.6903, -0.0160, -0.2451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 247 ] state=tensor([[-1.8550, -0.6903, -0.0160, -0.2451]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8688, -0.8852, -0.0209,  0.0425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 248 ] state=tensor([[-1.8688, -0.8852, -0.0209,  0.0425]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8865, -0.6898, -0.0200, -0.2567]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 249 ] state=tensor([[-1.8865, -0.6898, -0.0200, -0.2567]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9003, -0.8846, -0.0251,  0.0296]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 250 ] state=tensor([[-1.9003, -0.8846, -0.0251,  0.0296]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9180, -0.6891, -0.0246, -0.2709]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 251 ] state=tensor([[-1.9180, -0.6891, -0.0246, -0.2709]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9318, -0.8839, -0.0300,  0.0140]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 252 ] state=tensor([[-1.9318, -0.8839, -0.0300,  0.0140]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9495, -0.6883, -0.0297, -0.2880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 253 ] state=tensor([[-1.9495, -0.6883, -0.0297, -0.2880]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9633, -0.8830, -0.0355, -0.0048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 254 ] state=tensor([[-1.9633, -0.8830, -0.0355, -0.0048]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9809, -0.6874, -0.0356, -0.3085]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 255 ] state=tensor([[-1.9809, -0.6874, -0.0356, -0.3085]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9947, -0.4918, -0.0417, -0.6122]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 123 ][ timestamp 256 ] state=tensor([[-1.9947, -0.4918, -0.0417, -0.6122]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0045, -0.6863, -0.0540, -0.3329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 257 ] state=tensor([[-2.0045, -0.6863, -0.0540, -0.3329]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0182, -0.8806, -0.0606, -0.0577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 258 ] state=tensor([[-2.0182, -0.8806, -0.0606, -0.0577]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0358, -1.0748, -0.0618,  0.2152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 259 ] state=tensor([[-2.0358, -1.0748, -0.0618,  0.2152]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0573, -0.8789, -0.0575, -0.0963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 260 ] state=tensor([[-2.0573, -0.8789, -0.0575, -0.0963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0749, -1.0731, -0.0594,  0.1777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 261 ] state=tensor([[-2.0749, -1.0731, -0.0594,  0.1777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0964, -1.2674, -0.0558,  0.4511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 262 ] state=tensor([[-2.0964, -1.2674, -0.0558,  0.4511]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1217, -1.0715, -0.0468,  0.1413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 263 ] state=tensor([[-2.1217, -1.0715, -0.0468,  0.1413]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1432, -0.8757, -0.0440, -0.1657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 264 ] state=tensor([[-2.1432, -0.8757, -0.0440, -0.1657]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1607, -1.0702, -0.0473,  0.1127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 265 ] state=tensor([[-2.1607, -1.0702, -0.0473,  0.1127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1821, -1.2646, -0.0451,  0.3901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 266 ] state=tensor([[-2.1821, -1.2646, -0.0451,  0.3901]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2074, -1.4591, -0.0373,  0.6683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 267 ] state=tensor([[-2.2074, -1.4591, -0.0373,  0.6683]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2366, -1.2635, -0.0239,  0.3641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 268 ] state=tensor([[-2.2366, -1.2635, -0.0239,  0.3641]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2618, -1.0680, -0.0166,  0.0640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 269 ] state=tensor([[-2.2618, -1.0680, -0.0166,  0.0640]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2832, -0.8726, -0.0153, -0.2339]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 270 ] state=tensor([[-2.2832, -0.8726, -0.0153, -0.2339]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3006, -0.6773, -0.0200, -0.5314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 271 ] state=tensor([[-2.3006, -0.6773, -0.0200, -0.5314]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3142, -0.4819, -0.0306, -0.8303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 272 ] state=tensor([[-2.3142, -0.4819, -0.0306, -0.8303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3238, -0.6766, -0.0472, -0.5474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 273 ] state=tensor([[-2.3238, -0.6766, -0.0472, -0.5474]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3374, -0.4808, -0.0582, -0.8546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 274 ] state=tensor([[-2.3374, -0.4808, -0.0582, -0.8546]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3470, -0.6751, -0.0753, -0.5807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 275 ] state=tensor([[-2.3470, -0.6751, -0.0753, -0.5807]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3605, -0.4790, -0.0869, -0.8962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 276 ] state=tensor([[-2.3605, -0.4790, -0.0869, -0.8962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3701, -0.6729, -0.1048, -0.6320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 277 ] state=tensor([[-2.3701, -0.6729, -0.1048, -0.6320]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3835, -0.8664, -0.1175, -0.3741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 123 ][ timestamp 278 ] state=tensor([[-2.3835, -0.8664, -0.1175, -0.3741]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 123: Exploration_rate=0.05. Score=278.\n",
      "[ episode 124 ] state=tensor([[-0.0495, -0.0200,  0.0087,  0.0220]])\n",
      "[ episode 124 ][ timestamp 1 ] state=tensor([[-0.0495, -0.0200,  0.0087,  0.0220]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0499,  0.1750,  0.0091, -0.2680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 2 ] state=tensor([[-0.0499,  0.1750,  0.0091, -0.2680]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0464,  0.3700,  0.0038, -0.5577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 3 ] state=tensor([[-0.0464,  0.3700,  0.0038, -0.5577]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0390,  0.5651, -0.0074, -0.8492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 4 ] state=tensor([[-0.0390,  0.5651, -0.0074, -0.8492]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0277,  0.3701, -0.0244, -0.5589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 5 ] state=tensor([[-0.0277,  0.3701, -0.0244, -0.5589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0203,  0.1753, -0.0356, -0.2740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 6 ] state=tensor([[-0.0203,  0.1753, -0.0356, -0.2740]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0168,  0.3709, -0.0410, -0.5777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 7 ] state=tensor([[-0.0168,  0.3709, -0.0410, -0.5777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0094,  0.1764, -0.0526, -0.2982]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 8 ] state=tensor([[-0.0094,  0.1764, -0.0526, -0.2982]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0058, -0.0180, -0.0586, -0.0226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 9 ] state=tensor([[-0.0058, -0.0180, -0.0586, -0.0226]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0062,  0.1780, -0.0590, -0.3331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 10 ] state=tensor([[-0.0062,  0.1780, -0.0590, -0.3331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0026, -0.0163, -0.0657, -0.0596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 11 ] state=tensor([[-0.0026, -0.0163, -0.0657, -0.0596]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0030, -0.2104, -0.0669,  0.2116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 12 ] state=tensor([[-0.0030, -0.2104, -0.0669,  0.2116]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0072, -0.0144, -0.0626, -0.1014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 13 ] state=tensor([[-0.0072, -0.0144, -0.0626, -0.1014]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0075, -0.2086, -0.0647,  0.1709]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 14 ] state=tensor([[-0.0075, -0.2086, -0.0647,  0.1709]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0116, -0.4027, -0.0612,  0.4425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 15 ] state=tensor([[-0.0116, -0.4027, -0.0612,  0.4425]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0197, -0.2068, -0.0524,  0.1312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 16 ] state=tensor([[-0.0197, -0.2068, -0.0524,  0.1312]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0238, -0.4011, -0.0498,  0.4069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 17 ] state=tensor([[-0.0238, -0.4011, -0.0498,  0.4069]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0318, -0.2053, -0.0416,  0.0989]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 124 ][ timestamp 18 ] state=tensor([[-0.0318, -0.2053, -0.0416,  0.0989]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0359, -0.3998, -0.0397,  0.3782]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 19 ] state=tensor([[-0.0359, -0.3998, -0.0397,  0.3782]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0439, -0.2042, -0.0321,  0.0732]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 20 ] state=tensor([[-0.0439, -0.2042, -0.0321,  0.0732]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0480, -0.0086, -0.0306, -0.2294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 21 ] state=tensor([[-0.0480, -0.0086, -0.0306, -0.2294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0482,  0.1870, -0.0352, -0.5316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 22 ] state=tensor([[-0.0482,  0.1870, -0.0352, -0.5316]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0445, -0.0076, -0.0459, -0.2502]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 23 ] state=tensor([[-0.0445, -0.0076, -0.0459, -0.2502]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0446,  0.1881, -0.0509, -0.5570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 24 ] state=tensor([[-0.0446,  0.1881, -0.0509, -0.5570]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0408, -0.0063, -0.0620, -0.2807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 25 ] state=tensor([[-0.0408, -0.0063, -0.0620, -0.2807]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0410, -0.2005, -0.0676, -0.0082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 26 ] state=tensor([[-0.0410, -0.2005, -0.0676, -0.0082]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0450, -0.0044, -0.0678, -0.3215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 27 ] state=tensor([[-0.0450, -0.0044, -0.0678, -0.3215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0451, -0.1985, -0.0742, -0.0509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 28 ] state=tensor([[-0.0451, -0.1985, -0.0742, -0.0509]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0490, -0.3925, -0.0752,  0.2175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 29 ] state=tensor([[-0.0490, -0.3925, -0.0752,  0.2175]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0569, -0.5865, -0.0709,  0.4855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 30 ] state=tensor([[-0.0569, -0.5865, -0.0709,  0.4855]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0686, -0.3904, -0.0612,  0.1714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 31 ] state=tensor([[-0.0686, -0.3904, -0.0612,  0.1714]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0764, -0.5846, -0.0577,  0.4441]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 32 ] state=tensor([[-0.0764, -0.5846, -0.0577,  0.4441]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0881, -0.7789, -0.0489,  0.7181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 33 ] state=tensor([[-0.0881, -0.7789, -0.0489,  0.7181]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1037, -0.9733, -0.0345,  0.9950]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 34 ] state=tensor([[-0.1037, -0.9733, -0.0345,  0.9950]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1232, -0.7777, -0.0146,  0.6917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 35 ] state=tensor([[-0.1232, -0.7777, -0.0146,  0.6917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3872e-01, -9.7266e-01, -7.5983e-04,  9.7973e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 36 ] state=tensor([[-1.3872e-01, -9.7266e-01, -7.5983e-04,  9.7973e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1582, -0.7775,  0.0188,  0.6868]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 37 ] state=tensor([[-0.1582, -0.7775,  0.0188,  0.6868]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1737, -0.5827,  0.0326,  0.4001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 38 ] state=tensor([[-0.1737, -0.5827,  0.0326,  0.4001]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1854, -0.3880,  0.0406,  0.1179]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 39 ] state=tensor([[-0.1854, -0.3880,  0.0406,  0.1179]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1931, -0.1935,  0.0429, -0.1617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 40 ] state=tensor([[-0.1931, -0.1935,  0.0429, -0.1617]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1970,  0.0010,  0.0397, -0.4406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 41 ] state=tensor([[-0.1970,  0.0010,  0.0397, -0.4406]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1970,  0.1955,  0.0309, -0.7205]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 42 ] state=tensor([[-0.1970,  0.1955,  0.0309, -0.7205]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1931,  0.3902,  0.0165, -1.0033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 43 ] state=tensor([[-0.1931,  0.3902,  0.0165, -1.0033]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1853,  0.1949, -0.0036, -0.7055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 44 ] state=tensor([[-0.1853,  0.1949, -0.0036, -0.7055]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8138e-01, -2.1504e-04, -1.7701e-02, -4.1393e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 45 ] state=tensor([[-1.8138e-01, -2.1504e-04, -1.7701e-02, -4.1393e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1814, -0.1951, -0.0260, -0.1269]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 46 ] state=tensor([[-0.1814, -0.1951, -0.0260, -0.1269]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1853, -0.3898, -0.0285,  0.1575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 47 ] state=tensor([[-0.1853, -0.3898, -0.0285,  0.1575]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1931, -0.5845, -0.0254,  0.4410]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 48 ] state=tensor([[-0.1931, -0.5845, -0.0254,  0.4410]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2048, -0.7793, -0.0165,  0.7256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 49 ] state=tensor([[-0.2048, -0.7793, -0.0165,  0.7256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2204, -0.5839, -0.0020,  0.4278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 50 ] state=tensor([[-0.2204, -0.5839, -0.0020,  0.4278]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2320, -0.7790,  0.0065,  0.7198]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 51 ] state=tensor([[-0.2320, -0.7790,  0.0065,  0.7198]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2476, -0.5840,  0.0209,  0.4292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 52 ] state=tensor([[-0.2476, -0.5840,  0.0209,  0.4292]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2593, -0.3892,  0.0295,  0.1432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 53 ] state=tensor([[-0.2593, -0.3892,  0.0295,  0.1432]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2671, -0.1945,  0.0324, -0.1400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 54 ] state=tensor([[-0.2671, -0.1945,  0.0324, -0.1400]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.7097e-01,  1.5716e-04,  2.9566e-02, -4.2234e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 55 ] state=tensor([[-2.7097e-01,  1.5716e-04,  2.9566e-02, -4.2234e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2710,  0.1948,  0.0211, -0.7056]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 56 ] state=tensor([[-0.2710,  0.1948,  0.0211, -0.7056]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2671, -0.0006,  0.0070, -0.4063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 57 ] state=tensor([[-0.2671, -0.0006,  0.0070, -0.4063]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2671, -0.1958, -0.0011, -0.1114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 58 ] state=tensor([[-0.2671, -0.1958, -0.0011, -0.1114]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2710, -0.3909, -0.0033,  0.1809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 59 ] state=tensor([[-0.2710, -0.3909, -0.0033,  0.1809]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.7881e-01, -1.9572e-01,  2.7128e-04, -1.1283e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 60 ] state=tensor([[-2.7881e-01, -1.9572e-01,  2.7128e-04, -1.1283e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2827, -0.3908, -0.0020,  0.1799]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 61 ] state=tensor([[-0.2827, -0.3908, -0.0020,  0.1799]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2905, -0.5859,  0.0016,  0.4720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 62 ] state=tensor([[-0.2905, -0.5859,  0.0016,  0.4720]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3023, -0.3908,  0.0111,  0.1798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 63 ] state=tensor([[-0.3023, -0.3908,  0.0111,  0.1798]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3101, -0.1959,  0.0146, -0.1094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 64 ] state=tensor([[-0.3101, -0.1959,  0.0146, -0.1094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3140, -0.0010,  0.0125, -0.3974]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 65 ] state=tensor([[-0.3140, -0.0010,  0.0125, -0.3974]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3140,  0.1940,  0.0045, -0.6861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 66 ] state=tensor([[-0.3140,  0.1940,  0.0045, -0.6861]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3101, -0.0012, -0.0092, -0.3920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 67 ] state=tensor([[-0.3101, -0.0012, -0.0092, -0.3920]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3102, -0.1962, -0.0170, -0.1022]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 68 ] state=tensor([[-0.3102, -0.1962, -0.0170, -0.1022]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3141, -0.0008, -0.0191, -0.4003]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 69 ] state=tensor([[-0.3141, -0.0008, -0.0191, -0.4003]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3141, -0.1957, -0.0271, -0.1136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 70 ] state=tensor([[-0.3141, -0.1957, -0.0271, -0.1136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.1802e-01, -1.8198e-04, -2.9370e-02, -4.1476e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 71 ] state=tensor([[-3.1802e-01, -1.8198e-04, -2.9370e-02, -4.1476e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3180, -0.1949, -0.0377, -0.1315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 72 ] state=tensor([[-0.3180, -0.1949, -0.0377, -0.1315]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3219,  0.0008, -0.0403, -0.4358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 73 ] state=tensor([[-0.3219,  0.0008, -0.0403, -0.4358]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3219, -0.1938, -0.0490, -0.1561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 74 ] state=tensor([[-0.3219, -0.1938, -0.0490, -0.1561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3258, -0.3882, -0.0521,  0.1207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 75 ] state=tensor([[-0.3258, -0.3882, -0.0521,  0.1207]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3335, -0.5825, -0.0497,  0.3965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 76 ] state=tensor([[-0.3335, -0.5825, -0.0497,  0.3965]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3452, -0.7769, -0.0418,  0.6731]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 77 ] state=tensor([[-0.3452, -0.7769, -0.0418,  0.6731]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3607, -0.5812, -0.0283,  0.3676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 78 ] state=tensor([[-0.3607, -0.5812, -0.0283,  0.3676]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3724, -0.7759, -0.0210,  0.6512]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 79 ] state=tensor([[-0.3724, -0.7759, -0.0210,  0.6512]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3879, -0.5805, -0.0079,  0.3520]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 80 ] state=tensor([[-0.3879, -0.5805, -0.0079,  0.3520]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3995, -0.3853, -0.0009,  0.0568]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 81 ] state=tensor([[-0.3995, -0.3853, -0.0009,  0.0568]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-4.0718e-01, -5.8037e-01,  2.2898e-04,  3.4922e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 82 ] state=tensor([[-4.0718e-01, -5.8037e-01,  2.2898e-04,  3.4922e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4188, -0.3853,  0.0072,  0.0566]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 124 ][ timestamp 83 ] state=tensor([[-0.4188, -0.3853,  0.0072,  0.0566]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4265, -0.5805,  0.0083,  0.3516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 84 ] state=tensor([[-0.4265, -0.5805,  0.0083,  0.3516]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4381, -0.3855,  0.0154,  0.0615]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 85 ] state=tensor([[-0.4381, -0.3855,  0.0154,  0.0615]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4458, -0.1906,  0.0166, -0.2263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 86 ] state=tensor([[-0.4458, -0.1906,  0.0166, -0.2263]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4496,  0.0043,  0.0121, -0.5137]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 87 ] state=tensor([[-0.4496,  0.0043,  0.0121, -0.5137]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4495, -0.1910,  0.0018, -0.2172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 88 ] state=tensor([[-0.4495, -0.1910,  0.0018, -0.2172]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4534,  0.0041, -0.0025, -0.5093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 89 ] state=tensor([[-0.4534,  0.0041, -0.0025, -0.5093]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4533, -0.1910, -0.0127, -0.2174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 90 ] state=tensor([[-0.4533, -0.1910, -0.0127, -0.2174]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4571, -0.3859, -0.0171,  0.0712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 91 ] state=tensor([[-0.4571, -0.3859, -0.0171,  0.0712]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4648, -0.5808, -0.0156,  0.3585]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 92 ] state=tensor([[-0.4648, -0.5808, -0.0156,  0.3585]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4764, -0.3854, -0.0085,  0.0609]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 93 ] state=tensor([[-0.4764, -0.3854, -0.0085,  0.0609]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4841, -0.5804, -0.0073,  0.3509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 94 ] state=tensor([[-0.4841, -0.5804, -0.0073,  0.3509]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.9575e-01, -3.8522e-01, -2.4230e-04,  5.5915e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 95 ] state=tensor([[-4.9575e-01, -3.8522e-01, -2.4230e-04,  5.5915e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5035, -0.5803,  0.0009,  0.3485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 96 ] state=tensor([[-0.5035, -0.5803,  0.0009,  0.3485]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5151, -0.7755,  0.0078,  0.6415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 97 ] state=tensor([[-0.5151, -0.7755,  0.0078,  0.6415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5306, -0.5805,  0.0207,  0.3513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 98 ] state=tensor([[-0.5306, -0.5805,  0.0207,  0.3513]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5422, -0.3856,  0.0277,  0.0652]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 99 ] state=tensor([[-0.5422, -0.3856,  0.0277,  0.0652]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5499, -0.1909,  0.0290, -0.2186]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 100 ] state=tensor([[-0.5499, -0.1909,  0.0290, -0.2186]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5537,  0.0038,  0.0246, -0.5020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 101 ] state=tensor([[-0.5537,  0.0038,  0.0246, -0.5020]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5536, -0.1917,  0.0146, -0.2017]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 102 ] state=tensor([[-0.5536, -0.1917,  0.0146, -0.2017]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5575,  0.0032,  0.0106, -0.4897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 103 ] state=tensor([[-0.5575,  0.0032,  0.0106, -0.4897]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5574, -0.1920,  0.0008, -0.1937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 104 ] state=tensor([[-0.5574, -0.1920,  0.0008, -0.1937]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5612,  0.0031, -0.0031, -0.4862]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 105 ] state=tensor([[-0.5612,  0.0031, -0.0031, -0.4862]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5612, -0.1920, -0.0128, -0.1945]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 106 ] state=tensor([[-0.5612, -0.1920, -0.0128, -0.1945]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5650, -0.3870, -0.0167,  0.0941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 107 ] state=tensor([[-0.5650, -0.3870, -0.0167,  0.0941]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5728, -0.5818, -0.0148,  0.3815]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 108 ] state=tensor([[-0.5728, -0.5818, -0.0148,  0.3815]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5844, -0.3865, -0.0072,  0.0842]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 109 ] state=tensor([[-0.5844, -0.3865, -0.0072,  0.0842]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5921, -0.5815, -0.0055,  0.3746]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 110 ] state=tensor([[-0.5921, -0.5815, -0.0055,  0.3746]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6038, -0.3863,  0.0020,  0.0801]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 111 ] state=tensor([[-0.6038, -0.3863,  0.0020,  0.0801]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6115, -0.5815,  0.0036,  0.3734]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 112 ] state=tensor([[-0.6115, -0.5815,  0.0036,  0.3734]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6231, -0.3864,  0.0110,  0.0819]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 113 ] state=tensor([[-0.6231, -0.3864,  0.0110,  0.0819]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6308, -0.1914,  0.0127, -0.2073]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 114 ] state=tensor([[-0.6308, -0.1914,  0.0127, -0.2073]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6347, -0.3867,  0.0085,  0.0894]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 115 ] state=tensor([[-0.6347, -0.3867,  0.0085,  0.0894]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6424, -0.1917,  0.0103, -0.2006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 116 ] state=tensor([[-0.6424, -0.1917,  0.0103, -0.2006]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6462,  0.0032,  0.0063, -0.4900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 117 ] state=tensor([[-0.6462,  0.0032,  0.0063, -0.4900]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6462, -0.1920, -0.0035, -0.1954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 118 ] state=tensor([[-0.6462, -0.1920, -0.0035, -0.1954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6500, -0.3870, -0.0074,  0.0962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 119 ] state=tensor([[-0.6500, -0.3870, -0.0074,  0.0962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6578, -0.5821, -0.0055,  0.3866]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 120 ] state=tensor([[-0.6578, -0.5821, -0.0055,  0.3866]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6694, -0.3869,  0.0023,  0.0921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 121 ] state=tensor([[-0.6694, -0.3869,  0.0023,  0.0921]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6771, -0.5820,  0.0041,  0.3855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 122 ] state=tensor([[-0.6771, -0.5820,  0.0041,  0.3855]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6888, -0.3870,  0.0118,  0.0941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 123 ] state=tensor([[-0.6888, -0.3870,  0.0118,  0.0941]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6965, -0.1920,  0.0137, -0.1948]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 124 ] state=tensor([[-0.6965, -0.1920,  0.0137, -0.1948]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7004,  0.0029,  0.0098, -0.4831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 125 ] state=tensor([[-0.7004,  0.0029,  0.0098, -0.4831]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.0030e-01, -1.9234e-01,  1.2847e-04, -1.8737e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 126 ] state=tensor([[-7.0030e-01, -1.9234e-01,  1.2847e-04, -1.8737e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7041, -0.3875, -0.0036,  0.1054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 127 ] state=tensor([[-0.7041, -0.3875, -0.0036,  0.1054]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7119, -0.1923, -0.0015, -0.1885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 128 ] state=tensor([[-0.7119, -0.1923, -0.0015, -0.1885]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7157,  0.0029, -0.0053, -0.4816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 129 ] state=tensor([[-0.7157,  0.0029, -0.0053, -0.4816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7157, -0.1922, -0.0149, -0.1906]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 130 ] state=tensor([[-0.7157, -0.1922, -0.0149, -0.1906]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7195, -0.3871, -0.0187,  0.0973]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 131 ] state=tensor([[-0.7195, -0.3871, -0.0187,  0.0973]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7273, -0.5819, -0.0168,  0.3840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 132 ] state=tensor([[-0.7273, -0.5819, -0.0168,  0.3840]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7389, -0.3866, -0.0091,  0.0861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 133 ] state=tensor([[-0.7389, -0.3866, -0.0091,  0.0861]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7466, -0.5816, -0.0074,  0.3759]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 134 ] state=tensor([[-0.7466, -0.5816, -0.0074,  0.3759]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.5827e-01, -3.8635e-01,  1.4193e-04,  8.0916e-02]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 124 ][ timestamp 135 ] state=tensor([[-7.5827e-01, -3.8635e-01,  1.4193e-04,  8.0916e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7660, -0.1912,  0.0018, -0.2117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 136 ] state=tensor([[-0.7660, -0.1912,  0.0018, -0.2117]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7698, -0.3864, -0.0025,  0.0815]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 137 ] state=tensor([[-0.7698, -0.3864, -0.0025,  0.0815]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7775, -0.1912, -0.0008, -0.2119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 138 ] state=tensor([[-0.7775, -0.1912, -0.0008, -0.2119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7814,  0.0039, -0.0051, -0.5049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 139 ] state=tensor([[-0.7814,  0.0039, -0.0051, -0.5049]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7813, -0.1911, -0.0152, -0.2138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 140 ] state=tensor([[-0.7813, -0.1911, -0.0152, -0.2138]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7851, -0.3860, -0.0195,  0.0740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 141 ] state=tensor([[-0.7851, -0.3860, -0.0195,  0.0740]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7928, -0.5809, -0.0180,  0.3605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 142 ] state=tensor([[-0.7928, -0.5809, -0.0180,  0.3605]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8045, -0.3855, -0.0108,  0.0622]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 143 ] state=tensor([[-0.8045, -0.3855, -0.0108,  0.0622]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8122, -0.5805, -0.0095,  0.3515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 144 ] state=tensor([[-0.8122, -0.5805, -0.0095,  0.3515]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8238, -0.3852, -0.0025,  0.0558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 145 ] state=tensor([[-0.8238, -0.3852, -0.0025,  0.0558]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8315, -0.1901, -0.0014, -0.2377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 146 ] state=tensor([[-0.8315, -0.1901, -0.0014, -0.2377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8353, -0.3852, -0.0061,  0.0546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 147 ] state=tensor([[-0.8353, -0.3852, -0.0061,  0.0546]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8430, -0.1900, -0.0050, -0.2400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 148 ] state=tensor([[-0.8430, -0.1900, -0.0050, -0.2400]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8468, -0.3850, -0.0098,  0.0511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 149 ] state=tensor([[-0.8468, -0.3850, -0.0098,  0.0511]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8545, -0.1897, -0.0088, -0.2447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 150 ] state=tensor([[-0.8545, -0.1897, -0.0088, -0.2447]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8583, -0.3847, -0.0137,  0.0452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 151 ] state=tensor([[-0.8583, -0.3847, -0.0137,  0.0452]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8660, -0.1894, -0.0128, -0.2518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 152 ] state=tensor([[-0.8660, -0.1894, -0.0128, -0.2518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8698, -0.3844, -0.0178,  0.0368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 153 ] state=tensor([[-0.8698, -0.3844, -0.0178,  0.0368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8774, -0.5792, -0.0171,  0.3238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 154 ] state=tensor([[-0.8774, -0.5792, -0.0171,  0.3238]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8890, -0.3839, -0.0106,  0.0258]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 155 ] state=tensor([[-0.8890, -0.3839, -0.0106,  0.0258]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8967, -0.1886, -0.0101, -0.2702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 156 ] state=tensor([[-0.8967, -0.1886, -0.0101, -0.2702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9005, -0.3836, -0.0155,  0.0193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 157 ] state=tensor([[-0.9005, -0.3836, -0.0155,  0.0193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9082, -0.5785, -0.0151,  0.3070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 158 ] state=tensor([[-0.9082, -0.5785, -0.0151,  0.3070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9197, -0.3831, -0.0090,  0.0096]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 159 ] state=tensor([[-0.9197, -0.3831, -0.0090,  0.0096]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9274, -0.1879, -0.0088, -0.2859]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 160 ] state=tensor([[-0.9274, -0.1879, -0.0088, -0.2859]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9311, -0.3829, -0.0145,  0.0040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 161 ] state=tensor([[-0.9311, -0.3829, -0.0145,  0.0040]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9388, -0.5778, -0.0144,  0.2921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 162 ] state=tensor([[-0.9388, -0.5778, -0.0144,  0.2921]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9504, -0.3825, -0.0086, -0.0051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 163 ] state=tensor([[-0.9504, -0.3825, -0.0086, -0.0051]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9580, -0.1872, -0.0087, -0.3005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 164 ] state=tensor([[-0.9580, -0.1872, -0.0087, -0.3005]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9617, -0.3822, -0.0147, -0.0106]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 165 ] state=tensor([[-0.9617, -0.3822, -0.0147, -0.0106]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9694, -0.5771, -0.0149,  0.2774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 166 ] state=tensor([[-0.9694, -0.5771, -0.0149,  0.2774]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9809, -0.7720, -0.0094,  0.5654]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 167 ] state=tensor([[-0.9809, -0.7720, -0.0094,  0.5654]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9964, -0.5768,  0.0019,  0.2697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 168 ] state=tensor([[-0.9964, -0.5768,  0.0019,  0.2697]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0079, -0.3817,  0.0073, -0.0223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 169 ] state=tensor([[-1.0079, -0.3817,  0.0073, -0.0223]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0155, -0.1867,  0.0069, -0.3127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 170 ] state=tensor([[-1.0155, -0.1867,  0.0069, -0.3127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0193e+00, -3.8188e-01,  6.2993e-04, -1.7854e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 171 ] state=tensor([[-1.0193e+00, -3.8188e-01,  6.2993e-04, -1.7854e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0269e+00, -1.8676e-01,  2.7284e-04, -3.1034e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 172 ] state=tensor([[-1.0269e+00, -1.8676e-01,  2.7284e-04, -3.1034e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0306, -0.3819, -0.0059, -0.0176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 173 ] state=tensor([[-1.0306, -0.3819, -0.0059, -0.0176]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0383, -0.1867, -0.0063, -0.3121]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 174 ] state=tensor([[-1.0383, -0.1867, -0.0063, -0.3121]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0420, -0.3817, -0.0125, -0.0214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 175 ] state=tensor([[-1.0420, -0.3817, -0.0125, -0.0214]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0497, -0.1864, -0.0130, -0.3180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 176 ] state=tensor([[-1.0497, -0.1864, -0.0130, -0.3180]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0534, -0.3813, -0.0193, -0.0295]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 177 ] state=tensor([[-1.0534, -0.3813, -0.0193, -0.0295]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0610, -0.5762, -0.0199,  0.2571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 178 ] state=tensor([[-1.0610, -0.5762, -0.0199,  0.2571]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0725, -0.3808, -0.0148, -0.0418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 179 ] state=tensor([[-1.0725, -0.3808, -0.0148, -0.0418]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0802, -0.5757, -0.0156,  0.2462]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 180 ] state=tensor([[-1.0802, -0.5757, -0.0156,  0.2462]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0917, -0.3804, -0.0107, -0.0514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 181 ] state=tensor([[-1.0917, -0.3804, -0.0107, -0.0514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0993, -0.5753, -0.0117,  0.2379]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 182 ] state=tensor([[-1.0993, -0.5753, -0.0117,  0.2379]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1108, -0.3800, -0.0069, -0.0585]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 183 ] state=tensor([[-1.1108, -0.3800, -0.0069, -0.0585]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1184, -0.5751, -0.0081,  0.2320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 184 ] state=tensor([[-1.1184, -0.5751, -0.0081,  0.2320]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1299, -0.3798, -0.0035, -0.0632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 185 ] state=tensor([[-1.1299, -0.3798, -0.0035, -0.0632]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1375, -0.5749, -0.0047,  0.2284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 186 ] state=tensor([[-1.1375, -0.5749, -0.0047,  0.2284]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1490e+00, -3.7970e-01, -1.7481e-04, -6.5807e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 187 ] state=tensor([[-1.1490e+00, -3.7970e-01, -1.7481e-04, -6.5807e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1566, -0.5748, -0.0015,  0.2268]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 188 ] state=tensor([[-1.1566, -0.5748, -0.0015,  0.2268]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1681, -0.3797,  0.0030, -0.0663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 189 ] state=tensor([[-1.1681, -0.3797,  0.0030, -0.0663]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1757, -0.5748,  0.0017,  0.2273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 190 ] state=tensor([[-1.1757, -0.5748,  0.0017,  0.2273]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1872, -0.3797,  0.0063, -0.0648]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 191 ] state=tensor([[-1.1872, -0.3797,  0.0063, -0.0648]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1947, -0.5750,  0.0050,  0.2298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 192 ] state=tensor([[-1.1947, -0.5750,  0.0050,  0.2298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2062, -0.7702,  0.0096,  0.5241]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 193 ] state=tensor([[-1.2062, -0.7702,  0.0096,  0.5241]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2217, -0.5752,  0.0200,  0.2344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 194 ] state=tensor([[-1.2217, -0.5752,  0.0200,  0.2344]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2332, -0.3803,  0.0247, -0.0519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 195 ] state=tensor([[-1.2332, -0.3803,  0.0247, -0.0519]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2408, -0.1856,  0.0237, -0.3367]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 124 ][ timestamp 196 ] state=tensor([[-1.2408, -0.1856,  0.0237, -0.3367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2445, -0.3810,  0.0170, -0.0366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 197 ] state=tensor([[-1.2445, -0.3810,  0.0170, -0.0366]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2521, -0.1862,  0.0162, -0.3239]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 198 ] state=tensor([[-1.2521, -0.1862,  0.0162, -0.3239]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2558, -0.3815,  0.0098, -0.0261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 199 ] state=tensor([[-1.2558, -0.3815,  0.0098, -0.0261]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2634, -0.1865,  0.0092, -0.3157]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 200 ] state=tensor([[-1.2634, -0.1865,  0.0092, -0.3157]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2672,  0.0085,  0.0029, -0.6055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 201 ] state=tensor([[-1.2672,  0.0085,  0.0029, -0.6055]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2670, -0.1867, -0.0092, -0.3119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 202 ] state=tensor([[-1.2670, -0.1867, -0.0092, -0.3119]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2707, -0.3817, -0.0154, -0.0221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 203 ] state=tensor([[-1.2707, -0.3817, -0.0154, -0.0221]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2784, -0.1863, -0.0159, -0.3196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 204 ] state=tensor([[-1.2784, -0.1863, -0.0159, -0.3196]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2821, -0.3812, -0.0223, -0.0320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 205 ] state=tensor([[-1.2821, -0.3812, -0.0223, -0.0320]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2897, -0.1858, -0.0229, -0.3316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 206 ] state=tensor([[-1.2897, -0.1858, -0.0229, -0.3316]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2934,  0.0096, -0.0295, -0.6314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 207 ] state=tensor([[-1.2934,  0.0096, -0.0295, -0.6314]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2933, -0.1851, -0.0422, -0.3482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 208 ] state=tensor([[-1.2933, -0.1851, -0.0422, -0.3482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2970,  0.0106, -0.0491, -0.6538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 209 ] state=tensor([[-1.2970,  0.0106, -0.0491, -0.6538]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2967, -0.1838, -0.0622, -0.3770]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 210 ] state=tensor([[-1.2967, -0.1838, -0.0622, -0.3770]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3004,  0.0122, -0.0697, -0.6887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 211 ] state=tensor([[-1.3004,  0.0122, -0.0697, -0.6887]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3002, -0.1819, -0.0835, -0.4187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 212 ] state=tensor([[-1.3002, -0.1819, -0.0835, -0.4187]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3038, -0.3758, -0.0919, -0.1535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 213 ] state=tensor([[-1.3038, -0.3758, -0.0919, -0.1535]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3113, -0.5694, -0.0950,  0.1088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 214 ] state=tensor([[-1.3113, -0.5694, -0.0950,  0.1088]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3227, -0.7631, -0.0928,  0.3701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 215 ] state=tensor([[-1.3227, -0.7631, -0.0928,  0.3701]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3380, -0.9568, -0.0854,  0.6322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 216 ] state=tensor([[-1.3380, -0.9568, -0.0854,  0.6322]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3571, -0.7606, -0.0727,  0.3139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 217 ] state=tensor([[-1.3571, -0.7606, -0.0727,  0.3139]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3723, -0.9546, -0.0665,  0.5828]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 218 ] state=tensor([[-1.3723, -0.9546, -0.0665,  0.5828]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3914, -0.7586, -0.0548,  0.2699]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 219 ] state=tensor([[-1.3914, -0.7586, -0.0548,  0.2699]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4066, -0.5627, -0.0494, -0.0396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 220 ] state=tensor([[-1.4066, -0.5627, -0.0494, -0.0396]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4178, -0.3670, -0.0502, -0.3474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 221 ] state=tensor([[-1.4178, -0.3670, -0.0502, -0.3474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4252, -0.5613, -0.0571, -0.0710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 222 ] state=tensor([[-1.4252, -0.5613, -0.0571, -0.0710]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4364, -0.3654, -0.0586, -0.3811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 223 ] state=tensor([[-1.4364, -0.3654, -0.0586, -0.3811]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4437, -0.5597, -0.0662, -0.1075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 224 ] state=tensor([[-1.4437, -0.5597, -0.0662, -0.1075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4549, -0.7538, -0.0683,  0.1636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 225 ] state=tensor([[-1.4549, -0.7538, -0.0683,  0.1636]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4700, -0.5578, -0.0651, -0.1498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 226 ] state=tensor([[-1.4700, -0.5578, -0.0651, -0.1498]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4811, -0.7519, -0.0681,  0.1217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 227 ] state=tensor([[-1.4811, -0.7519, -0.0681,  0.1217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4962, -0.5559, -0.0656, -0.1917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 228 ] state=tensor([[-1.4962, -0.5559, -0.0656, -0.1917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5073, -0.7500, -0.0695,  0.0796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 229 ] state=tensor([[-1.5073, -0.7500, -0.0695,  0.0796]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5223, -0.5539, -0.0679, -0.2342]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 230 ] state=tensor([[-1.5223, -0.5539, -0.0679, -0.2342]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5334, -0.7480, -0.0726,  0.0363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 231 ] state=tensor([[-1.5334, -0.7480, -0.0726,  0.0363]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5483, -0.9420, -0.0718,  0.3053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 232 ] state=tensor([[-1.5483, -0.9420, -0.0718,  0.3053]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5672, -1.1361, -0.0657,  0.5745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 233 ] state=tensor([[-1.5672, -1.1361, -0.0657,  0.5745]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5899, -0.9401, -0.0542,  0.2618]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 234 ] state=tensor([[-1.5899, -0.9401, -0.0542,  0.2618]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6087, -0.7442, -0.0490, -0.0475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 235 ] state=tensor([[-1.6087, -0.7442, -0.0490, -0.0475]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6236, -0.5485, -0.0499, -0.3552]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 236 ] state=tensor([[-1.6236, -0.5485, -0.0499, -0.3552]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6346, -0.7428, -0.0570, -0.0787]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 237 ] state=tensor([[-1.6346, -0.7428, -0.0570, -0.0787]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6494, -0.9371, -0.0586,  0.1955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 238 ] state=tensor([[-1.6494, -0.9371, -0.0586,  0.1955]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6682, -1.1313, -0.0547,  0.4691]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 239 ] state=tensor([[-1.6682, -1.1313, -0.0547,  0.4691]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6908, -0.9355, -0.0453,  0.1597]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 240 ] state=tensor([[-1.6908, -0.9355, -0.0453,  0.1597]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7095, -0.7397, -0.0421, -0.1469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 241 ] state=tensor([[-1.7095, -0.7397, -0.0421, -0.1469]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7243, -0.5440, -0.0451, -0.4526]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 242 ] state=tensor([[-1.7243, -0.5440, -0.0451, -0.4526]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7352, -0.7385, -0.0541, -0.1745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 243 ] state=tensor([[-1.7352, -0.7385, -0.0541, -0.1745]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7499, -0.9328, -0.0576,  0.1007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 244 ] state=tensor([[-1.7499, -0.9328, -0.0576,  0.1007]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7686, -0.7369, -0.0556, -0.2096]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 245 ] state=tensor([[-1.7686, -0.7369, -0.0556, -0.2096]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7833, -0.9312, -0.0598,  0.0650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 246 ] state=tensor([[-1.7833, -0.9312, -0.0598,  0.0650]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8019, -0.7353, -0.0585, -0.2459]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 247 ] state=tensor([[-1.8019, -0.7353, -0.0585, -0.2459]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8167, -0.9295, -0.0634,  0.0278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 248 ] state=tensor([[-1.8167, -0.9295, -0.0634,  0.0278]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8352, -1.1237, -0.0629,  0.2998]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 249 ] state=tensor([[-1.8352, -1.1237, -0.0629,  0.2998]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8577, -1.3178, -0.0569,  0.5720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 250 ] state=tensor([[-1.8577, -1.3178, -0.0569,  0.5720]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8841, -1.1220, -0.0454,  0.2620]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 251 ] state=tensor([[-1.8841, -1.1220, -0.0454,  0.2620]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9065, -0.9262, -0.0402, -0.0447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 252 ] state=tensor([[-1.9065, -0.9262, -0.0402, -0.0447]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9250, -0.7305, -0.0411, -0.3498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 253 ] state=tensor([[-1.9250, -0.7305, -0.0411, -0.3498]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9396, -0.9251, -0.0481, -0.0703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 254 ] state=tensor([[-1.9396, -0.9251, -0.0481, -0.0703]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9582, -0.7293, -0.0495, -0.3778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 255 ] state=tensor([[-1.9582, -0.7293, -0.0495, -0.3778]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9727, -0.9237, -0.0570, -0.1011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 256 ] state=tensor([[-1.9727, -0.9237, -0.0570, -0.1011]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9912, -1.1179, -0.0591,  0.1730]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 124 ][ timestamp 257 ] state=tensor([[-1.9912, -1.1179, -0.0591,  0.1730]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0136, -1.3122, -0.0556,  0.4465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 258 ] state=tensor([[-2.0136, -1.3122, -0.0556,  0.4465]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0398, -1.1163, -0.0467,  0.1369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 259 ] state=tensor([[-2.0398, -1.1163, -0.0467,  0.1369]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0621, -1.3107, -0.0439,  0.4145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 260 ] state=tensor([[-2.0621, -1.3107, -0.0439,  0.4145]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0884, -1.1150, -0.0356,  0.1083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 261 ] state=tensor([[-2.0884, -1.1150, -0.0356,  0.1083]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1107, -0.9194, -0.0335, -0.1955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 262 ] state=tensor([[-2.1107, -0.9194, -0.0335, -0.1955]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1290, -1.1140, -0.0374,  0.0865]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 263 ] state=tensor([[-2.1290, -1.1140, -0.0374,  0.0865]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1513, -0.9184, -0.0357, -0.2178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 264 ] state=tensor([[-2.1513, -0.9184, -0.0357, -0.2178]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1697, -0.7228, -0.0400, -0.5215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 265 ] state=tensor([[-2.1697, -0.7228, -0.0400, -0.5215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1841, -0.9173, -0.0504, -0.2417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 266 ] state=tensor([[-2.1841, -0.9173, -0.0504, -0.2417]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2025, -1.1117, -0.0553,  0.0347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 267 ] state=tensor([[-2.2025, -1.1117, -0.0553,  0.0347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2247, -0.9158, -0.0546, -0.2749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 268 ] state=tensor([[-2.2247, -0.9158, -0.0546, -0.2749]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2430e+00, -1.1101e+00, -6.0075e-02,  8.6088e-05]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 269 ] state=tensor([[-2.2430e+00, -1.1101e+00, -6.0075e-02,  8.6088e-05]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2652, -0.9142, -0.0601, -0.3109]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 270 ] state=tensor([[-2.2652, -0.9142, -0.0601, -0.3109]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2835, -1.1084, -0.0663, -0.0378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 271 ] state=tensor([[-2.2835, -1.1084, -0.0663, -0.0378]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3057, -1.3025, -0.0670,  0.2333]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 272 ] state=tensor([[-2.3057, -1.3025, -0.0670,  0.2333]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3317, -1.1065, -0.0624, -0.0798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 273 ] state=tensor([[-2.3317, -1.1065, -0.0624, -0.0798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3539, -1.3007, -0.0640,  0.1926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 274 ] state=tensor([[-2.3539, -1.3007, -0.0640,  0.1926]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3799, -1.1047, -0.0601, -0.1196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 124 ][ timestamp 275 ] state=tensor([[-2.3799, -1.1047, -0.0601, -0.1196]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 124: Exploration_rate=0.05. Score=275.\n",
      "[ episode 125 ] state=tensor([[-0.0141, -0.0483,  0.0326,  0.0233]])\n",
      "[ episode 125 ][ timestamp 1 ] state=tensor([[-0.0141, -0.0483,  0.0326,  0.0233]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0151,  0.1464,  0.0331, -0.2589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 2 ] state=tensor([[-0.0151,  0.1464,  0.0331, -0.2589]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0121,  0.3410,  0.0279, -0.5409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 3 ] state=tensor([[-0.0121,  0.3410,  0.0279, -0.5409]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0053,  0.5357,  0.0171, -0.8247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 4 ] state=tensor([[-0.0053,  0.5357,  0.0171, -0.8247]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 5.3881e-03,  7.3062e-01,  5.9447e-04, -1.1120e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 5 ] state=tensor([[ 5.3881e-03,  7.3062e-01,  5.9447e-04, -1.1120e+00]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0200,  0.5355, -0.0216, -0.8191]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 6 ] state=tensor([[ 0.0200,  0.5355, -0.0216, -0.8191]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0307,  0.3407, -0.0380, -0.5333]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 7 ] state=tensor([[ 0.0307,  0.3407, -0.0380, -0.5333]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0375,  0.5363, -0.0487, -0.8377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 8 ] state=tensor([[ 0.0375,  0.5363, -0.0487, -0.8377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0482,  0.3419, -0.0654, -0.5607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 9 ] state=tensor([[ 0.0482,  0.3419, -0.0654, -0.5607]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0551,  0.1477, -0.0767, -0.2894]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 10 ] state=tensor([[ 0.0551,  0.1477, -0.0767, -0.2894]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0580,  0.3439, -0.0824, -0.6052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 11 ] state=tensor([[ 0.0580,  0.3439, -0.0824, -0.6052]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0649,  0.1500, -0.0946, -0.3396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 12 ] state=tensor([[ 0.0649,  0.1500, -0.0946, -0.3396]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0679, -0.0437, -0.1013, -0.0782]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 13 ] state=tensor([[ 0.0679, -0.0437, -0.1013, -0.0782]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0670, -0.2372, -0.1029,  0.1809]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 14 ] state=tensor([[ 0.0670, -0.2372, -0.1029,  0.1809]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0623, -0.4307, -0.0993,  0.4394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 15 ] state=tensor([[ 0.0623, -0.4307, -0.0993,  0.4394]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0537, -0.6243, -0.0905,  0.6992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 16 ] state=tensor([[ 0.0537, -0.6243, -0.0905,  0.6992]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0412, -0.4281, -0.0765,  0.3795]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 17 ] state=tensor([[ 0.0412, -0.4281, -0.0765,  0.3795]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0326, -0.6220, -0.0689,  0.6471]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 18 ] state=tensor([[ 0.0326, -0.6220, -0.0689,  0.6471]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0202, -0.8161, -0.0560,  0.9173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 19 ] state=tensor([[ 0.0202, -0.8161, -0.0560,  0.9173]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0039, -1.0104, -0.0376,  1.1919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 20 ] state=tensor([[ 0.0039, -1.0104, -0.0376,  1.1919]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0163, -1.2050, -0.0138,  1.4725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 21 ] state=tensor([[-0.0163, -1.2050, -0.0138,  1.4725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0404, -1.0098,  0.0156,  1.1756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 22 ] state=tensor([[-0.0404, -1.0098,  0.0156,  1.1756]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0606, -0.8148,  0.0392,  0.8878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 23 ] state=tensor([[-0.0606, -0.8148,  0.0392,  0.8878]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0769, -0.6203,  0.0569,  0.6077]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 24 ] state=tensor([[-0.0769, -0.6203,  0.0569,  0.6077]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0893, -0.4260,  0.0691,  0.3335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 25 ] state=tensor([[-0.0893, -0.4260,  0.0691,  0.3335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0979, -0.2319,  0.0757,  0.0634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 26 ] state=tensor([[-0.0979, -0.2319,  0.0757,  0.0634]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1025, -0.0380,  0.0770, -0.2045]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 27 ] state=tensor([[-0.1025, -0.0380,  0.0770, -0.2045]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1032,  0.1560,  0.0729, -0.4719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 28 ] state=tensor([[-0.1032,  0.1560,  0.0729, -0.4719]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1001,  0.3500,  0.0635, -0.7408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 29 ] state=tensor([[-0.1001,  0.3500,  0.0635, -0.7408]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0931,  0.5442,  0.0487, -1.0128]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 30 ] state=tensor([[-0.0931,  0.5442,  0.0487, -1.0128]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0822,  0.3485,  0.0284, -0.7053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 31 ] state=tensor([[-0.0822,  0.3485,  0.0284, -0.7053]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0753,  0.5432,  0.0143, -0.9889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 32 ] state=tensor([[-0.0753,  0.5432,  0.0143, -0.9889]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0644,  0.3479, -0.0055, -0.6917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 33 ] state=tensor([[-0.0644,  0.3479, -0.0055, -0.6917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0575,  0.1528, -0.0193, -0.4008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 34 ] state=tensor([[-0.0575,  0.1528, -0.0193, -0.4008]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0544, -0.0420, -0.0273, -0.1142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 35 ] state=tensor([[-0.0544, -0.0420, -0.0273, -0.1142]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0552,  0.1535, -0.0296, -0.4154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 36 ] state=tensor([[-0.0552,  0.1535, -0.0296, -0.4154]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0522, -0.0412, -0.0379, -0.1322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 37 ] state=tensor([[-0.0522, -0.0412, -0.0379, -0.1322]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0530,  0.1544, -0.0406, -0.4366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 38 ] state=tensor([[-0.0530,  0.1544, -0.0406, -0.4366]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0499, -0.0401, -0.0493, -0.1570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 39 ] state=tensor([[-0.0499, -0.0401, -0.0493, -0.1570]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0507,  0.1557, -0.0524, -0.4648]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 40 ] state=tensor([[-0.0507,  0.1557, -0.0524, -0.4648]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0476,  0.3515, -0.0617, -0.7735]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 41 ] state=tensor([[-0.0476,  0.3515, -0.0617, -0.7735]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0406,  0.1573, -0.0772, -0.5009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 42 ] state=tensor([[-0.0406,  0.1573, -0.0772, -0.5009]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0374, -0.0367, -0.0872, -0.2335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 43 ] state=tensor([[-0.0374, -0.0367, -0.0872, -0.2335]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0381, -0.2304, -0.0919,  0.0305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 44 ] state=tensor([[-0.0381, -0.2304, -0.0919,  0.0305]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0428, -0.4241, -0.0913,  0.2928]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 45 ] state=tensor([[-0.0428, -0.4241, -0.0913,  0.2928]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0512, -0.6178, -0.0854,  0.5553]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 46 ] state=tensor([[-0.0512, -0.6178, -0.0854,  0.5553]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0636, -0.8117, -0.0743,  0.8199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 47 ] state=tensor([[-0.0636, -0.8117, -0.0743,  0.8199]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0798, -0.6156, -0.0579,  0.5048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 48 ] state=tensor([[-0.0798, -0.6156, -0.0579,  0.5048]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0921, -0.8099, -0.0478,  0.7787]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 49 ] state=tensor([[-0.0921, -0.8099, -0.0478,  0.7787]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1083, -0.6141, -0.0322,  0.4714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 50 ] state=tensor([[-0.1083, -0.6141, -0.0322,  0.4714]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1206, -0.8088, -0.0228,  0.7537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 51 ] state=tensor([[-0.1206, -0.8088, -0.0228,  0.7537]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1368, -0.6133, -0.0077,  0.4540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 52 ] state=tensor([[-0.1368, -0.6133, -0.0077,  0.4540]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1491, -0.4181,  0.0013,  0.1589]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 125 ][ timestamp 53 ] state=tensor([[-0.1491, -0.4181,  0.0013,  0.1589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1574, -0.6132,  0.0045,  0.4520]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 54 ] state=tensor([[-0.1574, -0.6132,  0.0045,  0.4520]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1697, -0.4182,  0.0136,  0.1607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 55 ] state=tensor([[-0.1697, -0.4182,  0.0136,  0.1607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1781, -0.2233,  0.0168, -0.1277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 56 ] state=tensor([[-0.1781, -0.2233,  0.0168, -0.1277]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1825, -0.4186,  0.0142,  0.1703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 57 ] state=tensor([[-0.1825, -0.4186,  0.0142,  0.1703]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1909, -0.2237,  0.0176, -0.1179]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 58 ] state=tensor([[-0.1909, -0.2237,  0.0176, -0.1179]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1954, -0.4191,  0.0153,  0.1803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 59 ] state=tensor([[-0.1954, -0.4191,  0.0153,  0.1803]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2037, -0.2242,  0.0189, -0.1076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 60 ] state=tensor([[-0.2037, -0.2242,  0.0189, -0.1076]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2082, -0.0293,  0.0167, -0.3942]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 61 ] state=tensor([[-0.2082, -0.0293,  0.0167, -0.3942]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2088, -0.2247,  0.0088, -0.0963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 62 ] state=tensor([[-0.2088, -0.2247,  0.0088, -0.0963]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2133, -0.4199,  0.0069,  0.1991]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 63 ] state=tensor([[-0.2133, -0.4199,  0.0069,  0.1991]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2217, -0.6152,  0.0109,  0.4940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 64 ] state=tensor([[-0.2217, -0.6152,  0.0109,  0.4940]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2340, -0.4202,  0.0208,  0.2048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 65 ] state=tensor([[-0.2340, -0.4202,  0.0208,  0.2048]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2424, -0.2254,  0.0249, -0.0813]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 66 ] state=tensor([[-0.2424, -0.2254,  0.0249, -0.0813]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2469, -0.0306,  0.0232, -0.3660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 67 ] state=tensor([[-0.2469, -0.0306,  0.0232, -0.3660]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2475,  0.1642,  0.0159, -0.6513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 68 ] state=tensor([[-0.2475,  0.1642,  0.0159, -0.6513]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2443, -0.0312,  0.0029, -0.3536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 69 ] state=tensor([[-0.2443, -0.0312,  0.0029, -0.3536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2449,  0.1639, -0.0042, -0.6454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 70 ] state=tensor([[-0.2449,  0.1639, -0.0042, -0.6454]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2416, -0.0312, -0.0171, -0.3540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 71 ] state=tensor([[-0.2416, -0.0312, -0.0171, -0.3540]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2422,  0.1642, -0.0242, -0.6521]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 72 ] state=tensor([[-0.2422,  0.1642, -0.0242, -0.6521]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2389, -0.0306, -0.0372, -0.3671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 73 ] state=tensor([[-0.2389, -0.0306, -0.0372, -0.3671]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2395, -0.2251, -0.0446, -0.0864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 74 ] state=tensor([[-0.2395, -0.2251, -0.0446, -0.0864]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2441, -0.4196, -0.0463,  0.1919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 75 ] state=tensor([[-0.2441, -0.4196, -0.0463,  0.1919]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2524, -0.2238, -0.0424, -0.1150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 76 ] state=tensor([[-0.2524, -0.2238, -0.0424, -0.1150]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2569, -0.4183, -0.0447,  0.1640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 77 ] state=tensor([[-0.2569, -0.4183, -0.0447,  0.1640]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2653, -0.6128, -0.0415,  0.4422]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 78 ] state=tensor([[-0.2653, -0.6128, -0.0415,  0.4422]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2775, -0.8073, -0.0326,  0.7216]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 79 ] state=tensor([[-0.2775, -0.8073, -0.0326,  0.7216]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2937, -0.6117, -0.0182,  0.4188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 80 ] state=tensor([[-0.2937, -0.6117, -0.0182,  0.4188]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3059, -0.4164, -0.0098,  0.1204]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 81 ] state=tensor([[-0.3059, -0.4164, -0.0098,  0.1204]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3143, -0.2211, -0.0074, -0.1753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 82 ] state=tensor([[-0.3143, -0.2211, -0.0074, -0.1753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3187, -0.4161, -0.0109,  0.1150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 83 ] state=tensor([[-0.3187, -0.4161, -0.0109,  0.1150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3270, -0.2208, -0.0086, -0.1811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 84 ] state=tensor([[-0.3270, -0.2208, -0.0086, -0.1811]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3314, -0.4158, -0.0122,  0.1089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 85 ] state=tensor([[-0.3314, -0.4158, -0.0122,  0.1089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3397, -0.6108, -0.0100,  0.3977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 86 ] state=tensor([[-0.3397, -0.6108, -0.0100,  0.3977]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3519, -0.4155, -0.0021,  0.1018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 87 ] state=tensor([[-0.3519, -0.4155, -0.0021,  0.1018]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.6026e-01, -2.2037e-01, -5.8909e-05, -1.9150e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 88 ] state=tensor([[-3.6026e-01, -2.2037e-01, -5.8909e-05, -1.9150e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3647, -0.4155, -0.0039,  0.1012]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 89 ] state=tensor([[-0.3647, -0.4155, -0.0039,  0.1012]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3730, -0.6106, -0.0019,  0.3926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 90 ] state=tensor([[-0.3730, -0.6106, -0.0019,  0.3926]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3852, -0.4154,  0.0060,  0.0993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 91 ] state=tensor([[-0.3852, -0.4154,  0.0060,  0.0993]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3935, -0.6106,  0.0080,  0.3939]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 92 ] state=tensor([[-0.3935, -0.6106,  0.0080,  0.3939]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4057, -0.4156,  0.0159,  0.1038]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 93 ] state=tensor([[-0.4057, -0.4156,  0.0159,  0.1038]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4140, -0.6110,  0.0179,  0.4014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 94 ] state=tensor([[-0.4140, -0.6110,  0.0179,  0.4014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4262, -0.4161,  0.0260,  0.1144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 95 ] state=tensor([[-0.4262, -0.4161,  0.0260,  0.1144]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4346, -0.2214,  0.0282, -0.1700]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 96 ] state=tensor([[-0.4346, -0.2214,  0.0282, -0.1700]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4390, -0.4169,  0.0248,  0.1315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 97 ] state=tensor([[-0.4390, -0.4169,  0.0248,  0.1315]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4473, -0.6123,  0.0275,  0.4319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 98 ] state=tensor([[-0.4473, -0.6123,  0.0275,  0.4319]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4596, -0.4176,  0.0361,  0.1480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 99 ] state=tensor([[-0.4596, -0.4176,  0.0361,  0.1480]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4679, -0.6132,  0.0391,  0.4519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 100 ] state=tensor([[-0.4679, -0.6132,  0.0391,  0.4519]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4802, -0.4187,  0.0481,  0.1718]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 101 ] state=tensor([[-0.4802, -0.4187,  0.0481,  0.1718]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4886, -0.2243,  0.0515, -0.1054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 102 ] state=tensor([[-0.4886, -0.2243,  0.0515, -0.1054]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4930, -0.4201,  0.0494,  0.2031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 103 ] state=tensor([[-0.4930, -0.4201,  0.0494,  0.2031]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5014, -0.2257,  0.0535, -0.0736]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 104 ] state=tensor([[-0.5014, -0.2257,  0.0535, -0.0736]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5060, -0.0314,  0.0520, -0.3489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 105 ] state=tensor([[-0.5060, -0.0314,  0.0520, -0.3489]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5066,  0.1629,  0.0451, -0.6247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 106 ] state=tensor([[-0.5066,  0.1629,  0.0451, -0.6247]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5033, -0.0328,  0.0326, -0.3182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 107 ] state=tensor([[-0.5033, -0.0328,  0.0326, -0.3182]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5040,  0.1619,  0.0262, -0.6005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 108 ] state=tensor([[-0.5040,  0.1619,  0.0262, -0.6005]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5007, -0.0336,  0.0142, -0.2996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 109 ] state=tensor([[-0.5007, -0.0336,  0.0142, -0.2996]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5014, -0.2289,  0.0082, -0.0025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 110 ] state=tensor([[-0.5014, -0.2289,  0.0082, -0.0025]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5060, -0.4242,  0.0081,  0.2927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 111 ] state=tensor([[-0.5060, -0.4242,  0.0081,  0.2927]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5145, -0.6194,  0.0140,  0.5880]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 112 ] state=tensor([[-0.5145, -0.6194,  0.0140,  0.5880]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5269, -0.4245,  0.0258,  0.2997]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 113 ] state=tensor([[-0.5269, -0.4245,  0.0258,  0.2997]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5354, -0.2297,  0.0317,  0.0153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 114 ] state=tensor([[-0.5354, -0.2297,  0.0317,  0.0153]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5400, -0.4253,  0.0321,  0.3178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 115 ] state=tensor([[-0.5400, -0.4253,  0.0321,  0.3178]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5485, -0.2307,  0.0384,  0.0354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 116 ] state=tensor([[-0.5485, -0.2307,  0.0384,  0.0354]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5531, -0.4263,  0.0391,  0.3400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 117 ] state=tensor([[-0.5531, -0.4263,  0.0391,  0.3400]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5616, -0.2318,  0.0459,  0.0599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 118 ] state=tensor([[-0.5616, -0.2318,  0.0459,  0.0599]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5662, -0.0373,  0.0471, -0.2180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 119 ] state=tensor([[-0.5662, -0.0373,  0.0471, -0.2180]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5670,  0.1571,  0.0428, -0.4954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 120 ] state=tensor([[-0.5670,  0.1571,  0.0428, -0.4954]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5638,  0.3516,  0.0328, -0.7743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 121 ] state=tensor([[-0.5638,  0.3516,  0.0328, -0.7743]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5568,  0.1560,  0.0174, -0.4715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 122 ] state=tensor([[-0.5568,  0.1560,  0.0174, -0.4715]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5537, -0.0393,  0.0079, -0.1734]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 123 ] state=tensor([[-0.5537, -0.0393,  0.0079, -0.1734]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5545, -0.2346,  0.0045,  0.1218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 124 ] state=tensor([[-0.5545, -0.2346,  0.0045,  0.1218]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5592, -0.0395,  0.0069, -0.1695]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 125 ] state=tensor([[-0.5592, -0.0395,  0.0069, -0.1695]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5600,  0.1555,  0.0035, -0.4600]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 126 ] state=tensor([[-0.5600,  0.1555,  0.0035, -0.4600]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5568, -0.0397, -0.0057, -0.1662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 127 ] state=tensor([[-0.5568, -0.0397, -0.0057, -0.1662]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5576, -0.2347, -0.0090,  0.1247]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 128 ] state=tensor([[-0.5576, -0.2347, -0.0090,  0.1247]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5623, -0.4297, -0.0065,  0.4145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 129 ] state=tensor([[-0.5623, -0.4297, -0.0065,  0.4145]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5709, -0.2345,  0.0018,  0.1197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 130 ] state=tensor([[-0.5709, -0.2345,  0.0018,  0.1197]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5756, -0.4296,  0.0042,  0.4130]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 131 ] state=tensor([[-0.5756, -0.4296,  0.0042,  0.4130]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5842, -0.2346,  0.0124,  0.1216]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 132 ] state=tensor([[-0.5842, -0.2346,  0.0124,  0.1216]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5889, -0.0396,  0.0149, -0.1671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 133 ] state=tensor([[-0.5889, -0.0396,  0.0149, -0.1671]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5897, -0.2350,  0.0115,  0.1302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 134 ] state=tensor([[-0.5897, -0.2350,  0.0115,  0.1302]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5944, -0.0400,  0.0141, -0.1588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 135 ] state=tensor([[-0.5944, -0.0400,  0.0141, -0.1588]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5952, -0.2353,  0.0109,  0.1383]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 136 ] state=tensor([[-0.5952, -0.2353,  0.0109,  0.1383]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5999, -0.0404,  0.0137, -0.1509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 137 ] state=tensor([[-0.5999, -0.0404,  0.0137, -0.1509]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6007, -0.2357,  0.0107,  0.1460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 138 ] state=tensor([[-0.6007, -0.2357,  0.0107,  0.1460]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6054, -0.4309,  0.0136,  0.4421]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 139 ] state=tensor([[-0.6054, -0.4309,  0.0136,  0.4421]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6140, -0.2360,  0.0224,  0.1537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 140 ] state=tensor([[-0.6140, -0.2360,  0.0224,  0.1537]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6188, -0.0412,  0.0255, -0.1318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 141 ] state=tensor([[-0.6188, -0.0412,  0.0255, -0.1318]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6196, -0.2367,  0.0229,  0.1688]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 142 ] state=tensor([[-0.6196, -0.2367,  0.0229,  0.1688]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6243, -0.0419,  0.0263, -0.1166]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 143 ] state=tensor([[-0.6243, -0.0419,  0.0263, -0.1166]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6252,  0.1528,  0.0239, -0.4008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 144 ] state=tensor([[-0.6252,  0.1528,  0.0239, -0.4008]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6221, -0.0426,  0.0159, -0.1007]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 145 ] state=tensor([[-0.6221, -0.0426,  0.0159, -0.1007]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6230,  0.1523,  0.0139, -0.3883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 146 ] state=tensor([[-0.6230,  0.1523,  0.0139, -0.3883]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6199,  0.3472,  0.0061, -0.6766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 147 ] state=tensor([[-0.6199,  0.3472,  0.0061, -0.6766]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6130,  0.1520, -0.0074, -0.3820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 148 ] state=tensor([[-0.6130,  0.1520, -0.0074, -0.3820]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6099, -0.0430, -0.0150, -0.0916]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 149 ] state=tensor([[-0.6099, -0.0430, -0.0150, -0.0916]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6108, -0.2379, -0.0169,  0.1963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 150 ] state=tensor([[-0.6108, -0.2379, -0.0169,  0.1963]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6155, -0.0426, -0.0129, -0.1017]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 151 ] state=tensor([[-0.6155, -0.0426, -0.0129, -0.1017]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6164, -0.2375, -0.0150,  0.1869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 152 ] state=tensor([[-0.6164, -0.2375, -0.0150,  0.1869]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6211, -0.4324, -0.0112,  0.4748]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 153 ] state=tensor([[-0.6211, -0.4324, -0.0112,  0.4748]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6298, -0.2371, -0.0017,  0.1786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 154 ] state=tensor([[-0.6298, -0.2371, -0.0017,  0.1786]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6345, -0.0420,  0.0018, -0.1147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 155 ] state=tensor([[-0.6345, -0.0420,  0.0018, -0.1147]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-6.3538e-01, -2.3715e-01, -4.6873e-04,  1.7861e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 156 ] state=tensor([[-6.3538e-01, -2.3715e-01, -4.6873e-04,  1.7861e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6401, -0.0420,  0.0031, -0.1142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 157 ] state=tensor([[-0.6401, -0.0420,  0.0031, -0.1142]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6410, -0.2372,  0.0008,  0.1794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 158 ] state=tensor([[-0.6410, -0.2372,  0.0008,  0.1794]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6457, -0.0421,  0.0044, -0.1130]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 159 ] state=tensor([[-0.6457, -0.0421,  0.0044, -0.1130]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6465, -0.2373,  0.0021,  0.1811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 160 ] state=tensor([[-0.6465, -0.2373,  0.0021,  0.1811]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6513, -0.0422,  0.0058, -0.1109]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 161 ] state=tensor([[-0.6513, -0.0422,  0.0058, -0.1109]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6521, -0.2374,  0.0036,  0.1836]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 162 ] state=tensor([[-0.6521, -0.2374,  0.0036,  0.1836]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6569, -0.0423,  0.0072, -0.1080]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 163 ] state=tensor([[-0.6569, -0.0423,  0.0072, -0.1080]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6577, -0.2375,  0.0051,  0.1870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 164 ] state=tensor([[-0.6577, -0.2375,  0.0051,  0.1870]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6625, -0.0425,  0.0088, -0.1041]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 165 ] state=tensor([[-0.6625, -0.0425,  0.0088, -0.1041]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6633, -0.2377,  0.0067,  0.1913]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 166 ] state=tensor([[-0.6633, -0.2377,  0.0067,  0.1913]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6681, -0.0427,  0.0105, -0.0992]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 167 ] state=tensor([[-0.6681, -0.0427,  0.0105, -0.0992]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6689,  0.1523,  0.0086, -0.3886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 168 ] state=tensor([[-0.6689,  0.1523,  0.0086, -0.3886]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6659,  0.3473,  0.0008, -0.6785]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 169 ] state=tensor([[-0.6659,  0.3473,  0.0008, -0.6785]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6590,  0.1521, -0.0128, -0.3856]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 170 ] state=tensor([[-0.6590,  0.1521, -0.0128, -0.3856]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6559, -0.0428, -0.0205, -0.0970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 171 ] state=tensor([[-0.6559, -0.0428, -0.0205, -0.0970]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6568, -0.2376, -0.0224,  0.1892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 172 ] state=tensor([[-0.6568, -0.2376, -0.0224,  0.1892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6615, -0.4324, -0.0186,  0.4747]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 173 ] state=tensor([[-0.6615, -0.4324, -0.0186,  0.4747]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6702, -0.2370, -0.0092,  0.1762]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 174 ] state=tensor([[-0.6702, -0.2370, -0.0092,  0.1762]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6749, -0.0418, -0.0056, -0.1194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 175 ] state=tensor([[-0.6749, -0.0418, -0.0056, -0.1194]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6757, -0.2368, -0.0080,  0.1715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 176 ] state=tensor([[-0.6757, -0.2368, -0.0080,  0.1715]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6805, -0.0416, -0.0046, -0.1237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 177 ] state=tensor([[-0.6805, -0.0416, -0.0046, -0.1237]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6813, -0.2366, -0.0071,  0.1676]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 178 ] state=tensor([[-0.6813, -0.2366, -0.0071,  0.1676]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6860, -0.4317, -0.0037,  0.4580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 179 ] state=tensor([[-0.6860, -0.4317, -0.0037,  0.4580]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6947, -0.2365,  0.0055,  0.1642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 180 ] state=tensor([[-0.6947, -0.2365,  0.0055,  0.1642]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6994, -0.0415,  0.0087, -0.1268]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 181 ] state=tensor([[-0.6994, -0.0415,  0.0087, -0.1268]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7002, -0.2367,  0.0062,  0.1686]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 182 ] state=tensor([[-0.7002, -0.2367,  0.0062,  0.1686]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7050, -0.0417,  0.0096, -0.1221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 183 ] state=tensor([[-0.7050, -0.0417,  0.0096, -0.1221]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7058,  0.1533,  0.0071, -0.4117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 184 ] state=tensor([[-0.7058,  0.1533,  0.0071, -0.4117]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7027, -0.0419, -0.0011, -0.1168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 185 ] state=tensor([[-0.7027, -0.0419, -0.0011, -0.1168]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7036,  0.1532, -0.0034, -0.4098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 186 ] state=tensor([[-0.7036,  0.1532, -0.0034, -0.4098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7005, -0.0418, -0.0116, -0.1182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 187 ] state=tensor([[-0.7005, -0.0418, -0.0116, -0.1182]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7013, -0.2368, -0.0140,  0.1708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 188 ] state=tensor([[-0.7013, -0.2368, -0.0140,  0.1708]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7061, -0.4317, -0.0106,  0.4590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 189 ] state=tensor([[-0.7061, -0.4317, -0.0106,  0.4590]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7147, -0.2364, -0.0014,  0.1630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 190 ] state=tensor([[-0.7147, -0.2364, -0.0014,  0.1630]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7194, -0.0413,  0.0019, -0.1301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 191 ] state=tensor([[-0.7194, -0.0413,  0.0019, -0.1301]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7203, -0.2364, -0.0007,  0.1631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 192 ] state=tensor([[-0.7203, -0.2364, -0.0007,  0.1631]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7250, -0.0413,  0.0025, -0.1298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 193 ] state=tensor([[-0.7250, -0.0413,  0.0025, -0.1298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.2583e-01, -2.3647e-01, -8.2404e-05,  1.6368e-01]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 125 ][ timestamp 194 ] state=tensor([[-7.2583e-01, -2.3647e-01, -8.2404e-05,  1.6368e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7306, -0.0414,  0.0032, -0.1290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 195 ] state=tensor([[-0.7306, -0.0414,  0.0032, -0.1290]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.3138e-01, -2.3652e-01,  6.1063e-04,  1.6466e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 196 ] state=tensor([[-7.3138e-01, -2.3652e-01,  6.1063e-04,  1.6466e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7361, -0.0414,  0.0039, -0.1278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 197 ] state=tensor([[-0.7361, -0.0414,  0.0039, -0.1278]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7369,  0.1537,  0.0013, -0.4193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 198 ] state=tensor([[-0.7369,  0.1537,  0.0013, -0.4193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7339, -0.0415, -0.0070, -0.1262]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 199 ] state=tensor([[-0.7339, -0.0415, -0.0070, -0.1262]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7347,  0.1537, -0.0096, -0.4211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 200 ] state=tensor([[-0.7347,  0.1537, -0.0096, -0.4211]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7316, -0.0412, -0.0180, -0.1314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 201 ] state=tensor([[-0.7316, -0.0412, -0.0180, -0.1314]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7324, -0.2361, -0.0206,  0.1555]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 202 ] state=tensor([[-0.7324, -0.2361, -0.0206,  0.1555]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7372, -0.0407, -0.0175, -0.1436]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 203 ] state=tensor([[-0.7372, -0.0407, -0.0175, -0.1436]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7380, -0.2356, -0.0204,  0.1435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 204 ] state=tensor([[-0.7380, -0.2356, -0.0204,  0.1435]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7427, -0.0402, -0.0175, -0.1555]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 205 ] state=tensor([[-0.7427, -0.0402, -0.0175, -0.1555]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7435, -0.2350, -0.0206,  0.1316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 206 ] state=tensor([[-0.7435, -0.2350, -0.0206,  0.1316]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7482, -0.0396, -0.0180, -0.1675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 207 ] state=tensor([[-0.7482, -0.0396, -0.0180, -0.1675]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7490, -0.2345, -0.0213,  0.1195]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 208 ] state=tensor([[-0.7490, -0.2345, -0.0213,  0.1195]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7537, -0.4293, -0.0189,  0.4053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 209 ] state=tensor([[-0.7537, -0.4293, -0.0189,  0.4053]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7623, -0.2339, -0.0108,  0.1067]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 210 ] state=tensor([[-0.7623, -0.2339, -0.0108,  0.1067]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7669, -0.0386, -0.0087, -0.1893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 211 ] state=tensor([[-0.7669, -0.0386, -0.0087, -0.1893]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7677,  0.1566, -0.0125, -0.4848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 212 ] state=tensor([[-0.7677,  0.1566, -0.0125, -0.4848]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7646, -0.0383, -0.0222, -0.1960]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 213 ] state=tensor([[-0.7646, -0.0383, -0.0222, -0.1960]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7653, -0.2331, -0.0261,  0.0896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 214 ] state=tensor([[-0.7653, -0.2331, -0.0261,  0.0896]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7700, -0.0376, -0.0243, -0.2112]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 215 ] state=tensor([[-0.7700, -0.0376, -0.0243, -0.2112]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7708, -0.2324, -0.0285,  0.0737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 216 ] state=tensor([[-0.7708, -0.2324, -0.0285,  0.0737]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7754, -0.4271, -0.0271,  0.3572]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 217 ] state=tensor([[-0.7754, -0.4271, -0.0271,  0.3572]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7840, -0.2316, -0.0199,  0.0561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 218 ] state=tensor([[-0.7840, -0.2316, -0.0199,  0.0561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7886, -0.0362, -0.0188, -0.2428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 219 ] state=tensor([[-0.7886, -0.0362, -0.0188, -0.2428]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7893, -0.2310, -0.0236,  0.0439]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 220 ] state=tensor([[-0.7893, -0.2310, -0.0236,  0.0439]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7939, -0.4258, -0.0228,  0.3291]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 221 ] state=tensor([[-0.7939, -0.4258, -0.0228,  0.3291]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8024, -0.2304, -0.0162,  0.0293]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 222 ] state=tensor([[-0.8024, -0.2304, -0.0162,  0.0293]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8071, -0.4253, -0.0156,  0.3168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 223 ] state=tensor([[-0.8071, -0.4253, -0.0156,  0.3168]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8156, -0.2299, -0.0093,  0.0193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 224 ] state=tensor([[-0.8156, -0.2299, -0.0093,  0.0193]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8202, -0.0347, -0.0089, -0.2763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 225 ] state=tensor([[-0.8202, -0.0347, -0.0089, -0.2763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8209, -0.2297, -0.0144,  0.0135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 226 ] state=tensor([[-0.8209, -0.2297, -0.0144,  0.0135]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8254, -0.4246, -0.0141,  0.3016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 227 ] state=tensor([[-0.8254, -0.4246, -0.0141,  0.3016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8339, -0.2293, -0.0081,  0.0045]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 228 ] state=tensor([[-0.8339, -0.2293, -0.0081,  0.0045]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8385, -0.4243, -0.0080,  0.2946]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 229 ] state=tensor([[-0.8385, -0.4243, -0.0080,  0.2946]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8470, -0.6193, -0.0021,  0.5848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 230 ] state=tensor([[-0.8470, -0.6193, -0.0021,  0.5848]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8594, -0.4241,  0.0096,  0.2914]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 231 ] state=tensor([[-0.8594, -0.4241,  0.0096,  0.2914]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8679, -0.2291,  0.0154,  0.0018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 232 ] state=tensor([[-0.8679, -0.2291,  0.0154,  0.0018]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8725, -0.4245,  0.0154,  0.2993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 233 ] state=tensor([[-0.8725, -0.4245,  0.0154,  0.2993]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8809, -0.2296,  0.0214,  0.0115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 234 ] state=tensor([[-0.8809, -0.2296,  0.0214,  0.0115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8855, -0.0348,  0.0217, -0.2743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 235 ] state=tensor([[-0.8855, -0.0348,  0.0217, -0.2743]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8862,  0.1600,  0.0162, -0.5601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 236 ] state=tensor([[-0.8862,  0.1600,  0.0162, -0.5601]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8830, -0.0353,  0.0050, -0.2624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 237 ] state=tensor([[-0.8830, -0.0353,  0.0050, -0.2624]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-8.8374e-01, -2.3050e-01, -2.8236e-04,  3.1868e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 238 ] state=tensor([[-8.8374e-01, -2.3050e-01, -2.8236e-04,  3.1868e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.8835e-01, -3.5372e-02,  3.5501e-04, -2.6090e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 239 ] state=tensor([[-8.8835e-01, -3.5372e-02,  3.5501e-04, -2.6090e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8891,  0.1597, -0.0049, -0.5535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 240 ] state=tensor([[-0.8891,  0.1597, -0.0049, -0.5535]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8859, -0.0353, -0.0159, -0.2623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 241 ] state=tensor([[-0.8859, -0.0353, -0.0159, -0.2623]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8866, -0.2302, -0.0212,  0.0253]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 242 ] state=tensor([[-0.8866, -0.2302, -0.0212,  0.0253]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8912, -0.4250, -0.0207,  0.3112]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 243 ] state=tensor([[-0.8912, -0.4250, -0.0207,  0.3112]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8997, -0.6198, -0.0144,  0.5973]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 244 ] state=tensor([[-0.8997, -0.6198, -0.0144,  0.5973]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9121, -0.4245, -0.0025,  0.3001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 245 ] state=tensor([[-0.9121, -0.4245, -0.0025,  0.3001]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9206, -0.2294,  0.0035,  0.0066]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 246 ] state=tensor([[-0.9206, -0.2294,  0.0035,  0.0066]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9251, -0.4245,  0.0036,  0.3004]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 247 ] state=tensor([[-0.9251, -0.4245,  0.0036,  0.3004]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9336, -0.2295,  0.0096,  0.0089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 248 ] state=tensor([[-0.9336, -0.2295,  0.0096,  0.0089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9382, -0.4247,  0.0098,  0.3046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 249 ] state=tensor([[-0.9382, -0.4247,  0.0098,  0.3046]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9467, -0.2297,  0.0159,  0.0150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 250 ] state=tensor([[-0.9467, -0.2297,  0.0159,  0.0150]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9513, -0.0348,  0.0162, -0.2726]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 251 ] state=tensor([[-0.9513, -0.0348,  0.0162, -0.2726]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9520, -0.2302,  0.0108,  0.0252]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 252 ] state=tensor([[-0.9520, -0.2302,  0.0108,  0.0252]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9566, -0.0352,  0.0113, -0.2641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 253 ] state=tensor([[-0.9566, -0.0352,  0.0113, -0.2641]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9573,  0.1597,  0.0060, -0.5532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 254 ] state=tensor([[-0.9573,  0.1597,  0.0060, -0.5532]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9541, -0.0355, -0.0051, -0.2587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 255 ] state=tensor([[-0.9541, -0.0355, -0.0051, -0.2587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9548, -0.2305, -0.0103,  0.0324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 256 ] state=tensor([[-0.9548, -0.2305, -0.0103,  0.0324]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9594, -0.4255, -0.0096,  0.3218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 257 ] state=tensor([[-0.9594, -0.4255, -0.0096,  0.3218]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9680, -0.2302, -0.0032,  0.0261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 258 ] state=tensor([[-0.9680, -0.2302, -0.0032,  0.0261]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9726, -0.0351, -0.0027, -0.2675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 259 ] state=tensor([[-0.9726, -0.0351, -0.0027, -0.2675]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9733, -0.2302, -0.0080,  0.0243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 260 ] state=tensor([[-0.9733, -0.2302, -0.0080,  0.0243]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9779, -0.0349, -0.0075, -0.2709]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 261 ] state=tensor([[-0.9779, -0.0349, -0.0075, -0.2709]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9786, -0.2299, -0.0129,  0.0194]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 262 ] state=tensor([[-0.9786, -0.2299, -0.0129,  0.0194]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9832, -0.4249, -0.0125,  0.3080]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 263 ] state=tensor([[-0.9832, -0.4249, -0.0125,  0.3080]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9917, -0.2296, -0.0064,  0.0114]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 264 ] state=tensor([[-0.9917, -0.2296, -0.0064,  0.0114]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9962, -0.0344, -0.0062, -0.2833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 265 ] state=tensor([[-0.9962, -0.0344, -0.0062, -0.2833]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9969,  0.1609, -0.0118, -0.5779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 266 ] state=tensor([[-0.9969,  0.1609, -0.0118, -0.5779]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9937, -0.0341, -0.0234, -0.2890]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 125 ][ timestamp 267 ] state=tensor([[-0.9937, -0.0341, -0.0234, -0.2890]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9944, -0.2289, -0.0292, -0.0038]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 268 ] state=tensor([[-0.9944, -0.2289, -0.0292, -0.0038]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9990, -0.4236, -0.0292,  0.2796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 269 ] state=tensor([[-0.9990, -0.4236, -0.0292,  0.2796]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0075, -0.2280, -0.0236, -0.0222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 270 ] state=tensor([[-1.0075, -0.2280, -0.0236, -0.0222]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0120, -0.4228, -0.0241,  0.2629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 271 ] state=tensor([[-1.0120, -0.4228, -0.0241,  0.2629]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0205, -0.2274, -0.0188, -0.0373]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 272 ] state=tensor([[-1.0205, -0.2274, -0.0188, -0.0373]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0250, -0.4222, -0.0196,  0.2494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 273 ] state=tensor([[-1.0250, -0.4222, -0.0196,  0.2494]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0335, -0.2268, -0.0146, -0.0494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 274 ] state=tensor([[-1.0335, -0.2268, -0.0146, -0.0494]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0380, -0.4217, -0.0156,  0.2387]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 275 ] state=tensor([[-1.0380, -0.4217, -0.0156,  0.2387]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0464, -0.2264, -0.0108, -0.0589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 276 ] state=tensor([[-1.0464, -0.2264, -0.0108, -0.0589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0510, -0.4213, -0.0120,  0.2304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 277 ] state=tensor([[-1.0510, -0.4213, -0.0120,  0.2304]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0594, -0.2261, -0.0074, -0.0661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 278 ] state=tensor([[-1.0594, -0.2261, -0.0074, -0.0661]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0639, -0.4211, -0.0087,  0.2243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 279 ] state=tensor([[-1.0639, -0.4211, -0.0087,  0.2243]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0723, -0.2258, -0.0042, -0.0711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 280 ] state=tensor([[-1.0723, -0.2258, -0.0042, -0.0711]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0768, -0.4209, -0.0056,  0.2202]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 281 ] state=tensor([[-1.0768, -0.4209, -0.0056,  0.2202]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0853, -0.2257, -0.0012, -0.0742]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 282 ] state=tensor([[-1.0853, -0.2257, -0.0012, -0.0742]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0898, -0.4208, -0.0027,  0.2181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 283 ] state=tensor([[-1.0898, -0.4208, -0.0027,  0.2181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0982, -0.2256,  0.0016, -0.0755]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 284 ] state=tensor([[-1.0982, -0.2256,  0.0016, -0.0755]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1027e+00, -4.2078e-01,  1.3795e-04,  2.1772e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 285 ] state=tensor([[-1.1027e+00, -4.2078e-01,  1.3795e-04,  2.1772e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1111, -0.2257,  0.0045, -0.0749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 286 ] state=tensor([[-1.1111, -0.2257,  0.0045, -0.0749]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1156, -0.0306,  0.0030, -0.3662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 287 ] state=tensor([[-1.1156, -0.0306,  0.0030, -0.3662]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1162, -0.2258, -0.0043, -0.0726]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 288 ] state=tensor([[-1.1162, -0.2258, -0.0043, -0.0726]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1208, -0.4208, -0.0058,  0.2188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 289 ] state=tensor([[-1.1208, -0.4208, -0.0058,  0.2188]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1292, -0.2256, -0.0014, -0.0757]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 290 ] state=tensor([[-1.1292, -0.2256, -0.0014, -0.0757]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1337, -0.0305, -0.0029, -0.3689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 291 ] state=tensor([[-1.1337, -0.0305, -0.0029, -0.3689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1343, -0.2256, -0.0103, -0.0771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 292 ] state=tensor([[-1.1343, -0.2256, -0.0103, -0.0771]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1388, -0.4205, -0.0118,  0.2123]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 293 ] state=tensor([[-1.1388, -0.4205, -0.0118,  0.2123]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1472, -0.2252, -0.0076, -0.0841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 294 ] state=tensor([[-1.1472, -0.2252, -0.0076, -0.0841]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1517, -0.4203, -0.0093,  0.2062]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 295 ] state=tensor([[-1.1517, -0.4203, -0.0093,  0.2062]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1601, -0.2250, -0.0052, -0.0894]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 296 ] state=tensor([[-1.1601, -0.2250, -0.0052, -0.0894]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1646, -0.4200, -0.0069,  0.2017]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 297 ] state=tensor([[-1.1646, -0.4200, -0.0069,  0.2017]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1730, -0.2248, -0.0029, -0.0932]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 298 ] state=tensor([[-1.1730, -0.2248, -0.0029, -0.0932]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1775, -0.4199, -0.0048,  0.1986]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 299 ] state=tensor([[-1.1775, -0.4199, -0.0048,  0.1986]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1859e+00, -2.2472e-01, -7.9922e-04, -9.5630e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 300 ] state=tensor([[-1.1859e+00, -2.2472e-01, -7.9922e-04, -9.5630e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1904, -0.0296, -0.0027, -0.3886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 301 ] state=tensor([[-1.1904, -0.0296, -0.0027, -0.3886]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1910, -0.2247, -0.0105, -0.0967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 302 ] state=tensor([[-1.1910, -0.2247, -0.0105, -0.0967]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1955, -0.4196, -0.0124,  0.1926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 303 ] state=tensor([[-1.1955, -0.4196, -0.0124,  0.1926]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2039, -0.2243, -0.0086, -0.1040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 304 ] state=tensor([[-1.2039, -0.2243, -0.0086, -0.1040]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2084, -0.4193, -0.0106,  0.1860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 305 ] state=tensor([[-1.2084, -0.4193, -0.0106,  0.1860]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2168, -0.2241, -0.0069, -0.1100]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 306 ] state=tensor([[-1.2168, -0.2241, -0.0069, -0.1100]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2213, -0.4191, -0.0091,  0.1805]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 307 ] state=tensor([[-1.2213, -0.4191, -0.0091,  0.1805]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2296, -0.2238, -0.0055, -0.1151]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 308 ] state=tensor([[-1.2296, -0.2238, -0.0055, -0.1151]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2341, -0.0286, -0.0078, -0.4095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 309 ] state=tensor([[-1.2341, -0.0286, -0.0078, -0.4095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2347, -0.2236, -0.0160, -0.1193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 310 ] state=tensor([[-1.2347, -0.2236, -0.0160, -0.1193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2392, -0.4185, -0.0184,  0.1683]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 311 ] state=tensor([[-1.2392, -0.4185, -0.0184,  0.1683]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2475, -0.2232, -0.0150, -0.1301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 312 ] state=tensor([[-1.2475, -0.2232, -0.0150, -0.1301]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2520, -0.4181, -0.0176,  0.1578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 313 ] state=tensor([[-1.2520, -0.4181, -0.0176,  0.1578]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2604, -0.6129, -0.0145,  0.4449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 314 ] state=tensor([[-1.2604, -0.6129, -0.0145,  0.4449]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2726, -0.4176, -0.0056,  0.1477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 315 ] state=tensor([[-1.2726, -0.4176, -0.0056,  0.1477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2810, -0.2224, -0.0026, -0.1468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 316 ] state=tensor([[-1.2810, -0.2224, -0.0026, -0.1468]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2854, -0.4175, -0.0056,  0.1451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 317 ] state=tensor([[-1.2854, -0.4175, -0.0056,  0.1451]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2938, -0.6125, -0.0027,  0.4360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 318 ] state=tensor([[-1.2938, -0.6125, -0.0027,  0.4360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3060, -0.4174,  0.0061,  0.1425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 319 ] state=tensor([[-1.3060, -0.4174,  0.0061,  0.1425]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3144, -0.6126,  0.0089,  0.4371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 320 ] state=tensor([[-1.3144, -0.6126,  0.0089,  0.4371]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3266, -0.4176,  0.0177,  0.1472]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 321 ] state=tensor([[-1.3266, -0.4176,  0.0177,  0.1472]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3350, -0.2227,  0.0206, -0.1399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 322 ] state=tensor([[-1.3350, -0.2227,  0.0206, -0.1399]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3394, -0.0279,  0.0178, -0.4260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 323 ] state=tensor([[-1.3394, -0.0279,  0.0178, -0.4260]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3400, -0.2233,  0.0093, -0.1277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 324 ] state=tensor([[-1.3400, -0.2233,  0.0093, -0.1277]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3444, -0.0283,  0.0067, -0.4175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 325 ] state=tensor([[-1.3444, -0.0283,  0.0067, -0.4175]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3450, -0.2235, -0.0016, -0.1227]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 326 ] state=tensor([[-1.3450, -0.2235, -0.0016, -0.1227]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3495, -0.0283, -0.0041, -0.4159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 327 ] state=tensor([[-1.3495, -0.0283, -0.0041, -0.4159]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3500, -0.2234, -0.0124, -0.1245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 328 ] state=tensor([[-1.3500, -0.2234, -0.0124, -0.1245]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3545, -0.0281, -0.0149, -0.4210]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 329 ] state=tensor([[-1.3545, -0.0281, -0.0149, -0.4210]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3551, -0.2230, -0.0233, -0.1331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 330 ] state=tensor([[-1.3551, -0.2230, -0.0233, -0.1331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3595, -0.4178, -0.0260,  0.1522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 331 ] state=tensor([[-1.3595, -0.4178, -0.0260,  0.1522]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3679, -0.6125, -0.0229,  0.4365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 332 ] state=tensor([[-1.3679, -0.6125, -0.0229,  0.4365]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3801, -0.4171, -0.0142,  0.1367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 333 ] state=tensor([[-1.3801, -0.4171, -0.0142,  0.1367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3885, -0.6120, -0.0115,  0.4249]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 334 ] state=tensor([[-1.3885, -0.6120, -0.0115,  0.4249]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4007, -0.4167, -0.0030,  0.1286]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 335 ] state=tensor([[-1.4007, -0.4167, -0.0030,  0.1286]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4091e+00, -2.2157e-01, -3.8267e-04, -1.6498e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 336 ] state=tensor([[-1.4091e+00, -2.2157e-01, -3.8267e-04, -1.6498e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4135, -0.0264, -0.0037, -0.4578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 337 ] state=tensor([[-1.4135, -0.0264, -0.0037, -0.4578]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4140, -0.2215, -0.0128, -0.1663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 338 ] state=tensor([[-1.4140, -0.2215, -0.0128, -0.1663]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4184, -0.0262, -0.0162, -0.4630]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 125 ][ timestamp 339 ] state=tensor([[-1.4184, -0.0262, -0.0162, -0.4630]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4190, -0.2211, -0.0254, -0.1754]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 340 ] state=tensor([[-1.4190, -0.2211, -0.0254, -0.1754]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4234, -0.4159, -0.0289,  0.1091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 341 ] state=tensor([[-1.4234, -0.4159, -0.0289,  0.1091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4317, -0.6105, -0.0267,  0.3925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 342 ] state=tensor([[-1.4317, -0.6105, -0.0267,  0.3925]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4439, -0.4151, -0.0189,  0.0915]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 343 ] state=tensor([[-1.4439, -0.4151, -0.0189,  0.0915]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4522, -0.2197, -0.0171, -0.2070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 344 ] state=tensor([[-1.4522, -0.2197, -0.0171, -0.2070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4566, -0.0243, -0.0212, -0.5051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 345 ] state=tensor([[-1.4566, -0.0243, -0.0212, -0.5051]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4571, -0.2191, -0.0313, -0.2191]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 346 ] state=tensor([[-1.4571, -0.2191, -0.0313, -0.2191]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4615, -0.0236, -0.0357, -0.5215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 347 ] state=tensor([[-1.4615, -0.0236, -0.0357, -0.5215]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4620, -0.2182, -0.0461, -0.2403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 348 ] state=tensor([[-1.4620, -0.2182, -0.0461, -0.2403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4663, -0.4126, -0.0509,  0.0375]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 349 ] state=tensor([[-1.4663, -0.4126, -0.0509,  0.0375]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4746, -0.2168, -0.0502, -0.2708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 350 ] state=tensor([[-1.4746, -0.2168, -0.0502, -0.2708]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4789, -0.4112, -0.0556,  0.0056]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 351 ] state=tensor([[-1.4789, -0.4112, -0.0556,  0.0056]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4871, -0.6054, -0.0555,  0.2803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 352 ] state=tensor([[-1.4871, -0.6054, -0.0555,  0.2803]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4992, -0.4096, -0.0499, -0.0294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 353 ] state=tensor([[-1.4992, -0.4096, -0.0499, -0.0294]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5074, -0.6039, -0.0505,  0.2472]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 354 ] state=tensor([[-1.5074, -0.6039, -0.0505,  0.2472]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5195, -0.7983, -0.0455,  0.5235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 355 ] state=tensor([[-1.5195, -0.7983, -0.0455,  0.5235]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5355, -0.9928, -0.0351,  0.8015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 356 ] state=tensor([[-1.5355, -0.9928, -0.0351,  0.8015]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5553, -1.1874, -0.0190,  1.0830]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 357 ] state=tensor([[-1.5553, -1.1874, -0.0190,  1.0830]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5791, -1.3823,  0.0026,  1.3696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 358 ] state=tensor([[-1.5791, -1.3823,  0.0026,  1.3696]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6067, -1.1872,  0.0300,  1.0778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 359 ] state=tensor([[-1.6067, -1.1872,  0.0300,  1.0778]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6305, -1.3827,  0.0516,  1.3797]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 360 ] state=tensor([[-1.6305, -1.3827,  0.0516,  1.3797]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6581, -1.5784,  0.0792,  1.6881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 361 ] state=tensor([[-1.6581, -1.5784,  0.0792,  1.6881]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6897, -1.3843,  0.1129,  1.4210]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 362 ] state=tensor([[-1.6897, -1.3843,  0.1129,  1.4210]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7174, -1.5806,  0.1414,  1.7468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 363 ] state=tensor([[-1.7174, -1.5806,  0.1414,  1.7468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7490, -1.3873,  0.1763,  1.5012]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 364 ] state=tensor([[-1.7490, -1.3873,  0.1763,  1.5012]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7767, -1.5841,  0.2063,  1.8434]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 125 ][ timestamp 365 ] state=tensor([[-1.7767, -1.5841,  0.2063,  1.8434]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 125: Exploration_rate=0.05. Score=365.\n",
      "[ episode 126 ] state=tensor([[ 0.0459, -0.0138, -0.0368,  0.0156]])\n",
      "[ episode 126 ][ timestamp 1 ] state=tensor([[ 0.0459, -0.0138, -0.0368,  0.0156]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0456,  0.1818, -0.0365, -0.2885]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 2 ] state=tensor([[ 0.0456,  0.1818, -0.0365, -0.2885]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0493,  0.3774, -0.0423, -0.5925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 3 ] state=tensor([[ 0.0493,  0.3774, -0.0423, -0.5925]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0568,  0.1829, -0.0541, -0.3134]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 4 ] state=tensor([[ 0.0568,  0.1829, -0.0541, -0.3134]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0605, -0.0114, -0.0604, -0.0383]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 5 ] state=tensor([[ 0.0605, -0.0114, -0.0604, -0.0383]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0602, -0.2056, -0.0612,  0.2348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 6 ] state=tensor([[ 0.0602, -0.2056, -0.0612,  0.2348]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0561, -0.3998, -0.0565,  0.5075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 7 ] state=tensor([[ 0.0561, -0.3998, -0.0565,  0.5075]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0481, -0.2039, -0.0463,  0.1976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 8 ] state=tensor([[ 0.0481, -0.2039, -0.0463,  0.1976]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0441, -0.0082, -0.0424, -0.1093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 9 ] state=tensor([[ 0.0441, -0.0082, -0.0424, -0.1093]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0439, -0.2026, -0.0446,  0.1697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 10 ] state=tensor([[ 0.0439, -0.2026, -0.0446,  0.1697]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0398, -0.3971, -0.0412,  0.4480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 11 ] state=tensor([[ 0.0398, -0.3971, -0.0412,  0.4480]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0319, -0.2014, -0.0322,  0.1426]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 12 ] state=tensor([[ 0.0319, -0.2014, -0.0322,  0.1426]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0279, -0.3961, -0.0294,  0.4250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 13 ] state=tensor([[ 0.0279, -0.3961, -0.0294,  0.4250]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0200, -0.5908, -0.0209,  0.7082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 14 ] state=tensor([[ 0.0200, -0.5908, -0.0209,  0.7082]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0081, -0.3954, -0.0067,  0.4091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 15 ] state=tensor([[ 0.0081, -0.3954, -0.0067,  0.4091]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0002, -0.2001,  0.0015,  0.1143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 16 ] state=tensor([[ 0.0002, -0.2001,  0.0015,  0.1143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0038, -0.0050,  0.0038, -0.1779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 17 ] state=tensor([[-0.0038, -0.0050,  0.0038, -0.1779]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.8729e-03,  1.9003e-01,  2.1627e-04, -4.6941e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 18 ] state=tensor([[-3.8729e-03,  1.9003e-01,  2.1627e-04, -4.6941e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.2398e-05, -5.0983e-03, -9.1720e-03, -1.7666e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 19 ] state=tensor([[-7.2398e-05, -5.0983e-03, -9.1720e-03, -1.7666e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7436e-04, -2.0009e-01, -1.2705e-02,  1.1311e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 20 ] state=tensor([[-1.7436e-04, -2.0009e-01, -1.2705e-02,  1.1311e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0042, -0.3950, -0.0104,  0.4018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 21 ] state=tensor([[-0.0042, -0.3950, -0.0104,  0.4018]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0121, -0.1998, -0.0024,  0.1058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 22 ] state=tensor([[-0.0121, -0.1998, -0.0024,  0.1058]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0161, -0.0046, -0.0003, -0.1876]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 23 ] state=tensor([[-0.0161, -0.0046, -0.0003, -0.1876]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0162,  0.1905, -0.0040, -0.4804]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 24 ] state=tensor([[-0.0162,  0.1905, -0.0040, -0.4804]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0124, -0.0045, -0.0137, -0.1890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 25 ] state=tensor([[-0.0124, -0.0045, -0.0137, -0.1890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0124,  0.1908, -0.0174, -0.4860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 26 ] state=tensor([[-0.0124,  0.1908, -0.0174, -0.4860]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0086, -0.0041, -0.0272, -0.1988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 27 ] state=tensor([[-0.0086, -0.0041, -0.0272, -0.1988]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0087, -0.1988, -0.0311,  0.0852]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 28 ] state=tensor([[-0.0087, -0.1988, -0.0311,  0.0852]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0127, -0.0033, -0.0294, -0.2172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 29 ] state=tensor([[-0.0127, -0.0033, -0.0294, -0.2172]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0128,  0.1923, -0.0338, -0.5190]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 30 ] state=tensor([[-0.0128,  0.1923, -0.0338, -0.5190]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0089, -0.0024, -0.0441, -0.2371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 31 ] state=tensor([[-0.0089, -0.0024, -0.0441, -0.2371]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0090, -0.1968, -0.0489,  0.0413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 32 ] state=tensor([[-0.0090, -0.1968, -0.0489,  0.0413]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0129, -0.3912, -0.0481,  0.3182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 33 ] state=tensor([[-0.0129, -0.3912, -0.0481,  0.3182]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0207, -0.1954, -0.0417,  0.0107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 34 ] state=tensor([[-0.0207, -0.1954, -0.0417,  0.0107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0246, -0.3899, -0.0415,  0.2900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 35 ] state=tensor([[-0.0246, -0.3899, -0.0415,  0.2900]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0324, -0.1943, -0.0357, -0.0155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 36 ] state=tensor([[-0.0324, -0.1943, -0.0357, -0.0155]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0363,  0.0014, -0.0360, -0.3192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 37 ] state=tensor([[-0.0363,  0.0014, -0.0360, -0.3192]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0363, -0.1932, -0.0424, -0.0381]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 38 ] state=tensor([[-0.0363, -0.1932, -0.0424, -0.0381]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0401, -0.3877, -0.0431,  0.2409]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 39 ] state=tensor([[-0.0401, -0.3877, -0.0431,  0.2409]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0479, -0.5822, -0.0383,  0.5197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 40 ] state=tensor([[-0.0479, -0.5822, -0.0383,  0.5197]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0595, -0.3866, -0.0279,  0.2152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 41 ] state=tensor([[-0.0595, -0.3866, -0.0279,  0.2152]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0673, -0.5813, -0.0236,  0.4989]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 42 ] state=tensor([[-0.0673, -0.5813, -0.0236,  0.4989]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0789, -0.3858, -0.0137,  0.1989]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 43 ] state=tensor([[-0.0789, -0.3858, -0.0137,  0.1989]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0866, -0.5808, -0.0097,  0.4872]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 44 ] state=tensor([[-0.0866, -0.5808, -0.0097,  0.4872]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-9.8233e-02, -7.7574e-01,  6.7725e-05,  7.7682e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 45 ] state=tensor([[-9.8233e-02, -7.7574e-01,  6.7725e-05,  7.7682e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1137, -0.5806,  0.0156,  0.4842]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 46 ] state=tensor([[-0.1137, -0.5806,  0.0156,  0.4842]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1254, -0.3857,  0.0253,  0.1964]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 126 ][ timestamp 47 ] state=tensor([[-0.1254, -0.3857,  0.0253,  0.1964]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1331, -0.1910,  0.0292, -0.0882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 48 ] state=tensor([[-0.1331, -0.1910,  0.0292, -0.0882]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1369,  0.0037,  0.0275, -0.3715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 49 ] state=tensor([[-0.1369,  0.0037,  0.0275, -0.3715]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1368, -0.1918,  0.0200, -0.0703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 50 ] state=tensor([[-0.1368, -0.1918,  0.0200, -0.0703]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1407,  0.0031,  0.0186, -0.3566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 51 ] state=tensor([[-0.1407,  0.0031,  0.0186, -0.3566]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1406, -0.1923,  0.0115, -0.0581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 52 ] state=tensor([[-0.1406, -0.1923,  0.0115, -0.0581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1444,  0.0026,  0.0103, -0.3471]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 53 ] state=tensor([[-0.1444,  0.0026,  0.0103, -0.3471]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1444,  0.1976,  0.0034, -0.6365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 54 ] state=tensor([[-0.1444,  0.1976,  0.0034, -0.6365]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1404,  0.0024, -0.0093, -0.3428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 55 ] state=tensor([[-0.1404,  0.0024, -0.0093, -0.3428]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1404, -0.1926, -0.0162, -0.0531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 56 ] state=tensor([[-0.1404, -0.1926, -0.0162, -0.0531]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1442,  0.0028, -0.0173, -0.3508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 57 ] state=tensor([[-0.1442,  0.0028, -0.0173, -0.3508]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1442, -0.1921, -0.0243, -0.0636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 58 ] state=tensor([[-0.1442, -0.1921, -0.0243, -0.0636]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1480, -0.3868, -0.0256,  0.2213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 59 ] state=tensor([[-0.1480, -0.3868, -0.0256,  0.2213]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1558, -0.1914, -0.0211, -0.0793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 60 ] state=tensor([[-0.1558, -0.1914, -0.0211, -0.0793]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1596, -0.3862, -0.0227,  0.2066]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 61 ] state=tensor([[-0.1596, -0.3862, -0.0227,  0.2066]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1673, -0.5810, -0.0186,  0.4920]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 62 ] state=tensor([[-0.1673, -0.5810, -0.0186,  0.4920]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1789, -0.7758, -0.0087,  0.7788]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 63 ] state=tensor([[-0.1789, -0.7758, -0.0087,  0.7788]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1944, -0.5806,  0.0068,  0.4834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 64 ] state=tensor([[-0.1944, -0.5806,  0.0068,  0.4834]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2061, -0.3856,  0.0165,  0.1929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 65 ] state=tensor([[-0.2061, -0.3856,  0.0165,  0.1929]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2138, -0.1907,  0.0204, -0.0946]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 66 ] state=tensor([[-0.2138, -0.1907,  0.0204, -0.0946]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2176,  0.0042,  0.0185, -0.3808]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 67 ] state=tensor([[-0.2176,  0.0042,  0.0185, -0.3808]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2175, -0.1912,  0.0109, -0.0823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 68 ] state=tensor([[-0.2175, -0.1912,  0.0109, -0.0823]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2213, -0.3865,  0.0092,  0.2138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 69 ] state=tensor([[-0.2213, -0.3865,  0.0092,  0.2138]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2291, -0.1915,  0.0135, -0.0760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 70 ] state=tensor([[-0.2291, -0.1915,  0.0135, -0.0760]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2329, -0.3868,  0.0120,  0.2209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 71 ] state=tensor([[-0.2329, -0.3868,  0.0120,  0.2209]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2406, -0.1919,  0.0164, -0.0680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 72 ] state=tensor([[-0.2406, -0.1919,  0.0164, -0.0680]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2445,  0.0030,  0.0150, -0.3554]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 73 ] state=tensor([[-0.2445,  0.0030,  0.0150, -0.3554]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2444,  0.1979,  0.0079, -0.6433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 74 ] state=tensor([[-0.2444,  0.1979,  0.0079, -0.6433]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2404,  0.0027, -0.0050, -0.3482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 75 ] state=tensor([[-0.2404,  0.0027, -0.0050, -0.3482]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2404, -0.1924, -0.0119, -0.0571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 76 ] state=tensor([[-0.2404, -0.1924, -0.0119, -0.0571]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2442, -0.3873, -0.0131,  0.2318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 77 ] state=tensor([[-0.2442, -0.3873, -0.0131,  0.2318]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2520, -0.1920, -0.0084, -0.0649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 78 ] state=tensor([[-0.2520, -0.1920, -0.0084, -0.0649]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2558, -0.3870, -0.0097,  0.2251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 79 ] state=tensor([[-0.2558, -0.3870, -0.0097,  0.2251]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2636, -0.1918, -0.0052, -0.0707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 80 ] state=tensor([[-0.2636, -0.1918, -0.0052, -0.0707]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2674,  0.0034, -0.0066, -0.3650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 81 ] state=tensor([[-0.2674,  0.0034, -0.0066, -0.3650]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2673, -0.1916, -0.0139, -0.0744]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 82 ] state=tensor([[-0.2673, -0.1916, -0.0139, -0.0744]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2712,  0.0037, -0.0154, -0.3714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 83 ] state=tensor([[-0.2712,  0.0037, -0.0154, -0.3714]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2711, -0.1912, -0.0228, -0.0837]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 84 ] state=tensor([[-0.2711, -0.1912, -0.0228, -0.0837]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2749, -0.3860, -0.0245,  0.2017]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 85 ] state=tensor([[-0.2749, -0.3860, -0.0245,  0.2017]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2826, -0.5807, -0.0205,  0.4866]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 86 ] state=tensor([[-0.2826, -0.5807, -0.0205,  0.4866]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2942, -0.3853, -0.0108,  0.1875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 87 ] state=tensor([[-0.2942, -0.3853, -0.0108,  0.1875]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3019, -0.1900, -0.0070, -0.1086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 88 ] state=tensor([[-0.3019, -0.1900, -0.0070, -0.1086]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3057, -0.3851, -0.0092,  0.1819]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 89 ] state=tensor([[-0.3057, -0.3851, -0.0092,  0.1819]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3135, -0.1898, -0.0055, -0.1137]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 90 ] state=tensor([[-0.3135, -0.1898, -0.0055, -0.1137]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3172, -0.3849, -0.0078,  0.1773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 91 ] state=tensor([[-0.3172, -0.3849, -0.0078,  0.1773]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3249, -0.1896, -0.0043, -0.1179]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 92 ] state=tensor([[-0.3249, -0.1896, -0.0043, -0.1179]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3287, -0.3847, -0.0066,  0.1735]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 93 ] state=tensor([[-0.3287, -0.3847, -0.0066,  0.1735]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3364, -0.1895, -0.0032, -0.1213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 94 ] state=tensor([[-0.3364, -0.1895, -0.0032, -0.1213]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3402,  0.0057, -0.0056, -0.4150]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 95 ] state=tensor([[-0.3402,  0.0057, -0.0056, -0.4150]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3401, -0.1893, -0.0139, -0.1241]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 96 ] state=tensor([[-0.3401, -0.1893, -0.0139, -0.1241]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3439, -0.3843, -0.0164,  0.1642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 97 ] state=tensor([[-0.3439, -0.3843, -0.0164,  0.1642]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3516, -0.1889, -0.0131, -0.1336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 98 ] state=tensor([[-0.3516, -0.1889, -0.0131, -0.1336]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3554,  0.0064, -0.0157, -0.4304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 99 ] state=tensor([[-0.3554,  0.0064, -0.0157, -0.4304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3552, -0.1885, -0.0244, -0.1427]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 100 ] state=tensor([[-0.3552, -0.1885, -0.0244, -0.1427]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3590, -0.3833, -0.0272,  0.1422]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 101 ] state=tensor([[-0.3590, -0.3833, -0.0272,  0.1422]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3667, -0.1878, -0.0244, -0.1589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 102 ] state=tensor([[-0.3667, -0.1878, -0.0244, -0.1589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3704, -0.3825, -0.0275,  0.1260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 103 ] state=tensor([[-0.3704, -0.3825, -0.0275,  0.1260]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3781, -0.5772, -0.0250,  0.4098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 104 ] state=tensor([[-0.3781, -0.5772, -0.0250,  0.4098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3896, -0.7720, -0.0168,  0.6945]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 105 ] state=tensor([[-0.3896, -0.7720, -0.0168,  0.6945]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4051, -0.5766, -0.0029,  0.3966]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 106 ] state=tensor([[-0.4051, -0.5766, -0.0029,  0.3966]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4166, -0.7717,  0.0050,  0.6883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 107 ] state=tensor([[-0.4166, -0.7717,  0.0050,  0.6883]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4320, -0.5767,  0.0188,  0.3972]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 108 ] state=tensor([[-0.4320, -0.5767,  0.0188,  0.3972]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4436, -0.3818,  0.0267,  0.1105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 109 ] state=tensor([[-0.4436, -0.3818,  0.0267,  0.1105]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4512, -0.1871,  0.0289, -0.1736]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 110 ] state=tensor([[-0.4512, -0.1871,  0.0289, -0.1736]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4549,  0.0076,  0.0254, -0.4570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 111 ] state=tensor([[-0.4549,  0.0076,  0.0254, -0.4570]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4548, -0.1879,  0.0163, -0.1564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 112 ] state=tensor([[-0.4548, -0.1879,  0.0163, -0.1564]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4585, -0.3832,  0.0132,  0.1413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 113 ] state=tensor([[-0.4585, -0.3832,  0.0132,  0.1413]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4662, -0.1883,  0.0160, -0.1472]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 114 ] state=tensor([[-0.4662, -0.1883,  0.0160, -0.1472]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4700,  0.0066,  0.0131, -0.4347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 115 ] state=tensor([[-0.4700,  0.0066,  0.0131, -0.4347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4698, -0.1887,  0.0044, -0.1380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 116 ] state=tensor([[-0.4698, -0.1887,  0.0044, -0.1380]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4736, -0.3839,  0.0016,  0.1561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 117 ] state=tensor([[-0.4736, -0.3839,  0.0016,  0.1561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4813, -0.1888,  0.0047, -0.1361]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 118 ] state=tensor([[-0.4813, -0.1888,  0.0047, -0.1361]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4851, -0.3840,  0.0020,  0.1581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 119 ] state=tensor([[-0.4851, -0.3840,  0.0020,  0.1581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4927, -0.1889,  0.0052, -0.1340]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 126 ][ timestamp 120 ] state=tensor([[-0.4927, -0.1889,  0.0052, -0.1340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4965, -0.3841,  0.0025,  0.1603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 121 ] state=tensor([[-0.4965, -0.3841,  0.0025,  0.1603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5042, -0.5792,  0.0057,  0.4538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 122 ] state=tensor([[-0.5042, -0.5792,  0.0057,  0.4538]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5158, -0.3842,  0.0148,  0.1629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 123 ] state=tensor([[-0.5158, -0.3842,  0.0148,  0.1629]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5235, -0.1893,  0.0180, -0.1251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 124 ] state=tensor([[-0.5235, -0.1893,  0.0180, -0.1251]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5273, -0.3847,  0.0155,  0.1732]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 125 ] state=tensor([[-0.5273, -0.3847,  0.0155,  0.1732]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5349, -0.5800,  0.0190,  0.4708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 126 ] state=tensor([[-0.5349, -0.5800,  0.0190,  0.4708]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5465, -0.3852,  0.0284,  0.1841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 127 ] state=tensor([[-0.5465, -0.3852,  0.0284,  0.1841]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5543, -0.1905,  0.0321, -0.0994]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 128 ] state=tensor([[-0.5543, -0.1905,  0.0321, -0.0994]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5581,  0.0042,  0.0301, -0.3818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 129 ] state=tensor([[-0.5581,  0.0042,  0.0301, -0.3818]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5580, -0.1913,  0.0225, -0.0798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 130 ] state=tensor([[-0.5580, -0.1913,  0.0225, -0.0798]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5618,  0.0034,  0.0209, -0.3653]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 131 ] state=tensor([[-0.5618,  0.0034,  0.0209, -0.3653]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5617, -0.1920,  0.0136, -0.0661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 132 ] state=tensor([[-0.5617, -0.1920,  0.0136, -0.0661]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5656, -0.3873,  0.0122,  0.2308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 133 ] state=tensor([[-0.5656, -0.3873,  0.0122,  0.2308]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5733, -0.1923,  0.0169, -0.0580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 134 ] state=tensor([[-0.5733, -0.1923,  0.0169, -0.0580]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5772, -0.3877,  0.0157,  0.2399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 135 ] state=tensor([[-0.5772, -0.3877,  0.0157,  0.2399]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5849, -0.1928,  0.0205, -0.0478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 136 ] state=tensor([[-0.5849, -0.1928,  0.0205, -0.0478]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5888, -0.3882,  0.0195,  0.2513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 137 ] state=tensor([[-0.5888, -0.3882,  0.0195,  0.2513]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5965, -0.1934,  0.0246, -0.0351]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 138 ] state=tensor([[-0.5965, -0.1934,  0.0246, -0.0351]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6004,  0.0014,  0.0239, -0.3200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 139 ] state=tensor([[-0.6004,  0.0014,  0.0239, -0.3200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6004, -0.1941,  0.0175, -0.0199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 140 ] state=tensor([[-0.6004, -0.1941,  0.0175, -0.0199]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6043,  0.0008,  0.0171, -0.3070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 141 ] state=tensor([[-0.6043,  0.0008,  0.0171, -0.3070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6042,  0.1957,  0.0109, -0.5942]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 142 ] state=tensor([[-0.6042,  0.1957,  0.0109, -0.5942]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-6.0033e-01,  4.0857e-04, -9.6146e-04, -2.9813e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 143 ] state=tensor([[-6.0033e-01,  4.0857e-04, -9.6146e-04, -2.9813e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6003, -0.1947, -0.0069, -0.0057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 144 ] state=tensor([[-0.6003, -0.1947, -0.0069, -0.0057]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6042, -0.3897, -0.0070,  0.2847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 145 ] state=tensor([[-0.6042, -0.3897, -0.0070,  0.2847]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6120, -0.1945, -0.0013, -0.0102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 146 ] state=tensor([[-0.6120, -0.1945, -0.0013, -0.0102]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6159, -0.3896, -0.0015,  0.2821]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 147 ] state=tensor([[-0.6159, -0.3896, -0.0015,  0.2821]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6237, -0.1945,  0.0041, -0.0111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 148 ] state=tensor([[-0.6237, -0.1945,  0.0041, -0.0111]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6276, -0.3896,  0.0039,  0.2829]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 149 ] state=tensor([[-0.6276, -0.3896,  0.0039,  0.2829]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6354, -0.1946,  0.0095, -0.0086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 150 ] state=tensor([[-0.6354, -0.1946,  0.0095, -0.0086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-6.3927e-01,  4.1135e-04,  9.3608e-03, -2.9821e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 151 ] state=tensor([[-6.3927e-01,  4.1135e-04,  9.3608e-03, -2.9821e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6393, -0.1948,  0.0034, -0.0026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 152 ] state=tensor([[-0.6393, -0.1948,  0.0034, -0.0026]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-6.4316e-01,  2.3030e-04,  3.3447e-03, -2.9420e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 153 ] state=tensor([[-6.4316e-01,  2.3030e-04,  3.3447e-03, -2.9420e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-6.4315e-01, -1.9494e-01, -2.5393e-03, -4.6462e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 154 ] state=tensor([[-6.4315e-01, -1.9494e-01, -2.5393e-03, -4.6462e-04]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-6.4705e-01,  2.1910e-04, -2.5486e-03, -2.9395e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 155 ] state=tensor([[-6.4705e-01,  2.1910e-04, -2.5486e-03, -2.9395e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6470,  0.1954, -0.0084, -0.5874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 156 ] state=tensor([[-0.6470,  0.1954, -0.0084, -0.5874]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-6.4314e-01,  3.7437e-04, -2.0176e-02, -2.9742e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 157 ] state=tensor([[-6.4314e-01,  3.7437e-04, -2.0176e-02, -2.9742e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6431, -0.1945, -0.0261, -0.0112]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 158 ] state=tensor([[-0.6431, -0.1945, -0.0261, -0.0112]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6470, -0.3892, -0.0263,  0.2732]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 159 ] state=tensor([[-0.6470, -0.3892, -0.0263,  0.2732]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6548, -0.1937, -0.0209, -0.0277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 160 ] state=tensor([[-0.6548, -0.1937, -0.0209, -0.0277]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6587, -0.3885, -0.0214,  0.2583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 161 ] state=tensor([[-0.6587, -0.3885, -0.0214,  0.2583]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6664, -0.5833, -0.0163,  0.5442]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 162 ] state=tensor([[-0.6664, -0.5833, -0.0163,  0.5442]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6781, -0.3880, -0.0054,  0.2464]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 163 ] state=tensor([[-0.6781, -0.3880, -0.0054,  0.2464]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-6.8587e-01, -1.9278e-01, -4.6192e-04, -4.7991e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 164 ] state=tensor([[-6.8587e-01, -1.9278e-01, -4.6192e-04, -4.7991e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6897, -0.3879, -0.0014,  0.2445]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 165 ] state=tensor([[-0.6897, -0.3879, -0.0014,  0.2445]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6975, -0.1928,  0.0035, -0.0486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 166 ] state=tensor([[-0.6975, -0.1928,  0.0035, -0.0486]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7013, -0.3879,  0.0025,  0.2452]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 167 ] state=tensor([[-0.7013, -0.3879,  0.0025,  0.2452]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7091, -0.1928,  0.0074, -0.0467]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 168 ] state=tensor([[-0.7091, -0.1928,  0.0074, -0.0467]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7130,  0.0022,  0.0065, -0.3370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 169 ] state=tensor([[-0.7130,  0.0022,  0.0065, -0.3370]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.1292e-01, -1.9304e-01, -2.7360e-04, -4.2327e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 170 ] state=tensor([[-7.1292e-01, -1.9304e-01, -2.7360e-04, -4.2327e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7168, -0.3882, -0.0011,  0.2503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 171 ] state=tensor([[-0.7168, -0.3882, -0.0011,  0.2503]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7245, -0.1930,  0.0039, -0.0428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 172 ] state=tensor([[-0.7245, -0.1930,  0.0039, -0.0428]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7284, -0.3882,  0.0030,  0.2511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 173 ] state=tensor([[-0.7284, -0.3882,  0.0030,  0.2511]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7362, -0.5834,  0.0081,  0.5448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 174 ] state=tensor([[-0.7362, -0.5834,  0.0081,  0.5448]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7478, -0.3884,  0.0189,  0.2546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 175 ] state=tensor([[-0.7478, -0.3884,  0.0189,  0.2546]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7556, -0.1935,  0.0240, -0.0320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 176 ] state=tensor([[-0.7556, -0.1935,  0.0240, -0.0320]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7595,  0.0013,  0.0234, -0.3170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 177 ] state=tensor([[-0.7595,  0.0013,  0.0234, -0.3170]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7594,  0.1960,  0.0171, -0.6022]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 178 ] state=tensor([[-0.7594,  0.1960,  0.0171, -0.6022]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.5552e-01,  6.8387e-04,  5.0166e-03, -3.0421e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 179 ] state=tensor([[-7.5552e-01,  6.8387e-04,  5.0166e-03, -3.0421e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7555, -0.1945, -0.0011, -0.0099]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 126 ][ timestamp 180 ] state=tensor([[-0.7555, -0.1945, -0.0011, -0.0099]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7594, -0.3896, -0.0013,  0.2824]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 181 ] state=tensor([[-0.7594, -0.3896, -0.0013,  0.2824]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7672, -0.1945,  0.0044, -0.0107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 182 ] state=tensor([[-0.7672, -0.1945,  0.0044, -0.0107]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7711, -0.3897,  0.0042,  0.2834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 183 ] state=tensor([[-0.7711, -0.3897,  0.0042,  0.2834]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7789, -0.1946,  0.0098, -0.0080]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 184 ] state=tensor([[-0.7789, -0.1946,  0.0098, -0.0080]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.8277e-01,  3.8145e-04,  9.6753e-03, -2.9755e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 185 ] state=tensor([[-7.8277e-01,  3.8145e-04,  9.6753e-03, -2.9755e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7828, -0.1949,  0.0037, -0.0018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 186 ] state=tensor([[-0.7828, -0.1949,  0.0037, -0.0018]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.8666e-01,  1.9126e-04,  3.6875e-03, -2.9334e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 187 ] state=tensor([[-7.8666e-01,  1.9126e-04,  3.6875e-03, -2.9334e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.8665e-01, -1.9498e-01, -2.1793e-03,  5.0321e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 188 ] state=tensor([[-7.8665e-01, -1.9498e-01, -2.1793e-03,  5.0321e-04]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7906, -0.3901, -0.0022,  0.2925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 189 ] state=tensor([[-0.7906, -0.3901, -0.0022,  0.2925]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7984, -0.1949,  0.0037, -0.0009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 190 ] state=tensor([[-0.7984, -0.1949,  0.0037, -0.0009]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.0225e-01,  1.4809e-04,  3.6633e-03, -2.9239e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 191 ] state=tensor([[-8.0225e-01,  1.4809e-04,  3.6633e-03, -2.9239e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8022, -0.1950, -0.0022,  0.0014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 192 ] state=tensor([[-0.8022, -0.1950, -0.0022,  0.0014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.0615e-01,  1.2731e-04, -2.1555e-03, -2.9192e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 193 ] state=tensor([[-8.0615e-01,  1.2731e-04, -2.1555e-03, -2.9192e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-8.0615e-01, -1.9496e-01, -7.9939e-03,  7.9154e-05]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 194 ] state=tensor([[-8.0615e-01, -1.9496e-01, -7.9939e-03,  7.9154e-05]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8100, -0.3900, -0.0080,  0.2902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 195 ] state=tensor([[-0.8100, -0.3900, -0.0080,  0.2902]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8178, -0.1947, -0.0022, -0.0050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 196 ] state=tensor([[-0.8178, -0.1947, -0.0022, -0.0050]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.2174e-01,  4.1801e-04, -2.2870e-03, -2.9834e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 197 ] state=tensor([[-8.2174e-01,  4.1801e-04, -2.2870e-03, -2.9834e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8217, -0.1947, -0.0083, -0.0064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 198 ] state=tensor([[-0.8217, -0.1947, -0.0083, -0.0064]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.2563e-01,  5.6808e-04, -8.3813e-03, -3.0165e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 199 ] state=tensor([[-8.2563e-01,  5.6808e-04, -8.3813e-03, -3.0165e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8256, -0.1944, -0.0144, -0.0116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 200 ] state=tensor([[-0.8256, -0.1944, -0.0144, -0.0116]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8295,  0.0009, -0.0146, -0.3088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 201 ] state=tensor([[-0.8295,  0.0009, -0.0146, -0.3088]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8295, -0.1940, -0.0208, -0.0208]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 202 ] state=tensor([[-0.8295, -0.1940, -0.0208, -0.0208]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8334,  0.0014, -0.0212, -0.3200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 203 ] state=tensor([[-0.8334,  0.0014, -0.0212, -0.3200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8333, -0.1934, -0.0276, -0.0341]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 204 ] state=tensor([[-0.8333, -0.1934, -0.0276, -0.0341]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8372, -0.3881, -0.0283,  0.2498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 205 ] state=tensor([[-0.8372, -0.3881, -0.0283,  0.2498]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8450, -0.1926, -0.0233, -0.0517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 206 ] state=tensor([[-0.8450, -0.1926, -0.0233, -0.0517]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8488, -0.3874, -0.0244,  0.2335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 207 ] state=tensor([[-0.8488, -0.3874, -0.0244,  0.2335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8566, -0.1919, -0.0197, -0.0667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 208 ] state=tensor([[-0.8566, -0.1919, -0.0197, -0.0667]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8604, -0.3868, -0.0210,  0.2197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 209 ] state=tensor([[-0.8604, -0.3868, -0.0210,  0.2197]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8681, -0.5816, -0.0166,  0.5057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 210 ] state=tensor([[-0.8681, -0.5816, -0.0166,  0.5057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8798, -0.3862, -0.0065,  0.2078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 211 ] state=tensor([[-0.8798, -0.3862, -0.0065,  0.2078]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8875, -0.1910, -0.0024, -0.0870]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 212 ] state=tensor([[-0.8875, -0.1910, -0.0024, -0.0870]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8913, -0.3861, -0.0041,  0.2050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 213 ] state=tensor([[-0.8913, -0.3861, -0.0041,  0.2050]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.9904e-01, -1.9093e-01,  2.0639e-07, -8.8993e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 214 ] state=tensor([[-8.9904e-01, -1.9093e-01,  2.0639e-07, -8.8993e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9029, -0.3860, -0.0018,  0.2037]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 215 ] state=tensor([[-0.9029, -0.3860, -0.0018,  0.2037]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9106, -0.1909,  0.0023, -0.0896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 216 ] state=tensor([[-0.9106, -0.1909,  0.0023, -0.0896]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-9.1440e-01, -3.8606e-01,  5.0304e-04,  2.0385e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 217 ] state=tensor([[-9.1440e-01, -3.8606e-01,  5.0304e-04,  2.0385e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9221, -0.1909,  0.0046, -0.0887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 218 ] state=tensor([[-0.9221, -0.1909,  0.0046, -0.0887]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9259, -0.3861,  0.0028,  0.2055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 219 ] state=tensor([[-0.9259, -0.3861,  0.0028,  0.2055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9337, -0.1910,  0.0069, -0.0863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 220 ] state=tensor([[-0.9337, -0.1910,  0.0069, -0.0863]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9375,  0.0040,  0.0052, -0.3768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 221 ] state=tensor([[-0.9375,  0.0040,  0.0052, -0.3768]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9374, -0.1912, -0.0023, -0.0825]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 222 ] state=tensor([[-0.9374, -0.1912, -0.0023, -0.0825]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9412, -0.3863, -0.0040,  0.2094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 223 ] state=tensor([[-0.9412, -0.3863, -0.0040,  0.2094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-9.4895e-01, -1.9113e-01,  1.8992e-04, -8.4523e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 224 ] state=tensor([[-9.4895e-01, -1.9113e-01,  1.8992e-04, -8.4523e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9528, -0.3863, -0.0015,  0.2082]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 225 ] state=tensor([[-0.9528, -0.3863, -0.0015,  0.2082]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9605, -0.1911,  0.0027, -0.0849]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 226 ] state=tensor([[-0.9605, -0.1911,  0.0027, -0.0849]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9643, -0.3863,  0.0010,  0.2086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 227 ] state=tensor([[-0.9643, -0.3863,  0.0010,  0.2086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9720, -0.1912,  0.0051, -0.0838]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 228 ] state=tensor([[-0.9720, -0.1912,  0.0051, -0.0838]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9759, -0.3864,  0.0035,  0.2105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 229 ] state=tensor([[-0.9759, -0.3864,  0.0035,  0.2105]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9836, -0.1913,  0.0077, -0.0811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 230 ] state=tensor([[-0.9836, -0.1913,  0.0077, -0.0811]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9874, -0.3865,  0.0060,  0.2140]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 231 ] state=tensor([[-0.9874, -0.3865,  0.0060,  0.2140]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9952, -0.1915,  0.0103, -0.0768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 232 ] state=tensor([[-0.9952, -0.1915,  0.0103, -0.0768]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9990,  0.0035,  0.0088, -0.3662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 233 ] state=tensor([[-0.9990,  0.0035,  0.0088, -0.3662]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9989, -0.1918,  0.0015, -0.0707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 234 ] state=tensor([[-0.9989, -0.1918,  0.0015, -0.0707]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0027e+00,  3.3465e-03,  5.6822e-05, -3.6294e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 235 ] state=tensor([[-1.0027e+00,  3.3465e-03,  5.6822e-05, -3.6294e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0027, -0.1918, -0.0072, -0.0702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 236 ] state=tensor([[-1.0027, -0.1918, -0.0072, -0.0702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0065, -0.3868, -0.0086,  0.2202]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 237 ] state=tensor([[-1.0065, -0.3868, -0.0086,  0.2202]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0143, -0.1916, -0.0042, -0.0752]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 238 ] state=tensor([[-1.0143, -0.1916, -0.0042, -0.0752]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0181,  0.0036, -0.0057, -0.3692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 239 ] state=tensor([[-1.0181,  0.0036, -0.0057, -0.3692]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0180, -0.1914, -0.0131, -0.0784]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 240 ] state=tensor([[-1.0180, -0.1914, -0.0131, -0.0784]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0218, -0.3863, -0.0147,  0.2102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 241 ] state=tensor([[-1.0218, -0.3863, -0.0147,  0.2102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0296, -0.1910, -0.0105, -0.0871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 242 ] state=tensor([[-1.0296, -0.1910, -0.0105, -0.0871]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0334, -0.3860, -0.0122,  0.2023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 243 ] state=tensor([[-1.0334, -0.3860, -0.0122,  0.2023]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0411, -0.5809, -0.0082,  0.4911]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 244 ] state=tensor([[-1.0411, -0.5809, -0.0082,  0.4911]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0527, -0.3857,  0.0017,  0.1958]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 126 ][ timestamp 245 ] state=tensor([[-1.0527, -0.3857,  0.0017,  0.1958]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0604, -0.1906,  0.0056, -0.0963]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 246 ] state=tensor([[-1.0604, -0.1906,  0.0056, -0.0963]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0643,  0.0044,  0.0037, -0.3872]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 247 ] state=tensor([[-1.0643,  0.0044,  0.0037, -0.3872]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0642, -0.1907, -0.0041, -0.0934]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 248 ] state=tensor([[-1.0642, -0.1907, -0.0041, -0.0934]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0680,  0.0045, -0.0060, -0.3874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 249 ] state=tensor([[-1.0680,  0.0045, -0.0060, -0.3874]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0679, -0.1906, -0.0137, -0.0966]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 250 ] state=tensor([[-1.0679, -0.1906, -0.0137, -0.0966]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0717, -0.3855, -0.0156,  0.1918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 251 ] state=tensor([[-1.0717, -0.3855, -0.0156,  0.1918]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0794, -0.1902, -0.0118, -0.1058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 252 ] state=tensor([[-1.0794, -0.1902, -0.0118, -0.1058]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0832, -0.3851, -0.0139,  0.1831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 253 ] state=tensor([[-1.0832, -0.3851, -0.0139,  0.1831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0909, -0.1898, -0.0103, -0.1139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 254 ] state=tensor([[-1.0909, -0.1898, -0.0103, -0.1139]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0947,  0.0055, -0.0125, -0.4098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 255 ] state=tensor([[-1.0947,  0.0055, -0.0125, -0.4098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0946, -0.1895, -0.0207, -0.1211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 256 ] state=tensor([[-1.0946, -0.1895, -0.0207, -0.1211]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0984, -0.3843, -0.0231,  0.1650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 257 ] state=tensor([[-1.0984, -0.3843, -0.0231,  0.1650]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1061, -0.1888, -0.0198, -0.1349]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 258 ] state=tensor([[-1.1061, -0.1888, -0.0198, -0.1349]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1099, -0.3837, -0.0225,  0.1514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 259 ] state=tensor([[-1.1099, -0.3837, -0.0225,  0.1514]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1175, -0.1882, -0.0195, -0.1483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 260 ] state=tensor([[-1.1175, -0.1882, -0.0195, -0.1483]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1213,  0.0072, -0.0225, -0.4471]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 261 ] state=tensor([[-1.1213,  0.0072, -0.0225, -0.4471]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1211,  0.2026, -0.0314, -0.7467]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 262 ] state=tensor([[-1.1211,  0.2026, -0.0314, -0.7467]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1171,  0.0079, -0.0464, -0.4641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 263 ] state=tensor([[-1.1171,  0.0079, -0.0464, -0.4641]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1169, -0.1865, -0.0556, -0.1864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 264 ] state=tensor([[-1.1169, -0.1865, -0.0556, -0.1864]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1207,  0.0093, -0.0594, -0.4961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 265 ] state=tensor([[-1.1207,  0.0093, -0.0594, -0.4961]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1205, -0.1849, -0.0693, -0.2227]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 266 ] state=tensor([[-1.1205, -0.1849, -0.0693, -0.2227]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1242, -0.3790, -0.0737,  0.0473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 267 ] state=tensor([[-1.1242, -0.3790, -0.0737,  0.0473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1318, -0.5729, -0.0728,  0.3159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 268 ] state=tensor([[-1.1318, -0.5729, -0.0728,  0.3159]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1432, -0.7670, -0.0665,  0.5847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 269 ] state=tensor([[-1.1432, -0.7670, -0.0665,  0.5847]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1586, -0.5710, -0.0548,  0.2719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 270 ] state=tensor([[-1.1586, -0.5710, -0.0548,  0.2719]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1700, -0.3751, -0.0493, -0.0376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 271 ] state=tensor([[-1.1700, -0.3751, -0.0493, -0.0376]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1775, -0.5695, -0.0501,  0.2391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 272 ] state=tensor([[-1.1775, -0.5695, -0.0501,  0.2391]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1889, -0.3737, -0.0453, -0.0689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 273 ] state=tensor([[-1.1889, -0.3737, -0.0453, -0.0689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1963, -0.5681, -0.0467,  0.2091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 274 ] state=tensor([[-1.1963, -0.5681, -0.0467,  0.2091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2077, -0.7626, -0.0425,  0.4867]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 275 ] state=tensor([[-1.2077, -0.7626, -0.0425,  0.4867]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2230, -0.5669, -0.0328,  0.1810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 276 ] state=tensor([[-1.2230, -0.5669, -0.0328,  0.1810]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2343, -0.7615, -0.0292,  0.4631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 277 ] state=tensor([[-1.2343, -0.7615, -0.0292,  0.4631]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2495, -0.5660, -0.0199,  0.1614]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 278 ] state=tensor([[-1.2495, -0.5660, -0.0199,  0.1614]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2608, -0.3706, -0.0167, -0.1375]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 279 ] state=tensor([[-1.2608, -0.3706, -0.0167, -0.1375]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2683, -0.1752, -0.0194, -0.4354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 280 ] state=tensor([[-1.2683, -0.1752, -0.0194, -0.4354]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2718, -0.3701, -0.0281, -0.1489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 281 ] state=tensor([[-1.2718, -0.3701, -0.0281, -0.1489]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2792, -0.5648, -0.0311,  0.1348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 282 ] state=tensor([[-1.2792, -0.5648, -0.0311,  0.1348]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2905, -0.7594, -0.0284,  0.4175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 283 ] state=tensor([[-1.2905, -0.7594, -0.0284,  0.4175]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3056, -0.5639, -0.0201,  0.1160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 284 ] state=tensor([[-1.3056, -0.5639, -0.0201,  0.1160]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3169, -0.3685, -0.0177, -0.1829]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 285 ] state=tensor([[-1.3169, -0.3685, -0.0177, -0.1829]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3243, -0.5634, -0.0214,  0.1041]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 286 ] state=tensor([[-1.3243, -0.5634, -0.0214,  0.1041]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3356, -0.7582, -0.0193,  0.3900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 287 ] state=tensor([[-1.3356, -0.7582, -0.0193,  0.3900]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3507, -0.5628, -0.0115,  0.0912]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 288 ] state=tensor([[-1.3507, -0.5628, -0.0115,  0.0912]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3620, -0.7578, -0.0097,  0.3803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 289 ] state=tensor([[-1.3620, -0.7578, -0.0097,  0.3803]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3771, -0.5625, -0.0021,  0.0845]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 290 ] state=tensor([[-1.3771, -0.5625, -0.0021,  0.0845]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3884e+00, -3.6735e-01, -3.9384e-04, -2.0879e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 291 ] state=tensor([[-1.3884e+00, -3.6735e-01, -3.9384e-04, -2.0879e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3957, -0.1722, -0.0046, -0.5016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 292 ] state=tensor([[-1.3957, -0.1722, -0.0046, -0.5016]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3992, -0.3673, -0.0146, -0.2104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 293 ] state=tensor([[-1.3992, -0.3673, -0.0146, -0.2104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4065, -0.1719, -0.0188, -0.5076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 294 ] state=tensor([[-1.4065, -0.1719, -0.0188, -0.5076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4100, -0.3668, -0.0290, -0.2209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 295 ] state=tensor([[-1.4100, -0.3668, -0.0290, -0.2209]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4173, -0.1713, -0.0334, -0.5226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 296 ] state=tensor([[-1.4173, -0.1713, -0.0334, -0.5226]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4207, -0.3659, -0.0438, -0.2406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 297 ] state=tensor([[-1.4207, -0.3659, -0.0438, -0.2406]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4280, -0.5604, -0.0486,  0.0379]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 298 ] state=tensor([[-1.4280, -0.5604, -0.0486,  0.0379]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4393, -0.7548, -0.0479,  0.3149]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 299 ] state=tensor([[-1.4393, -0.7548, -0.0479,  0.3149]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4543, -0.5590, -0.0416,  0.0075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 300 ] state=tensor([[-1.4543, -0.5590, -0.0416,  0.0075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4655, -0.7535, -0.0414,  0.2868]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 301 ] state=tensor([[-1.4655, -0.7535, -0.0414,  0.2868]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4806, -0.5578, -0.0357, -0.0187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 302 ] state=tensor([[-1.4806, -0.5578, -0.0357, -0.0187]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4918, -0.3622, -0.0361, -0.3224]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 303 ] state=tensor([[-1.4918, -0.3622, -0.0361, -0.3224]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4990, -0.5568, -0.0425, -0.0413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 304 ] state=tensor([[-1.4990, -0.5568, -0.0425, -0.0413]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5101, -0.7513, -0.0434,  0.2376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 305 ] state=tensor([[-1.5101, -0.7513, -0.0434,  0.2376]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5252, -0.5556, -0.0386, -0.0684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 306 ] state=tensor([[-1.5252, -0.5556, -0.0386, -0.0684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5363, -0.7501, -0.0400,  0.2119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 307 ] state=tensor([[-1.5363, -0.7501, -0.0400,  0.2119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5513, -0.5544, -0.0357, -0.0932]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 308 ] state=tensor([[-1.5513, -0.5544, -0.0357, -0.0932]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5624, -0.3588, -0.0376, -0.3969]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 309 ] state=tensor([[-1.5624, -0.3588, -0.0376, -0.3969]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5695, -0.5534, -0.0455, -0.1163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 310 ] state=tensor([[-1.5695, -0.5534, -0.0455, -0.1163]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5806, -0.7478, -0.0479,  0.1617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 311 ] state=tensor([[-1.5806, -0.7478, -0.0479,  0.1617]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5956, -0.9422, -0.0446,  0.4389]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 312 ] state=tensor([[-1.5956, -0.9422, -0.0446,  0.4389]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6144, -0.7465, -0.0358,  0.1325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 313 ] state=tensor([[-1.6144, -0.7465, -0.0358,  0.1325]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6293, -0.9411, -0.0332,  0.4136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 314 ] state=tensor([[-1.6293, -0.9411, -0.0332,  0.4136]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6482, -1.1357, -0.0249,  0.6957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 315 ] state=tensor([[-1.6482, -1.1357, -0.0249,  0.6957]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6709, -0.9403, -0.0110,  0.3952]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 126 ][ timestamp 316 ] state=tensor([[-1.6709, -0.9403, -0.0110,  0.3952]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6897, -0.7450, -0.0031,  0.0991]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 317 ] state=tensor([[-1.6897, -0.7450, -0.0031,  0.0991]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7046e+00, -5.4985e-01, -1.1241e-03, -1.9455e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 318 ] state=tensor([[-1.7046e+00, -5.4985e-01, -1.1241e-03, -1.9455e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7156, -0.3547, -0.0050, -0.4876]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 319 ] state=tensor([[-1.7156, -0.3547, -0.0050, -0.4876]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7227, -0.5498, -0.0148, -0.1965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 320 ] state=tensor([[-1.7227, -0.5498, -0.0148, -0.1965]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7337, -0.3544, -0.0187, -0.4938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 321 ] state=tensor([[-1.7337, -0.3544, -0.0187, -0.4938]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7408, -0.1590, -0.0286, -0.7923]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 322 ] state=tensor([[-1.7408, -0.1590, -0.0286, -0.7923]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7439, -0.3538, -0.0444, -0.5088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 323 ] state=tensor([[-1.7439, -0.3538, -0.0444, -0.5088]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7510, -0.5482, -0.0546, -0.2304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 324 ] state=tensor([[-1.7510, -0.5482, -0.0546, -0.2304]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7620, -0.3524, -0.0592, -0.5398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 325 ] state=tensor([[-1.7620, -0.3524, -0.0592, -0.5398]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7690, -0.5466, -0.0700, -0.2663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 326 ] state=tensor([[-1.7690, -0.5466, -0.0700, -0.2663]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7800, -0.7407, -0.0753,  0.0035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 327 ] state=tensor([[-1.7800, -0.7407, -0.0753,  0.0035]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7948, -0.9346, -0.0753,  0.2715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 328 ] state=tensor([[-1.7948, -0.9346, -0.0753,  0.2715]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8135, -1.1286, -0.0698,  0.5395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 329 ] state=tensor([[-1.8135, -1.1286, -0.0698,  0.5395]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8360, -0.9326, -0.0590,  0.2257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 330 ] state=tensor([[-1.8360, -0.9326, -0.0590,  0.2257]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8547, -0.7367, -0.0545, -0.0850]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 331 ] state=tensor([[-1.8547, -0.7367, -0.0545, -0.0850]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8694, -0.9310, -0.0562,  0.1900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 332 ] state=tensor([[-1.8694, -0.9310, -0.0562,  0.1900]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8880, -0.7351, -0.0524, -0.1199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 333 ] state=tensor([[-1.8880, -0.7351, -0.0524, -0.1199]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9027, -0.5393, -0.0548, -0.4287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 334 ] state=tensor([[-1.9027, -0.5393, -0.0548, -0.4287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9135, -0.3434, -0.0634, -0.7381]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 335 ] state=tensor([[-1.9135, -0.3434, -0.0634, -0.7381]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9204, -0.1475, -0.0782, -1.0500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 336 ] state=tensor([[-1.9204, -0.1475, -0.0782, -1.0500]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9233, -0.3415, -0.0992, -0.7829]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 337 ] state=tensor([[-1.9233, -0.3415, -0.0992, -0.7829]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9302, -0.5351, -0.1148, -0.5230]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 338 ] state=tensor([[-1.9302, -0.5351, -0.1148, -0.5230]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9409, -0.7284, -0.1253, -0.2686]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 339 ] state=tensor([[-1.9409, -0.7284, -0.1253, -0.2686]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9554, -0.9216, -0.1306, -0.0179]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 340 ] state=tensor([[-1.9554, -0.9216, -0.1306, -0.0179]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9739, -1.1146, -0.1310,  0.2309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 341 ] state=tensor([[-1.9739, -1.1146, -0.1310,  0.2309]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9962, -1.3076, -0.1264,  0.4796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 342 ] state=tensor([[-1.9962, -1.3076, -0.1264,  0.4796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0223, -1.5008, -0.1168,  0.7299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 343 ] state=tensor([[-2.0223, -1.5008, -0.1168,  0.7299]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0523, -1.3042, -0.1022,  0.4029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 344 ] state=tensor([[-2.0523, -1.3042, -0.1022,  0.4029]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0784, -1.1078, -0.0941,  0.0798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 345 ] state=tensor([[-2.0784, -1.1078, -0.0941,  0.0798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1006, -1.3015, -0.0925,  0.3413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 346 ] state=tensor([[-2.1006, -1.3015, -0.0925,  0.3413]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1266, -1.4952, -0.0857,  0.6035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 347 ] state=tensor([[-2.1266, -1.4952, -0.0857,  0.6035]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1565, -1.2990, -0.0736,  0.2851]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 348 ] state=tensor([[-2.1565, -1.2990, -0.0736,  0.2851]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1825, -1.1029, -0.0679, -0.0299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 349 ] state=tensor([[-2.1825, -1.1029, -0.0679, -0.0299]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2045, -1.2970, -0.0685,  0.2406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 350 ] state=tensor([[-2.2045, -1.2970, -0.0685,  0.2406]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2305, -1.4910, -0.0637,  0.5109]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 351 ] state=tensor([[-2.2305, -1.4910, -0.0637,  0.5109]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2603, -1.2951, -0.0535,  0.1988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 352 ] state=tensor([[-2.2603, -1.2951, -0.0535,  0.1988]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2862, -1.0992, -0.0495, -0.1102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 353 ] state=tensor([[-2.2862, -1.0992, -0.0495, -0.1102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3082, -0.9034, -0.0517, -0.4181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 354 ] state=tensor([[-2.3082, -0.9034, -0.0517, -0.4181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3263, -0.7076, -0.0601, -0.7267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 355 ] state=tensor([[-2.3263, -0.7076, -0.0601, -0.7267]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3404, -0.5117, -0.0746, -1.0376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 356 ] state=tensor([[-2.3404, -0.5117, -0.0746, -1.0376]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3506, -0.7058, -0.0954, -0.7693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 357 ] state=tensor([[-2.3506, -0.7058, -0.0954, -0.7693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3648, -0.8995, -0.1108, -0.5081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 358 ] state=tensor([[-2.3648, -0.8995, -0.1108, -0.5081]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3828, -0.7030, -0.1209, -0.8335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 359 ] state=tensor([[-2.3828, -0.7030, -0.1209, -0.8335]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3968, -0.8962, -0.1376, -0.5812]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 126 ][ timestamp 360 ] state=tensor([[-2.3968, -0.8962, -0.1376, -0.5812]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 126: Exploration_rate=0.05. Score=360.\n",
      "[ episode 127 ] state=tensor([[ 0.0127,  0.0442,  0.0250, -0.0431]])\n",
      "[ episode 127 ][ timestamp 1 ] state=tensor([[ 0.0127,  0.0442,  0.0250, -0.0431]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0136,  0.2389,  0.0241, -0.3278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 2 ] state=tensor([[ 0.0136,  0.2389,  0.0241, -0.3278]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0184,  0.4337,  0.0175, -0.6127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 3 ] state=tensor([[ 0.0184,  0.4337,  0.0175, -0.6127]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0270,  0.6286,  0.0053, -0.8999]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 4 ] state=tensor([[ 0.0270,  0.6286,  0.0053, -0.8999]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0396,  0.8236, -0.0127, -1.1909]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 5 ] state=tensor([[ 0.0396,  0.8236, -0.0127, -1.1909]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0561,  0.6287, -0.0365, -0.9022]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 6 ] state=tensor([[ 0.0561,  0.6287, -0.0365, -0.9022]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0686,  0.4341, -0.0546, -0.6212]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 7 ] state=tensor([[ 0.0686,  0.4341, -0.0546, -0.6212]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0773,  0.2397, -0.0670, -0.3462]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 8 ] state=tensor([[ 0.0773,  0.2397, -0.0670, -0.3462]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0821,  0.0456, -0.0739, -0.0754]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 9 ] state=tensor([[ 0.0821,  0.0456, -0.0739, -0.0754]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0830, -0.1484, -0.0754,  0.1931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 10 ] state=tensor([[ 0.0830, -0.1484, -0.0754,  0.1931]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0801, -0.3423, -0.0716,  0.4611]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 11 ] state=tensor([[ 0.0801, -0.3423, -0.0716,  0.4611]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0732, -0.1463, -0.0623,  0.1467]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 12 ] state=tensor([[ 0.0732, -0.1463, -0.0623,  0.1467]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0703, -0.3404, -0.0594,  0.4191]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 13 ] state=tensor([[ 0.0703, -0.3404, -0.0594,  0.4191]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0635, -0.1445, -0.0510,  0.1083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 14 ] state=tensor([[ 0.0635, -0.1445, -0.0510,  0.1083]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0606, -0.3389, -0.0489,  0.3844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 15 ] state=tensor([[ 0.0606, -0.3389, -0.0489,  0.3844]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0538, -0.5333, -0.0412,  0.6613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 16 ] state=tensor([[ 0.0538, -0.5333, -0.0412,  0.6613]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0432, -0.3376, -0.0279,  0.3560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 17 ] state=tensor([[ 0.0432, -0.3376, -0.0279,  0.3560]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0364, -0.5323, -0.0208,  0.6397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 18 ] state=tensor([[ 0.0364, -0.5323, -0.0208,  0.6397]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0258, -0.3369, -0.0080,  0.3405]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 19 ] state=tensor([[ 0.0258, -0.3369, -0.0080,  0.3405]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0190, -0.1417, -0.0012,  0.0453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 20 ] state=tensor([[ 0.0190, -0.1417, -0.0012,  0.0453]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0162,  0.0534, -0.0003, -0.2477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 21 ] state=tensor([[ 0.0162,  0.0534, -0.0003, -0.2477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0172,  0.2486, -0.0053, -0.5405]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 22 ] state=tensor([[ 0.0172,  0.2486, -0.0053, -0.5405]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0222,  0.4438, -0.0161, -0.8349]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 23 ] state=tensor([[ 0.0222,  0.4438, -0.0161, -0.8349]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0311,  0.2489, -0.0328, -0.5473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 24 ] state=tensor([[ 0.0311,  0.2489, -0.0328, -0.5473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0361,  0.0542, -0.0437, -0.2651]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 25 ] state=tensor([[ 0.0361,  0.0542, -0.0437, -0.2651]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0372,  0.2499, -0.0490, -0.5713]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 26 ] state=tensor([[ 0.0372,  0.2499, -0.0490, -0.5713]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0422,  0.0555, -0.0605, -0.2944]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 127 ][ timestamp 27 ] state=tensor([[ 0.0422,  0.0555, -0.0605, -0.2944]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0433, -0.1387, -0.0663, -0.0214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 28 ] state=tensor([[ 0.0433, -0.1387, -0.0663, -0.0214]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0405, -0.3328, -0.0668,  0.2496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 29 ] state=tensor([[ 0.0405, -0.3328, -0.0668,  0.2496]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0338, -0.5269, -0.0618,  0.5205]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 30 ] state=tensor([[ 0.0338, -0.5269, -0.0618,  0.5205]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0233, -0.7211, -0.0514,  0.7931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 31 ] state=tensor([[ 0.0233, -0.7211, -0.0514,  0.7931]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0089, -0.9155, -0.0355,  1.0692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 32 ] state=tensor([[ 0.0089, -0.9155, -0.0355,  1.0692]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0094, -0.7199, -0.0141,  0.7656]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 33 ] state=tensor([[-0.0094, -0.7199, -0.0141,  0.7656]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0238, -0.5246,  0.0012,  0.4685]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 34 ] state=tensor([[-0.0238, -0.5246,  0.0012,  0.4685]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0343, -0.3295,  0.0106,  0.1762]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 35 ] state=tensor([[-0.0343, -0.3295,  0.0106,  0.1762]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0409, -0.1345,  0.0141, -0.1131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 36 ] state=tensor([[-0.0409, -0.1345,  0.0141, -0.1131]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0436,  0.0604,  0.0118, -0.4013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 37 ] state=tensor([[-0.0436,  0.0604,  0.0118, -0.4013]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0424, -0.1349,  0.0038, -0.1049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 38 ] state=tensor([[-0.0424, -0.1349,  0.0038, -0.1049]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0451, -0.3301,  0.0017,  0.1889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 39 ] state=tensor([[-0.0451, -0.3301,  0.0017,  0.1889]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0517, -0.5252,  0.0055,  0.4822]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 40 ] state=tensor([[-0.0517, -0.5252,  0.0055,  0.4822]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0622, -0.7204,  0.0151,  0.7766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 41 ] state=tensor([[-0.0622, -0.7204,  0.0151,  0.7766]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0766, -0.5255,  0.0307,  0.4887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 42 ] state=tensor([[-0.0766, -0.5255,  0.0307,  0.4887]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0871, -0.3308,  0.0404,  0.2058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 43 ] state=tensor([[-0.0871, -0.3308,  0.0404,  0.2058]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0937, -0.1363,  0.0445, -0.0739]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 44 ] state=tensor([[-0.0937, -0.1363,  0.0445, -0.0739]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0965,  0.0582,  0.0431, -0.3522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 45 ] state=tensor([[-0.0965,  0.0582,  0.0431, -0.3522]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0953,  0.2527,  0.0360, -0.6310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 46 ] state=tensor([[-0.0953,  0.2527,  0.0360, -0.6310]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0902,  0.0570,  0.0234, -0.3271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 47 ] state=tensor([[-0.0902,  0.0570,  0.0234, -0.3271]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0891,  0.2518,  0.0169, -0.6124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 48 ] state=tensor([[-0.0891,  0.2518,  0.0169, -0.6124]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0841,  0.0565,  0.0046, -0.3144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 49 ] state=tensor([[-0.0841,  0.0565,  0.0046, -0.3144]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0829, -0.1387, -0.0017, -0.0203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 50 ] state=tensor([[-0.0829, -0.1387, -0.0017, -0.0203]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0857, -0.3338, -0.0021,  0.2719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 51 ] state=tensor([[-0.0857, -0.3338, -0.0021,  0.2719]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0924, -0.1387,  0.0034, -0.0215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 52 ] state=tensor([[-0.0924, -0.1387,  0.0034, -0.0215]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0952,  0.0564,  0.0029, -0.3131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 53 ] state=tensor([[-0.0952,  0.0564,  0.0029, -0.3131]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0940, -0.1388, -0.0033, -0.0195]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 54 ] state=tensor([[-0.0940, -0.1388, -0.0033, -0.0195]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0968,  0.0564, -0.0037, -0.3132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 55 ] state=tensor([[-0.0968,  0.0564, -0.0037, -0.3132]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0957,  0.2516, -0.0100, -0.6071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 56 ] state=tensor([[-0.0957,  0.2516, -0.0100, -0.6071]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0906,  0.4469, -0.0221, -0.9029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 57 ] state=tensor([[-0.0906,  0.4469, -0.0221, -0.9029]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0817,  0.2520, -0.0402, -0.6172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 58 ] state=tensor([[-0.0817,  0.2520, -0.0402, -0.6172]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0767,  0.0575, -0.0525, -0.3375]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 59 ] state=tensor([[-0.0767,  0.0575, -0.0525, -0.3375]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0755, -0.1368, -0.0593, -0.0618]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 60 ] state=tensor([[-0.0755, -0.1368, -0.0593, -0.0618]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0782, -0.3311, -0.0605,  0.2116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 61 ] state=tensor([[-0.0782, -0.3311, -0.0605,  0.2116]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0849, -0.5253, -0.0563,  0.4846]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 62 ] state=tensor([[-0.0849, -0.5253, -0.0563,  0.4846]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0954, -0.7196, -0.0466,  0.7590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 63 ] state=tensor([[-0.0954, -0.7196, -0.0466,  0.7590]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1098, -0.5238, -0.0314,  0.4520]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 64 ] state=tensor([[-0.1098, -0.5238, -0.0314,  0.4520]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1202, -0.3283, -0.0224,  0.1496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 65 ] state=tensor([[-0.1202, -0.3283, -0.0224,  0.1496]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1268, -0.5231, -0.0194,  0.4352]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 66 ] state=tensor([[-0.1268, -0.5231, -0.0194,  0.4352]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1373, -0.3277, -0.0107,  0.1364]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 67 ] state=tensor([[-0.1373, -0.3277, -0.0107,  0.1364]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1438, -0.1324, -0.0080, -0.1596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 68 ] state=tensor([[-0.1438, -0.1324, -0.0080, -0.1596]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1465, -0.3274, -0.0111,  0.1306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 69 ] state=tensor([[-0.1465, -0.3274, -0.0111,  0.1306]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1530, -0.5224, -0.0085,  0.4197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 70 ] state=tensor([[-0.1530, -0.5224, -0.0085,  0.4197]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6347e-01, -3.2712e-01, -1.3824e-04,  1.2434e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 71 ] state=tensor([[-1.6347e-01, -3.2712e-01, -1.3824e-04,  1.2434e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1700, -0.1320,  0.0023, -0.1684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 72 ] state=tensor([[-0.1700, -0.1320,  0.0023, -0.1684]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1726,  0.0631, -0.0010, -0.4603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 73 ] state=tensor([[-0.1726,  0.0631, -0.0010, -0.4603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1714, -0.1320, -0.0102, -0.1680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 74 ] state=tensor([[-0.1714, -0.1320, -0.0102, -0.1680]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1740,  0.0632, -0.0136, -0.4639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 75 ] state=tensor([[-0.1740,  0.0632, -0.0136, -0.4639]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1728, -0.1317, -0.0229, -0.1755]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 76 ] state=tensor([[-0.1728, -0.1317, -0.0229, -0.1755]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1754, -0.3265, -0.0264,  0.1099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 77 ] state=tensor([[-0.1754, -0.3265, -0.0264,  0.1099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1819, -0.1310, -0.0242, -0.1910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 78 ] state=tensor([[-0.1819, -0.1310, -0.0242, -0.1910]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1845,  0.0645, -0.0280, -0.4912]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 79 ] state=tensor([[-0.1845,  0.0645, -0.0280, -0.4912]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1833, -0.1302, -0.0378, -0.2075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 80 ] state=tensor([[-0.1833, -0.1302, -0.0378, -0.2075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1859, -0.3248, -0.0420,  0.0731]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 81 ] state=tensor([[-0.1859, -0.3248, -0.0420,  0.0731]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1924, -0.5193, -0.0405,  0.3522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 82 ] state=tensor([[-0.1924, -0.5193, -0.0405,  0.3522]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2027, -0.7138, -0.0335,  0.6318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 83 ] state=tensor([[-0.2027, -0.7138, -0.0335,  0.6318]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2170, -0.9085, -0.0208,  0.9138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 84 ] state=tensor([[-0.2170, -0.9085, -0.0208,  0.9138]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2352, -0.7131, -0.0025,  0.6147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 85 ] state=tensor([[-0.2352, -0.7131, -0.0025,  0.6147]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2494, -0.5179,  0.0097,  0.3212]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 86 ] state=tensor([[-0.2494, -0.5179,  0.0097,  0.3212]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2598, -0.3229,  0.0162,  0.0316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 87 ] state=tensor([[-0.2598, -0.3229,  0.0162,  0.0316]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2663, -0.1280,  0.0168, -0.2560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 88 ] state=tensor([[-0.2663, -0.1280,  0.0168, -0.2560]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2688,  0.0668,  0.0117, -0.5433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 89 ] state=tensor([[-0.2688,  0.0668,  0.0117, -0.5433]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2675, -0.1284,  0.0008, -0.2470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 90 ] state=tensor([[-0.2675, -0.1284,  0.0008, -0.2470]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2701,  0.0667, -0.0041, -0.5394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 91 ] state=tensor([[-0.2701,  0.0667, -0.0041, -0.5394]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2687, -0.1284, -0.0149, -0.2480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 92 ] state=tensor([[-0.2687, -0.1284, -0.0149, -0.2480]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2713,  0.0669, -0.0199, -0.5454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 93 ] state=tensor([[-0.2713,  0.0669, -0.0199, -0.5454]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2700, -0.1279, -0.0308, -0.2590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 94 ] state=tensor([[-0.2700, -0.1279, -0.0308, -0.2590]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2725, -0.3226, -0.0360,  0.0238]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 127 ][ timestamp 95 ] state=tensor([[-0.2725, -0.3226, -0.0360,  0.0238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2790, -0.5172, -0.0355,  0.3049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 96 ] state=tensor([[-0.2790, -0.5172, -0.0355,  0.3049]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2893, -0.3215, -0.0294,  0.0013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 97 ] state=tensor([[-0.2893, -0.3215, -0.0294,  0.0013]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2957, -0.5162, -0.0294,  0.2846]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 98 ] state=tensor([[-0.2957, -0.5162, -0.0294,  0.2846]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3061, -0.3207, -0.0237, -0.0172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 99 ] state=tensor([[-0.3061, -0.3207, -0.0237, -0.0172]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3125, -0.5155, -0.0240,  0.2679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 100 ] state=tensor([[-0.3125, -0.5155, -0.0240,  0.2679]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3228, -0.7103, -0.0187,  0.5529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 101 ] state=tensor([[-0.3228, -0.7103, -0.0187,  0.5529]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3370, -0.5149, -0.0076,  0.2544]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 102 ] state=tensor([[-0.3370, -0.5149, -0.0076,  0.2544]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3473, -0.7099, -0.0025,  0.5447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 103 ] state=tensor([[-0.3473, -0.7099, -0.0025,  0.5447]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3615, -0.5147,  0.0084,  0.2512]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 104 ] state=tensor([[-0.3615, -0.5147,  0.0084,  0.2512]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3718, -0.3197,  0.0134, -0.0388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 105 ] state=tensor([[-0.3718, -0.3197,  0.0134, -0.0388]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3782, -0.5150,  0.0126,  0.2581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 106 ] state=tensor([[-0.3782, -0.5150,  0.0126,  0.2581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3885, -0.3201,  0.0178, -0.0306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 107 ] state=tensor([[-0.3885, -0.3201,  0.0178, -0.0306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3949, -0.1252,  0.0172, -0.3176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 108 ] state=tensor([[-0.3949, -0.1252,  0.0172, -0.3176]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3974, -0.3206,  0.0108, -0.0196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 109 ] state=tensor([[-0.3974, -0.3206,  0.0108, -0.0196]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4038, -0.1256,  0.0104, -0.3088]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 110 ] state=tensor([[-0.4038, -0.1256,  0.0104, -0.3088]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4063, -0.3209,  0.0043, -0.0129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 111 ] state=tensor([[-0.4063, -0.3209,  0.0043, -0.0129]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4127, -0.5161,  0.0040,  0.2812]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 112 ] state=tensor([[-0.4127, -0.5161,  0.0040,  0.2812]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4230, -0.3210,  0.0096, -0.0103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 113 ] state=tensor([[-0.4230, -0.3210,  0.0096, -0.0103]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4295, -0.5163,  0.0094,  0.2854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 114 ] state=tensor([[-0.4295, -0.5163,  0.0094,  0.2854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4398, -0.3213,  0.0151, -0.0042]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 115 ] state=tensor([[-0.4398, -0.3213,  0.0151, -0.0042]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4462, -0.1264,  0.0150, -0.2921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 116 ] state=tensor([[-0.4462, -0.1264,  0.0150, -0.2921]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4487, -0.3217,  0.0092,  0.0053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 117 ] state=tensor([[-0.4487, -0.3217,  0.0092,  0.0053]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4552, -0.5170,  0.0093,  0.3008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 118 ] state=tensor([[-0.4552, -0.5170,  0.0093,  0.3008]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4655, -0.3220,  0.0153,  0.0111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 119 ] state=tensor([[-0.4655, -0.3220,  0.0153,  0.0111]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4720, -0.5173,  0.0155,  0.3086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 120 ] state=tensor([[-0.4720, -0.5173,  0.0155,  0.3086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4823, -0.3224,  0.0217,  0.0209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 121 ] state=tensor([[-0.4823, -0.3224,  0.0217,  0.0209]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4888, -0.5179,  0.0221,  0.3203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 122 ] state=tensor([[-0.4888, -0.5179,  0.0221,  0.3203]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4991, -0.3231,  0.0285,  0.0347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 123 ] state=tensor([[-0.4991, -0.3231,  0.0285,  0.0347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5056, -0.1284,  0.0292, -0.2489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 124 ] state=tensor([[-0.5056, -0.1284,  0.0292, -0.2489]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5081,  0.0663,  0.0243, -0.5322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 125 ] state=tensor([[-0.5081,  0.0663,  0.0243, -0.5322]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5068,  0.2611,  0.0136, -0.8171]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 126 ] state=tensor([[-0.5068,  0.2611,  0.0136, -0.8171]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5016,  0.0658, -0.0027, -0.5202]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 127 ] state=tensor([[-0.5016,  0.0658, -0.0027, -0.5202]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5003, -0.1293, -0.0131, -0.2284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 128 ] state=tensor([[-0.5003, -0.1293, -0.0131, -0.2284]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5029, -0.3242, -0.0177,  0.0602]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 129 ] state=tensor([[-0.5029, -0.3242, -0.0177,  0.0602]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5093, -0.5191, -0.0165,  0.3472]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 130 ] state=tensor([[-0.5093, -0.5191, -0.0165,  0.3472]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5197, -0.3237, -0.0095,  0.0494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 131 ] state=tensor([[-0.5197, -0.3237, -0.0095,  0.0494]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5262, -0.5187, -0.0086,  0.3390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 132 ] state=tensor([[-0.5262, -0.5187, -0.0086,  0.3390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5366, -0.3235, -0.0018,  0.0437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 133 ] state=tensor([[-0.5366, -0.3235, -0.0018,  0.0437]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5430, -0.1283, -0.0009, -0.2496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 134 ] state=tensor([[-0.5430, -0.1283, -0.0009, -0.2496]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5456, -0.3234, -0.0059,  0.0428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 135 ] state=tensor([[-0.5456, -0.3234, -0.0059,  0.0428]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5521, -0.1282, -0.0050, -0.2517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 136 ] state=tensor([[-0.5521, -0.1282, -0.0050, -0.2517]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5546, -0.3233, -0.0101,  0.0394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 137 ] state=tensor([[-0.5546, -0.3233, -0.0101,  0.0394]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5611, -0.5182, -0.0093,  0.3288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 138 ] state=tensor([[-0.5611, -0.5182, -0.0093,  0.3288]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5715, -0.3230, -0.0027,  0.0332]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 139 ] state=tensor([[-0.5715, -0.3230, -0.0027,  0.0332]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5779, -0.5181, -0.0020,  0.3251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 140 ] state=tensor([[-0.5779, -0.5181, -0.0020,  0.3251]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5883, -0.3229,  0.0045,  0.0317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 141 ] state=tensor([[-0.5883, -0.3229,  0.0045,  0.0317]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5948, -0.5181,  0.0051,  0.3258]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 142 ] state=tensor([[-0.5948, -0.5181,  0.0051,  0.3258]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6051, -0.3231,  0.0116,  0.0348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 143 ] state=tensor([[-0.6051, -0.3231,  0.0116,  0.0348]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6116, -0.5183,  0.0123,  0.3311]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 144 ] state=tensor([[-0.6116, -0.5183,  0.0123,  0.3311]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6219, -0.3234,  0.0189,  0.0423]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 145 ] state=tensor([[-0.6219, -0.3234,  0.0189,  0.0423]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6284, -0.1286,  0.0198, -0.2444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 146 ] state=tensor([[-0.6284, -0.1286,  0.0198, -0.2444]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6310, -0.3240,  0.0149,  0.0545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 147 ] state=tensor([[-0.6310, -0.3240,  0.0149,  0.0545]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6375, -0.1291,  0.0160, -0.2335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 148 ] state=tensor([[-0.6375, -0.1291,  0.0160, -0.2335]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6400, -0.3244,  0.0113,  0.0642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 149 ] state=tensor([[-0.6400, -0.3244,  0.0113,  0.0642]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6465, -0.5197,  0.0126,  0.3604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 150 ] state=tensor([[-0.6465, -0.5197,  0.0126,  0.3604]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6569, -0.3247,  0.0198,  0.0718]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 151 ] state=tensor([[-0.6569, -0.3247,  0.0198,  0.0718]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6634, -0.1299,  0.0212, -0.2146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 152 ] state=tensor([[-0.6634, -0.1299,  0.0212, -0.2146]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6660, -0.3253,  0.0169,  0.0847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 153 ] state=tensor([[-0.6660, -0.3253,  0.0169,  0.0847]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6725, -0.1305,  0.0186, -0.2026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 154 ] state=tensor([[-0.6725, -0.1305,  0.0186, -0.2026]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6751, -0.3258,  0.0146,  0.0959]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 155 ] state=tensor([[-0.6751, -0.3258,  0.0146,  0.0959]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6817, -0.5212,  0.0165,  0.3931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 156 ] state=tensor([[-0.6817, -0.5212,  0.0165,  0.3931]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6921, -0.3263,  0.0244,  0.1057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 157 ] state=tensor([[-0.6921, -0.3263,  0.0244,  0.1057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6986, -0.1315,  0.0265, -0.1792]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 158 ] state=tensor([[-0.6986, -0.1315,  0.0265, -0.1792]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7012,  0.0632,  0.0229, -0.4634]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 159 ] state=tensor([[-0.7012,  0.0632,  0.0229, -0.4634]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7000, -0.1322,  0.0136, -0.1636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 160 ] state=tensor([[-0.7000, -0.1322,  0.0136, -0.1636]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7026,  0.0627,  0.0103, -0.4520]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 161 ] state=tensor([[-0.7026,  0.0627,  0.0103, -0.4520]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7014, -0.1326,  0.0013, -0.1560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 162 ] state=tensor([[-0.7014, -0.1326,  0.0013, -0.1560]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7040, -0.3277, -0.0018,  0.1371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 163 ] state=tensor([[-0.7040, -0.3277, -0.0018,  0.1371]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7106, -0.5228,  0.0009,  0.4292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 164 ] state=tensor([[-0.7106, -0.5228,  0.0009,  0.4292]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7210, -0.3277,  0.0095,  0.1368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 165 ] state=tensor([[-0.7210, -0.3277,  0.0095,  0.1368]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7276, -0.1327,  0.0122, -0.1529]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 127 ][ timestamp 166 ] state=tensor([[-0.7276, -0.1327,  0.0122, -0.1529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7302, -0.3280,  0.0092,  0.1436]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 167 ] state=tensor([[-0.7302, -0.3280,  0.0092,  0.1436]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7368, -0.1330,  0.0121, -0.1461]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 168 ] state=tensor([[-0.7368, -0.1330,  0.0121, -0.1461]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7394, -0.3283,  0.0091,  0.1503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 169 ] state=tensor([[-0.7394, -0.3283,  0.0091,  0.1503]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7460, -0.1333,  0.0121, -0.1394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 170 ] state=tensor([[-0.7460, -0.1333,  0.0121, -0.1394]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7487,  0.0616,  0.0094, -0.4283]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 171 ] state=tensor([[-0.7487,  0.0616,  0.0094, -0.4283]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7474, -0.1336,  0.0008, -0.1327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 172 ] state=tensor([[-0.7474, -0.1336,  0.0008, -0.1327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7501,  0.0615, -0.0019, -0.4251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 173 ] state=tensor([[-0.7501,  0.0615, -0.0019, -0.4251]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7489, -0.1336, -0.0104, -0.1330]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 174 ] state=tensor([[-0.7489, -0.1336, -0.0104, -0.1330]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7516, -0.3286, -0.0130,  0.1564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 175 ] state=tensor([[-0.7516, -0.3286, -0.0130,  0.1564]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7581, -0.1333, -0.0099, -0.1404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 176 ] state=tensor([[-0.7581, -0.1333, -0.0099, -0.1404]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7608, -0.3282, -0.0127,  0.1492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 177 ] state=tensor([[-0.7608, -0.3282, -0.0127,  0.1492]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7674, -0.5232, -0.0097,  0.4378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 178 ] state=tensor([[-0.7674, -0.5232, -0.0097,  0.4378]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7778, -0.3279, -0.0010,  0.1421]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 179 ] state=tensor([[-0.7778, -0.3279, -0.0010,  0.1421]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7844, -0.1328,  0.0019, -0.1509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 180 ] state=tensor([[-0.7844, -0.1328,  0.0019, -0.1509]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7870, -0.3279, -0.0011,  0.1424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 181 ] state=tensor([[-0.7870, -0.3279, -0.0011,  0.1424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7936, -0.1328,  0.0017, -0.1506]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 182 ] state=tensor([[-0.7936, -0.1328,  0.0017, -0.1506]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7963, -0.3280, -0.0013,  0.1426]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 183 ] state=tensor([[-0.7963, -0.3280, -0.0013,  0.1426]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8028, -0.1328,  0.0016, -0.1505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 184 ] state=tensor([[-0.8028, -0.1328,  0.0016, -0.1505]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8055, -0.3280, -0.0015,  0.1427]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 185 ] state=tensor([[-0.8055, -0.3280, -0.0015,  0.1427]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8120, -0.5231,  0.0014,  0.4349]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 186 ] state=tensor([[-0.8120, -0.5231,  0.0014,  0.4349]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8225, -0.3280,  0.0101,  0.1426]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 187 ] state=tensor([[-0.8225, -0.3280,  0.0101,  0.1426]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8290, -0.1330,  0.0129, -0.1468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 188 ] state=tensor([[-0.8290, -0.1330,  0.0129, -0.1468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8317,  0.0620,  0.0100, -0.4354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 189 ] state=tensor([[-0.8317,  0.0620,  0.0100, -0.4354]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8305, -0.1333,  0.0013, -0.1396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 190 ] state=tensor([[-0.8305, -0.1333,  0.0013, -0.1396]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8331, -0.3284, -0.0015,  0.1535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 191 ] state=tensor([[-0.8331, -0.3284, -0.0015,  0.1535]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8397, -0.1333,  0.0016, -0.1397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 192 ] state=tensor([[-0.8397, -0.1333,  0.0016, -0.1397]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8424, -0.3284, -0.0012,  0.1535]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 193 ] state=tensor([[-0.8424, -0.3284, -0.0012,  0.1535]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8489, -0.1333,  0.0019, -0.1395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 194 ] state=tensor([[-0.8489, -0.1333,  0.0019, -0.1395]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8516,  0.0618, -0.0009, -0.4316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 195 ] state=tensor([[-0.8516,  0.0618, -0.0009, -0.4316]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8504, -0.1333, -0.0096, -0.1393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 196 ] state=tensor([[-0.8504, -0.1333, -0.0096, -0.1393]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8530, -0.3283, -0.0124,  0.1504]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 197 ] state=tensor([[-0.8530, -0.3283, -0.0124,  0.1504]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8596, -0.1330, -0.0093, -0.1462]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 198 ] state=tensor([[-0.8596, -0.1330, -0.0093, -0.1462]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8623, -0.3280, -0.0123,  0.1436]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 199 ] state=tensor([[-0.8623, -0.3280, -0.0123,  0.1436]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8688, -0.5229, -0.0094,  0.4323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 200 ] state=tensor([[-0.8688, -0.5229, -0.0094,  0.4323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.7928e-01, -3.2768e-01, -7.5110e-04,  1.3672e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 201 ] state=tensor([[-8.7928e-01, -3.2768e-01, -7.5110e-04,  1.3672e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8858, -0.1326,  0.0020, -0.1562]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 202 ] state=tensor([[-0.8858, -0.1326,  0.0020, -0.1562]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8885,  0.0625, -0.0011, -0.4483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 203 ] state=tensor([[-0.8885,  0.0625, -0.0011, -0.4483]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8872, -0.1326, -0.0101, -0.1559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 204 ] state=tensor([[-0.8872, -0.1326, -0.0101, -0.1559]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8899, -0.3275, -0.0132,  0.1335]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 205 ] state=tensor([[-0.8899, -0.3275, -0.0132,  0.1335]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8964, -0.1322, -0.0106, -0.1633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 206 ] state=tensor([[-0.8964, -0.1322, -0.0106, -0.1633]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8991, -0.3272, -0.0138,  0.1261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 207 ] state=tensor([[-0.8991, -0.3272, -0.0138,  0.1261]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9056, -0.5221, -0.0113,  0.4143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 208 ] state=tensor([[-0.9056, -0.5221, -0.0113,  0.4143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9161, -0.3268, -0.0030,  0.1181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 209 ] state=tensor([[-0.9161, -0.3268, -0.0030,  0.1181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-9.2261e-01, -1.3168e-01, -6.4920e-04, -1.7551e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 210 ] state=tensor([[-9.2261e-01, -1.3168e-01, -6.4920e-04, -1.7551e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9252,  0.0635, -0.0042, -0.4684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 211 ] state=tensor([[-0.9252,  0.0635, -0.0042, -0.4684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9240, -0.1316, -0.0135, -0.1770]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 212 ] state=tensor([[-0.9240, -0.1316, -0.0135, -0.1770]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9266,  0.0637, -0.0171, -0.4740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 213 ] state=tensor([[-0.9266,  0.0637, -0.0171, -0.4740]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9253, -0.1312, -0.0265, -0.1867]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 214 ] state=tensor([[-0.9253, -0.1312, -0.0265, -0.1867]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9280, -0.3259, -0.0303,  0.0975]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 215 ] state=tensor([[-0.9280, -0.3259, -0.0303,  0.0975]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9345, -0.5206, -0.0283,  0.3805]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 216 ] state=tensor([[-0.9345, -0.5206, -0.0283,  0.3805]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9449, -0.3251, -0.0207,  0.0790]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 217 ] state=tensor([[-0.9449, -0.3251, -0.0207,  0.0790]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9514, -0.1297, -0.0191, -0.2202]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 218 ] state=tensor([[-0.9514, -0.1297, -0.0191, -0.2202]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9540, -0.3245, -0.0235,  0.0664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 219 ] state=tensor([[-0.9540, -0.3245, -0.0235,  0.0664]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9605, -0.5193, -0.0222,  0.3516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 220 ] state=tensor([[-0.9605, -0.5193, -0.0222,  0.3516]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9709, -0.3238, -0.0152,  0.0520]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 221 ] state=tensor([[-0.9709, -0.3238, -0.0152,  0.0520]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9773, -0.1285, -0.0141, -0.2454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 222 ] state=tensor([[-0.9773, -0.1285, -0.0141, -0.2454]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9799,  0.0668, -0.0191, -0.5426]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 223 ] state=tensor([[-0.9799,  0.0668, -0.0191, -0.5426]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9786, -0.1280, -0.0299, -0.2559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 224 ] state=tensor([[-0.9786, -0.1280, -0.0299, -0.2559]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9811, -0.3227, -0.0350,  0.0272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 225 ] state=tensor([[-0.9811, -0.3227, -0.0350,  0.0272]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9876, -0.5173, -0.0345,  0.3086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 226 ] state=tensor([[-0.9876, -0.5173, -0.0345,  0.3086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9979, -0.3217, -0.0283,  0.0052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 227 ] state=tensor([[-0.9979, -0.3217, -0.0283,  0.0052]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0044, -0.5164, -0.0282,  0.2889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 228 ] state=tensor([[-1.0044, -0.5164, -0.0282,  0.2889]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0147, -0.3209, -0.0224, -0.0126]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 229 ] state=tensor([[-1.0147, -0.3209, -0.0224, -0.0126]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0211, -0.5157, -0.0227,  0.2729]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 230 ] state=tensor([[-1.0211, -0.5157, -0.0227,  0.2729]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0314, -0.7105, -0.0172,  0.5584]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 127 ][ timestamp 231 ] state=tensor([[-1.0314, -0.7105, -0.0172,  0.5584]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0456, -0.5151, -0.0061,  0.2603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 232 ] state=tensor([[-1.0456, -0.5151, -0.0061,  0.2603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0559e+00, -7.1018e-01, -8.4490e-04,  5.5109e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 233 ] state=tensor([[-1.0559e+00, -7.1018e-01, -8.4490e-04,  5.5109e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0701, -0.5150,  0.0102,  0.2581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 234 ] state=tensor([[-1.0701, -0.5150,  0.0102,  0.2581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0804, -0.3201,  0.0153, -0.0313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 235 ] state=tensor([[-1.0804, -0.3201,  0.0153, -0.0313]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0868, -0.5154,  0.0147,  0.2662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 236 ] state=tensor([[-1.0868, -0.5154,  0.0147,  0.2662]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0971, -0.3205,  0.0200, -0.0218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 237 ] state=tensor([[-1.0971, -0.3205,  0.0200, -0.0218]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1036, -0.5159,  0.0196,  0.2771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 238 ] state=tensor([[-1.1036, -0.5159,  0.0196,  0.2771]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1139, -0.3211,  0.0251, -0.0093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 239 ] state=tensor([[-1.1139, -0.3211,  0.0251, -0.0093]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1203, -0.1263,  0.0250, -0.2940]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 240 ] state=tensor([[-1.1203, -0.1263,  0.0250, -0.2940]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1228,  0.0684,  0.0191, -0.5787]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 241 ] state=tensor([[-1.1228,  0.0684,  0.0191, -0.5787]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1215, -0.1269,  0.0075, -0.2801]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 242 ] state=tensor([[-1.1215, -0.1269,  0.0075, -0.2801]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1240,  0.0681,  0.0019, -0.5704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 243 ] state=tensor([[-1.1240,  0.0681,  0.0019, -0.5704]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1226, -0.1271, -0.0095, -0.2771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 244 ] state=tensor([[-1.1226, -0.1271, -0.0095, -0.2771]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1252,  0.0682, -0.0150, -0.5727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 245 ] state=tensor([[-1.1252,  0.0682, -0.0150, -0.5727]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1238, -0.1267, -0.0265, -0.2848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 246 ] state=tensor([[-1.1238, -0.1267, -0.0265, -0.2848]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1263e+00, -3.2146e-01, -3.2199e-02, -6.3560e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 247 ] state=tensor([[-1.1263e+00, -3.2146e-01, -3.2199e-02, -6.3560e-04]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1328, -0.5161, -0.0322,  0.2817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 248 ] state=tensor([[-1.1328, -0.5161, -0.0322,  0.2817]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1431, -0.3205, -0.0266, -0.0209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 249 ] state=tensor([[-1.1431, -0.3205, -0.0266, -0.0209]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1495, -0.5153, -0.0270,  0.2632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 250 ] state=tensor([[-1.1495, -0.5153, -0.0270,  0.2632]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1598, -0.3198, -0.0217, -0.0378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 251 ] state=tensor([[-1.1598, -0.3198, -0.0217, -0.0378]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1662, -0.5146, -0.0225,  0.2479]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 252 ] state=tensor([[-1.1662, -0.5146, -0.0225,  0.2479]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1765, -0.3191, -0.0175, -0.0518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 253 ] state=tensor([[-1.1765, -0.3191, -0.0175, -0.0518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1829, -0.5140, -0.0186,  0.2353]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 254 ] state=tensor([[-1.1829, -0.5140, -0.0186,  0.2353]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1932, -0.7089, -0.0139,  0.5221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 255 ] state=tensor([[-1.1932, -0.7089, -0.0139,  0.5221]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2073, -0.5135, -0.0034,  0.2251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 256 ] state=tensor([[-1.2073, -0.5135, -0.0034,  0.2251]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2176e+00, -3.1837e-01,  1.0828e-03, -6.8694e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 257 ] state=tensor([[-1.2176e+00, -3.1837e-01,  1.0828e-03, -6.8694e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2240e+00, -1.2327e-01, -2.9111e-04, -3.6103e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 258 ] state=tensor([[-1.2240e+00, -1.2327e-01, -2.9111e-04, -3.6103e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2264,  0.0719, -0.0075, -0.6538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 259 ] state=tensor([[-1.2264,  0.0719, -0.0075, -0.6538]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2250, -0.1232, -0.0206, -0.3635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 260 ] state=tensor([[-1.2250, -0.1232, -0.0206, -0.3635]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2275, -0.3180, -0.0279, -0.0774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 261 ] state=tensor([[-1.2275, -0.3180, -0.0279, -0.0774]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2338, -0.5127, -0.0294,  0.2064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 262 ] state=tensor([[-1.2338, -0.5127, -0.0294,  0.2064]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2441, -0.7074, -0.0253,  0.4896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 263 ] state=tensor([[-1.2441, -0.7074, -0.0253,  0.4896]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2582, -0.5119, -0.0155,  0.1891]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 264 ] state=tensor([[-1.2582, -0.5119, -0.0155,  0.1891]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2685, -0.3166, -0.0117, -0.1084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 265 ] state=tensor([[-1.2685, -0.3166, -0.0117, -0.1084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2748, -0.1213, -0.0139, -0.4048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 266 ] state=tensor([[-1.2748, -0.1213, -0.0139, -0.4048]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2772, -0.3162, -0.0220, -0.1165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 267 ] state=tensor([[-1.2772, -0.3162, -0.0220, -0.1165]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2835, -0.1208, -0.0243, -0.4160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 268 ] state=tensor([[-1.2835, -0.1208, -0.0243, -0.4160]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2860,  0.0747, -0.0326, -0.7163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 269 ] state=tensor([[-1.2860,  0.0747, -0.0326, -0.7163]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2845, -0.1200, -0.0469, -0.4340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 270 ] state=tensor([[-1.2845, -0.1200, -0.0469, -0.4340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2869, -0.3144, -0.0556, -0.1565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 271 ] state=tensor([[-1.2869, -0.3144, -0.0556, -0.1565]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2932, -0.5087, -0.0588,  0.1181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 272 ] state=tensor([[-1.2932, -0.5087, -0.0588,  0.1181]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3033, -0.7029, -0.0564,  0.3917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 273 ] state=tensor([[-1.3033, -0.7029, -0.0564,  0.3917]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3174, -0.5070, -0.0486,  0.0818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 274 ] state=tensor([[-1.3174, -0.5070, -0.0486,  0.0818]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3275, -0.7014, -0.0469,  0.3588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 275 ] state=tensor([[-1.3275, -0.7014, -0.0469,  0.3588]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3416, -0.5057, -0.0397,  0.0517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 276 ] state=tensor([[-1.3416, -0.5057, -0.0397,  0.0517]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3517, -0.7002, -0.0387,  0.3316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 277 ] state=tensor([[-1.3517, -0.7002, -0.0387,  0.3316]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3657, -0.5046, -0.0321,  0.0269]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 278 ] state=tensor([[-1.3657, -0.5046, -0.0321,  0.0269]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3758, -0.6992, -0.0315,  0.3093]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 279 ] state=tensor([[-1.3758, -0.6992, -0.0315,  0.3093]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3897, -0.8939, -0.0254,  0.5919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 280 ] state=tensor([[-1.3897, -0.8939, -0.0254,  0.5919]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4076, -1.0886, -0.0135,  0.8765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 281 ] state=tensor([[-1.4076, -1.0886, -0.0135,  0.8765]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4294, -0.8933,  0.0040,  0.5796]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 127 ][ timestamp 282 ] state=tensor([[-1.4294, -0.8933,  0.0040,  0.5796]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4473, -0.6983,  0.0156,  0.2881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 283 ] state=tensor([[-1.4473, -0.6983,  0.0156,  0.2881]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4612e+00, -5.0336e-01,  2.1365e-02,  4.2561e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 284 ] state=tensor([[-1.4612e+00, -5.0336e-01,  2.1365e-02,  4.2561e-04]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4713, -0.3086,  0.0214, -0.2854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 285 ] state=tensor([[-1.4713, -0.3086,  0.0214, -0.2854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4775, -0.1137,  0.0157, -0.5713]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 286 ] state=tensor([[-1.4775, -0.1137,  0.0157, -0.5713]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4797, -0.3091,  0.0042, -0.2737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 287 ] state=tensor([[-1.4797, -0.3091,  0.0042, -0.2737]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4859e+00, -1.1402e-01, -1.2363e-03, -5.6507e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 288 ] state=tensor([[-1.4859e+00, -1.1402e-01, -1.2363e-03, -5.6507e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4882,  0.0811, -0.0125, -0.8581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 289 ] state=tensor([[-1.4882,  0.0811, -0.0125, -0.8581]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4866, -0.1138, -0.0297, -0.5694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 290 ] state=tensor([[-1.4866, -0.1138, -0.0297, -0.5694]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4889,  0.0817, -0.0411, -0.8713]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 291 ] state=tensor([[-1.4889,  0.0817, -0.0411, -0.8713]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4872, -0.1128, -0.0585, -0.5918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 292 ] state=tensor([[-1.4872, -0.1128, -0.0585, -0.5918]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4895,  0.0831, -0.0704, -0.9024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 293 ] state=tensor([[-1.4895,  0.0831, -0.0704, -0.9024]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4878, -0.1111, -0.0884, -0.6326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 294 ] state=tensor([[-1.4878, -0.1111, -0.0884, -0.6326]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4900, -0.3048, -0.1011, -0.3690]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 295 ] state=tensor([[-1.4900, -0.3048, -0.1011, -0.3690]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4961, -0.4984, -0.1084, -0.1098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 296 ] state=tensor([[-1.4961, -0.4984, -0.1084, -0.1098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5061, -0.6918, -0.1106,  0.1468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 297 ] state=tensor([[-1.5061, -0.6918, -0.1106,  0.1468]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5199, -0.8852, -0.1077,  0.4026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 298 ] state=tensor([[-1.5199, -0.8852, -0.1077,  0.4026]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5376, -1.0786, -0.0996,  0.6595]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 299 ] state=tensor([[-1.5376, -1.0786, -0.0996,  0.6595]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5592, -1.2722, -0.0865,  0.9192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 300 ] state=tensor([[-1.5592, -1.2722, -0.0865,  0.9192]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5847, -1.4661, -0.0681,  1.1835]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 301 ] state=tensor([[-1.5847, -1.4661, -0.0681,  1.1835]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6140, -1.2701, -0.0444,  0.8703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 302 ] state=tensor([[-1.6140, -1.2701, -0.0444,  0.8703]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6394, -1.0744, -0.0270,  0.5640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 303 ] state=tensor([[-1.6394, -1.0744, -0.0270,  0.5640]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6609, -0.8790, -0.0157,  0.2629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 304 ] state=tensor([[-1.6609, -0.8790, -0.0157,  0.2629]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6785, -0.6836, -0.0104, -0.0347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 305 ] state=tensor([[-1.6785, -0.6836, -0.0104, -0.0347]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6921, -0.4883, -0.0111, -0.3306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 306 ] state=tensor([[-1.6921, -0.4883, -0.0111, -0.3306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7019, -0.2931, -0.0178, -0.6268]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 307 ] state=tensor([[-1.7019, -0.2931, -0.0178, -0.6268]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7078, -0.0977, -0.0303, -0.9250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 308 ] state=tensor([[-1.7078, -0.0977, -0.0303, -0.9250]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7097, -0.2924, -0.0488, -0.6420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 309 ] state=tensor([[-1.7097, -0.2924, -0.0488, -0.6420]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7156, -0.4868, -0.0616, -0.3651]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 310 ] state=tensor([[-1.7156, -0.4868, -0.0616, -0.3651]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7253, -0.6810, -0.0689, -0.0924]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 311 ] state=tensor([[-1.7253, -0.6810, -0.0689, -0.0924]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7389, -0.8751, -0.0708,  0.1777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 312 ] state=tensor([[-1.7389, -0.8751, -0.0708,  0.1777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7564, -1.0691, -0.0672,  0.4473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 313 ] state=tensor([[-1.7564, -1.0691, -0.0672,  0.4473]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7778, -1.2632, -0.0583,  0.7180]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 314 ] state=tensor([[-1.7778, -1.2632, -0.0583,  0.7180]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8031, -1.0673, -0.0439,  0.4076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 315 ] state=tensor([[-1.8031, -1.0673, -0.0439,  0.4076]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8244, -0.8716, -0.0358,  0.1014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 316 ] state=tensor([[-1.8244, -0.8716, -0.0358,  0.1014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8418, -0.6760, -0.0337, -0.2024]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 317 ] state=tensor([[-1.8418, -0.6760, -0.0337, -0.2024]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8554, -0.4804, -0.0378, -0.5055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 318 ] state=tensor([[-1.8554, -0.4804, -0.0378, -0.5055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8650, -0.2848, -0.0479, -0.8099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 319 ] state=tensor([[-1.8650, -0.2848, -0.0479, -0.8099]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8707, -0.4792, -0.0641, -0.5326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 320 ] state=tensor([[-1.8707, -0.4792, -0.0641, -0.5326]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8803, -0.2833, -0.0747, -0.8448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 321 ] state=tensor([[-1.8803, -0.2833, -0.0747, -0.8448]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8859, -0.4773, -0.0916, -0.5765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 322 ] state=tensor([[-1.8859, -0.4773, -0.0916, -0.5765]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8955, -0.6710, -0.1032, -0.3141]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 323 ] state=tensor([[-1.8955, -0.6710, -0.1032, -0.3141]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9089, -0.8645, -0.1095, -0.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 324 ] state=tensor([[-1.9089, -0.8645, -0.1095, -0.0556]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9262, -1.0579, -0.1106,  0.2006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 325 ] state=tensor([[-1.9262, -1.0579, -0.1106,  0.2006]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9473, -1.2513, -0.1066,  0.4565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 326 ] state=tensor([[-1.9473, -1.2513, -0.1066,  0.4565]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9724, -1.4448, -0.0974,  0.7138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 327 ] state=tensor([[-1.9724, -1.4448, -0.0974,  0.7138]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0013, -1.6384, -0.0831,  0.9743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 328 ] state=tensor([[-2.0013, -1.6384, -0.0831,  0.9743]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0340, -1.4423, -0.0637,  0.6567]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 329 ] state=tensor([[-2.0340, -1.4423, -0.0637,  0.6567]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0629, -1.2463, -0.0505,  0.3446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 330 ] state=tensor([[-2.0629, -1.2463, -0.0505,  0.3446]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0878, -1.0505, -0.0436,  0.0365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 331 ] state=tensor([[-2.0878, -1.0505, -0.0436,  0.0365]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1088, -0.8548, -0.0429, -0.2697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 332 ] state=tensor([[-2.1088, -0.8548, -0.0429, -0.2697]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1259, -0.6591, -0.0483, -0.5756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 333 ] state=tensor([[-2.1259, -0.6591, -0.0483, -0.5756]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1391, -0.4633, -0.0598, -0.8831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 334 ] state=tensor([[-2.1391, -0.4633, -0.0598, -0.8831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1484, -0.2675, -0.0775, -1.1939]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 335 ] state=tensor([[-2.1484, -0.2675, -0.0775, -1.1939]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1537, -0.4615, -0.1014, -0.9265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 336 ] state=tensor([[-2.1537, -0.4615, -0.1014, -0.9265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1629, -0.2652, -0.1199, -1.2492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 337 ] state=tensor([[-2.1629, -0.2652, -0.1199, -1.2492]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1682, -0.4586, -0.1449, -0.9964]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 338 ] state=tensor([[-2.1682, -0.4586, -0.1449, -0.9964]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1774, -0.2618, -0.1648, -1.3308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 339 ] state=tensor([[-2.1774, -0.2618, -0.1648, -1.3308]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1826, -0.0651, -0.1914, -1.6702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 127 ][ timestamp 340 ] state=tensor([[-2.1826, -0.0651, -0.1914, -1.6702]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 127: Exploration_rate=0.05. Score=340.\n",
      "[ episode 128 ] state=tensor([[ 0.0342,  0.0289, -0.0150,  0.0057]])\n",
      "[ episode 128 ][ timestamp 1 ] state=tensor([[ 0.0342,  0.0289, -0.0150,  0.0057]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0347,  0.2242, -0.0149, -0.2917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 2 ] state=tensor([[ 0.0347,  0.2242, -0.0149, -0.2917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0392,  0.0293, -0.0207, -0.0037]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 3 ] state=tensor([[ 0.0392,  0.0293, -0.0207, -0.0037]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0398,  0.2247, -0.0208, -0.3028]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 4 ] state=tensor([[ 0.0398,  0.2247, -0.0208, -0.3028]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0443,  0.0299, -0.0268, -0.0168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 5 ] state=tensor([[ 0.0443,  0.0299, -0.0268, -0.0168]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0449,  0.2254, -0.0272, -0.3178]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 6 ] state=tensor([[ 0.0449,  0.2254, -0.0272, -0.3178]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0494,  0.0307, -0.0335, -0.0338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 7 ] state=tensor([[ 0.0494,  0.0307, -0.0335, -0.0338]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0500,  0.2263, -0.0342, -0.3369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 8 ] state=tensor([[ 0.0500,  0.2263, -0.0342, -0.3369]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0546,  0.4219, -0.0410, -0.6402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 9 ] state=tensor([[ 0.0546,  0.4219, -0.0410, -0.6402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0630,  0.2273, -0.0538, -0.3607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 10 ] state=tensor([[ 0.0630,  0.2273, -0.0538, -0.3607]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0675,  0.0330, -0.0610, -0.0854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 11 ] state=tensor([[ 0.0675,  0.0330, -0.0610, -0.0854]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0682, -0.1612, -0.0627,  0.1874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 12 ] state=tensor([[ 0.0682, -0.1612, -0.0627,  0.1874]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0650,  0.0348, -0.0589, -0.1243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 13 ] state=tensor([[ 0.0650,  0.0348, -0.0589, -0.1243]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0657,  0.2307, -0.0614, -0.4350]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 128 ][ timestamp 14 ] state=tensor([[ 0.0657,  0.2307, -0.0614, -0.4350]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0703,  0.0365, -0.0701, -0.1623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 15 ] state=tensor([[ 0.0703,  0.0365, -0.0701, -0.1623]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0710,  0.2326, -0.0734, -0.4763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 16 ] state=tensor([[ 0.0710,  0.2326, -0.0734, -0.4763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0757,  0.0385, -0.0829, -0.2076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 17 ] state=tensor([[ 0.0757,  0.0385, -0.0829, -0.2076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0764, -0.1553, -0.0870,  0.0579]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 18 ] state=tensor([[ 0.0764, -0.1553, -0.0870,  0.0579]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0733, -0.3491, -0.0859,  0.3219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 19 ] state=tensor([[ 0.0733, -0.3491, -0.0859,  0.3219]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0664, -0.5429, -0.0794,  0.5863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 20 ] state=tensor([[ 0.0664, -0.5429, -0.0794,  0.5863]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0555, -0.7368, -0.0677,  0.8529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 21 ] state=tensor([[ 0.0555, -0.7368, -0.0677,  0.8529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0408, -0.9309, -0.0507,  1.1236]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 22 ] state=tensor([[ 0.0408, -0.9309, -0.0507,  1.1236]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0221, -1.1254, -0.0282,  1.3999]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 23 ] state=tensor([[ 0.0221, -1.1254, -0.0282,  1.3999]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.6281e-04, -9.2990e-01, -1.8736e-04,  1.0986e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 24 ] state=tensor([[-3.6281e-04, -9.2990e-01, -1.8736e-04,  1.0986e+00]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0190, -0.7348,  0.0218,  0.8058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 25 ] state=tensor([[-0.0190, -0.7348,  0.0218,  0.8058]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0337, -0.5400,  0.0379,  0.5201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 26 ] state=tensor([[-0.0337, -0.5400,  0.0379,  0.5201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0445, -0.3454,  0.0483,  0.2396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 27 ] state=tensor([[-0.0445, -0.3454,  0.0483,  0.2396]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0514, -0.1510,  0.0531, -0.0375]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 28 ] state=tensor([[-0.0514, -0.1510,  0.0531, -0.0375]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0544,  0.0433,  0.0523, -0.3130]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 29 ] state=tensor([[-0.0544,  0.0433,  0.0523, -0.3130]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0535,  0.2377,  0.0461, -0.5887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 30 ] state=tensor([[-0.0535,  0.2377,  0.0461, -0.5887]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0488,  0.0419,  0.0343, -0.2818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 31 ] state=tensor([[-0.0488,  0.0419,  0.0343, -0.2818]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0479,  0.2365,  0.0287, -0.5635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 32 ] state=tensor([[-0.0479,  0.2365,  0.0287, -0.5635]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0432,  0.0410,  0.0174, -0.2619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 33 ] state=tensor([[-0.0432,  0.0410,  0.0174, -0.2619]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0424, -0.1543,  0.0122,  0.0362]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 34 ] state=tensor([[-0.0424, -0.1543,  0.0122,  0.0362]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0455, -0.3496,  0.0129,  0.3327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 35 ] state=tensor([[-0.0455, -0.3496,  0.0129,  0.3327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0525, -0.1547,  0.0195,  0.0441]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 36 ] state=tensor([[-0.0525, -0.1547,  0.0195,  0.0441]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0555,  0.0401,  0.0204, -0.2424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 37 ] state=tensor([[-0.0555,  0.0401,  0.0204, -0.2424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0547,  0.2350,  0.0156, -0.5285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 38 ] state=tensor([[-0.0547,  0.2350,  0.0156, -0.5285]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0500,  0.0396,  0.0050, -0.2310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 39 ] state=tensor([[-0.0500,  0.0396,  0.0050, -0.2310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.9251e-02,  2.3468e-01,  3.8683e-04, -5.2208e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 40 ] state=tensor([[-4.9251e-02,  2.3468e-01,  3.8683e-04, -5.2208e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0446,  0.0396, -0.0101, -0.2293]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 41 ] state=tensor([[-0.0446,  0.0396, -0.0101, -0.2293]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0438,  0.2348, -0.0146, -0.5251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 42 ] state=tensor([[-0.0438,  0.2348, -0.0146, -0.5251]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0391,  0.0399, -0.0251, -0.2371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 43 ] state=tensor([[-0.0391,  0.0399, -0.0251, -0.2371]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0383,  0.2354, -0.0299, -0.5376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 44 ] state=tensor([[-0.0383,  0.2354, -0.0299, -0.5376]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0336,  0.0407, -0.0406, -0.2545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 45 ] state=tensor([[-0.0336,  0.0407, -0.0406, -0.2545]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0328, -0.1538, -0.0457,  0.0251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 46 ] state=tensor([[-0.0328, -0.1538, -0.0457,  0.0251]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0358, -0.3483, -0.0452,  0.3030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 47 ] state=tensor([[-0.0358, -0.3483, -0.0452,  0.3030]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0428, -0.5427, -0.0392,  0.5811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 48 ] state=tensor([[-0.0428, -0.5427, -0.0392,  0.5811]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0536, -0.3471, -0.0275,  0.2764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 49 ] state=tensor([[-0.0536, -0.3471, -0.0275,  0.2764]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0606, -0.1516, -0.0220, -0.0249]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 50 ] state=tensor([[-0.0606, -0.1516, -0.0220, -0.0249]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0636,  0.0439, -0.0225, -0.3244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 51 ] state=tensor([[-0.0636,  0.0439, -0.0225, -0.3244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0627, -0.1509, -0.0290, -0.0389]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 52 ] state=tensor([[-0.0627, -0.1509, -0.0290, -0.0389]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0658, -0.3456, -0.0298,  0.2445]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 53 ] state=tensor([[-0.0658, -0.3456, -0.0298,  0.2445]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0727, -0.5403, -0.0249,  0.5276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 54 ] state=tensor([[-0.0727, -0.5403, -0.0249,  0.5276]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0835, -0.3448, -0.0143,  0.2272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 55 ] state=tensor([[-0.0835, -0.3448, -0.0143,  0.2272]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0904, -0.5398, -0.0098,  0.5153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 56 ] state=tensor([[-0.0904, -0.5398, -0.0098,  0.5153]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0117e-01, -7.3474e-01,  5.1468e-04,  8.0490e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 57 ] state=tensor([[-1.0117e-01, -7.3474e-01,  5.1468e-04,  8.0490e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1159, -0.5396,  0.0166,  0.5124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 58 ] state=tensor([[-0.1159, -0.5396,  0.0166,  0.5124]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1267, -0.3447,  0.0269,  0.2250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 59 ] state=tensor([[-0.1267, -0.3447,  0.0269,  0.2250]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1336, -0.1500,  0.0314, -0.0591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 60 ] state=tensor([[-0.1336, -0.1500,  0.0314, -0.0591]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1366,  0.0446,  0.0302, -0.3417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 61 ] state=tensor([[-0.1366,  0.0446,  0.0302, -0.3417]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1357,  0.2393,  0.0233, -0.6248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 62 ] state=tensor([[-0.1357,  0.2393,  0.0233, -0.6248]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1309,  0.0439,  0.0108, -0.3248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 63 ] state=tensor([[-0.1309,  0.0439,  0.0108, -0.3248]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1300,  0.2389,  0.0044, -0.6141]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 64 ] state=tensor([[-0.1300,  0.2389,  0.0044, -0.6141]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1252,  0.0437, -0.0079, -0.3200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 65 ] state=tensor([[-0.1252,  0.0437, -0.0079, -0.3200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1243, -0.1513, -0.0143, -0.0298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 66 ] state=tensor([[-0.1243, -0.1513, -0.0143, -0.0298]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1274,  0.0440, -0.0149, -0.3270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 67 ] state=tensor([[-0.1274,  0.0440, -0.0149, -0.3270]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1265,  0.2393, -0.0215, -0.6244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 68 ] state=tensor([[-0.1265,  0.2393, -0.0215, -0.6244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1217,  0.0445, -0.0340, -0.3385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 69 ] state=tensor([[-0.1217,  0.0445, -0.0340, -0.3385]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1208, -0.1501, -0.0407, -0.0567]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 70 ] state=tensor([[-0.1208, -0.1501, -0.0407, -0.0567]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1238, -0.3446, -0.0419,  0.2228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 71 ] state=tensor([[-0.1238, -0.3446, -0.0419,  0.2228]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1307, -0.5391, -0.0374,  0.5020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 72 ] state=tensor([[-0.1307, -0.5391, -0.0374,  0.5020]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1415, -0.3435, -0.0274,  0.1978]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 73 ] state=tensor([[-0.1415, -0.3435, -0.0274,  0.1978]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1484, -0.1480, -0.0234, -0.1034]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 74 ] state=tensor([[-0.1484, -0.1480, -0.0234, -0.1034]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1513, -0.3428, -0.0255,  0.1818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 75 ] state=tensor([[-0.1513, -0.3428, -0.0255,  0.1818]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1582, -0.1473, -0.0218, -0.1188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 76 ] state=tensor([[-0.1582, -0.1473, -0.0218, -0.1188]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1611, -0.3421, -0.0242,  0.1669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 77 ] state=tensor([[-0.1611, -0.3421, -0.0242,  0.1669]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1680, -0.1466, -0.0209, -0.1333]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 78 ] state=tensor([[-0.1680, -0.1466, -0.0209, -0.1333]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1709, -0.3415, -0.0235,  0.1527]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 79 ] state=tensor([[-0.1709, -0.3415, -0.0235,  0.1527]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1777, -0.1460, -0.0205, -0.1473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 80 ] state=tensor([[-0.1777, -0.1460, -0.0205, -0.1473]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1807,  0.0494, -0.0234, -0.4464]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 81 ] state=tensor([[-0.1807,  0.0494, -0.0234, -0.4464]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1797, -0.1454, -0.0324, -0.1612]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 82 ] state=tensor([[-0.1797, -0.1454, -0.0324, -0.1612]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1826,  0.0502, -0.0356, -0.4639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 83 ] state=tensor([[-0.1826,  0.0502, -0.0356, -0.4639]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1816, -0.1444, -0.0449, -0.1826]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 84 ] state=tensor([[-0.1816, -0.1444, -0.0449, -0.1826]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1845,  0.0513, -0.0485, -0.4891]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 85 ] state=tensor([[-0.1845,  0.0513, -0.0485, -0.4891]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1834, -0.1431, -0.0583, -0.2121]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 86 ] state=tensor([[-0.1834, -0.1431, -0.0583, -0.2121]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1863, -0.3373, -0.0625,  0.0617]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 128 ][ timestamp 87 ] state=tensor([[-0.1863, -0.3373, -0.0625,  0.0617]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1930, -0.5315, -0.0613,  0.3340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 88 ] state=tensor([[-0.1930, -0.5315, -0.0613,  0.3340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2037, -0.7257, -0.0546,  0.6067]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 89 ] state=tensor([[-0.2037, -0.7257, -0.0546,  0.6067]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2182, -0.9200, -0.0425,  0.8817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 90 ] state=tensor([[-0.2182, -0.9200, -0.0425,  0.8817]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2366, -0.7243, -0.0249,  0.5760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 91 ] state=tensor([[-0.2366, -0.7243, -0.0249,  0.5760]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2511, -0.5289, -0.0133,  0.2756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 92 ] state=tensor([[-0.2511, -0.5289, -0.0133,  0.2756]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2616, -0.3336, -0.0078, -0.0213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 93 ] state=tensor([[-0.2616, -0.3336, -0.0078, -0.0213]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2683, -0.5286, -0.0082,  0.2689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 94 ] state=tensor([[-0.2683, -0.5286, -0.0082,  0.2689]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2789, -0.3333, -0.0029, -0.0264]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 95 ] state=tensor([[-0.2789, -0.3333, -0.0029, -0.0264]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2856, -0.1382, -0.0034, -0.3200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 96 ] state=tensor([[-0.2856, -0.1382, -0.0034, -0.3200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2883, -0.3333, -0.0098, -0.0283]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 97 ] state=tensor([[-0.2883, -0.3333, -0.0098, -0.0283]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2950, -0.1380, -0.0104, -0.3241]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 98 ] state=tensor([[-0.2950, -0.1380, -0.0104, -0.3241]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2977, -0.3330, -0.0168, -0.0347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 99 ] state=tensor([[-0.2977, -0.3330, -0.0168, -0.0347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3044, -0.5278, -0.0175,  0.2526]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 100 ] state=tensor([[-0.3044, -0.5278, -0.0175,  0.2526]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3150, -0.3325, -0.0125, -0.0455]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 101 ] state=tensor([[-0.3150, -0.3325, -0.0125, -0.0455]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3216, -0.5274, -0.0134,  0.2432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 102 ] state=tensor([[-0.3216, -0.5274, -0.0134,  0.2432]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3322, -0.3321, -0.0085, -0.0537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 103 ] state=tensor([[-0.3322, -0.3321, -0.0085, -0.0537]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3388, -0.5271, -0.0096,  0.2363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 104 ] state=tensor([[-0.3388, -0.5271, -0.0096,  0.2363]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3493, -0.3319, -0.0049, -0.0594]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 105 ] state=tensor([[-0.3493, -0.3319, -0.0049, -0.0594]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3560, -0.5269, -0.0061,  0.2317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 106 ] state=tensor([[-0.3560, -0.5269, -0.0061,  0.2317]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3665, -0.7219, -0.0014,  0.5225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 107 ] state=tensor([[-0.3665, -0.7219, -0.0014,  0.5225]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3810, -0.5268,  0.0090,  0.2293]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 108 ] state=tensor([[-0.3810, -0.5268,  0.0090,  0.2293]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3915, -0.7220,  0.0136,  0.5249]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 109 ] state=tensor([[-0.3915, -0.7220,  0.0136,  0.5249]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4059, -0.5271,  0.0241,  0.2365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 110 ] state=tensor([[-0.4059, -0.5271,  0.0241,  0.2365]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4165, -0.3323,  0.0288, -0.0485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 111 ] state=tensor([[-0.4165, -0.3323,  0.0288, -0.0485]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4231, -0.1376,  0.0279, -0.3320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 112 ] state=tensor([[-0.4231, -0.1376,  0.0279, -0.3320]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4259, -0.3332,  0.0212, -0.0306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 113 ] state=tensor([[-0.4259, -0.3332,  0.0212, -0.0306]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4325, -0.5286,  0.0206,  0.2687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 114 ] state=tensor([[-0.4325, -0.5286,  0.0206,  0.2687]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4431, -0.7240,  0.0260,  0.5678]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 115 ] state=tensor([[-0.4431, -0.7240,  0.0260,  0.5678]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4576, -0.5292,  0.0373,  0.2834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 116 ] state=tensor([[-0.4576, -0.5292,  0.0373,  0.2834]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4682, -0.3347,  0.0430,  0.0027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 117 ] state=tensor([[-0.4682, -0.3347,  0.0430,  0.0027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4749, -0.1402,  0.0431, -0.2761]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 118 ] state=tensor([[-0.4749, -0.1402,  0.0431, -0.2761]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4777,  0.0543,  0.0375, -0.5549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 119 ] state=tensor([[-0.4777,  0.0543,  0.0375, -0.5549]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4766, -0.1413,  0.0264, -0.2506]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 120 ] state=tensor([[-0.4766, -0.1413,  0.0264, -0.2506]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4794,  0.0534,  0.0214, -0.5348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 121 ] state=tensor([[-0.4794,  0.0534,  0.0214, -0.5348]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4783, -0.1420,  0.0107, -0.2355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 122 ] state=tensor([[-0.4783, -0.1420,  0.0107, -0.2355]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4812,  0.0530,  0.0060, -0.5248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 123 ] state=tensor([[-0.4812,  0.0530,  0.0060, -0.5248]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4801, -0.1423, -0.0045, -0.2302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 124 ] state=tensor([[-0.4801, -0.1423, -0.0045, -0.2302]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4830,  0.0529, -0.0091, -0.5243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 125 ] state=tensor([[-0.4830,  0.0529, -0.0091, -0.5243]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4819, -0.1421, -0.0196, -0.2345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 126 ] state=tensor([[-0.4819, -0.1421, -0.0196, -0.2345]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4848, -0.3369, -0.0243,  0.0520]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 127 ] state=tensor([[-0.4848, -0.3369, -0.0243,  0.0520]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4915, -0.1414, -0.0232, -0.2483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 128 ] state=tensor([[-0.4915, -0.1414, -0.0232, -0.2483]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4943, -0.3362, -0.0282,  0.0370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 129 ] state=tensor([[-0.4943, -0.3362, -0.0282,  0.0370]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5010, -0.5309, -0.0274,  0.3207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 130 ] state=tensor([[-0.5010, -0.5309, -0.0274,  0.3207]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5117, -0.7256, -0.0210,  0.6046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 131 ] state=tensor([[-0.5117, -0.7256, -0.0210,  0.6046]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5262, -0.5302, -0.0089,  0.3053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 132 ] state=tensor([[-0.5262, -0.5302, -0.0089,  0.3053]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5368, -0.3350, -0.0028,  0.0099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 133 ] state=tensor([[-0.5368, -0.3350, -0.0028,  0.0099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5435, -0.1398, -0.0026, -0.2837]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 134 ] state=tensor([[-0.5435, -0.1398, -0.0026, -0.2837]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5463, -0.3349, -0.0083,  0.0081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 135 ] state=tensor([[-0.5463, -0.3349, -0.0083,  0.0081]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5530, -0.5299, -0.0081,  0.2982]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 136 ] state=tensor([[-0.5530, -0.5299, -0.0081,  0.2982]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5636, -0.3347, -0.0022,  0.0029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 137 ] state=tensor([[-0.5636, -0.3347, -0.0022,  0.0029]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5703, -0.5298, -0.0021,  0.2949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 138 ] state=tensor([[-0.5703, -0.5298, -0.0021,  0.2949]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5809, -0.3346,  0.0038,  0.0016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 139 ] state=tensor([[-0.5809, -0.3346,  0.0038,  0.0016]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5876, -0.5298,  0.0038,  0.2955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 140 ] state=tensor([[-0.5876, -0.5298,  0.0038,  0.2955]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5982, -0.7250,  0.0097,  0.5893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 141 ] state=tensor([[-0.5982, -0.7250,  0.0097,  0.5893]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6127, -0.5300,  0.0215,  0.2997]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 142 ] state=tensor([[-0.6127, -0.5300,  0.0215,  0.2997]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6233, -0.3352,  0.0275,  0.0139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 143 ] state=tensor([[-0.6233, -0.3352,  0.0275,  0.0139]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6300, -0.5307,  0.0278,  0.3151]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 144 ] state=tensor([[-0.6300, -0.5307,  0.0278,  0.3151]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6406, -0.3360,  0.0341,  0.0313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 145 ] state=tensor([[-0.6406, -0.3360,  0.0341,  0.0313]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6473, -0.1413,  0.0347, -0.2504]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 146 ] state=tensor([[-0.6473, -0.1413,  0.0347, -0.2504]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6501, -0.3369,  0.0297,  0.0530]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 147 ] state=tensor([[-0.6501, -0.3369,  0.0297,  0.0530]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6569, -0.1423,  0.0308, -0.2301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 148 ] state=tensor([[-0.6569, -0.1423,  0.0308, -0.2301]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6597, -0.3378,  0.0262,  0.0721]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 149 ] state=tensor([[-0.6597, -0.3378,  0.0262,  0.0721]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6665, -0.1431,  0.0276, -0.2122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 150 ] state=tensor([[-0.6665, -0.1431,  0.0276, -0.2122]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6693, -0.3386,  0.0234,  0.0890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 151 ] state=tensor([[-0.6693, -0.3386,  0.0234,  0.0890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6761, -0.1438,  0.0251, -0.1962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 152 ] state=tensor([[-0.6761, -0.1438,  0.0251, -0.1962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6790, -0.3393,  0.0212,  0.1043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 153 ] state=tensor([[-0.6790, -0.3393,  0.0212,  0.1043]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6858, -0.1445,  0.0233, -0.1816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 154 ] state=tensor([[-0.6858, -0.1445,  0.0233, -0.1816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6886, -0.3399,  0.0197,  0.1183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 155 ] state=tensor([[-0.6886, -0.3399,  0.0197,  0.1183]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6954, -0.1451,  0.0220, -0.1681]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 128 ][ timestamp 156 ] state=tensor([[-0.6954, -0.1451,  0.0220, -0.1681]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6983, -0.3405,  0.0187,  0.1315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 157 ] state=tensor([[-0.6983, -0.3405,  0.0187,  0.1315]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7052, -0.1457,  0.0213, -0.1553]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 158 ] state=tensor([[-0.7052, -0.1457,  0.0213, -0.1553]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7081, -0.3411,  0.0182,  0.1441]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 159 ] state=tensor([[-0.7081, -0.3411,  0.0182,  0.1441]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7149, -0.1462,  0.0211, -0.1428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 160 ] state=tensor([[-0.7149, -0.1462,  0.0211, -0.1428]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7178,  0.0486,  0.0182, -0.4288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 161 ] state=tensor([[-0.7178,  0.0486,  0.0182, -0.4288]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7168, -0.1468,  0.0096, -0.1304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 162 ] state=tensor([[-0.7168, -0.1468,  0.0096, -0.1304]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7198,  0.0482,  0.0070, -0.4200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 163 ] state=tensor([[-0.7198,  0.0482,  0.0070, -0.4200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7188, -0.1470, -0.0014, -0.1251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 164 ] state=tensor([[-0.7188, -0.1470, -0.0014, -0.1251]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7217, -0.3421, -0.0039,  0.1671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 165 ] state=tensor([[-0.7217, -0.3421, -0.0039,  0.1671]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.2859e-01, -5.3719e-01, -5.2194e-04,  4.5858e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 166 ] state=tensor([[-7.2859e-01, -5.3719e-01, -5.2194e-04,  4.5858e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7393, -0.3421,  0.0086,  0.1657]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 167 ] state=tensor([[-0.7393, -0.3421,  0.0086,  0.1657]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7462, -0.1471,  0.0120, -0.1242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 168 ] state=tensor([[-0.7462, -0.1471,  0.0120, -0.1242]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7491, -0.3424,  0.0095,  0.1722]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 169 ] state=tensor([[-0.7491, -0.3424,  0.0095,  0.1722]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7560, -0.1474,  0.0129, -0.1175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 170 ] state=tensor([[-0.7560, -0.1474,  0.0129, -0.1175]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7589,  0.0476,  0.0106, -0.4060]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 171 ] state=tensor([[-0.7589,  0.0476,  0.0106, -0.4060]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7580, -0.1477,  0.0025, -0.1100]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 172 ] state=tensor([[-0.7580, -0.1477,  0.0025, -0.1100]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.6091e-01,  4.7384e-02,  2.5468e-04, -4.0194e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 173 ] state=tensor([[-7.6091e-01,  4.7384e-02,  2.5468e-04, -4.0194e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7600, -0.1477, -0.0078, -0.1092]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 174 ] state=tensor([[-0.7600, -0.1477, -0.0078, -0.1092]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7629,  0.0475, -0.0100, -0.4043]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 175 ] state=tensor([[-0.7629,  0.0475, -0.0100, -0.4043]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7620, -0.1475, -0.0181, -0.1148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 176 ] state=tensor([[-0.7620, -0.1475, -0.0181, -0.1148]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7649, -0.3423, -0.0203,  0.1722]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 177 ] state=tensor([[-0.7649, -0.3423, -0.0203,  0.1722]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7718, -0.5372, -0.0169,  0.4583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 178 ] state=tensor([[-0.7718, -0.5372, -0.0169,  0.4583]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7825, -0.3418, -0.0077,  0.1604]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 179 ] state=tensor([[-0.7825, -0.3418, -0.0077,  0.1604]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7893, -0.1466, -0.0045, -0.1347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 180 ] state=tensor([[-0.7893, -0.1466, -0.0045, -0.1347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7923, -0.3416, -0.0072,  0.1565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 181 ] state=tensor([[-0.7923, -0.3416, -0.0072,  0.1565]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7991, -0.1464, -0.0041, -0.1384]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 182 ] state=tensor([[-0.7991, -0.1464, -0.0041, -0.1384]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8020, -0.3415, -0.0069,  0.1530]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 183 ] state=tensor([[-0.8020, -0.3415, -0.0069,  0.1530]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8089, -0.5365, -0.0038,  0.4435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 184 ] state=tensor([[-0.8089, -0.5365, -0.0038,  0.4435]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8196, -0.3413,  0.0051,  0.1496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 185 ] state=tensor([[-0.8196, -0.3413,  0.0051,  0.1496]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8264, -0.1463,  0.0081, -0.1415]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 186 ] state=tensor([[-0.8264, -0.1463,  0.0081, -0.1415]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8294,  0.0487,  0.0052, -0.4316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 187 ] state=tensor([[-0.8294,  0.0487,  0.0052, -0.4316]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8284, -0.1465, -0.0034, -0.1373]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 188 ] state=tensor([[-0.8284, -0.1465, -0.0034, -0.1373]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8313, -0.3415, -0.0062,  0.1543]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 189 ] state=tensor([[-0.8313, -0.3415, -0.0062,  0.1543]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8381, -0.1463, -0.0031, -0.1403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 190 ] state=tensor([[-0.8381, -0.1463, -0.0031, -0.1403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8411, -0.3414, -0.0059,  0.1514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 191 ] state=tensor([[-0.8411, -0.3414, -0.0059,  0.1514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8479, -0.5364, -0.0028,  0.4422]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 192 ] state=tensor([[-0.8479, -0.5364, -0.0028,  0.4422]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8586, -0.3413,  0.0060,  0.1486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 193 ] state=tensor([[-0.8586, -0.3413,  0.0060,  0.1486]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8654, -0.5365,  0.0090,  0.4432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 194 ] state=tensor([[-0.8654, -0.5365,  0.0090,  0.4432]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8762, -0.3415,  0.0178,  0.1534]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 195 ] state=tensor([[-0.8762, -0.3415,  0.0178,  0.1534]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8830, -0.1466,  0.0209, -0.1336]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 196 ] state=tensor([[-0.8830, -0.1466,  0.0209, -0.1336]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8859, -0.3420,  0.0182,  0.1656]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 197 ] state=tensor([[-0.8859, -0.3420,  0.0182,  0.1656]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8928, -0.1472,  0.0215, -0.1213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 198 ] state=tensor([[-0.8928, -0.1472,  0.0215, -0.1213]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8957,  0.0476,  0.0191, -0.4071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 199 ] state=tensor([[-0.8957,  0.0476,  0.0191, -0.4071]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8948,  0.2425,  0.0110, -0.6937]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 200 ] state=tensor([[-0.8948,  0.2425,  0.0110, -0.6937]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8899,  0.0472, -0.0029, -0.3976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 201 ] state=tensor([[-0.8899,  0.0472, -0.0029, -0.3976]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8890, -0.1479, -0.0108, -0.1058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 202 ] state=tensor([[-0.8890, -0.1479, -0.0108, -0.1058]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8919, -0.3429, -0.0130,  0.1834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 203 ] state=tensor([[-0.8919, -0.3429, -0.0130,  0.1834]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8988, -0.5378, -0.0093,  0.4720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 204 ] state=tensor([[-0.8988, -0.5378, -0.0093,  0.4720]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-9.0955e-01, -3.4254e-01,  1.4249e-04,  1.7639e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 205 ] state=tensor([[-9.0955e-01, -3.4254e-01,  1.4249e-04,  1.7639e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9164, -0.1474,  0.0037, -0.1162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 206 ] state=tensor([[-0.9164, -0.1474,  0.0037, -0.1162]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9193, -0.3426,  0.0013,  0.1776]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 207 ] state=tensor([[-0.9193, -0.3426,  0.0013,  0.1776]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9262, -0.5377,  0.0049,  0.4707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 208 ] state=tensor([[-0.9262, -0.5377,  0.0049,  0.4707]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9370, -0.3427,  0.0143,  0.1796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 209 ] state=tensor([[-0.9370, -0.3427,  0.0143,  0.1796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9438, -0.5380,  0.0179,  0.4767]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 210 ] state=tensor([[-0.9438, -0.5380,  0.0179,  0.4767]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9546, -0.3431,  0.0274,  0.1897]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 211 ] state=tensor([[-0.9546, -0.3431,  0.0274,  0.1897]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9614, -0.1484,  0.0312, -0.0942]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 212 ] state=tensor([[-0.9614, -0.1484,  0.0312, -0.0942]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9644,  0.0462,  0.0293, -0.3768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 213 ] state=tensor([[-0.9644,  0.0462,  0.0293, -0.3768]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9635,  0.2409,  0.0218, -0.6601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 214 ] state=tensor([[-0.9635,  0.2409,  0.0218, -0.6601]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9587,  0.4357,  0.0086, -0.9459]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 215 ] state=tensor([[-0.9587,  0.4357,  0.0086, -0.9459]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9499,  0.2405, -0.0103, -0.6505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 216 ] state=tensor([[-0.9499,  0.2405, -0.0103, -0.6505]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9451,  0.0455, -0.0233, -0.3611]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 217 ] state=tensor([[-0.9451,  0.0455, -0.0233, -0.3611]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9442, -0.1493, -0.0305, -0.0758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 218 ] state=tensor([[-0.9442, -0.1493, -0.0305, -0.0758]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9472, -0.3439, -0.0321,  0.2071]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 219 ] state=tensor([[-0.9472, -0.3439, -0.0321,  0.2071]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9541, -0.1484, -0.0279, -0.0955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 220 ] state=tensor([[-0.9541, -0.1484, -0.0279, -0.0955]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9571, -0.3431, -0.0298,  0.1882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 221 ] state=tensor([[-0.9571, -0.3431, -0.0298,  0.1882]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9639, -0.5378, -0.0261,  0.4713]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 128 ][ timestamp 222 ] state=tensor([[-0.9639, -0.5378, -0.0261,  0.4713]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9747, -0.7325, -0.0166,  0.7557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 223 ] state=tensor([[-0.9747, -0.7325, -0.0166,  0.7557]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9893, -0.5372, -0.0015,  0.4578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 224 ] state=tensor([[-0.9893, -0.5372, -0.0015,  0.4578]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0001, -0.3420,  0.0076,  0.1647]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 225 ] state=tensor([[-1.0001, -0.3420,  0.0076,  0.1647]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0069, -0.5372,  0.0109,  0.4597]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 226 ] state=tensor([[-1.0069, -0.5372,  0.0109,  0.4597]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0176, -0.3423,  0.0201,  0.1705]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 227 ] state=tensor([[-1.0176, -0.3423,  0.0201,  0.1705]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0245, -0.1474,  0.0235, -0.1157]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 228 ] state=tensor([[-1.0245, -0.1474,  0.0235, -0.1157]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0274,  0.0473,  0.0212, -0.4009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 229 ] state=tensor([[-1.0274,  0.0473,  0.0212, -0.4009]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0265,  0.2421,  0.0132, -0.6868]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 230 ] state=tensor([[-1.0265,  0.2421,  0.0132, -0.6868]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0217e+00,  4.6844e-02, -5.3432e-04, -3.9002e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 231 ] state=tensor([[-1.0217e+00,  4.6844e-02, -5.3432e-04, -3.9002e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0207, -0.1483, -0.0083, -0.0975]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 232 ] state=tensor([[-1.0207, -0.1483, -0.0083, -0.0975]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0237,  0.0470, -0.0103, -0.3928]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 233 ] state=tensor([[-1.0237,  0.0470, -0.0103, -0.3928]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0227,  0.2422, -0.0181, -0.6887]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 234 ] state=tensor([[-1.0227,  0.2422, -0.0181, -0.6887]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0179,  0.0474, -0.0319, -0.4018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 235 ] state=tensor([[-1.0179,  0.0474, -0.0319, -0.4018]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0169, -0.1473, -0.0400, -0.1193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 236 ] state=tensor([[-1.0169, -0.1473, -0.0400, -0.1193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0199, -0.3418, -0.0423,  0.1605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 237 ] state=tensor([[-1.0199, -0.3418, -0.0423,  0.1605]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0267, -0.5363, -0.0391,  0.4395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 238 ] state=tensor([[-1.0267, -0.5363, -0.0391,  0.4395]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0375, -0.3406, -0.0303,  0.1347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 239 ] state=tensor([[-1.0375, -0.3406, -0.0303,  0.1347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0443, -0.5353, -0.0276,  0.4177]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 240 ] state=tensor([[-1.0443, -0.5353, -0.0276,  0.4177]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0550, -0.3398, -0.0193,  0.1164]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 241 ] state=tensor([[-1.0550, -0.3398, -0.0193,  0.1164]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0618, -0.5347, -0.0170,  0.4030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 242 ] state=tensor([[-1.0618, -0.5347, -0.0170,  0.4030]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0725, -0.3393, -0.0089,  0.1050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 243 ] state=tensor([[-1.0725, -0.3393, -0.0089,  0.1050]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0793, -0.1441, -0.0068, -0.1905]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 244 ] state=tensor([[-1.0793, -0.1441, -0.0068, -0.1905]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0821,  0.0512, -0.0106, -0.4853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 245 ] state=tensor([[-1.0821,  0.0512, -0.0106, -0.4853]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0811,  0.2464, -0.0203, -0.7813]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 246 ] state=tensor([[-1.0811,  0.2464, -0.0203, -0.7813]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0762,  0.0516, -0.0359, -0.4951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 247 ] state=tensor([[-1.0762,  0.0516, -0.0359, -0.4951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0752, -0.1430, -0.0458, -0.2139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 248 ] state=tensor([[-1.0752, -0.1430, -0.0458, -0.2139]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0780, -0.3374, -0.0501,  0.0639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 249 ] state=tensor([[-1.0780, -0.3374, -0.0501,  0.0639]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0848, -0.5318, -0.0488,  0.3404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 250 ] state=tensor([[-1.0848, -0.5318, -0.0488,  0.3404]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0954, -0.7262, -0.0420,  0.6173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 251 ] state=tensor([[-1.0954, -0.7262, -0.0420,  0.6173]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1099, -0.5305, -0.0297,  0.3117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 252 ] state=tensor([[-1.1099, -0.5305, -0.0297,  0.3117]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1205, -0.3350, -0.0235,  0.0098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 253 ] state=tensor([[-1.1205, -0.3350, -0.0235,  0.0098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1272, -0.5298, -0.0233,  0.2949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 254 ] state=tensor([[-1.1272, -0.5298, -0.0233,  0.2949]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1378, -0.3343, -0.0174, -0.0050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 255 ] state=tensor([[-1.1378, -0.3343, -0.0174, -0.0050]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1445, -0.1390, -0.0175, -0.3031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 256 ] state=tensor([[-1.1445, -0.1390, -0.0175, -0.3031]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1473, -0.3338, -0.0235, -0.0160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 257 ] state=tensor([[-1.1473, -0.3338, -0.0235, -0.0160]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1540, -0.5286, -0.0238,  0.2692]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 258 ] state=tensor([[-1.1540, -0.5286, -0.0238,  0.2692]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1645, -0.3331, -0.0185, -0.0309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 259 ] state=tensor([[-1.1645, -0.3331, -0.0185, -0.0309]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1712, -0.5280, -0.0191,  0.2559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 260 ] state=tensor([[-1.1712, -0.5280, -0.0191,  0.2559]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1818, -0.7228, -0.0140,  0.5425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 261 ] state=tensor([[-1.1818, -0.7228, -0.0140,  0.5425]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1962, -0.5275, -0.0031,  0.2454]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 262 ] state=tensor([[-1.1962, -0.5275, -0.0031,  0.2454]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2068, -0.3324,  0.0018, -0.0482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 263 ] state=tensor([[-1.2068, -0.3324,  0.0018, -0.0482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2134e+00, -1.3726e-01,  8.3091e-04, -3.4034e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 264 ] state=tensor([[-1.2134e+00, -1.3726e-01,  8.3091e-04, -3.4034e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2162,  0.0578, -0.0060, -0.6328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 265 ] state=tensor([[-1.2162,  0.0578, -0.0060, -0.6328]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2150, -0.1372, -0.0186, -0.3420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 266 ] state=tensor([[-1.2150, -0.1372, -0.0186, -0.3420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2177,  0.0582, -0.0255, -0.6405]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 267 ] state=tensor([[-1.2177,  0.0582, -0.0255, -0.6405]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2166, -0.1366, -0.0383, -0.3559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 268 ] state=tensor([[-1.2166, -0.1366, -0.0383, -0.3559]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2193, -0.3311, -0.0454, -0.0755]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 269 ] state=tensor([[-1.2193, -0.3311, -0.0454, -0.0755]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2259, -0.5256, -0.0469,  0.2025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 270 ] state=tensor([[-1.2259, -0.5256, -0.0469,  0.2025]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2364, -0.7200, -0.0429,  0.4800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 271 ] state=tensor([[-1.2364, -0.7200, -0.0429,  0.4800]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2508, -0.9145, -0.0333,  0.7589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 272 ] state=tensor([[-1.2508, -0.9145, -0.0333,  0.7589]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2691, -0.7189, -0.0181,  0.4559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 273 ] state=tensor([[-1.2691, -0.7189, -0.0181,  0.4559]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2835, -0.5235, -0.0090,  0.1576]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 274 ] state=tensor([[-1.2835, -0.5235, -0.0090,  0.1576]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2940, -0.3283, -0.0058, -0.1379]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 275 ] state=tensor([[-1.2940, -0.3283, -0.0058, -0.1379]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3006, -0.1331, -0.0086, -0.4324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 276 ] state=tensor([[-1.3006, -0.1331, -0.0086, -0.4324]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3032,  0.0622, -0.0172, -0.7278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 277 ] state=tensor([[-1.3032,  0.0622, -0.0172, -0.7278]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3020, -0.1327, -0.0318, -0.4406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 278 ] state=tensor([[-1.3020, -0.1327, -0.0318, -0.4406]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3046, -0.3274, -0.0406, -0.1581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 279 ] state=tensor([[-1.3046, -0.3274, -0.0406, -0.1581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3112, -0.1317, -0.0437, -0.4633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 280 ] state=tensor([[-1.3112, -0.1317, -0.0437, -0.4633]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3138, -0.3262, -0.0530, -0.1847]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 281 ] state=tensor([[-1.3138, -0.3262, -0.0530, -0.1847]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3203, -0.5205, -0.0567,  0.0908]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 282 ] state=tensor([[-1.3203, -0.5205, -0.0567,  0.0908]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3307, -0.7148, -0.0549,  0.3651]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 283 ] state=tensor([[-1.3307, -0.7148, -0.0549,  0.3651]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3450, -0.5189, -0.0476,  0.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 284 ] state=tensor([[-1.3450, -0.5189, -0.0476,  0.0556]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3554, -0.7133, -0.0465,  0.3329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 285 ] state=tensor([[-1.3554, -0.7133, -0.0465,  0.3329]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3697, -0.9078, -0.0398,  0.6106]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 286 ] state=tensor([[-1.3697, -0.9078, -0.0398,  0.6106]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3878, -1.1023, -0.0276,  0.8905]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 287 ] state=tensor([[-1.3878, -1.1023, -0.0276,  0.8905]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4099, -0.9068, -0.0098,  0.5892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 288 ] state=tensor([[-1.4099, -0.9068, -0.0098,  0.5892]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4280, -0.7116,  0.0020,  0.2935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 289 ] state=tensor([[-1.4280, -0.7116,  0.0020,  0.2935]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4422e+00, -5.1646e-01,  7.8574e-03,  1.4129e-03]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 290 ] state=tensor([[-1.4422e+00, -5.1646e-01,  7.8574e-03,  1.4129e-03]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4526, -0.3215,  0.0079, -0.2888]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 291 ] state=tensor([[-1.4526, -0.3215,  0.0079, -0.2888]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4590, -0.1264,  0.0021, -0.5790]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 128 ][ timestamp 292 ] state=tensor([[-1.4590, -0.1264,  0.0021, -0.5790]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4615,  0.0686, -0.0095, -0.8710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 293 ] state=tensor([[-1.4615,  0.0686, -0.0095, -0.8710]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4602, -0.1263, -0.0269, -0.5813]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 294 ] state=tensor([[-1.4602, -0.1263, -0.0269, -0.5813]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4627,  0.0691, -0.0385, -0.8823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 295 ] state=tensor([[-1.4627,  0.0691, -0.0385, -0.8823]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4613, -0.1254, -0.0562, -0.6020]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 296 ] state=tensor([[-1.4613, -0.1254, -0.0562, -0.6020]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4638, -0.3197, -0.0682, -0.3275]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 297 ] state=tensor([[-1.4638, -0.3197, -0.0682, -0.3275]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4702, -0.5138, -0.0748, -0.0571]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 298 ] state=tensor([[-1.4702, -0.5138, -0.0748, -0.0571]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4805, -0.7078, -0.0759,  0.2111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 299 ] state=tensor([[-1.4805, -0.7078, -0.0759,  0.2111]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4946, -0.9017, -0.0717,  0.4789]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 300 ] state=tensor([[-1.4946, -0.9017, -0.0717,  0.4789]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5127, -1.0958, -0.0621,  0.7482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 301 ] state=tensor([[-1.5127, -1.0958, -0.0621,  0.7482]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5346, -1.2900, -0.0471,  1.0207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 302 ] state=tensor([[-1.5346, -1.2900, -0.0471,  1.0207]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5604, -1.0943, -0.0267,  0.7136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 303 ] state=tensor([[-1.5604, -1.0943, -0.0267,  0.7136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5823, -0.8988, -0.0124,  0.4126]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 304 ] state=tensor([[-1.5823, -0.8988, -0.0124,  0.4126]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6003, -0.7035, -0.0042,  0.1160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 305 ] state=tensor([[-1.6003, -0.7035, -0.0042,  0.1160]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6143, -0.5083, -0.0019, -0.1780]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 306 ] state=tensor([[-1.6143, -0.5083, -0.0019, -0.1780]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6245, -0.3132, -0.0054, -0.4712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 307 ] state=tensor([[-1.6245, -0.3132, -0.0054, -0.4712]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6308, -0.1180, -0.0149, -0.7656]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 308 ] state=tensor([[-1.6308, -0.1180, -0.0149, -0.7656]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6331,  0.0773, -0.0302, -1.0630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 309 ] state=tensor([[-1.6331,  0.0773, -0.0302, -1.0630]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6316,  0.2729, -0.0514, -1.3650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 310 ] state=tensor([[-1.6316,  0.2729, -0.0514, -1.3650]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6261,  0.0784, -0.0787, -1.0888]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 311 ] state=tensor([[-1.6261,  0.0784, -0.0787, -1.0888]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6245, -0.1156, -0.1005, -0.8218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 312 ] state=tensor([[-1.6245, -0.1156, -0.1005, -0.8218]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6269, -0.3092, -0.1169, -0.5624]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 313 ] state=tensor([[-1.6269, -0.3092, -0.1169, -0.5624]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6330, -0.5025, -0.1282, -0.3087]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 314 ] state=tensor([[-1.6330, -0.5025, -0.1282, -0.3087]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6431, -0.6956, -0.1344, -0.0590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 315 ] state=tensor([[-1.6431, -0.6956, -0.1344, -0.0590]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6570, -0.8886, -0.1355,  0.1884]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 316 ] state=tensor([[-1.6570, -0.8886, -0.1355,  0.1884]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6748, -1.0815, -0.1318,  0.4355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 317 ] state=tensor([[-1.6748, -1.0815, -0.1318,  0.4355]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6964, -1.2745, -0.1231,  0.6839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 318 ] state=tensor([[-1.6964, -1.2745, -0.1231,  0.6839]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7219, -1.4678, -0.1094,  0.9354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 319 ] state=tensor([[-1.7219, -1.4678, -0.1094,  0.9354]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7512, -1.6612, -0.0907,  1.1919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 320 ] state=tensor([[-1.7512, -1.6612, -0.0907,  1.1919]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7845, -1.4651, -0.0668,  0.8722]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 321 ] state=tensor([[-1.7845, -1.4651, -0.0668,  0.8722]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8138, -1.2691, -0.0494,  0.5593]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 322 ] state=tensor([[-1.8138, -1.2691, -0.0494,  0.5593]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8392, -1.4635, -0.0382,  0.8360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 323 ] state=tensor([[-1.8392, -1.4635, -0.0382,  0.8360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8684, -1.2679, -0.0215,  0.5315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 324 ] state=tensor([[-1.8684, -1.2679, -0.0215,  0.5315]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8938, -1.0725, -0.0109,  0.2321]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 325 ] state=tensor([[-1.8938, -1.0725, -0.0109,  0.2321]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9152, -0.8772, -0.0062, -0.0639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 326 ] state=tensor([[-1.9152, -0.8772, -0.0062, -0.0639]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9328, -0.6820, -0.0075, -0.3586]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 327 ] state=tensor([[-1.9328, -0.6820, -0.0075, -0.3586]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9464, -0.4868, -0.0147, -0.6536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 328 ] state=tensor([[-1.9464, -0.4868, -0.0147, -0.6536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9562, -0.2914, -0.0277, -0.9509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 329 ] state=tensor([[-1.9562, -0.2914, -0.0277, -0.9509]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9620, -0.4862, -0.0468, -0.6670]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 330 ] state=tensor([[-1.9620, -0.4862, -0.0468, -0.6670]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9717, -0.2904, -0.0601, -0.9741]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 331 ] state=tensor([[-1.9717, -0.2904, -0.0601, -0.9741]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9775, -0.4847, -0.0796, -0.7009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 332 ] state=tensor([[-1.9775, -0.4847, -0.0796, -0.7009]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9872, -0.6786, -0.0936, -0.4342]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 333 ] state=tensor([[-1.9872, -0.6786, -0.0936, -0.4342]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0008, -0.8723, -0.1023, -0.1725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 334 ] state=tensor([[-2.0008, -0.8723, -0.1023, -0.1725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0182, -0.6759, -0.1057, -0.4956]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 335 ] state=tensor([[-2.0182, -0.6759, -0.1057, -0.4956]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0317, -0.8694, -0.1156, -0.2380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 336 ] state=tensor([[-2.0317, -0.8694, -0.1156, -0.2380]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0491, -1.0627, -0.1204,  0.0161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 337 ] state=tensor([[-2.0491, -1.0627, -0.1204,  0.0161]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0704, -1.2559, -0.1201,  0.2685]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 338 ] state=tensor([[-2.0704, -1.2559, -0.1201,  0.2685]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0955, -1.4491, -0.1147,  0.5210]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 339 ] state=tensor([[-2.0955, -1.4491, -0.1147,  0.5210]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1245, -1.6424, -0.1043,  0.7755]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 340 ] state=tensor([[-2.1245, -1.6424, -0.1043,  0.7755]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1573, -1.4460, -0.0888,  0.4519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 341 ] state=tensor([[-2.1573, -1.4460, -0.0888,  0.4519]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1863, -1.6398, -0.0797,  0.7153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 342 ] state=tensor([[-2.1863, -1.6398, -0.0797,  0.7153]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2190, -1.8337, -0.0654,  0.9818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 343 ] state=tensor([[-2.2190, -1.8337, -0.0654,  0.9818]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2557, -1.6378, -0.0458,  0.6694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 344 ] state=tensor([[-2.2557, -1.6378, -0.0458,  0.6694]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2885, -1.4421, -0.0324,  0.3626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 345 ] state=tensor([[-2.2885, -1.4421, -0.0324,  0.3626]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3173, -1.2465, -0.0252,  0.0599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 346 ] state=tensor([[-2.3173, -1.2465, -0.0252,  0.0599]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3423, -1.0510, -0.0240, -0.2406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 347 ] state=tensor([[-2.3423, -1.0510, -0.0240, -0.2406]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3633, -0.8556, -0.0288, -0.5408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 348 ] state=tensor([[-2.3633, -0.8556, -0.0288, -0.5408]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3804, -0.6601, -0.0396, -0.8424]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 349 ] state=tensor([[-2.3804, -0.6601, -0.0396, -0.8424]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.3936, -0.4644, -0.0564, -1.1473]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 128 ][ timestamp 350 ] state=tensor([[-2.3936, -0.4644, -0.0564, -1.1473]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 128: Exploration_rate=0.05. Score=350.\n",
      "[ episode 129 ] state=tensor([[-0.0270,  0.0148, -0.0301, -0.0074]])\n",
      "[ episode 129 ][ timestamp 1 ] state=tensor([[-0.0270,  0.0148, -0.0301, -0.0074]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0267,  0.2103, -0.0302, -0.3095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 2 ] state=tensor([[-0.0267,  0.2103, -0.0302, -0.3095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0225,  0.0156, -0.0364, -0.0265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 3 ] state=tensor([[-0.0225,  0.0156, -0.0364, -0.0265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0222,  0.2113, -0.0370, -0.3304]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 4 ] state=tensor([[-0.0222,  0.2113, -0.0370, -0.3304]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0180,  0.0167, -0.0436, -0.0496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 5 ] state=tensor([[-0.0180,  0.0167, -0.0436, -0.0496]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0177,  0.2124, -0.0446, -0.3557]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 6 ] state=tensor([[-0.0177,  0.2124, -0.0446, -0.3557]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0134,  0.0180, -0.0517, -0.0774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 7 ] state=tensor([[-0.0134,  0.0180, -0.0517, -0.0774]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0130,  0.2138, -0.0532, -0.3860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 8 ] state=tensor([[-0.0130,  0.2138, -0.0532, -0.3860]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0088,  0.0195, -0.0610, -0.1105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 9 ] state=tensor([[-0.0088,  0.0195, -0.0610, -0.1105]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0084,  0.2154, -0.0632, -0.4218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 10 ] state=tensor([[-0.0084,  0.2154, -0.0632, -0.4218]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0041,  0.0212, -0.0716, -0.1497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 11 ] state=tensor([[-0.0041,  0.0212, -0.0716, -0.1497]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0037, -0.1728, -0.0746,  0.1196]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 12 ] state=tensor([[-0.0037, -0.1728, -0.0746,  0.1196]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0071, -0.3668, -0.0722,  0.3878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 13 ] state=tensor([[-0.0071, -0.3668, -0.0722,  0.3878]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0144, -0.5608, -0.0644,  0.6569]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 129 ][ timestamp 14 ] state=tensor([[-0.0144, -0.5608, -0.0644,  0.6569]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0257, -0.3649, -0.0513,  0.3446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 15 ] state=tensor([[-0.0257, -0.3649, -0.0513,  0.3446]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0330, -0.5592, -0.0444,  0.6207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 16 ] state=tensor([[-0.0330, -0.5592, -0.0444,  0.6207]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0441, -0.3635, -0.0320,  0.3144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 17 ] state=tensor([[-0.0441, -0.3635, -0.0320,  0.3144]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0514, -0.5582, -0.0257,  0.5968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 18 ] state=tensor([[-0.0514, -0.5582, -0.0257,  0.5968]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0626, -0.3627, -0.0138,  0.2961]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 19 ] state=tensor([[-0.0626, -0.3627, -0.0138,  0.2961]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0698, -0.5576, -0.0079,  0.5844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 20 ] state=tensor([[-0.0698, -0.5576, -0.0079,  0.5844]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0810, -0.3624,  0.0038,  0.2893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 21 ] state=tensor([[-0.0810, -0.3624,  0.0038,  0.2893]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0882, -0.1673,  0.0096, -0.0022]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 22 ] state=tensor([[-0.0882, -0.1673,  0.0096, -0.0022]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0916,  0.0277,  0.0096, -0.2918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 23 ] state=tensor([[-0.0916,  0.0277,  0.0096, -0.2918]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0910,  0.2227,  0.0037, -0.5815]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 24 ] state=tensor([[-0.0910,  0.2227,  0.0037, -0.5815]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0866,  0.4177, -0.0079, -0.8730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 25 ] state=tensor([[-0.0866,  0.4177, -0.0079, -0.8730]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0782,  0.2227, -0.0254, -0.5828]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 26 ] state=tensor([[-0.0782,  0.2227, -0.0254, -0.5828]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0738,  0.0280, -0.0370, -0.2982]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 27 ] state=tensor([[-0.0738,  0.0280, -0.0370, -0.2982]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0732, -0.1666, -0.0430, -0.0174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 28 ] state=tensor([[-0.0732, -0.1666, -0.0430, -0.0174]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0765, -0.3611, -0.0433,  0.2614]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 29 ] state=tensor([[-0.0765, -0.3611, -0.0433,  0.2614]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0838, -0.1654, -0.0381, -0.0446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 30 ] state=tensor([[-0.0838, -0.1654, -0.0381, -0.0446]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0871,  0.0303, -0.0390, -0.3491]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 31 ] state=tensor([[-0.0871,  0.0303, -0.0390, -0.3491]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0865, -0.1643, -0.0460, -0.0689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 32 ] state=tensor([[-0.0865, -0.1643, -0.0460, -0.0689]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0897, -0.3587, -0.0473,  0.2089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 33 ] state=tensor([[-0.0897, -0.3587, -0.0473,  0.2089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0969, -0.5531, -0.0432,  0.4863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 34 ] state=tensor([[-0.0969, -0.5531, -0.0432,  0.4863]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1080, -0.3574, -0.0334,  0.1803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 35 ] state=tensor([[-0.1080, -0.3574, -0.0334,  0.1803]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1151, -0.5521, -0.0298,  0.4623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 36 ] state=tensor([[-0.1151, -0.5521, -0.0298,  0.4623]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1262, -0.3565, -0.0206,  0.1603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 37 ] state=tensor([[-0.1262, -0.3565, -0.0206,  0.1603]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1333, -0.1611, -0.0174, -0.1388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 38 ] state=tensor([[-0.1333, -0.1611, -0.0174, -0.1388]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1365, -0.3560, -0.0202,  0.1484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 39 ] state=tensor([[-0.1365, -0.3560, -0.0202,  0.1484]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1436, -0.5508, -0.0172,  0.4346]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 40 ] state=tensor([[-0.1436, -0.5508, -0.0172,  0.4346]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1547, -0.7457, -0.0085,  0.7219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 41 ] state=tensor([[-0.1547, -0.7457, -0.0085,  0.7219]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1696, -0.5504,  0.0059,  0.4265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 42 ] state=tensor([[-0.1696, -0.5504,  0.0059,  0.4265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1806, -0.3554,  0.0145,  0.1357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 43 ] state=tensor([[-0.1806, -0.3554,  0.0145,  0.1357]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1877, -0.5507,  0.0172,  0.4329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 44 ] state=tensor([[-0.1877, -0.5507,  0.0172,  0.4329]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1987, -0.7461,  0.0258,  0.7310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 45 ] state=tensor([[-0.1987, -0.7461,  0.0258,  0.7310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2136, -0.5513,  0.0405,  0.4465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 46 ] state=tensor([[-0.2136, -0.5513,  0.0405,  0.4465]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2246, -0.3568,  0.0494,  0.1669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 47 ] state=tensor([[-0.2246, -0.3568,  0.0494,  0.1669]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2318, -0.1624,  0.0527, -0.1098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 48 ] state=tensor([[-0.2318, -0.1624,  0.0527, -0.1098]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2350,  0.0319,  0.0505, -0.3854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 49 ] state=tensor([[-0.2350,  0.0319,  0.0505, -0.3854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2344,  0.2263,  0.0428, -0.6617]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 50 ] state=tensor([[-0.2344,  0.2263,  0.0428, -0.6617]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2299,  0.4208,  0.0296, -0.9406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 51 ] state=tensor([[-0.2299,  0.4208,  0.0296, -0.9406]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2215,  0.2253,  0.0108, -0.6388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 52 ] state=tensor([[-0.2215,  0.2253,  0.0108, -0.6388]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2169,  0.0300, -0.0020, -0.3427]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 53 ] state=tensor([[-0.2169,  0.0300, -0.0020, -0.3427]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2163,  0.2251, -0.0088, -0.6360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 54 ] state=tensor([[-0.2163,  0.2251, -0.0088, -0.6360]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2118,  0.0301, -0.0216, -0.3462]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 55 ] state=tensor([[-0.2118,  0.0301, -0.0216, -0.3462]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2112,  0.2256, -0.0285, -0.6456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 56 ] state=tensor([[-0.2112,  0.2256, -0.0285, -0.6456]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2067,  0.0308, -0.0414, -0.3620]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 57 ] state=tensor([[-0.2067,  0.0308, -0.0414, -0.3620]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2061, -0.1637, -0.0486, -0.0826]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 58 ] state=tensor([[-0.2061, -0.1637, -0.0486, -0.0826]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2094,  0.0321, -0.0503, -0.3903]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 59 ] state=tensor([[-0.2094,  0.0321, -0.0503, -0.3903]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2087, -0.1623, -0.0581, -0.1139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 60 ] state=tensor([[-0.2087, -0.1623, -0.0581, -0.1139]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2120,  0.0337, -0.0604, -0.4243]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 61 ] state=tensor([[-0.2120,  0.0337, -0.0604, -0.4243]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2113, -0.1606, -0.0689, -0.1512]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 62 ] state=tensor([[-0.2113, -0.1606, -0.0689, -0.1512]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2145, -0.3546, -0.0719,  0.1190]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 63 ] state=tensor([[-0.2145, -0.3546, -0.0719,  0.1190]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2216, -0.5487, -0.0695,  0.3881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 64 ] state=tensor([[-0.2216, -0.5487, -0.0695,  0.3881]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2326, -0.7427, -0.0617,  0.6581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 65 ] state=tensor([[-0.2326, -0.7427, -0.0617,  0.6581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2475, -0.5468, -0.0486,  0.3466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 66 ] state=tensor([[-0.2475, -0.5468, -0.0486,  0.3466]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2584, -0.7412, -0.0417,  0.6236]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 67 ] state=tensor([[-0.2584, -0.7412, -0.0417,  0.6236]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2732, -0.9357, -0.0292,  0.9029]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 68 ] state=tensor([[-0.2732, -0.9357, -0.0292,  0.9029]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2919, -0.7402, -0.0111,  0.6012]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 69 ] state=tensor([[-0.2919, -0.7402, -0.0111,  0.6012]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3067, -0.5449,  0.0009,  0.3050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 70 ] state=tensor([[-0.3067, -0.5449,  0.0009,  0.3050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3176, -0.7401,  0.0070,  0.5980]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 71 ] state=tensor([[-0.3176, -0.7401,  0.0070,  0.5980]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3324, -0.5450,  0.0190,  0.3075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 72 ] state=tensor([[-0.3324, -0.5450,  0.0190,  0.3075]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3433, -0.3502,  0.0251,  0.0209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 73 ] state=tensor([[-0.3433, -0.3502,  0.0251,  0.0209]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3503, -0.5457,  0.0255,  0.3214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 74 ] state=tensor([[-0.3503, -0.5457,  0.0255,  0.3214]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3612, -0.3509,  0.0320,  0.0368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 75 ] state=tensor([[-0.3612, -0.3509,  0.0320,  0.0368]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3683, -0.1563,  0.0327, -0.2456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 76 ] state=tensor([[-0.3683, -0.1563,  0.0327, -0.2456]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3714,  0.0384,  0.0278, -0.5278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 77 ] state=tensor([[-0.3714,  0.0384,  0.0278, -0.5278]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3706,  0.2331,  0.0172, -0.8116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 78 ] state=tensor([[-0.3706,  0.2331,  0.0172, -0.8116]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3660,  0.0377,  0.0010, -0.5135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 79 ] state=tensor([[-0.3660,  0.0377,  0.0010, -0.5135]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3652, -0.1574, -0.0093, -0.2205]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 80 ] state=tensor([[-0.3652, -0.1574, -0.0093, -0.2205]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3684,  0.0378, -0.0137, -0.5161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 81 ] state=tensor([[-0.3684,  0.0378, -0.0137, -0.5161]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3676, -0.1571, -0.0240, -0.2278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 82 ] state=tensor([[-0.3676, -0.1571, -0.0240, -0.2278]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3707,  0.0384, -0.0286, -0.5280]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 83 ] state=tensor([[-0.3707,  0.0384, -0.0286, -0.5280]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3700, -0.1563, -0.0391, -0.2444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 84 ] state=tensor([[-0.3700, -0.1563, -0.0391, -0.2444]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3731, -0.3509, -0.0440,  0.0357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 85 ] state=tensor([[-0.3731, -0.3509, -0.0440,  0.0357]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3801, -0.5453, -0.0433,  0.3142]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 129 ][ timestamp 86 ] state=tensor([[-0.3801, -0.5453, -0.0433,  0.3142]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3910, -0.3496, -0.0370,  0.0081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 87 ] state=tensor([[-0.3910, -0.3496, -0.0370,  0.0081]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3980, -0.5442, -0.0369,  0.2889]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 88 ] state=tensor([[-0.3980, -0.5442, -0.0369,  0.2889]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4089, -0.7388, -0.0311,  0.5697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 89 ] state=tensor([[-0.4089, -0.7388, -0.0311,  0.5697]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4237, -0.5432, -0.0197,  0.2674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 90 ] state=tensor([[-0.4237, -0.5432, -0.0197,  0.2674]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4345, -0.3478, -0.0143, -0.0314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 91 ] state=tensor([[-0.4345, -0.3478, -0.0143, -0.0314]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4415, -0.5427, -0.0150,  0.2567]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 92 ] state=tensor([[-0.4415, -0.5427, -0.0150,  0.2567]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4524, -0.3474, -0.0098, -0.0406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 93 ] state=tensor([[-0.4524, -0.3474, -0.0098, -0.0406]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4593, -0.5424, -0.0106,  0.2489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 94 ] state=tensor([[-0.4593, -0.5424, -0.0106,  0.2489]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4701, -0.3471, -0.0057, -0.0471]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 95 ] state=tensor([[-0.4701, -0.3471, -0.0057, -0.0471]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4771, -0.5422, -0.0066,  0.2438]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 96 ] state=tensor([[-0.4771, -0.5422, -0.0066,  0.2438]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4879, -0.7372, -0.0017,  0.5344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 97 ] state=tensor([[-0.4879, -0.7372, -0.0017,  0.5344]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5027, -0.5420,  0.0090,  0.2412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 98 ] state=tensor([[-0.5027, -0.5420,  0.0090,  0.2412]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5135, -0.3471,  0.0138, -0.0487]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 99 ] state=tensor([[-0.5135, -0.3471,  0.0138, -0.0487]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5205, -0.5424,  0.0128,  0.2483]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 100 ] state=tensor([[-0.5205, -0.5424,  0.0128,  0.2483]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5313, -0.3474,  0.0178, -0.0403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 101 ] state=tensor([[-0.5313, -0.3474,  0.0178, -0.0403]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5383, -0.1526,  0.0170, -0.3273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 102 ] state=tensor([[-0.5383, -0.1526,  0.0170, -0.3273]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5413,  0.0423,  0.0104, -0.6146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 103 ] state=tensor([[-0.5413,  0.0423,  0.0104, -0.6146]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5405, -0.1530, -0.0019, -0.3186]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 104 ] state=tensor([[-0.5405, -0.1530, -0.0019, -0.3186]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5435, -0.3481, -0.0082, -0.0265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 105 ] state=tensor([[-0.5435, -0.3481, -0.0082, -0.0265]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5505, -0.5431, -0.0088,  0.2635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 106 ] state=tensor([[-0.5505, -0.5431, -0.0088,  0.2635]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5613, -0.3478, -0.0035, -0.0319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 107 ] state=tensor([[-0.5613, -0.3478, -0.0035, -0.0319]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5683, -0.5429, -0.0041,  0.2597]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 108 ] state=tensor([[-0.5683, -0.5429, -0.0041,  0.2597]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5792, -0.3477,  0.0011, -0.0343]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 109 ] state=tensor([[-0.5792, -0.3477,  0.0011, -0.0343]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-5.8611e-01, -5.4284e-01,  3.7520e-04,  2.5872e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 110 ] state=tensor([[-5.8611e-01, -5.4284e-01,  3.7520e-04,  2.5872e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5970, -0.3477,  0.0055, -0.0338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 111 ] state=tensor([[-0.5970, -0.3477,  0.0055, -0.0338]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6039, -0.1527,  0.0049, -0.3248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 112 ] state=tensor([[-0.6039, -0.1527,  0.0049, -0.3248]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6070, -0.3479, -0.0016, -0.0306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 113 ] state=tensor([[-0.6070, -0.3479, -0.0016, -0.0306]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6139, -0.5430, -0.0022,  0.2616]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 114 ] state=tensor([[-0.6139, -0.5430, -0.0022,  0.2616]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6248, -0.7381,  0.0030,  0.5536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 115 ] state=tensor([[-0.6248, -0.7381,  0.0030,  0.5536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6396, -0.5430,  0.0141,  0.2619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 116 ] state=tensor([[-0.6396, -0.5430,  0.0141,  0.2619]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6504, -0.3481,  0.0193, -0.0264]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 117 ] state=tensor([[-0.6504, -0.3481,  0.0193, -0.0264]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6574, -0.5435,  0.0188,  0.2724]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 118 ] state=tensor([[-0.6574, -0.5435,  0.0188,  0.2724]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6682, -0.3486,  0.0242, -0.0143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 119 ] state=tensor([[-0.6682, -0.3486,  0.0242, -0.0143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6752, -0.1538,  0.0239, -0.2993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 120 ] state=tensor([[-0.6752, -0.1538,  0.0239, -0.2993]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6783, -0.3493,  0.0180,  0.0008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 121 ] state=tensor([[-0.6783, -0.3493,  0.0180,  0.0008]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6853, -0.1544,  0.0180, -0.2861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 122 ] state=tensor([[-0.6853, -0.1544,  0.0180, -0.2861]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6884, -0.3498,  0.0122,  0.0122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 123 ] state=tensor([[-0.6884, -0.3498,  0.0122,  0.0122]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6954, -0.1549,  0.0125, -0.2766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 124 ] state=tensor([[-0.6954, -0.1549,  0.0125, -0.2766]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6985, -0.3502,  0.0070,  0.0200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 125 ] state=tensor([[-0.6985, -0.3502,  0.0070,  0.0200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7055, -0.5454,  0.0074,  0.3149]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 126 ] state=tensor([[-0.7055, -0.5454,  0.0074,  0.3149]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7164, -0.3504,  0.0137,  0.0245]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 127 ] state=tensor([[-0.7164, -0.3504,  0.0137,  0.0245]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7234, -0.1554,  0.0141, -0.2638]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 128 ] state=tensor([[-0.7234, -0.1554,  0.0141, -0.2638]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7265, -0.3508,  0.0089,  0.0333]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 129 ] state=tensor([[-0.7265, -0.3508,  0.0089,  0.0333]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7335, -0.5460,  0.0095,  0.3287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 130 ] state=tensor([[-0.7335, -0.5460,  0.0095,  0.3287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7444, -0.3510,  0.0161,  0.0391]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 131 ] state=tensor([[-0.7444, -0.3510,  0.0161,  0.0391]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7514, -0.5464,  0.0169,  0.3368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 132 ] state=tensor([[-0.7514, -0.5464,  0.0169,  0.3368]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7624, -0.3515,  0.0236,  0.0495]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 133 ] state=tensor([[-0.7624, -0.3515,  0.0236,  0.0495]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7694, -0.1567,  0.0246, -0.2356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 134 ] state=tensor([[-0.7694, -0.1567,  0.0246, -0.2356]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7725, -0.3522,  0.0199,  0.0647]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 135 ] state=tensor([[-0.7725, -0.3522,  0.0199,  0.0647]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7796, -0.1574,  0.0212, -0.2216]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 136 ] state=tensor([[-0.7796, -0.1574,  0.0212, -0.2216]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7827,  0.0375,  0.0168, -0.5076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 137 ] state=tensor([[-0.7827,  0.0375,  0.0168, -0.5076]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7820,  0.2323,  0.0066, -0.7949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 138 ] state=tensor([[-0.7820,  0.2323,  0.0066, -0.7949]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7773,  0.0371, -0.0093, -0.5001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 139 ] state=tensor([[-0.7773,  0.0371, -0.0093, -0.5001]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7766, -0.1579, -0.0193, -0.2104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 140 ] state=tensor([[-0.7766, -0.1579, -0.0193, -0.2104]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7797, -0.3527, -0.0235,  0.0761]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 141 ] state=tensor([[-0.7797, -0.3527, -0.0235,  0.0761]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7868, -0.5475, -0.0220,  0.3613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 142 ] state=tensor([[-0.7868, -0.5475, -0.0220,  0.3613]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7977, -0.3521, -0.0147,  0.0618]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 143 ] state=tensor([[-0.7977, -0.3521, -0.0147,  0.0618]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8048, -0.1567, -0.0135, -0.2355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 144 ] state=tensor([[-0.8048, -0.1567, -0.0135, -0.2355]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8079, -0.3517, -0.0182,  0.0529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 145 ] state=tensor([[-0.8079, -0.3517, -0.0182,  0.0529]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8150, -0.1563, -0.0172, -0.2455]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 146 ] state=tensor([[-0.8150, -0.1563, -0.0172, -0.2455]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8181, -0.3511, -0.0221,  0.0417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 147 ] state=tensor([[-0.8181, -0.3511, -0.0221,  0.0417]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8251, -0.5459, -0.0212,  0.3274]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 148 ] state=tensor([[-0.8251, -0.5459, -0.0212,  0.3274]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8360, -0.3505, -0.0147,  0.0281]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 149 ] state=tensor([[-0.8360, -0.3505, -0.0147,  0.0281]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8430, -0.5454, -0.0141,  0.3161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 150 ] state=tensor([[-0.8430, -0.5454, -0.0141,  0.3161]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8539, -0.3501, -0.0078,  0.0190]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 151 ] state=tensor([[-0.8539, -0.3501, -0.0078,  0.0190]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8609, -0.5451, -0.0074,  0.3092]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 152 ] state=tensor([[-0.8609, -0.5451, -0.0074,  0.3092]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8718, -0.3499, -0.0012,  0.0142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 153 ] state=tensor([[-0.8718, -0.3499, -0.0012,  0.0142]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8788, -0.5450, -0.0010,  0.3064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 154 ] state=tensor([[-0.8788, -0.5450, -0.0010,  0.3064]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8897, -0.3499,  0.0052,  0.0135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 155 ] state=tensor([[-0.8897, -0.3499,  0.0052,  0.0135]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8967, -0.1548,  0.0054, -0.2776]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 129 ][ timestamp 156 ] state=tensor([[-0.8967, -0.1548,  0.0054, -0.2776]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-8.9984e-01, -3.5002e-01, -1.1424e-04,  1.6808e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 157 ] state=tensor([[-8.9984e-01, -3.5002e-01, -1.1424e-04,  1.6808e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-9.0684e-01, -1.5489e-01,  2.2192e-04, -2.7591e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 158 ] state=tensor([[-9.0684e-01, -1.5489e-01,  2.2192e-04, -2.7591e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9099,  0.0402, -0.0053, -0.5685]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 159 ] state=tensor([[-0.9099,  0.0402, -0.0053, -0.5685]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9091, -0.1548, -0.0167, -0.2775]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 160 ] state=tensor([[-0.9091, -0.1548, -0.0167, -0.2775]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9122,  0.0405, -0.0222, -0.5754]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 161 ] state=tensor([[-0.9122,  0.0405, -0.0222, -0.5754]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9114, -0.1543, -0.0337, -0.2898]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 162 ] state=tensor([[-0.9114, -0.1543, -0.0337, -0.2898]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9145, -0.3489, -0.0395, -0.0079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 163 ] state=tensor([[-0.9145, -0.3489, -0.0395, -0.0079]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9215, -0.5434, -0.0397,  0.2720]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 164 ] state=tensor([[-0.9215, -0.5434, -0.0397,  0.2720]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9324, -0.3478, -0.0342, -0.0329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 165 ] state=tensor([[-0.9324, -0.3478, -0.0342, -0.0329]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9393, -0.5424, -0.0349,  0.2488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 166 ] state=tensor([[-0.9393, -0.5424, -0.0349,  0.2488]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9502, -0.3468, -0.0299, -0.0547]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 167 ] state=tensor([[-0.9502, -0.3468, -0.0299, -0.0547]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9571, -0.5415, -0.0310,  0.2284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 168 ] state=tensor([[-0.9571, -0.5415, -0.0310,  0.2284]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9679, -0.7361, -0.0264,  0.5111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 169 ] state=tensor([[-0.9679, -0.7361, -0.0264,  0.5111]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9826, -0.5406, -0.0162,  0.2102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 170 ] state=tensor([[-0.9826, -0.5406, -0.0162,  0.2102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9935, -0.3453, -0.0120, -0.0875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 171 ] state=tensor([[-0.9935, -0.3453, -0.0120, -0.0875]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0004, -0.1500, -0.0138, -0.3840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 172 ] state=tensor([[-1.0004, -0.1500, -0.0138, -0.3840]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0034, -0.3449, -0.0215, -0.0957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 173 ] state=tensor([[-1.0034, -0.3449, -0.0215, -0.0957]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0103, -0.1495, -0.0234, -0.3951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 174 ] state=tensor([[-1.0103, -0.1495, -0.0234, -0.3951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0132, -0.3443, -0.0313, -0.1098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 175 ] state=tensor([[-1.0132, -0.3443, -0.0313, -0.1098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0201, -0.5389, -0.0335,  0.1728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 176 ] state=tensor([[-1.0201, -0.5389, -0.0335,  0.1728]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0309, -0.7336, -0.0300,  0.4548]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 177 ] state=tensor([[-1.0309, -0.7336, -0.0300,  0.4548]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0456, -0.9283, -0.0209,  0.7378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 178 ] state=tensor([[-1.0456, -0.9283, -0.0209,  0.7378]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0641, -0.7328, -0.0062,  0.4387]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 179 ] state=tensor([[-1.0641, -0.7328, -0.0062,  0.4387]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0788, -0.5376,  0.0026,  0.1440]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 180 ] state=tensor([[-1.0788, -0.5376,  0.0026,  0.1440]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0896, -0.3426,  0.0055, -0.1478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 181 ] state=tensor([[-1.0896, -0.3426,  0.0055, -0.1478]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0964, -0.5378,  0.0025,  0.1466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 182 ] state=tensor([[-1.0964, -0.5378,  0.0025,  0.1466]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1072, -0.3427,  0.0055, -0.1453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 183 ] state=tensor([[-1.1072, -0.3427,  0.0055, -0.1453]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1140, -0.1476,  0.0026, -0.4362]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 184 ] state=tensor([[-1.1140, -0.1476,  0.0026, -0.4362]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1170, -0.3428, -0.0062, -0.1428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 185 ] state=tensor([[-1.1170, -0.3428, -0.0062, -0.1428]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1238, -0.1476, -0.0090, -0.4374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 186 ] state=tensor([[-1.1238, -0.1476, -0.0090, -0.4374]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1268, -0.3426, -0.0178, -0.1475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 187 ] state=tensor([[-1.1268, -0.3426, -0.0178, -0.1475]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1336, -0.5374, -0.0207,  0.1395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 188 ] state=tensor([[-1.1336, -0.5374, -0.0207,  0.1395]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1444, -0.7323, -0.0179,  0.4256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 189 ] state=tensor([[-1.1444, -0.7323, -0.0179,  0.4256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1590, -0.5369, -0.0094,  0.1273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 190 ] state=tensor([[-1.1590, -0.5369, -0.0094,  0.1273]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1698, -0.7319, -0.0069,  0.4170]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 191 ] state=tensor([[-1.1698, -0.7319, -0.0069,  0.4170]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1844, -0.5366,  0.0015,  0.1221]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 192 ] state=tensor([[-1.1844, -0.5366,  0.0015,  0.1221]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1951, -0.3415,  0.0039, -0.1701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 193 ] state=tensor([[-1.1951, -0.3415,  0.0039, -0.1701]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2020e+00, -1.4648e-01,  5.1607e-04, -4.6152e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 194 ] state=tensor([[-1.2020e+00, -1.4648e-01,  5.1607e-04, -4.6152e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2049,  0.0486, -0.0087, -0.7540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 195 ] state=tensor([[-1.2049,  0.0486, -0.0087, -0.7540]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2039, -0.1464, -0.0238, -0.4641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 196 ] state=tensor([[-1.2039, -0.1464, -0.0238, -0.4641]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2068,  0.0491, -0.0331, -0.7642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 197 ] state=tensor([[-1.2068,  0.0491, -0.0331, -0.7642]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2059, -0.1456, -0.0484, -0.4821]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 198 ] state=tensor([[-1.2059, -0.1456, -0.0484, -0.4821]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2088, -0.3400, -0.0580, -0.2050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 199 ] state=tensor([[-1.2088, -0.3400, -0.0580, -0.2050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2156, -0.5342, -0.0621,  0.0688]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 200 ] state=tensor([[-1.2156, -0.5342, -0.0621,  0.0688]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2263, -0.7284, -0.0607,  0.3412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 201 ] state=tensor([[-1.2263, -0.7284, -0.0607,  0.3412]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2408, -0.5325, -0.0539,  0.0301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 202 ] state=tensor([[-1.2408, -0.5325, -0.0539,  0.0301]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2515, -0.7268, -0.0533,  0.3053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 203 ] state=tensor([[-1.2515, -0.7268, -0.0533,  0.3053]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2660, -0.9211, -0.0472,  0.5807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 204 ] state=tensor([[-1.2660, -0.9211, -0.0472,  0.5807]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2844, -0.7254, -0.0356,  0.2735]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 205 ] state=tensor([[-1.2844, -0.7254, -0.0356,  0.2735]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2989, -0.5297, -0.0301, -0.0302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 206 ] state=tensor([[-1.2989, -0.5297, -0.0301, -0.0302]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3095, -0.3342, -0.0307, -0.3322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 207 ] state=tensor([[-1.3095, -0.3342, -0.0307, -0.3322]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3162, -0.5289, -0.0374, -0.0494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 208 ] state=tensor([[-1.3162, -0.5289, -0.0374, -0.0494]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3268, -0.7234, -0.0384,  0.2313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 209 ] state=tensor([[-1.3268, -0.7234, -0.0384,  0.2313]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3413, -0.9180, -0.0337,  0.5116]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 210 ] state=tensor([[-1.3413, -0.9180, -0.0337,  0.5116]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3596, -0.7224, -0.0235,  0.2085]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 211 ] state=tensor([[-1.3596, -0.7224, -0.0235,  0.2085]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3741, -0.5270, -0.0193, -0.0915]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 212 ] state=tensor([[-1.3741, -0.5270, -0.0193, -0.0915]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3846, -0.7218, -0.0212,  0.1950]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 213 ] state=tensor([[-1.3846, -0.7218, -0.0212,  0.1950]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3991, -0.9166, -0.0173,  0.4810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 214 ] state=tensor([[-1.3991, -0.9166, -0.0173,  0.4810]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4174, -0.7213, -0.0076,  0.1829]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 215 ] state=tensor([[-1.4174, -0.7213, -0.0076,  0.1829]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4318, -0.5260, -0.0040, -0.1122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 216 ] state=tensor([[-1.4318, -0.5260, -0.0040, -0.1122]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4423, -0.7211, -0.0062,  0.1792]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 217 ] state=tensor([[-1.4423, -0.7211, -0.0062,  0.1792]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4568, -0.5259, -0.0026, -0.1154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 218 ] state=tensor([[-1.4568, -0.5259, -0.0026, -0.1154]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4673, -0.3307, -0.0049, -0.4089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 219 ] state=tensor([[-1.4673, -0.3307, -0.0049, -0.4089]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4739, -0.1355, -0.0131, -0.7032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 220 ] state=tensor([[-1.4739, -0.1355, -0.0131, -0.7032]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4766, -0.3305, -0.0272, -0.4146]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 221 ] state=tensor([[-1.4766, -0.3305, -0.0272, -0.4146]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4832, -0.1350, -0.0355, -0.7158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 222 ] state=tensor([[-1.4832, -0.1350, -0.0355, -0.7158]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4859, -0.3296, -0.0498, -0.4344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 223 ] state=tensor([[-1.4859, -0.3296, -0.0498, -0.4344]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4925, -0.5240, -0.0585, -0.1579]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 224 ] state=tensor([[-1.4925, -0.5240, -0.0585, -0.1579]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5030, -0.3281, -0.0616, -0.4684]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 225 ] state=tensor([[-1.5030, -0.3281, -0.0616, -0.4684]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5095, -0.5223, -0.0710, -0.1958]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 226 ] state=tensor([[-1.5095, -0.5223, -0.0710, -0.1958]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5200, -0.7163, -0.0749,  0.0737]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 129 ][ timestamp 227 ] state=tensor([[-1.5200, -0.7163, -0.0749,  0.0737]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5343, -0.9103, -0.0734,  0.3418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 228 ] state=tensor([[-1.5343, -0.9103, -0.0734,  0.3418]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5525, -1.1043, -0.0666,  0.6105]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 229 ] state=tensor([[-1.5525, -1.1043, -0.0666,  0.6105]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5746, -1.2984, -0.0544,  0.8815]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 230 ] state=tensor([[-1.5746, -1.2984, -0.0544,  0.8815]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6006, -1.4927, -0.0368,  1.1565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 231 ] state=tensor([[-1.6006, -1.4927, -0.0368,  1.1565]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6304, -1.2972, -0.0136,  0.8526]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 232 ] state=tensor([[-1.6304, -1.2972, -0.0136,  0.8526]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6564, -1.1019,  0.0034,  0.5556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 233 ] state=tensor([[-1.6564, -1.1019,  0.0034,  0.5556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6784, -0.9068,  0.0145,  0.2640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 234 ] state=tensor([[-1.6784, -0.9068,  0.0145,  0.2640]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6965, -0.7119,  0.0198, -0.0240]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 235 ] state=tensor([[-1.6965, -0.7119,  0.0198, -0.0240]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7108, -0.5170,  0.0193, -0.3104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 236 ] state=tensor([[-1.7108, -0.5170,  0.0193, -0.3104]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7211, -0.3222,  0.0131, -0.5969]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 237 ] state=tensor([[-1.7211, -0.3222,  0.0131, -0.5969]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7276e+00, -1.2726e-01,  1.1739e-03, -8.8547e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 238 ] state=tensor([[-1.7276e+00, -1.2726e-01,  1.1739e-03, -8.8547e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7301,  0.0678, -0.0165, -1.1778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 239 ] state=tensor([[-1.7301,  0.0678, -0.0165, -1.1778]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7287,  0.2632, -0.0401, -1.4756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 240 ] state=tensor([[-1.7287,  0.2632, -0.0401, -1.4756]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7235,  0.0686, -0.0696, -1.1957]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 241 ] state=tensor([[-1.7235,  0.0686, -0.0696, -1.1957]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7221, -0.1256, -0.0935, -0.9256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 242 ] state=tensor([[-1.7221, -0.1256, -0.0935, -0.9256]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7246, -0.3193, -0.1120, -0.6637]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 243 ] state=tensor([[-1.7246, -0.3193, -0.1120, -0.6637]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7310, -0.5127, -0.1253, -0.4083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 244 ] state=tensor([[-1.7310, -0.5127, -0.1253, -0.4083]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7413, -0.7059, -0.1335, -0.1576]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 245 ] state=tensor([[-1.7413, -0.7059, -0.1335, -0.1576]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7554, -0.8989, -0.1366,  0.0902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 246 ] state=tensor([[-1.7554, -0.8989, -0.1366,  0.0902]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7734, -1.0918, -0.1348,  0.3368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 247 ] state=tensor([[-1.7734, -1.0918, -0.1348,  0.3368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7952, -1.2848, -0.1281,  0.5841]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 248 ] state=tensor([[-1.7952, -1.2848, -0.1281,  0.5841]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8209, -1.4779, -0.1164,  0.8339]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 249 ] state=tensor([[-1.8209, -1.4779, -0.1164,  0.8339]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8504, -1.6712, -0.0997,  1.0878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 250 ] state=tensor([[-1.8504, -1.6712, -0.0997,  1.0878]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8839, -1.8649, -0.0780,  1.3476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 251 ] state=tensor([[-1.8839, -1.8649, -0.0780,  1.3476]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9212, -1.6689, -0.0510,  1.0316]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 252 ] state=tensor([[-1.9212, -1.6689, -0.0510,  1.0316]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9545, -1.4731, -0.0304,  0.7233]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 253 ] state=tensor([[-1.9545, -1.4731, -0.0304,  0.7233]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9840, -1.6678, -0.0159,  1.0063]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 254 ] state=tensor([[-1.9840, -1.6678, -0.0159,  1.0063]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0174, -1.4725,  0.0042,  0.7086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 255 ] state=tensor([[-2.0174, -1.4725,  0.0042,  0.7086]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0468, -1.2774,  0.0184,  0.4173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 256 ] state=tensor([[-2.0468, -1.2774,  0.0184,  0.4173]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0724, -1.0826,  0.0267,  0.1305]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 257 ] state=tensor([[-2.0724, -1.0826,  0.0267,  0.1305]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0940, -0.8878,  0.0293, -0.1537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 258 ] state=tensor([[-2.0940, -0.8878,  0.0293, -0.1537]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1118, -0.6931,  0.0263, -0.4370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 259 ] state=tensor([[-2.1118, -0.6931,  0.0263, -0.4370]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1256, -0.4984,  0.0175, -0.7213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 260 ] state=tensor([[-2.1256, -0.4984,  0.0175, -0.7213]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1356, -0.3035,  0.0031, -1.0084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 261 ] state=tensor([[-2.1356, -0.3035,  0.0031, -1.0084]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1417, -0.1085, -0.0171, -1.3001]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 262 ] state=tensor([[-2.1417, -0.1085, -0.0171, -1.3001]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1438,  0.0869, -0.0431, -1.5981]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 263 ] state=tensor([[-2.1438,  0.0869, -0.0431, -1.5981]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1421, -0.1077, -0.0750, -1.3191]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 264 ] state=tensor([[-2.1421, -0.1077, -0.0750, -1.3191]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1443, -0.3018, -0.1014, -1.0508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 265 ] state=tensor([[-2.1443, -0.3018, -0.1014, -1.0508]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1503, -0.1055, -0.1224, -1.3735]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 266 ] state=tensor([[-2.1503, -0.1055, -0.1224, -1.3735]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1524,  0.0909, -0.1499, -1.7019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 267 ] state=tensor([[-2.1524,  0.0909, -0.1499, -1.7019]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1506,  0.2874, -0.1839, -2.0372]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 129 ][ timestamp 268 ] state=tensor([[-2.1506,  0.2874, -0.1839, -2.0372]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 129: Exploration_rate=0.05. Score=268.\n",
      "[ episode 130 ] state=tensor([[ 0.0381, -0.0421, -0.0036, -0.0109]])\n",
      "[ episode 130 ][ timestamp 1 ] state=tensor([[ 0.0381, -0.0421, -0.0036, -0.0109]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0373,  0.1531, -0.0038, -0.3047]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 2 ] state=tensor([[ 0.0373,  0.1531, -0.0038, -0.3047]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0403,  0.3483, -0.0099, -0.5986]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 3 ] state=tensor([[ 0.0403,  0.3483, -0.0099, -0.5986]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0473,  0.1533, -0.0219, -0.3091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 4 ] state=tensor([[ 0.0473,  0.1533, -0.0219, -0.3091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0504, -0.0415, -0.0281, -0.0234]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 5 ] state=tensor([[ 0.0504, -0.0415, -0.0281, -0.0234]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0495,  0.1540, -0.0285, -0.3248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 6 ] state=tensor([[ 0.0495,  0.1540, -0.0285, -0.3248]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0526, -0.0407, -0.0350, -0.0412]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 7 ] state=tensor([[ 0.0526, -0.0407, -0.0350, -0.0412]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0518,  0.1549, -0.0358, -0.3447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 8 ] state=tensor([[ 0.0518,  0.1549, -0.0358, -0.3447]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0549,  0.3505, -0.0427, -0.6485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 9 ] state=tensor([[ 0.0549,  0.3505, -0.0427, -0.6485]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0619,  0.1560, -0.0557, -0.3696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 10 ] state=tensor([[ 0.0619,  0.1560, -0.0557, -0.3696]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0650, -0.0383, -0.0631, -0.0950]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 11 ] state=tensor([[ 0.0650, -0.0383, -0.0631, -0.0950]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0643, -0.2324, -0.0650,  0.1772]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 12 ] state=tensor([[ 0.0643, -0.2324, -0.0650,  0.1772]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0596, -0.4266, -0.0615,  0.4486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 13 ] state=tensor([[ 0.0596, -0.4266, -0.0615,  0.4486]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0511, -0.2306, -0.0525,  0.1372]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 14 ] state=tensor([[ 0.0511, -0.2306, -0.0525,  0.1372]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0465, -0.4250, -0.0497,  0.4129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 15 ] state=tensor([[ 0.0465, -0.4250, -0.0497,  0.4129]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0380, -0.2292, -0.0415,  0.1050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 16 ] state=tensor([[ 0.0380, -0.2292, -0.0415,  0.1050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0334, -0.4237, -0.0394,  0.3843]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 17 ] state=tensor([[ 0.0334, -0.4237, -0.0394,  0.3843]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0249, -0.6182, -0.0317,  0.6643]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 18 ] state=tensor([[ 0.0249, -0.6182, -0.0317,  0.6643]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0126, -0.8129, -0.0184,  0.9468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 19 ] state=tensor([[ 0.0126, -0.8129, -0.0184,  0.9468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.7046e-03, -6.1751e-01,  5.2709e-04,  6.4843e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 20 ] state=tensor([[-3.7046e-03, -6.1751e-01,  5.2709e-04,  6.4843e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0161, -0.4224,  0.0135,  0.3559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 21 ] state=tensor([[-0.0161, -0.4224,  0.0135,  0.3559]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0245, -0.2275,  0.0206,  0.0675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 22 ] state=tensor([[-0.0245, -0.2275,  0.0206,  0.0675]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0291, -0.4229,  0.0220,  0.3666]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 23 ] state=tensor([[-0.0291, -0.4229,  0.0220,  0.3666]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0375, -0.2281,  0.0293,  0.0810]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 24 ] state=tensor([[-0.0375, -0.2281,  0.0293,  0.0810]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0421, -0.4236,  0.0309,  0.3827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 25 ] state=tensor([[-0.0421, -0.4236,  0.0309,  0.3827]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0505, -0.2289,  0.0386,  0.1000]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 26 ] state=tensor([[-0.0505, -0.2289,  0.0386,  0.1000]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0551, -0.0344,  0.0406, -0.1803]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 27 ] state=tensor([[-0.0551, -0.0344,  0.0406, -0.1803]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0558,  0.1601,  0.0370, -0.4599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 28 ] state=tensor([[-0.0558,  0.1601,  0.0370, -0.4599]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0526, -0.0355,  0.0278, -0.1558]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 130 ][ timestamp 29 ] state=tensor([[-0.0526, -0.0355,  0.0278, -0.1558]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0533,  0.1592,  0.0246, -0.4396]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 30 ] state=tensor([[-0.0533,  0.1592,  0.0246, -0.4396]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0501, -0.0362,  0.0159, -0.1393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 31 ] state=tensor([[-0.0501, -0.0362,  0.0159, -0.1393]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0509,  0.1586,  0.0131, -0.4269]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 32 ] state=tensor([[-0.0509,  0.1586,  0.0131, -0.4269]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0477, -0.0367,  0.0045, -0.1301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 33 ] state=tensor([[-0.0477, -0.0367,  0.0045, -0.1301]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0484,  0.1584,  0.0019, -0.4214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 34 ] state=tensor([[-0.0484,  0.1584,  0.0019, -0.4214]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0453,  0.3535, -0.0065, -0.7135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 35 ] state=tensor([[-0.0453,  0.3535, -0.0065, -0.7135]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0382,  0.1585, -0.0208, -0.4228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 36 ] state=tensor([[-0.0382,  0.1585, -0.0208, -0.4228]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0350, -0.0364, -0.0292, -0.1368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 37 ] state=tensor([[-0.0350, -0.0364, -0.0292, -0.1368]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0357,  0.1592, -0.0320, -0.4385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 38 ] state=tensor([[-0.0357,  0.1592, -0.0320, -0.4385]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0326, -0.0355, -0.0407, -0.1561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 39 ] state=tensor([[-0.0326, -0.0355, -0.0407, -0.1561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0333,  0.1602, -0.0439, -0.4613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 40 ] state=tensor([[-0.0333,  0.1602, -0.0439, -0.4613]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0301, -0.0343, -0.0531, -0.1828]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 41 ] state=tensor([[-0.0301, -0.0343, -0.0531, -0.1828]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0307, -0.2286, -0.0567,  0.0927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 42 ] state=tensor([[-0.0307, -0.2286, -0.0567,  0.0927]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0353, -0.4229, -0.0549,  0.3669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 43 ] state=tensor([[-0.0353, -0.4229, -0.0549,  0.3669]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0438, -0.6172, -0.0475,  0.6418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 44 ] state=tensor([[-0.0438, -0.6172, -0.0475,  0.6418]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0561, -0.4214, -0.0347,  0.3346]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 45 ] state=tensor([[-0.0561, -0.4214, -0.0347,  0.3346]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0645, -0.6160, -0.0280,  0.6161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 46 ] state=tensor([[-0.0645, -0.6160, -0.0280,  0.6161]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0769, -0.4205, -0.0157,  0.3147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 47 ] state=tensor([[-0.0769, -0.4205, -0.0157,  0.3147]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0853, -0.2252, -0.0094,  0.0171]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 48 ] state=tensor([[-0.0853, -0.2252, -0.0094,  0.0171]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0898, -0.4202, -0.0091,  0.3068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 49 ] state=tensor([[-0.0898, -0.4202, -0.0091,  0.3068]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0982, -0.2249, -0.0029,  0.0113]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 50 ] state=tensor([[-0.0982, -0.2249, -0.0029,  0.0113]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1027, -0.0298, -0.0027, -0.2823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 51 ] state=tensor([[-0.1027, -0.0298, -0.0027, -0.2823]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1033,  0.1654, -0.0083, -0.5758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 52 ] state=tensor([[-0.1033,  0.1654, -0.0083, -0.5758]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1000, -0.0296, -0.0199, -0.2858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 53 ] state=tensor([[-0.1000, -0.0296, -0.0199, -0.2858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1006, -0.2244, -0.0256,  0.0006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 54 ] state=tensor([[-0.1006, -0.2244, -0.0256,  0.0006]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1051, -0.4192, -0.0256,  0.2851]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 55 ] state=tensor([[-0.1051, -0.4192, -0.0256,  0.2851]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1134, -0.2237, -0.0199, -0.0155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 56 ] state=tensor([[-0.1134, -0.2237, -0.0199, -0.0155]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1179, -0.4185, -0.0202,  0.2708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 57 ] state=tensor([[-0.1179, -0.4185, -0.0202,  0.2708]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1263, -0.2231, -0.0147, -0.0282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 58 ] state=tensor([[-0.1263, -0.2231, -0.0147, -0.0282]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1307, -0.4180, -0.0153,  0.2598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 59 ] state=tensor([[-0.1307, -0.4180, -0.0153,  0.2598]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1391, -0.2227, -0.0101, -0.0376]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 60 ] state=tensor([[-0.1391, -0.2227, -0.0101, -0.0376]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1436, -0.4177, -0.0109,  0.2518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 61 ] state=tensor([[-0.1436, -0.4177, -0.0109,  0.2518]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1519, -0.2224, -0.0058, -0.0443]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 62 ] state=tensor([[-0.1519, -0.2224, -0.0058, -0.0443]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1564, -0.0272, -0.0067, -0.3388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 63 ] state=tensor([[-0.1564, -0.0272, -0.0067, -0.3388]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1569, -0.2222, -0.0135, -0.0482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 64 ] state=tensor([[-0.1569, -0.2222, -0.0135, -0.0482]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1613, -0.0269, -0.0145, -0.3451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 65 ] state=tensor([[-0.1613, -0.0269, -0.0145, -0.3451]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1619, -0.2218, -0.0214, -0.0570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 66 ] state=tensor([[-0.1619, -0.2218, -0.0214, -0.0570]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1663, -0.4166, -0.0225,  0.2288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 67 ] state=tensor([[-0.1663, -0.4166, -0.0225,  0.2288]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1747, -0.6114, -0.0179,  0.5143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 68 ] state=tensor([[-0.1747, -0.6114, -0.0179,  0.5143]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1869, -0.8063, -0.0076,  0.8013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 69 ] state=tensor([[-0.1869, -0.8063, -0.0076,  0.8013]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2030, -0.6111,  0.0084,  0.5062]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 70 ] state=tensor([[-0.2030, -0.6111,  0.0084,  0.5062]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2152, -0.8063,  0.0185,  0.8016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 71 ] state=tensor([[-0.2152, -0.8063,  0.0185,  0.8016]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2314, -0.6114,  0.0345,  0.5148]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 72 ] state=tensor([[-0.2314, -0.6114,  0.0345,  0.5148]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2436, -0.4168,  0.0448,  0.2332]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 73 ] state=tensor([[-0.2436, -0.4168,  0.0448,  0.2332]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2519, -0.2224,  0.0495, -0.0451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 74 ] state=tensor([[-0.2519, -0.2224,  0.0495, -0.0451]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2564, -0.0280,  0.0486, -0.3217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 75 ] state=tensor([[-0.2564, -0.0280,  0.0486, -0.3217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2569,  0.1664,  0.0422, -0.5987]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 76 ] state=tensor([[-0.2569,  0.1664,  0.0422, -0.5987]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2536,  0.3609,  0.0302, -0.8778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 77 ] state=tensor([[-0.2536,  0.3609,  0.0302, -0.8778]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2464,  0.1654,  0.0126, -0.5758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 78 ] state=tensor([[-0.2464,  0.1654,  0.0126, -0.5758]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2431,  0.3603,  0.0011, -0.8645]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 79 ] state=tensor([[-0.2431,  0.3603,  0.0011, -0.8645]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2359,  0.1652, -0.0162, -0.5714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 80 ] state=tensor([[-0.2359,  0.1652, -0.0162, -0.5714]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2326, -0.0297, -0.0276, -0.2839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 81 ] state=tensor([[-0.2326, -0.0297, -0.0276, -0.2839]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.3316e-01, -2.2441e-01, -3.3272e-02, -1.5785e-05]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 82 ] state=tensor([[-2.3316e-01, -2.2441e-01, -3.3272e-02, -1.5785e-05]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2376, -0.4190, -0.0333,  0.2820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 83 ] state=tensor([[-0.2376, -0.4190, -0.0333,  0.2820]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2460, -0.2235, -0.0276, -0.0210]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 84 ] state=tensor([[-0.2460, -0.2235, -0.0276, -0.0210]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2505, -0.4182, -0.0281,  0.2628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 85 ] state=tensor([[-0.2505, -0.4182, -0.0281,  0.2628]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2589, -0.6129, -0.0228,  0.5465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 86 ] state=tensor([[-0.2589, -0.6129, -0.0228,  0.5465]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2711, -0.4175, -0.0119,  0.2468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 87 ] state=tensor([[-0.2711, -0.4175, -0.0119,  0.2468]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2795, -0.6124, -0.0069,  0.5357]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 88 ] state=tensor([[-0.2795, -0.6124, -0.0069,  0.5357]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2917, -0.4172,  0.0038,  0.2408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 89 ] state=tensor([[-0.2917, -0.4172,  0.0038,  0.2408]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3001, -0.2221,  0.0086, -0.0507]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 90 ] state=tensor([[-0.3001, -0.2221,  0.0086, -0.0507]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3045, -0.0271,  0.0076, -0.3406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 91 ] state=tensor([[-0.3045, -0.0271,  0.0076, -0.3406]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3050,  0.1679,  0.0008, -0.6309]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 92 ] state=tensor([[-0.3050,  0.1679,  0.0008, -0.6309]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3017, -0.0272, -0.0118, -0.3380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 93 ] state=tensor([[-0.3017, -0.0272, -0.0118, -0.3380]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3022,  0.1681, -0.0186, -0.6344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 94 ] state=tensor([[-0.3022,  0.1681, -0.0186, -0.6344]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2989, -0.0268, -0.0313, -0.3476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 95 ] state=tensor([[-0.2989, -0.0268, -0.0313, -0.3476]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2994, -0.2215, -0.0382, -0.0650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 96 ] state=tensor([[-0.2994, -0.2215, -0.0382, -0.0650]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3038, -0.4160, -0.0395,  0.2154]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 97 ] state=tensor([[-0.3038, -0.4160, -0.0395,  0.2154]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3122, -0.6106, -0.0352,  0.4954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 98 ] state=tensor([[-0.3122, -0.6106, -0.0352,  0.4954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3244, -0.8052, -0.0253,  0.7767]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 99 ] state=tensor([[-0.3244, -0.8052, -0.0253,  0.7767]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3405, -0.6097, -0.0098,  0.4762]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 130 ][ timestamp 100 ] state=tensor([[-0.3405, -0.6097, -0.0098,  0.4762]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-3.5266e-01, -4.1445e-01, -2.6717e-04,  1.8045e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 101 ] state=tensor([[-3.5266e-01, -4.1445e-01, -2.6717e-04,  1.8045e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3610, -0.6096,  0.0033,  0.4731]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 102 ] state=tensor([[-0.3610, -0.6096,  0.0033,  0.4731]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3731, -0.4145,  0.0128,  0.1814]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 103 ] state=tensor([[-0.3731, -0.4145,  0.0128,  0.1814]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3814, -0.2196,  0.0164, -0.1072]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 104 ] state=tensor([[-0.3814, -0.2196,  0.0164, -0.1072]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3858, -0.0247,  0.0143, -0.3946]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 105 ] state=tensor([[-0.3858, -0.0247,  0.0143, -0.3946]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3863,  0.1702,  0.0064, -0.6828]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 106 ] state=tensor([[-0.3863,  0.1702,  0.0064, -0.6828]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3829, -0.0250, -0.0073, -0.3881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 107 ] state=tensor([[-0.3829, -0.0250, -0.0073, -0.3881]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3834, -0.2200, -0.0150, -0.0977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 108 ] state=tensor([[-0.3834, -0.2200, -0.0150, -0.0977]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3878, -0.0246, -0.0170, -0.3951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 109 ] state=tensor([[-0.3878, -0.0246, -0.0170, -0.3951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3883, -0.2195, -0.0249, -0.1078]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 110 ] state=tensor([[-0.3883, -0.2195, -0.0249, -0.1078]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3927, -0.4143, -0.0270,  0.1769]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 111 ] state=tensor([[-0.3927, -0.4143, -0.0270,  0.1769]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4010, -0.6090, -0.0235,  0.4609]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 112 ] state=tensor([[-0.4010, -0.6090, -0.0235,  0.4609]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4132, -0.8038, -0.0143,  0.7461]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 113 ] state=tensor([[-0.4132, -0.8038, -0.0143,  0.7461]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-4.2924e-01, -9.9871e-01,  6.4418e-04,  1.0343e+00]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 114 ] state=tensor([[-4.2924e-01, -9.9871e-01,  6.4418e-04,  1.0343e+00]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4492, -0.8036,  0.0213,  0.7418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 115 ] state=tensor([[-0.4492, -0.8036,  0.0213,  0.7418]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4653, -0.6088,  0.0362,  0.4559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 116 ] state=tensor([[-0.4653, -0.6088,  0.0362,  0.4559]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4775, -0.4142,  0.0453,  0.1748]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 117 ] state=tensor([[-0.4775, -0.4142,  0.0453,  0.1748]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4857, -0.2197,  0.0488, -0.1032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 118 ] state=tensor([[-0.4857, -0.2197,  0.0488, -0.1032]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4901, -0.0253,  0.0467, -0.3801]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 119 ] state=tensor([[-0.4901, -0.0253,  0.0467, -0.3801]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4906,  0.1691,  0.0391, -0.6577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 120 ] state=tensor([[-0.4906,  0.1691,  0.0391, -0.6577]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4873,  0.3636,  0.0260, -0.9378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 121 ] state=tensor([[-0.4873,  0.3636,  0.0260, -0.9378]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4800,  0.5584,  0.0072, -1.2222]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 122 ] state=tensor([[-0.4800,  0.5584,  0.0072, -1.2222]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4688,  0.3632, -0.0172, -0.9273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 123 ] state=tensor([[-0.4688,  0.3632, -0.0172, -0.9273]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4616,  0.1683, -0.0358, -0.6401]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 124 ] state=tensor([[-0.4616,  0.1683, -0.0358, -0.6401]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4582, -0.0263, -0.0486, -0.3589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 125 ] state=tensor([[-0.4582, -0.0263, -0.0486, -0.3589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4587, -0.2207, -0.0558, -0.0819]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 126 ] state=tensor([[-0.4587, -0.2207, -0.0558, -0.0819]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4631, -0.4150, -0.0574,  0.1927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 127 ] state=tensor([[-0.4631, -0.4150, -0.0574,  0.1927]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4714, -0.6092, -0.0536,  0.4667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 128 ] state=tensor([[-0.4714, -0.6092, -0.0536,  0.4667]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4836, -0.8036, -0.0442,  0.7420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 129 ] state=tensor([[-0.4836, -0.8036, -0.0442,  0.7420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4997, -0.6079, -0.0294,  0.4358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 130 ] state=tensor([[-0.4997, -0.6079, -0.0294,  0.4358]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5118, -0.4123, -0.0207,  0.1340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 131 ] state=tensor([[-0.5118, -0.4123, -0.0207,  0.1340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5201, -0.6072, -0.0180,  0.4201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 132 ] state=tensor([[-0.5201, -0.6072, -0.0180,  0.4201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5322, -0.4118, -0.0096,  0.1218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 133 ] state=tensor([[-0.5322, -0.4118, -0.0096,  0.1218]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5405, -0.2165, -0.0071, -0.1739]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 134 ] state=tensor([[-0.5405, -0.2165, -0.0071, -0.1739]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5448, -0.4115, -0.0106,  0.1165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 135 ] state=tensor([[-0.5448, -0.4115, -0.0106,  0.1165]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5530, -0.6065, -0.0083,  0.4058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 136 ] state=tensor([[-0.5530, -0.6065, -0.0083,  0.4058]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-5.6516e-01, -4.1127e-01, -1.7890e-04,  1.1052e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 137 ] state=tensor([[-5.6516e-01, -4.1127e-01, -1.7890e-04,  1.1052e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5734, -0.6064,  0.0020,  0.4031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 138 ] state=tensor([[-0.5734, -0.6064,  0.0020,  0.4031]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5855, -0.4113,  0.0101,  0.1111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 139 ] state=tensor([[-0.5855, -0.4113,  0.0101,  0.1111]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5937, -0.6066,  0.0123,  0.4070]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 140 ] state=tensor([[-0.5937, -0.6066,  0.0123,  0.4070]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6059, -0.4116,  0.0205,  0.1182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 141 ] state=tensor([[-0.6059, -0.4116,  0.0205,  0.1182]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6141, -0.2168,  0.0228, -0.1680]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 142 ] state=tensor([[-0.6141, -0.2168,  0.0228, -0.1680]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6184, -0.4122,  0.0195,  0.1318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 143 ] state=tensor([[-0.6184, -0.4122,  0.0195,  0.1318]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6267, -0.2174,  0.0221, -0.1547]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 144 ] state=tensor([[-0.6267, -0.2174,  0.0221, -0.1547]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6310, -0.4128,  0.0190,  0.1449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 145 ] state=tensor([[-0.6310, -0.4128,  0.0190,  0.1449]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6393, -0.2180,  0.0219, -0.1417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 146 ] state=tensor([[-0.6393, -0.2180,  0.0219, -0.1417]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6437, -0.4134,  0.0191,  0.1578]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 147 ] state=tensor([[-0.6437, -0.4134,  0.0191,  0.1578]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6519, -0.2186,  0.0222, -0.1288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 148 ] state=tensor([[-0.6519, -0.2186,  0.0222, -0.1288]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6563, -0.4140,  0.0196,  0.1708]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 149 ] state=tensor([[-0.6563, -0.4140,  0.0196,  0.1708]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6646, -0.2192,  0.0231, -0.1156]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 150 ] state=tensor([[-0.6646, -0.2192,  0.0231, -0.1156]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6690, -0.4146,  0.0207,  0.1842]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 151 ] state=tensor([[-0.6690, -0.4146,  0.0207,  0.1842]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6772, -0.2198,  0.0244, -0.1018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 152 ] state=tensor([[-0.6772, -0.2198,  0.0244, -0.1018]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6816, -0.4153,  0.0224,  0.1985]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 153 ] state=tensor([[-0.6816, -0.4153,  0.0224,  0.1985]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6899, -0.2205,  0.0264, -0.0871]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 154 ] state=tensor([[-0.6899, -0.2205,  0.0264, -0.0871]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6944, -0.4160,  0.0246,  0.2138]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 155 ] state=tensor([[-0.6944, -0.4160,  0.0246,  0.2138]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7027, -0.2212,  0.0289, -0.0710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 156 ] state=tensor([[-0.7027, -0.2212,  0.0289, -0.0710]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7071, -0.4167,  0.0275,  0.2307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 157 ] state=tensor([[-0.7071, -0.4167,  0.0275,  0.2307]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7154, -0.2220,  0.0321, -0.0532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 158 ] state=tensor([[-0.7154, -0.2220,  0.0321, -0.0532]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7199, -0.0274,  0.0310, -0.3356]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 159 ] state=tensor([[-0.7199, -0.0274,  0.0310, -0.3356]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7204,  0.1673,  0.0243, -0.6184]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 160 ] state=tensor([[-0.7204,  0.1673,  0.0243, -0.6184]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7171, -0.0281,  0.0120, -0.3181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 161 ] state=tensor([[-0.7171, -0.0281,  0.0120, -0.3181]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7176, -0.2234,  0.0056, -0.0217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 162 ] state=tensor([[-0.7176, -0.2234,  0.0056, -0.0217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7221, -0.0284,  0.0052, -0.3126]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 163 ] state=tensor([[-0.7221, -0.0284,  0.0052, -0.3126]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7227, -0.2236, -0.0011, -0.0183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 164 ] state=tensor([[-0.7227, -0.2236, -0.0011, -0.0183]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7271, -0.4187, -0.0015,  0.2740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 165 ] state=tensor([[-0.7271, -0.4187, -0.0015,  0.2740]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7355, -0.6138,  0.0040,  0.5663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 166 ] state=tensor([[-0.7355, -0.6138,  0.0040,  0.5663]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7478, -0.4187,  0.0153,  0.2748]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 167 ] state=tensor([[-0.7478, -0.4187,  0.0153,  0.2748]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7562, -0.2238,  0.0208, -0.0130]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 168 ] state=tensor([[-0.7562, -0.2238,  0.0208, -0.0130]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7606, -0.0290,  0.0206, -0.2990]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 169 ] state=tensor([[-0.7606, -0.0290,  0.0206, -0.2990]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7612,  0.1658,  0.0146, -0.5851]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 170 ] state=tensor([[-0.7612,  0.1658,  0.0146, -0.5851]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7579, -0.0295,  0.0029, -0.2879]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 171 ] state=tensor([[-0.7579, -0.0295,  0.0029, -0.2879]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7585,  0.1656, -0.0029, -0.5796]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 130 ][ timestamp 172 ] state=tensor([[-0.7585,  0.1656, -0.0029, -0.5796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7552, -0.0295, -0.0145, -0.2879]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 173 ] state=tensor([[-0.7552, -0.0295, -0.0145, -0.2879]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.5578e-01, -2.2442e-01, -2.0210e-02,  2.2829e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 174 ] state=tensor([[-7.5578e-01, -2.2442e-01, -2.0210e-02,  2.2829e-04]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7603, -0.4192, -0.0202,  0.2865]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 175 ] state=tensor([[-0.7603, -0.4192, -0.0202,  0.2865]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7687, -0.6141, -0.0145,  0.5727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 176 ] state=tensor([[-0.7687, -0.6141, -0.0145,  0.5727]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7809, -0.8090, -0.0030,  0.8608]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 177 ] state=tensor([[-0.7809, -0.8090, -0.0030,  0.8608]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7971, -0.6138,  0.0142,  0.5672]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 178 ] state=tensor([[-0.7971, -0.6138,  0.0142,  0.5672]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8094, -0.4189,  0.0255,  0.2790]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 179 ] state=tensor([[-0.8094, -0.4189,  0.0255,  0.2790]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8178, -0.2242,  0.0311, -0.0055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 180 ] state=tensor([[-0.8178, -0.2242,  0.0311, -0.0055]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8223, -0.4197,  0.0310,  0.2968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 181 ] state=tensor([[-0.8223, -0.4197,  0.0310,  0.2968]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8306, -0.2250,  0.0369,  0.0141]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 182 ] state=tensor([[-0.8306, -0.2250,  0.0369,  0.0141]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8351, -0.0305,  0.0372, -0.2667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 183 ] state=tensor([[-0.8351, -0.0305,  0.0372, -0.2667]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8358,  0.1641,  0.0319, -0.5475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 184 ] state=tensor([[-0.8358,  0.1641,  0.0319, -0.5475]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8325,  0.3588,  0.0209, -0.8299]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 185 ] state=tensor([[-0.8325,  0.3588,  0.0209, -0.8299]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8253,  0.1634,  0.0043, -0.5307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 186 ] state=tensor([[-0.8253,  0.1634,  0.0043, -0.5307]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8220, -0.0318, -0.0063, -0.2367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 187 ] state=tensor([[-0.8220, -0.0318, -0.0063, -0.2367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8227, -0.2269, -0.0110,  0.0540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 188 ] state=tensor([[-0.8227, -0.2269, -0.0110,  0.0540]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8272, -0.0316, -0.0099, -0.2421]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 189 ] state=tensor([[-0.8272, -0.0316, -0.0099, -0.2421]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8278, -0.2266, -0.0148,  0.0474]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 190 ] state=tensor([[-0.8278, -0.2266, -0.0148,  0.0474]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8324, -0.4215, -0.0138,  0.3354]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 191 ] state=tensor([[-0.8324, -0.4215, -0.0138,  0.3354]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8408, -0.6164, -0.0071,  0.6237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 192 ] state=tensor([[-0.8408, -0.6164, -0.0071,  0.6237]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8531, -0.4212,  0.0054,  0.3288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 193 ] state=tensor([[-0.8531, -0.4212,  0.0054,  0.3288]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8615, -0.2261,  0.0119,  0.0378]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 194 ] state=tensor([[-0.8615, -0.2261,  0.0119,  0.0378]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8661, -0.0312,  0.0127, -0.2511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 195 ] state=tensor([[-0.8661, -0.0312,  0.0127, -0.2511]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8667,  0.1638,  0.0077, -0.5398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 196 ] state=tensor([[-0.8667,  0.1638,  0.0077, -0.5398]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8634, -0.0315, -0.0031, -0.2447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 197 ] state=tensor([[-0.8634, -0.0315, -0.0031, -0.2447]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8640,  0.1637, -0.0080, -0.5383]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 198 ] state=tensor([[-0.8640,  0.1637, -0.0080, -0.5383]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8608, -0.0313, -0.0188, -0.2482]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 199 ] state=tensor([[-0.8608, -0.0313, -0.0188, -0.2482]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8614, -0.2262, -0.0238,  0.0385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 200 ] state=tensor([[-0.8614, -0.2262, -0.0238,  0.0385]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8659, -0.0307, -0.0230, -0.2616]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 201 ] state=tensor([[-0.8659, -0.0307, -0.0230, -0.2616]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8665, -0.2255, -0.0282,  0.0238]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 202 ] state=tensor([[-0.8665, -0.2255, -0.0282,  0.0238]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8710, -0.4202, -0.0277,  0.3074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 203 ] state=tensor([[-0.8710, -0.4202, -0.0277,  0.3074]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8795, -0.6149, -0.0216,  0.5912]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 204 ] state=tensor([[-0.8795, -0.6149, -0.0216,  0.5912]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8918, -0.4195, -0.0098,  0.2918]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 205 ] state=tensor([[-0.8918, -0.4195, -0.0098,  0.2918]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9001, -0.6145, -0.0039,  0.5814]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 206 ] state=tensor([[-0.9001, -0.6145, -0.0039,  0.5814]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9124, -0.4193,  0.0077,  0.2875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 207 ] state=tensor([[-0.9124, -0.4193,  0.0077,  0.2875]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9208, -0.2243,  0.0135, -0.0027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 208 ] state=tensor([[-0.9208, -0.2243,  0.0135, -0.0027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9253, -0.0294,  0.0134, -0.2912]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 209 ] state=tensor([[-0.9253, -0.0294,  0.0134, -0.2912]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9259,  0.1656,  0.0076, -0.5796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 210 ] state=tensor([[-0.9259,  0.1656,  0.0076, -0.5796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9226, -0.0297, -0.0040, -0.2845]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 211 ] state=tensor([[-0.9226, -0.0297, -0.0040, -0.2845]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9232, -0.2247, -0.0097,  0.0069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 212 ] state=tensor([[-0.9232, -0.2247, -0.0097,  0.0069]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9277, -0.0295, -0.0096, -0.2888]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 213 ] state=tensor([[-0.9277, -0.0295, -0.0096, -0.2888]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-9.2825e-01, -2.2445e-01, -1.5347e-02,  8.1119e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 214 ] state=tensor([[-9.2825e-01, -2.2445e-01, -1.5347e-02,  8.1119e-04]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9327, -0.0291, -0.0153, -0.2967]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 215 ] state=tensor([[-0.9327, -0.0291, -0.0153, -0.2967]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9333, -0.2240, -0.0213, -0.0089]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 216 ] state=tensor([[-0.9333, -0.2240, -0.0213, -0.0089]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9378, -0.4188, -0.0214,  0.2770]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 217 ] state=tensor([[-0.9378, -0.4188, -0.0214,  0.2770]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9462, -0.2234, -0.0159, -0.0223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 218 ] state=tensor([[-0.9462, -0.2234, -0.0159, -0.0223]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9507, -0.0281, -0.0163, -0.3200]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 219 ] state=tensor([[-0.9507, -0.0281, -0.0163, -0.3200]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9512, -0.2229, -0.0227, -0.0325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 220 ] state=tensor([[-0.9512, -0.2229, -0.0227, -0.0325]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9557, -0.4177, -0.0234,  0.2529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 221 ] state=tensor([[-0.9557, -0.4177, -0.0234,  0.2529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9640, -0.6125, -0.0183,  0.5381]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 222 ] state=tensor([[-0.9640, -0.6125, -0.0183,  0.5381]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9763, -0.4171, -0.0076,  0.2397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 223 ] state=tensor([[-0.9763, -0.4171, -0.0076,  0.2397]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9846, -0.2219, -0.0028, -0.0553]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 224 ] state=tensor([[-0.9846, -0.2219, -0.0028, -0.0553]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9891, -0.0267, -0.0039, -0.3489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 225 ] state=tensor([[-0.9891, -0.0267, -0.0039, -0.3489]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9896, -0.2218, -0.0109, -0.0574]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 226 ] state=tensor([[-0.9896, -0.2218, -0.0109, -0.0574]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9940, -0.4168, -0.0120,  0.2318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 227 ] state=tensor([[-0.9940, -0.4168, -0.0120,  0.2318]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0024, -0.2215, -0.0074, -0.0647]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 228 ] state=tensor([[-1.0024, -0.2215, -0.0074, -0.0647]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0068, -0.0263, -0.0087, -0.3597]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 229 ] state=tensor([[-1.0068, -0.0263, -0.0087, -0.3597]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0073, -0.2213, -0.0159, -0.0697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 230 ] state=tensor([[-1.0073, -0.2213, -0.0159, -0.0697]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0117, -0.0259, -0.0173, -0.3674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 231 ] state=tensor([[-1.0117, -0.0259, -0.0173, -0.3674]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0123, -0.2208, -0.0246, -0.0802]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 232 ] state=tensor([[-1.0123, -0.2208, -0.0246, -0.0802]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0167, -0.0253, -0.0262, -0.3805]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 233 ] state=tensor([[-1.0167, -0.0253, -0.0262, -0.3805]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0172, -0.2201, -0.0338, -0.0962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 234 ] state=tensor([[-1.0172, -0.2201, -0.0338, -0.0962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0216, -0.4147, -0.0357,  0.1856]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 235 ] state=tensor([[-1.0216, -0.4147, -0.0357,  0.1856]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0299, -0.6093, -0.0320,  0.4668]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 236 ] state=tensor([[-1.0299, -0.6093, -0.0320,  0.4668]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0421, -0.4137, -0.0227,  0.1642]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 237 ] state=tensor([[-1.0421, -0.4137, -0.0227,  0.1642]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0503, -0.6085, -0.0194,  0.4496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 238 ] state=tensor([[-1.0503, -0.6085, -0.0194,  0.4496]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0625, -0.8033, -0.0104,  0.7361]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 239 ] state=tensor([[-1.0625, -0.8033, -0.0104,  0.7361]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0786, -0.6081,  0.0043,  0.4402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 240 ] state=tensor([[-1.0786, -0.6081,  0.0043,  0.4402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0907, -0.8033,  0.0131,  0.7342]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 241 ] state=tensor([[-1.0907, -0.8033,  0.0131,  0.7342]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1068, -0.6083,  0.0278,  0.4457]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 242 ] state=tensor([[-1.1068, -0.6083,  0.0278,  0.4457]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1190, -0.4136,  0.0367,  0.1619]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 243 ] state=tensor([[-1.1190, -0.4136,  0.0367,  0.1619]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1272, -0.2190,  0.0399, -0.1190]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 130 ][ timestamp 244 ] state=tensor([[-1.1272, -0.2190,  0.0399, -0.1190]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1316, -0.0245,  0.0376, -0.3988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 245 ] state=tensor([[-1.1316, -0.0245,  0.0376, -0.3988]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1321,  0.1701,  0.0296, -0.6794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 246 ] state=tensor([[-1.1321,  0.1701,  0.0296, -0.6794]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1287, -0.0254,  0.0160, -0.3776]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 247 ] state=tensor([[-1.1287, -0.0254,  0.0160, -0.3776]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1292,  0.1694,  0.0084, -0.6652]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 248 ] state=tensor([[-1.1292,  0.1694,  0.0084, -0.6652]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1258,  0.3645, -0.0049, -0.9552]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 249 ] state=tensor([[-1.1258,  0.3645, -0.0049, -0.9552]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1185,  0.1694, -0.0240, -0.6640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 250 ] state=tensor([[-1.1185,  0.1694, -0.0240, -0.6640]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1151, -0.0254, -0.0372, -0.3790]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 251 ] state=tensor([[-1.1151, -0.0254, -0.0372, -0.3790]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1157,  0.1702, -0.0448, -0.6832]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 252 ] state=tensor([[-1.1157,  0.1702, -0.0448, -0.6832]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1123, -0.0242, -0.0585, -0.4050]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 253 ] state=tensor([[-1.1123, -0.0242, -0.0585, -0.4050]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1127, -0.2185, -0.0666, -0.1313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 254 ] state=tensor([[-1.1127, -0.2185, -0.0666, -0.1313]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1171, -0.4126, -0.0692,  0.1397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 255 ] state=tensor([[-1.1171, -0.4126, -0.0692,  0.1397]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1254, -0.6066, -0.0664,  0.4098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 256 ] state=tensor([[-1.1254, -0.6066, -0.0664,  0.4098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1375, -0.8008, -0.0582,  0.6808]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 257 ] state=tensor([[-1.1375, -0.8008, -0.0582,  0.6808]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1535, -0.6049, -0.0446,  0.3703]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 258 ] state=tensor([[-1.1535, -0.6049, -0.0446,  0.3703]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1656, -0.7993, -0.0372,  0.6486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 259 ] state=tensor([[-1.1656, -0.7993, -0.0372,  0.6486]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1816, -0.6037, -0.0242,  0.3445]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 260 ] state=tensor([[-1.1816, -0.6037, -0.0242,  0.3445]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1937, -0.7985, -0.0173,  0.6294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 261 ] state=tensor([[-1.1937, -0.7985, -0.0173,  0.6294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2096, -0.6031, -0.0048,  0.3313]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 262 ] state=tensor([[-1.2096, -0.6031, -0.0048,  0.3313]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2217, -0.4079,  0.0019,  0.0371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 263 ] state=tensor([[-1.2217, -0.4079,  0.0019,  0.0371]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2299, -0.2129,  0.0026, -0.2549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 264 ] state=tensor([[-1.2299, -0.2129,  0.0026, -0.2549]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2341, -0.0178, -0.0025, -0.5468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 265 ] state=tensor([[-1.2341, -0.0178, -0.0025, -0.5468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2345,  0.1774, -0.0134, -0.8403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 266 ] state=tensor([[-1.2345,  0.1774, -0.0134, -0.8403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2309, -0.0175, -0.0302, -0.5518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 267 ] state=tensor([[-1.2309, -0.0175, -0.0302, -0.5518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2313, -0.2122, -0.0413, -0.2688]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 268 ] state=tensor([[-1.2313, -0.2122, -0.0413, -0.2688]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2355, -0.4067, -0.0466,  0.0106]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 269 ] state=tensor([[-1.2355, -0.4067, -0.0466,  0.0106]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2437, -0.6012, -0.0464,  0.2882]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 270 ] state=tensor([[-1.2437, -0.6012, -0.0464,  0.2882]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2557, -0.7956, -0.0407,  0.5659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 271 ] state=tensor([[-1.2557, -0.7956, -0.0407,  0.5659]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2716, -0.9901, -0.0293,  0.8455]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 272 ] state=tensor([[-1.2716, -0.9901, -0.0293,  0.8455]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2914, -0.7946, -0.0124,  0.5437]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 273 ] state=tensor([[-1.2914, -0.7946, -0.0124,  0.5437]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3073, -0.9896, -0.0016,  0.8324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 274 ] state=tensor([[-1.3073, -0.9896, -0.0016,  0.8324]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3271, -0.7944,  0.0151,  0.5393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 275 ] state=tensor([[-1.3271, -0.7944,  0.0151,  0.5393]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3430, -0.5995,  0.0259,  0.2514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 276 ] state=tensor([[-1.3430, -0.5995,  0.0259,  0.2514]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3550, -0.4048,  0.0309, -0.0330]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 277 ] state=tensor([[-1.3550, -0.4048,  0.0309, -0.0330]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3630, -0.2101,  0.0302, -0.3158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 278 ] state=tensor([[-1.3630, -0.2101,  0.0302, -0.3158]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3672, -0.0154,  0.0239, -0.5988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 279 ] state=tensor([[-1.3672, -0.0154,  0.0239, -0.5988]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3676,  0.1794,  0.0119, -0.8839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 280 ] state=tensor([[-1.3676,  0.1794,  0.0119, -0.8839]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3640, -0.0159, -0.0057, -0.5875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 281 ] state=tensor([[-1.3640, -0.0159, -0.0057, -0.5875]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3643, -0.2110, -0.0175, -0.2966]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 282 ] state=tensor([[-1.3643, -0.2110, -0.0175, -0.2966]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3685, -0.0156, -0.0234, -0.5947]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 283 ] state=tensor([[-1.3685, -0.0156, -0.0234, -0.5947]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3688, -0.2104, -0.0353, -0.3095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 284 ] state=tensor([[-1.3688, -0.2104, -0.0353, -0.3095]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3730, -0.4050, -0.0415, -0.0282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 285 ] state=tensor([[-1.3730, -0.4050, -0.0415, -0.0282]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3811, -0.5995, -0.0421,  0.2511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 286 ] state=tensor([[-1.3811, -0.5995, -0.0421,  0.2511]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3931, -0.4038, -0.0370, -0.0545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 287 ] state=tensor([[-1.3931, -0.4038, -0.0370, -0.0545]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4012, -0.5984, -0.0381,  0.2263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 288 ] state=tensor([[-1.4012, -0.5984, -0.0381,  0.2263]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4132, -0.7929, -0.0336,  0.5067]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 289 ] state=tensor([[-1.4132, -0.7929, -0.0336,  0.5067]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4290, -0.9876, -0.0235,  0.7886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 290 ] state=tensor([[-1.4290, -0.9876, -0.0235,  0.7886]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4488, -0.7921, -0.0077,  0.4886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 291 ] state=tensor([[-1.4488, -0.7921, -0.0077,  0.4886]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4646, -0.5969,  0.0021,  0.1935]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 292 ] state=tensor([[-1.4646, -0.5969,  0.0021,  0.1935]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4765, -0.4018,  0.0059, -0.0985]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 293 ] state=tensor([[-1.4765, -0.4018,  0.0059, -0.0985]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4846, -0.2068,  0.0040, -0.3893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 294 ] state=tensor([[-1.4846, -0.2068,  0.0040, -0.3893]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4887, -0.0117, -0.0038, -0.6807]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 295 ] state=tensor([[-1.4887, -0.0117, -0.0038, -0.6807]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4890, -0.2068, -0.0174, -0.3893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 296 ] state=tensor([[-1.4890, -0.2068, -0.0174, -0.3893]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4931, -0.4016, -0.0252, -0.1021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 297 ] state=tensor([[-1.4931, -0.4016, -0.0252, -0.1021]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5011, -0.2062, -0.0273, -0.4026]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 298 ] state=tensor([[-1.5011, -0.2062, -0.0273, -0.4026]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5052, -0.4009, -0.0353, -0.1187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 299 ] state=tensor([[-1.5052, -0.4009, -0.0353, -0.1187]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5133, -0.5955, -0.0377,  0.1627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 300 ] state=tensor([[-1.5133, -0.5955, -0.0377,  0.1627]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5252, -0.7900, -0.0344,  0.4432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 301 ] state=tensor([[-1.5252, -0.7900, -0.0344,  0.4432]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5410, -0.9847, -0.0256,  0.7249]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 302 ] state=tensor([[-1.5410, -0.9847, -0.0256,  0.7249]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5607, -0.7892, -0.0111,  0.4242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 303 ] state=tensor([[-1.5607, -0.7892, -0.0111,  0.4242]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5765, -0.5939, -0.0026,  0.1281]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 304 ] state=tensor([[-1.5765, -0.5939, -0.0026,  0.1281]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5883e+00, -3.9876e-01, -1.7924e-05, -1.6541e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 305 ] state=tensor([[-1.5883e+00, -3.9876e-01, -1.7924e-05, -1.6541e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5963, -0.2036, -0.0033, -0.4581]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 306 ] state=tensor([[-1.5963, -0.2036, -0.0033, -0.4581]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6004, -0.0085, -0.0125, -0.7518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 307 ] state=tensor([[-1.6004, -0.0085, -0.0125, -0.7518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6005, -0.2034, -0.0275, -0.4631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 308 ] state=tensor([[-1.6005, -0.2034, -0.0275, -0.4631]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6046, -0.3981, -0.0368, -0.1792]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 309 ] state=tensor([[-1.6046, -0.3981, -0.0368, -0.1792]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6126, -0.2025, -0.0404, -0.4833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 310 ] state=tensor([[-1.6126, -0.2025, -0.0404, -0.4833]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6166, -0.0068, -0.0500, -0.7884]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 311 ] state=tensor([[-1.6166, -0.0068, -0.0500, -0.7884]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6168, -0.2012, -0.0658, -0.5119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 312 ] state=tensor([[-1.6168, -0.2012, -0.0658, -0.5119]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6208, -0.3954, -0.0760, -0.2406]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 313 ] state=tensor([[-1.6208, -0.3954, -0.0760, -0.2406]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6287, -0.5893, -0.0809,  0.0271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 314 ] state=tensor([[-1.6287, -0.5893, -0.0809,  0.0271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6405, -0.7832, -0.0803,  0.2933]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 315 ] state=tensor([[-1.6405, -0.7832, -0.0803,  0.2933]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6561, -0.9771, -0.0744,  0.5596]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 130 ][ timestamp 316 ] state=tensor([[-1.6561, -0.9771, -0.0744,  0.5596]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6757, -1.1711, -0.0633,  0.8279]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 317 ] state=tensor([[-1.6757, -1.1711, -0.0633,  0.8279]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6991, -1.3653, -0.0467,  1.1000]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 318 ] state=tensor([[-1.6991, -1.3653, -0.0467,  1.1000]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7264, -1.5598, -0.0247,  1.3777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 319 ] state=tensor([[-1.7264, -1.5598, -0.0247,  1.3777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7576, -1.7546,  0.0029,  1.6626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 320 ] state=tensor([[-1.7576, -1.7546,  0.0029,  1.6626]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7927, -1.9497,  0.0361,  1.9561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 321 ] state=tensor([[-1.7927, -1.9497,  0.0361,  1.9561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8317, -1.7550,  0.0752,  1.6749]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 322 ] state=tensor([[-1.8317, -1.7550,  0.0752,  1.6749]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8668, -1.9509,  0.1087,  1.9900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 323 ] state=tensor([[-1.8668, -1.9509,  0.1087,  1.9900]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9058, -1.7571,  0.1485,  1.7329]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 324 ] state=tensor([[-1.9058, -1.7571,  0.1485,  1.7329]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9410, -1.5640,  0.1832,  1.4898]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 130 ][ timestamp 325 ] state=tensor([[-1.9410, -1.5640,  0.1832,  1.4898]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 130: Exploration_rate=0.05. Score=325.\n",
      "[ episode 131 ] state=tensor([[ 0.0110, -0.0150, -0.0371, -0.0477]])\n",
      "[ episode 131 ][ timestamp 1 ] state=tensor([[ 0.0110, -0.0150, -0.0371, -0.0477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0107,  0.1807, -0.0380, -0.3518]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 2 ] state=tensor([[ 0.0107,  0.1807, -0.0380, -0.3518]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0143, -0.0139, -0.0451, -0.0714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 3 ] state=tensor([[ 0.0143, -0.0139, -0.0451, -0.0714]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0140, -0.2084, -0.0465,  0.2068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 4 ] state=tensor([[ 0.0140, -0.2084, -0.0465,  0.2068]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0098, -0.4028, -0.0424,  0.4844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 5 ] state=tensor([[ 0.0098, -0.4028, -0.0424,  0.4844]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0018, -0.5973, -0.0327,  0.7635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 6 ] state=tensor([[ 0.0018, -0.5973, -0.0327,  0.7635]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0102, -0.4017, -0.0174,  0.4607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 7 ] state=tensor([[-0.0102, -0.4017, -0.0174,  0.4607]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0182, -0.2064, -0.0082,  0.1626]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 8 ] state=tensor([[-0.0182, -0.2064, -0.0082,  0.1626]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0223, -0.0111, -0.0049, -0.1327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 9 ] state=tensor([[-0.0223, -0.0111, -0.0049, -0.1327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0226,  0.1841, -0.0076, -0.4269]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 10 ] state=tensor([[-0.0226,  0.1841, -0.0076, -0.4269]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0189, -0.0109, -0.0161, -0.1367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 11 ] state=tensor([[-0.0189, -0.0109, -0.0161, -0.1367]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0191,  0.1844, -0.0189, -0.4344]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 12 ] state=tensor([[-0.0191,  0.1844, -0.0189, -0.4344]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0154, -0.0104, -0.0276, -0.1477]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 13 ] state=tensor([[-0.0154, -0.0104, -0.0276, -0.1477]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0156,  0.1851, -0.0305, -0.4490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 14 ] state=tensor([[-0.0156,  0.1851, -0.0305, -0.4490]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0119, -0.0096, -0.0395, -0.1660]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 15 ] state=tensor([[-0.0119, -0.0096, -0.0395, -0.1660]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0121, -0.2041, -0.0428,  0.1139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 16 ] state=tensor([[-0.0121, -0.2041, -0.0428,  0.1139]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0162, -0.3986, -0.0405,  0.3928]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 17 ] state=tensor([[-0.0162, -0.3986, -0.0405,  0.3928]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0242, -0.5932, -0.0327,  0.6724]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 18 ] state=tensor([[-0.0242, -0.5932, -0.0327,  0.6724]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0360, -0.3976, -0.0192,  0.3696]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 19 ] state=tensor([[-0.0360, -0.3976, -0.0192,  0.3696]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0440, -0.5924, -0.0118,  0.6562]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 20 ] state=tensor([[-0.0440, -0.5924, -0.0118,  0.6562]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0558, -0.3972,  0.0013,  0.3598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 21 ] state=tensor([[-0.0558, -0.3972,  0.0013,  0.3598]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0638, -0.2021,  0.0085,  0.0675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 22 ] state=tensor([[-0.0638, -0.2021,  0.0085,  0.0675]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0678, -0.0071,  0.0098, -0.2224]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 23 ] state=tensor([[-0.0678, -0.0071,  0.0098, -0.2224]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0680,  0.1879,  0.0054, -0.5120]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 24 ] state=tensor([[-0.0680,  0.1879,  0.0054, -0.5120]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0642,  0.3830, -0.0049, -0.8030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 25 ] state=tensor([[-0.0642,  0.3830, -0.0049, -0.8030]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0565,  0.1879, -0.0209, -0.5118]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 26 ] state=tensor([[-0.0565,  0.1879, -0.0209, -0.5118]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0528, -0.0069, -0.0311, -0.2258]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 27 ] state=tensor([[-0.0528, -0.0069, -0.0311, -0.2258]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0529,  0.1887, -0.0357, -0.5282]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 28 ] state=tensor([[-0.0529,  0.1887, -0.0357, -0.5282]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0491, -0.0060, -0.0462, -0.2469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 29 ] state=tensor([[-0.0491, -0.0060, -0.0462, -0.2469]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0493,  0.1898, -0.0512, -0.5538]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 30 ] state=tensor([[-0.0493,  0.1898, -0.0512, -0.5538]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0455, -0.0046, -0.0622, -0.2777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 31 ] state=tensor([[-0.0455, -0.0046, -0.0622, -0.2777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0456, -0.1987, -0.0678, -0.0053]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 32 ] state=tensor([[-0.0456, -0.1987, -0.0678, -0.0053]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0495, -0.3928, -0.0679,  0.2653]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 33 ] state=tensor([[-0.0495, -0.3928, -0.0679,  0.2653]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0574, -0.5869, -0.0626,  0.5358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 34 ] state=tensor([[-0.0574, -0.5869, -0.0626,  0.5358]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0691, -0.7811, -0.0519,  0.8081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 35 ] state=tensor([[-0.0691, -0.7811, -0.0519,  0.8081]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0847, -0.9755, -0.0357,  1.0840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 36 ] state=tensor([[-0.0847, -0.9755, -0.0357,  1.0840]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1043, -0.7799, -0.0140,  0.7804]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 37 ] state=tensor([[-0.1043, -0.7799, -0.0140,  0.7804]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1199, -0.9748,  0.0016,  1.0686]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 38 ] state=tensor([[-0.1199, -0.9748,  0.0016,  1.0686]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1394, -0.7797,  0.0229,  0.7764]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 39 ] state=tensor([[-0.1394, -0.7797,  0.0229,  0.7764]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1549, -0.5849,  0.0385,  0.4910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 40 ] state=tensor([[-0.1549, -0.5849,  0.0385,  0.4910]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1666, -0.3904,  0.0483,  0.2107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 41 ] state=tensor([[-0.1666, -0.3904,  0.0483,  0.2107]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1745, -0.1960,  0.0525, -0.0663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 42 ] state=tensor([[-0.1745, -0.1960,  0.0525, -0.0663]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1784, -0.0017,  0.0512, -0.3420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 43 ] state=tensor([[-0.1784, -0.0017,  0.0512, -0.3420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1784,  0.1927,  0.0443, -0.6181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 44 ] state=tensor([[-0.1784,  0.1927,  0.0443, -0.6181]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1746, -0.0030,  0.0320, -0.3118]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 45 ] state=tensor([[-0.1746, -0.0030,  0.0320, -0.3118]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1746,  0.1916,  0.0257, -0.5942]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 46 ] state=tensor([[-0.1746,  0.1916,  0.0257, -0.5942]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1708, -0.0038,  0.0139, -0.2936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 47 ] state=tensor([[-0.1708, -0.0038,  0.0139, -0.2936]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1709, -0.1991,  0.0080,  0.0035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 48 ] state=tensor([[-0.1709, -0.1991,  0.0080,  0.0035]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1748, -0.0041,  0.0081, -0.2867]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 49 ] state=tensor([[-0.1748, -0.0041,  0.0081, -0.2867]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1749,  0.1909,  0.0023, -0.5768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 50 ] state=tensor([[-0.1749,  0.1909,  0.0023, -0.5768]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1711, -0.0043, -0.0092, -0.2834]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 51 ] state=tensor([[-0.1711, -0.0043, -0.0092, -0.2834]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1712, -0.1993, -0.0149,  0.0064]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 52 ] state=tensor([[-0.1712, -0.1993, -0.0149,  0.0064]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1752, -0.0039, -0.0148, -0.2910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 53 ] state=tensor([[-0.1752, -0.0039, -0.0148, -0.2910]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1753,  0.1914, -0.0206, -0.5883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 54 ] state=tensor([[-0.1753,  0.1914, -0.0206, -0.5883]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1714, -0.0034, -0.0323, -0.3021]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 55 ] state=tensor([[-0.1714, -0.0034, -0.0323, -0.3021]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1715, -0.1981, -0.0384, -0.0198]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 56 ] state=tensor([[-0.1715, -0.1981, -0.0384, -0.0198]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1755, -0.3926, -0.0388,  0.2605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 57 ] state=tensor([[-0.1755, -0.3926, -0.0388,  0.2605]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1833, -0.5872, -0.0336,  0.5407]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 58 ] state=tensor([[-0.1833, -0.5872, -0.0336,  0.5407]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1951, -0.7818, -0.0228,  0.8226]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 59 ] state=tensor([[-0.1951, -0.7818, -0.0228,  0.8226]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2107, -0.5864, -0.0063,  0.5229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 60 ] state=tensor([[-0.2107, -0.5864, -0.0063,  0.5229]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2224, -0.7814,  0.0042,  0.8136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 61 ] state=tensor([[-0.2224, -0.7814,  0.0042,  0.8136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2380, -0.5864,  0.0204,  0.5222]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 131 ][ timestamp 62 ] state=tensor([[-0.2380, -0.5864,  0.0204,  0.5222]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2498, -0.3915,  0.0309,  0.2360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 63 ] state=tensor([[-0.2498, -0.3915,  0.0309,  0.2360]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2576, -0.1969,  0.0356, -0.0468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 64 ] state=tensor([[-0.2576, -0.1969,  0.0356, -0.0468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2615, -0.0023,  0.0347, -0.3280]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 65 ] state=tensor([[-0.2615, -0.0023,  0.0347, -0.3280]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2616, -0.1979,  0.0281, -0.0246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 66 ] state=tensor([[-0.2616, -0.1979,  0.0281, -0.0246]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2655, -0.0032,  0.0276, -0.3083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 67 ] state=tensor([[-0.2655, -0.0032,  0.0276, -0.3083]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2656,  0.1916,  0.0214, -0.5922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 68 ] state=tensor([[-0.2656,  0.1916,  0.0214, -0.5922]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2618, -0.0039,  0.0096, -0.2928]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 69 ] state=tensor([[-0.2618, -0.0039,  0.0096, -0.2928]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2619,  0.1911,  0.0037, -0.5824]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 70 ] state=tensor([[-0.2619,  0.1911,  0.0037, -0.5824]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2580, -0.0041, -0.0079, -0.2886]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 71 ] state=tensor([[-0.2580, -0.0041, -0.0079, -0.2886]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2581,  0.1912, -0.0137, -0.5838]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 72 ] state=tensor([[-0.2581,  0.1912, -0.0137, -0.5838]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2543, -0.0037, -0.0254, -0.2954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 73 ] state=tensor([[-0.2543, -0.0037, -0.0254, -0.2954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2544, -0.1985, -0.0313, -0.0108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 74 ] state=tensor([[-0.2544, -0.1985, -0.0313, -0.0108]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2583, -0.0029, -0.0315, -0.3132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 75 ] state=tensor([[-0.2583, -0.0029, -0.0315, -0.3132]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2584, -0.1976, -0.0377, -0.0306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 76 ] state=tensor([[-0.2584, -0.1976, -0.0377, -0.0306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2623, -0.0020, -0.0384, -0.3350]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 77 ] state=tensor([[-0.2623, -0.0020, -0.0384, -0.3350]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2624, -0.1965, -0.0451, -0.0546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 78 ] state=tensor([[-0.2624, -0.1965, -0.0451, -0.0546]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2663, -0.3910, -0.0462,  0.2235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 79 ] state=tensor([[-0.2663, -0.3910, -0.0462,  0.2235]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2741, -0.5854, -0.0417,  0.5013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 80 ] state=tensor([[-0.2741, -0.5854, -0.0417,  0.5013]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2858, -0.7799, -0.0317,  0.7805]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 81 ] state=tensor([[-0.2858, -0.7799, -0.0317,  0.7805]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3014, -0.5844, -0.0160,  0.4781]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 82 ] state=tensor([[-0.3014, -0.5844, -0.0160,  0.4781]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3131, -0.3890, -0.0065,  0.1804]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 83 ] state=tensor([[-0.3131, -0.3890, -0.0065,  0.1804]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3209, -0.1938, -0.0029, -0.1143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 84 ] state=tensor([[-0.3209, -0.1938, -0.0029, -0.1143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3248,  0.0014, -0.0052, -0.4079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 85 ] state=tensor([[-0.3248,  0.0014, -0.0052, -0.4079]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3248, -0.1937, -0.0133, -0.1169]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 86 ] state=tensor([[-0.3248, -0.1937, -0.0133, -0.1169]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3286, -0.3886, -0.0157,  0.1716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 87 ] state=tensor([[-0.3286, -0.3886, -0.0157,  0.1716]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3364, -0.1933, -0.0122, -0.1260]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 88 ] state=tensor([[-0.3364, -0.1933, -0.0122, -0.1260]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3403, -0.3882, -0.0147,  0.1628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 89 ] state=tensor([[-0.3403, -0.3882, -0.0147,  0.1628]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3480, -0.5831, -0.0115,  0.4508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 90 ] state=tensor([[-0.3480, -0.5831, -0.0115,  0.4508]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3597, -0.3878, -0.0025,  0.1545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 91 ] state=tensor([[-0.3597, -0.3878, -0.0025,  0.1545]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3675, -0.1927,  0.0006, -0.1390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 92 ] state=tensor([[-0.3675, -0.1927,  0.0006, -0.1390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3713,  0.0024, -0.0022, -0.4315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 93 ] state=tensor([[-0.3713,  0.0024, -0.0022, -0.4315]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3713, -0.1927, -0.0108, -0.1395]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 94 ] state=tensor([[-0.3713, -0.1927, -0.0108, -0.1395]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3751, -0.3876, -0.0136,  0.1498]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 95 ] state=tensor([[-0.3751, -0.3876, -0.0136,  0.1498]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3829, -0.1923, -0.0106, -0.1471]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 96 ] state=tensor([[-0.3829, -0.1923, -0.0106, -0.1471]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3867, -0.3873, -0.0135,  0.1422]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 97 ] state=tensor([[-0.3867, -0.3873, -0.0135,  0.1422]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3945, -0.5822, -0.0107,  0.4306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 98 ] state=tensor([[-0.3945, -0.5822, -0.0107,  0.4306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4061, -0.3869, -0.0021,  0.1345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 99 ] state=tensor([[-0.4061, -0.3869, -0.0021,  0.1345]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4138, -0.1918,  0.0006, -0.1588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 100 ] state=tensor([[-0.4138, -0.1918,  0.0006, -0.1588]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4177,  0.0033, -0.0026, -0.4513]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 101 ] state=tensor([[-0.4177,  0.0033, -0.0026, -0.4513]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4176,  0.1985, -0.0116, -0.7448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 102 ] state=tensor([[-0.4176,  0.1985, -0.0116, -0.7448]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4136,  0.0035, -0.0265, -0.4558]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 103 ] state=tensor([[-0.4136,  0.0035, -0.0265, -0.4558]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4136, -0.1912, -0.0356, -0.1716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 104 ] state=tensor([[-0.4136, -0.1912, -0.0356, -0.1716]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4174,  0.0044, -0.0390, -0.4753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 105 ] state=tensor([[-0.4174,  0.0044, -0.0390, -0.4753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4173, -0.1902, -0.0485, -0.1951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 106 ] state=tensor([[-0.4173, -0.1902, -0.0485, -0.1951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4211, -0.3845, -0.0524,  0.0819]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 107 ] state=tensor([[-0.4211, -0.3845, -0.0524,  0.0819]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4288, -0.5789, -0.0508,  0.3575]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 108 ] state=tensor([[-0.4288, -0.5789, -0.0508,  0.3575]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4404, -0.7732, -0.0437,  0.6338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 109 ] state=tensor([[-0.4404, -0.7732, -0.0437,  0.6338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4558, -0.9677, -0.0310,  0.9124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 110 ] state=tensor([[-0.4558, -0.9677, -0.0310,  0.9124]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4752, -0.7722, -0.0127,  0.6102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 111 ] state=tensor([[-0.4752, -0.7722, -0.0127,  0.6102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-4.9064e-01, -5.7690e-01, -5.2501e-04,  3.1349e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 112 ] state=tensor([[-4.9064e-01, -5.7690e-01, -5.2501e-04,  3.1349e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5022, -0.3818,  0.0057,  0.0206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 113 ] state=tensor([[-0.5022, -0.3818,  0.0057,  0.0206]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5098, -0.1867,  0.0062, -0.2702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 114 ] state=tensor([[-0.5098, -0.1867,  0.0062, -0.2702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5135, -0.3819,  0.0008,  0.0244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 115 ] state=tensor([[-0.5135, -0.3819,  0.0008,  0.0244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5212, -0.5771,  0.0012,  0.3173]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 116 ] state=tensor([[-0.5212, -0.5771,  0.0012,  0.3173]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5327, -0.3820,  0.0076,  0.0250]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 117 ] state=tensor([[-0.5327, -0.3820,  0.0076,  0.0250]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5404, -0.5772,  0.0081,  0.3201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 118 ] state=tensor([[-0.5404, -0.5772,  0.0081,  0.3201]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5519, -0.3822,  0.0145,  0.0300]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 119 ] state=tensor([[-0.5519, -0.3822,  0.0145,  0.0300]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5596, -0.5775,  0.0151,  0.3272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 120 ] state=tensor([[-0.5596, -0.5775,  0.0151,  0.3272]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5711, -0.3826,  0.0216,  0.0393]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 121 ] state=tensor([[-0.5711, -0.3826,  0.0216,  0.0393]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5788, -0.5780,  0.0224,  0.3387]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 122 ] state=tensor([[-0.5788, -0.5780,  0.0224,  0.3387]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5903, -0.3833,  0.0292,  0.0532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 123 ] state=tensor([[-0.5903, -0.3833,  0.0292,  0.0532]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5980, -0.1886,  0.0303, -0.2301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 124 ] state=tensor([[-0.5980, -0.1886,  0.0303, -0.2301]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6018, -0.3841,  0.0257,  0.0719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 125 ] state=tensor([[-0.6018, -0.3841,  0.0257,  0.0719]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6094, -0.1894,  0.0271, -0.2125]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 126 ] state=tensor([[-0.6094, -0.1894,  0.0271, -0.2125]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6132, -0.3849,  0.0228,  0.0886]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 131 ][ timestamp 127 ] state=tensor([[-0.6132, -0.3849,  0.0228,  0.0886]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6209, -0.1901,  0.0246, -0.1968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 128 ] state=tensor([[-0.6209, -0.1901,  0.0246, -0.1968]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6247, -0.3855,  0.0207,  0.1035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 129 ] state=tensor([[-0.6247, -0.3855,  0.0207,  0.1035]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6324, -0.1907,  0.0227, -0.1826]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 130 ] state=tensor([[-0.6324, -0.1907,  0.0227, -0.1826]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6362, -0.3862,  0.0191,  0.1172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 131 ] state=tensor([[-0.6362, -0.3862,  0.0191,  0.1172]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6440, -0.1913,  0.0214, -0.1694]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 132 ] state=tensor([[-0.6440, -0.1913,  0.0214, -0.1694]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6478, -0.3867,  0.0181,  0.1300]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 133 ] state=tensor([[-0.6478, -0.3867,  0.0181,  0.1300]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6555, -0.1919,  0.0207, -0.1570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 134 ] state=tensor([[-0.6555, -0.1919,  0.0207, -0.1570]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6594, -0.3873,  0.0175,  0.1422]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 135 ] state=tensor([[-0.6594, -0.3873,  0.0175,  0.1422]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6671, -0.5827,  0.0204,  0.4403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 136 ] state=tensor([[-0.6671, -0.5827,  0.0204,  0.4403]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6788, -0.3878,  0.0292,  0.1541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 137 ] state=tensor([[-0.6788, -0.3878,  0.0292,  0.1541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6865, -0.1931,  0.0322, -0.1292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 138 ] state=tensor([[-0.6865, -0.1931,  0.0322, -0.1292]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6904,  0.0015,  0.0297, -0.4115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 139 ] state=tensor([[-0.6904,  0.0015,  0.0297, -0.4115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6904,  0.1962,  0.0214, -0.6947]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 140 ] state=tensor([[-0.6904,  0.1962,  0.0214, -0.6947]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6864,  0.0008,  0.0075, -0.3954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 141 ] state=tensor([[-0.6864,  0.0008,  0.0075, -0.3954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-6.8642e-01, -1.9444e-01, -3.7111e-04, -1.0033e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 142 ] state=tensor([[-6.8642e-01, -1.9444e-01, -3.7111e-04, -1.0033e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6903, -0.3896, -0.0024,  0.1922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 143 ] state=tensor([[-0.6903, -0.3896, -0.0024,  0.1922]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6981, -0.5846,  0.0015,  0.4842]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 144 ] state=tensor([[-0.6981, -0.5846,  0.0015,  0.4842]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7098, -0.3895,  0.0112,  0.1919]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 145 ] state=tensor([[-0.7098, -0.3895,  0.0112,  0.1919]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7176, -0.5848,  0.0150,  0.4881]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 146 ] state=tensor([[-0.7176, -0.5848,  0.0150,  0.4881]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7293, -0.3899,  0.0248,  0.2002]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 147 ] state=tensor([[-0.7293, -0.3899,  0.0248,  0.2002]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7371, -0.1952,  0.0288, -0.0846]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 148 ] state=tensor([[-0.7371, -0.1952,  0.0288, -0.0846]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.4098e-01, -4.5543e-04,  2.7064e-02, -3.6804e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 149 ] state=tensor([[-7.4098e-01, -4.5543e-04,  2.7064e-02, -3.6804e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7410, -0.1960,  0.0197, -0.0670]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 150 ] state=tensor([[-0.7410, -0.1960,  0.0197, -0.0670]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7449, -0.3914,  0.0184,  0.2319]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 151 ] state=tensor([[-0.7449, -0.3914,  0.0184,  0.2319]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7527, -0.1965,  0.0230, -0.0550]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 152 ] state=tensor([[-0.7527, -0.1965,  0.0230, -0.0550]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7567, -0.0017,  0.0219, -0.3403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 153 ] state=tensor([[-0.7567, -0.0017,  0.0219, -0.3403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7567, -0.1971,  0.0151, -0.0408]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 154 ] state=tensor([[-0.7567, -0.1971,  0.0151, -0.0408]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7606, -0.0022,  0.0143, -0.3287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 155 ] state=tensor([[-0.7606, -0.0022,  0.0143, -0.3287]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7607, -0.1976,  0.0077, -0.0315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 156 ] state=tensor([[-0.7607, -0.1976,  0.0077, -0.0315]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7646, -0.3928,  0.0071,  0.2636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 157 ] state=tensor([[-0.7646, -0.3928,  0.0071,  0.2636]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7725, -0.5880,  0.0123,  0.5585]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 158 ] state=tensor([[-0.7725, -0.5880,  0.0123,  0.5585]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7843, -0.3931,  0.0235,  0.2697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 159 ] state=tensor([[-0.7843, -0.3931,  0.0235,  0.2697]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7921, -0.5885,  0.0289,  0.5697]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 160 ] state=tensor([[-0.7921, -0.5885,  0.0289,  0.5697]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8039, -0.3938,  0.0403,  0.2863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 161 ] state=tensor([[-0.8039, -0.3938,  0.0403,  0.2863]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8118, -0.1993,  0.0460,  0.0066]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 162 ] state=tensor([[-0.8118, -0.1993,  0.0460,  0.0066]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8157, -0.0049,  0.0462, -0.2712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 163 ] state=tensor([[-0.8157, -0.0049,  0.0462, -0.2712]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8158,  0.1896,  0.0407, -0.5490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 164 ] state=tensor([[-0.8158,  0.1896,  0.0407, -0.5490]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8120,  0.3841,  0.0298, -0.8286]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 165 ] state=tensor([[-0.8120,  0.3841,  0.0298, -0.8286]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8044,  0.1886,  0.0132, -0.5267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 166 ] state=tensor([[-0.8044,  0.1886,  0.0132, -0.5267]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8006, -0.0067,  0.0027, -0.2298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 167 ] state=tensor([[-0.8006, -0.0067,  0.0027, -0.2298]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8007, -0.2019, -0.0019,  0.0637]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 168 ] state=tensor([[-0.8007, -0.2019, -0.0019,  0.0637]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.0477e-01, -6.7232e-03, -6.6402e-04, -2.2962e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 169 ] state=tensor([[-8.0477e-01, -6.7232e-03, -6.6402e-04, -2.2962e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8049, -0.2018, -0.0053,  0.0629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 170 ] state=tensor([[-0.8049, -0.2018, -0.0053,  0.0629]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8089, -0.3969, -0.0040,  0.3539]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 171 ] state=tensor([[-0.8089, -0.3969, -0.0040,  0.3539]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8169, -0.2017,  0.0031,  0.0599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 172 ] state=tensor([[-0.8169, -0.2017,  0.0031,  0.0599]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8209, -0.3969,  0.0043,  0.3536]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 173 ] state=tensor([[-0.8209, -0.3969,  0.0043,  0.3536]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8288, -0.2018,  0.0113,  0.0623]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 174 ] state=tensor([[-0.8288, -0.2018,  0.0113,  0.0623]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8329, -0.3971,  0.0126,  0.3585]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 175 ] state=tensor([[-0.8329, -0.3971,  0.0126,  0.3585]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8408, -0.5924,  0.0198,  0.6551]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 176 ] state=tensor([[-0.8408, -0.5924,  0.0198,  0.6551]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8527, -0.3975,  0.0329,  0.3687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 177 ] state=tensor([[-0.8527, -0.3975,  0.0329,  0.3687]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8606, -0.2029,  0.0402,  0.0866]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 178 ] state=tensor([[-0.8606, -0.2029,  0.0402,  0.0866]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8647, -0.0084,  0.0420, -0.1931]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 179 ] state=tensor([[-0.8647, -0.0084,  0.0420, -0.1931]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8648, -0.2041,  0.0381,  0.1125]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 180 ] state=tensor([[-0.8648, -0.2041,  0.0381,  0.1125]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8689, -0.0095,  0.0404, -0.1679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 181 ] state=tensor([[-0.8689, -0.0095,  0.0404, -0.1679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8691,  0.1850,  0.0370, -0.4476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 182 ] state=tensor([[-0.8691,  0.1850,  0.0370, -0.4476]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8654,  0.3796,  0.0280, -0.7284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 183 ] state=tensor([[-0.8654,  0.3796,  0.0280, -0.7284]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8578,  0.1841,  0.0135, -0.4270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 184 ] state=tensor([[-0.8578,  0.1841,  0.0135, -0.4270]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8541, -0.0112,  0.0049, -0.1301]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 185 ] state=tensor([[-0.8541, -0.0112,  0.0049, -0.1301]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8544, -0.2064,  0.0023,  0.1641]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 186 ] state=tensor([[-0.8544, -0.2064,  0.0023,  0.1641]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8585, -0.0113,  0.0056, -0.1278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 187 ] state=tensor([[-0.8585, -0.0113,  0.0056, -0.1278]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8587,  0.1837,  0.0031, -0.4187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 188 ] state=tensor([[-0.8587,  0.1837,  0.0031, -0.4187]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8551, -0.0115, -0.0053, -0.1251]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 189 ] state=tensor([[-0.8551, -0.0115, -0.0053, -0.1251]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8553,  0.1837, -0.0078, -0.4195]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 190 ] state=tensor([[-0.8553,  0.1837, -0.0078, -0.4195]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8516, -0.0113, -0.0162, -0.1292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 191 ] state=tensor([[-0.8516, -0.0113, -0.0162, -0.1292]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8518,  0.1841, -0.0188, -0.4270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 192 ] state=tensor([[-0.8518,  0.1841, -0.0188, -0.4270]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8482, -0.0108, -0.0273, -0.1403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 193 ] state=tensor([[-0.8482, -0.0108, -0.0273, -0.1403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8484, -0.2055, -0.0301,  0.1436]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 194 ] state=tensor([[-0.8484, -0.2055, -0.0301,  0.1436]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8525, -0.4002, -0.0273,  0.4267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 195 ] state=tensor([[-0.8525, -0.4002, -0.0273,  0.4267]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8605, -0.2047, -0.0187,  0.1255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 196 ] state=tensor([[-0.8605, -0.2047, -0.0187,  0.1255]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8646, -0.3995, -0.0162,  0.4122]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 197 ] state=tensor([[-0.8646, -0.3995, -0.0162,  0.4122]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8726, -0.2042, -0.0080,  0.1145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 198 ] state=tensor([[-0.8726, -0.2042, -0.0080,  0.1145]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8766, -0.3992, -0.0057,  0.4046]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 131 ][ timestamp 199 ] state=tensor([[-0.8766, -0.3992, -0.0057,  0.4046]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8846, -0.2040,  0.0024,  0.1102]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 200 ] state=tensor([[-0.8846, -0.2040,  0.0024,  0.1102]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8887, -0.0089,  0.0046, -0.1817]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 201 ] state=tensor([[-0.8887, -0.0089,  0.0046, -0.1817]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8889,  0.1862,  0.0010, -0.4730]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 202 ] state=tensor([[-0.8889,  0.1862,  0.0010, -0.4730]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8852, -0.0090, -0.0085, -0.1800]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 203 ] state=tensor([[-0.8852, -0.0090, -0.0085, -0.1800]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8853,  0.1863, -0.0121, -0.4753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 204 ] state=tensor([[-0.8853,  0.1863, -0.0121, -0.4753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8816, -0.0087, -0.0216, -0.1865]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 205 ] state=tensor([[-0.8816, -0.0087, -0.0216, -0.1865]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8818, -0.2035, -0.0253,  0.0993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 206 ] state=tensor([[-0.8818, -0.2035, -0.0253,  0.0993]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8859, -0.0080, -0.0233, -0.2012]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 207 ] state=tensor([[-0.8859, -0.0080, -0.0233, -0.2012]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8860, -0.2028, -0.0274,  0.0840]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 208 ] state=tensor([[-0.8860, -0.2028, -0.0274,  0.0840]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8901, -0.3975, -0.0257,  0.3679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 209 ] state=tensor([[-0.8901, -0.3975, -0.0257,  0.3679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8980, -0.2020, -0.0183,  0.0673]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 210 ] state=tensor([[-0.8980, -0.2020, -0.0183,  0.0673]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9021, -0.0067, -0.0170, -0.2312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 211 ] state=tensor([[-0.9021, -0.0067, -0.0170, -0.2312]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9022, -0.2015, -0.0216,  0.0561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 212 ] state=tensor([[-0.9022, -0.2015, -0.0216,  0.0561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9062, -0.3963, -0.0205,  0.3419]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 213 ] state=tensor([[-0.9062, -0.3963, -0.0205,  0.3419]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9142, -0.2009, -0.0136,  0.0429]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 214 ] state=tensor([[-0.9142, -0.2009, -0.0136,  0.0429]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9182, -0.0056, -0.0128, -0.2541]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 215 ] state=tensor([[-0.9182, -0.0056, -0.0128, -0.2541]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9183,  0.1897, -0.0179, -0.5508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 216 ] state=tensor([[-0.9183,  0.1897, -0.0179, -0.5508]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9145, -0.0052, -0.0289, -0.2638]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 217 ] state=tensor([[-0.9145, -0.0052, -0.0289, -0.2638]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9146, -0.1999, -0.0341,  0.0197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 218 ] state=tensor([[-0.9146, -0.1999, -0.0341,  0.0197]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9186, -0.3945, -0.0338,  0.3014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 219 ] state=tensor([[-0.9186, -0.3945, -0.0338,  0.3014]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9265, -0.1989, -0.0277, -0.0018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 220 ] state=tensor([[-0.9265, -0.1989, -0.0277, -0.0018]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9305, -0.3936, -0.0278,  0.2820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 221 ] state=tensor([[-0.9305, -0.3936, -0.0278,  0.2820]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9383, -0.5883, -0.0221,  0.5658]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 222 ] state=tensor([[-0.9383, -0.5883, -0.0221,  0.5658]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9501, -0.3929, -0.0108,  0.2663]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 223 ] state=tensor([[-0.9501, -0.3929, -0.0108,  0.2663]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9580, -0.5879, -0.0055,  0.5555]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 224 ] state=tensor([[-0.9580, -0.5879, -0.0055,  0.5555]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9697, -0.3927,  0.0056,  0.2611]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 225 ] state=tensor([[-0.9697, -0.3927,  0.0056,  0.2611]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9776, -0.1976,  0.0109, -0.0298]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 226 ] state=tensor([[-0.9776, -0.1976,  0.0109, -0.0298]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9815, -0.0027,  0.0103, -0.3190]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 227 ] state=tensor([[-0.9815, -0.0027,  0.0103, -0.3190]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9816,  0.1923,  0.0039, -0.6084]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 228 ] state=tensor([[-0.9816,  0.1923,  0.0039, -0.6084]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9777, -0.0029, -0.0083, -0.3145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 229 ] state=tensor([[-0.9777, -0.0029, -0.0083, -0.3145]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9778,  0.1924, -0.0146, -0.6098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 230 ] state=tensor([[-0.9778,  0.1924, -0.0146, -0.6098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9739, -0.0025, -0.0268, -0.3218]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 231 ] state=tensor([[-0.9739, -0.0025, -0.0268, -0.3218]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9740, -0.1973, -0.0332, -0.0377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 232 ] state=tensor([[-0.9740, -0.1973, -0.0332, -0.0377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9779, -0.3919, -0.0340,  0.2444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 233 ] state=tensor([[-0.9779, -0.3919, -0.0340,  0.2444]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9858, -0.5865, -0.0291,  0.5261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 234 ] state=tensor([[-0.9858, -0.5865, -0.0291,  0.5261]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9975, -0.3910, -0.0186,  0.2244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 235 ] state=tensor([[-0.9975, -0.3910, -0.0186,  0.2244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0053, -0.5859, -0.0141,  0.5112]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 236 ] state=tensor([[-1.0053, -0.5859, -0.0141,  0.5112]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0171, -0.7808, -0.0038,  0.7994]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 237 ] state=tensor([[-1.0171, -0.7808, -0.0038,  0.7994]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0327, -0.5856,  0.0121,  0.5055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 238 ] state=tensor([[-1.0327, -0.5856,  0.0121,  0.5055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0444, -0.3907,  0.0223,  0.2167]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 239 ] state=tensor([[-1.0444, -0.3907,  0.0223,  0.2167]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0522, -0.1959,  0.0266, -0.0689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 240 ] state=tensor([[-1.0522, -0.1959,  0.0266, -0.0689]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0561, -0.0011,  0.0252, -0.3531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 241 ] state=tensor([[-1.0561, -0.0011,  0.0252, -0.3531]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0561, -0.1966,  0.0181, -0.0525]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 242 ] state=tensor([[-1.0561, -0.1966,  0.0181, -0.0525]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0601, -0.0017,  0.0171, -0.3394]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 243 ] state=tensor([[-1.0601, -0.0017,  0.0171, -0.3394]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0601,  0.1931,  0.0103, -0.6267]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 244 ] state=tensor([[-1.0601,  0.1931,  0.0103, -0.6267]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0562, -0.0021, -0.0022, -0.3308]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 245 ] state=tensor([[-1.0562, -0.0021, -0.0022, -0.3308]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0563,  0.1930, -0.0088, -0.6242]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 246 ] state=tensor([[-1.0563,  0.1930, -0.0088, -0.6242]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0524, -0.0020, -0.0213, -0.3343]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 247 ] state=tensor([[-1.0524, -0.0020, -0.0213, -0.3343]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0525, -0.1968, -0.0280, -0.0484]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 248 ] state=tensor([[-1.0525, -0.1968, -0.0280, -0.0484]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0564, -0.3915, -0.0290,  0.2353]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 249 ] state=tensor([[-1.0564, -0.3915, -0.0290,  0.2353]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0642, -0.5862, -0.0243,  0.5187]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 250 ] state=tensor([[-1.0642, -0.5862, -0.0243,  0.5187]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0759, -0.3907, -0.0139,  0.2185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 251 ] state=tensor([[-1.0759, -0.3907, -0.0139,  0.2185]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0838, -0.5857, -0.0095,  0.5068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 252 ] state=tensor([[-1.0838, -0.5857, -0.0095,  0.5068]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0955e+00, -3.9041e-01,  6.0732e-04,  2.1109e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 253 ] state=tensor([[-1.0955e+00, -3.9041e-01,  6.0732e-04,  2.1109e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1033, -0.1953,  0.0048, -0.0814]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 254 ] state=tensor([[-1.1033, -0.1953,  0.0048, -0.0814]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1072, -0.3905,  0.0032,  0.2128]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 255 ] state=tensor([[-1.1072, -0.3905,  0.0032,  0.2128]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1150, -0.1954,  0.0075, -0.0789]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 256 ] state=tensor([[-1.1150, -0.1954,  0.0075, -0.0789]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1189e+00, -3.9681e-04,  5.8798e-03, -3.6919e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 257 ] state=tensor([[-1.1189e+00, -3.9681e-04,  5.8798e-03, -3.6919e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1189,  0.1946, -0.0015, -0.6600]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 258 ] state=tensor([[-1.1189,  0.1946, -0.0015, -0.6600]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1150e+00, -4.5988e-04, -1.4704e-02, -3.6780e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 259 ] state=tensor([[-1.1150e+00, -4.5988e-04, -1.4704e-02, -3.6780e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1150, -0.1954, -0.0221, -0.0798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 260 ] state=tensor([[-1.1150, -0.1954, -0.0221, -0.0798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1189, -0.3902, -0.0237,  0.2058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 261 ] state=tensor([[-1.1189, -0.3902, -0.0237,  0.2058]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1267, -0.5849, -0.0195,  0.4910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 262 ] state=tensor([[-1.1267, -0.5849, -0.0195,  0.4910]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1384, -0.3896, -0.0097,  0.1922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 263 ] state=tensor([[-1.1384, -0.3896, -0.0097,  0.1922]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1462, -0.1943, -0.0059, -0.1035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 264 ] state=tensor([[-1.1462, -0.1943, -0.0059, -0.1035]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1501, -0.3893, -0.0079,  0.1873]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 265 ] state=tensor([[-1.1501, -0.3893, -0.0079,  0.1873]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1579, -0.1941, -0.0042, -0.1079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 266 ] state=tensor([[-1.1579, -0.1941, -0.0042, -0.1079]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1618e+00,  1.0865e-03, -6.3583e-03, -4.0189e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 267 ] state=tensor([[-1.1618e+00,  1.0865e-03, -6.3583e-03, -4.0189e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1618,  0.1963, -0.0144, -0.6966]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 268 ] state=tensor([[-1.1618,  0.1963, -0.0144, -0.6966]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1578,  0.0014, -0.0283, -0.4085]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 131 ][ timestamp 269 ] state=tensor([[-1.1578,  0.0014, -0.0283, -0.4085]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1578, -0.1933, -0.0365, -0.1248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 270 ] state=tensor([[-1.1578, -0.1933, -0.0365, -0.1248]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1617, -0.3879, -0.0390,  0.1561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 271 ] state=tensor([[-1.1617, -0.3879, -0.0390,  0.1561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1694, -0.5825, -0.0359,  0.4362]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 272 ] state=tensor([[-1.1694, -0.5825, -0.0359,  0.4362]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1811, -0.3868, -0.0271,  0.1325]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 273 ] state=tensor([[-1.1811, -0.3868, -0.0271,  0.1325]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1888, -0.5816, -0.0245,  0.4165]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 274 ] state=tensor([[-1.1888, -0.5816, -0.0245,  0.4165]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2005, -0.3861, -0.0162,  0.1162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 275 ] state=tensor([[-1.2005, -0.3861, -0.0162,  0.1162]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2082, -0.1908, -0.0138, -0.1816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 276 ] state=tensor([[-1.2082, -0.1908, -0.0138, -0.1816]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2120,  0.0046, -0.0175, -0.4786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 277 ] state=tensor([[-1.2120,  0.0046, -0.0175, -0.4786]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2119, -0.1903, -0.0270, -0.1915]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 278 ] state=tensor([[-1.2119, -0.1903, -0.0270, -0.1915]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2157,  0.0052, -0.0309, -0.4926]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 279 ] state=tensor([[-1.2157,  0.0052, -0.0309, -0.4926]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2156, -0.1895, -0.0407, -0.2098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 280 ] state=tensor([[-1.2156, -0.1895, -0.0407, -0.2098]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2194, -0.3840, -0.0449,  0.0698]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 281 ] state=tensor([[-1.2194, -0.3840, -0.0449,  0.0698]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2271, -0.5785, -0.0435,  0.3480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 282 ] state=tensor([[-1.2271, -0.5785, -0.0435,  0.3480]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2386, -0.3827, -0.0366,  0.0419]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 283 ] state=tensor([[-1.2386, -0.3827, -0.0366,  0.0419]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2463, -0.5773, -0.0357,  0.3228]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 284 ] state=tensor([[-1.2463, -0.5773, -0.0357,  0.3228]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2578, -0.7719, -0.0293,  0.6040]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 285 ] state=tensor([[-1.2578, -0.7719, -0.0293,  0.6040]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2733, -0.5764, -0.0172,  0.3023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 286 ] state=tensor([[-1.2733, -0.5764, -0.0172,  0.3023]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2848, -0.7713, -0.0111,  0.5895]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 287 ] state=tensor([[-1.2848, -0.7713, -0.0111,  0.5895]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3002e+00, -5.7599e-01,  6.3967e-04,  2.9329e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 288 ] state=tensor([[-1.3002e+00, -5.7599e-01,  6.3967e-04,  2.9329e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3118e+00, -3.8088e-01,  6.5055e-03,  8.1113e-04]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 289 ] state=tensor([[-1.3118e+00, -3.8088e-01,  6.5055e-03,  8.1113e-04]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3194, -0.1858,  0.0065, -0.2898]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 290 ] state=tensor([[-1.3194, -0.1858,  0.0065, -0.2898]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3231e+00,  9.1800e-03,  7.2550e-04, -5.8043e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 291 ] state=tensor([[-1.3231e+00,  9.1800e-03,  7.2550e-04, -5.8043e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3229, -0.1860, -0.0109, -0.2875]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 292 ] state=tensor([[-1.3229, -0.1860, -0.0109, -0.2875]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3266,  0.0093, -0.0166, -0.5836]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 293 ] state=tensor([[-1.3266,  0.0093, -0.0166, -0.5836]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3264, -0.1856, -0.0283, -0.2962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 294 ] state=tensor([[-1.3264, -0.1856, -0.0283, -0.2962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3301, -0.3803, -0.0342, -0.0126]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 295 ] state=tensor([[-1.3301, -0.3803, -0.0342, -0.0126]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3378, -0.1847, -0.0345, -0.3159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 296 ] state=tensor([[-1.3378, -0.1847, -0.0345, -0.3159]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3414, -0.3793, -0.0408, -0.0343]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 297 ] state=tensor([[-1.3414, -0.3793, -0.0408, -0.0343]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3490, -0.5738, -0.0415,  0.2453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 298 ] state=tensor([[-1.3490, -0.5738, -0.0415,  0.2453]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3605, -0.7683, -0.0366,  0.5246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 299 ] state=tensor([[-1.3605, -0.7683, -0.0366,  0.5246]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3759, -0.9629, -0.0261,  0.8055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 300 ] state=tensor([[-1.3759, -0.9629, -0.0261,  0.8055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3951, -0.7674, -0.0100,  0.5047]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 301 ] state=tensor([[-1.3951, -0.7674, -0.0100,  0.5047]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4105e+00, -5.7216e-01,  1.1744e-04,  2.0894e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 302 ] state=tensor([[-1.4105e+00, -5.7216e-01,  1.1744e-04,  2.0894e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4219, -0.3770,  0.0043, -0.0837]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 303 ] state=tensor([[-1.4219, -0.3770,  0.0043, -0.0837]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4295, -0.5722,  0.0026,  0.2103]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 304 ] state=tensor([[-1.4295, -0.5722,  0.0026,  0.2103]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4409, -0.7674,  0.0068,  0.5038]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 305 ] state=tensor([[-1.4409, -0.7674,  0.0068,  0.5038]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4563, -0.5724,  0.0169,  0.2133]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 306 ] state=tensor([[-1.4563, -0.5724,  0.0169,  0.2133]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4677, -0.3775,  0.0212, -0.0740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 307 ] state=tensor([[-1.4677, -0.3775,  0.0212, -0.0740]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4753, -0.1827,  0.0197, -0.3599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 308 ] state=tensor([[-1.4753, -0.1827,  0.0197, -0.3599]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4789,  0.0122,  0.0125, -0.6463]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 309 ] state=tensor([[-1.4789,  0.0122,  0.0125, -0.6463]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4787e+00, -1.8313e-01, -4.3351e-04, -3.4974e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 310 ] state=tensor([[-1.4787e+00, -1.8313e-01, -4.3351e-04, -3.4974e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4823, -0.3782, -0.0074, -0.0572]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 311 ] state=tensor([[-1.4823, -0.3782, -0.0074, -0.0572]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4899, -0.1830, -0.0086, -0.3522]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 312 ] state=tensor([[-1.4899, -0.1830, -0.0086, -0.3522]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4936,  0.0122, -0.0156, -0.6476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 313 ] state=tensor([[-1.4936,  0.0122, -0.0156, -0.6476]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4933, -0.1827, -0.0286, -0.3599]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 314 ] state=tensor([[-1.4933, -0.1827, -0.0286, -0.3599]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4970, -0.3774, -0.0358, -0.0763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 315 ] state=tensor([[-1.4970, -0.3774, -0.0358, -0.0763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5045, -0.5720, -0.0373,  0.2049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 316 ] state=tensor([[-1.5045, -0.5720, -0.0373,  0.2049]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5159, -0.7665, -0.0332,  0.4856]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 317 ] state=tensor([[-1.5159, -0.7665, -0.0332,  0.4856]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5313, -0.5710, -0.0235,  0.1826]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 318 ] state=tensor([[-1.5313, -0.5710, -0.0235,  0.1826]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5427, -0.3755, -0.0198, -0.1174]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 319 ] state=tensor([[-1.5427, -0.3755, -0.0198, -0.1174]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5502, -0.1801, -0.0222, -0.4163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 320 ] state=tensor([[-1.5502, -0.1801, -0.0222, -0.4163]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5538, -0.3749, -0.0305, -0.1307]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 321 ] state=tensor([[-1.5538, -0.3749, -0.0305, -0.1307]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5613, -0.1794, -0.0331, -0.4328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 322 ] state=tensor([[-1.5613, -0.1794, -0.0331, -0.4328]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5649, -0.3740, -0.0418, -0.1507]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 323 ] state=tensor([[-1.5649, -0.3740, -0.0418, -0.1507]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5724, -0.5685, -0.0448,  0.1285]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 324 ] state=tensor([[-1.5724, -0.5685, -0.0448,  0.1285]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5837, -0.7630, -0.0422,  0.4067]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 325 ] state=tensor([[-1.5837, -0.7630, -0.0422,  0.4067]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5990, -0.5673, -0.0341,  0.1010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 326 ] state=tensor([[-1.5990, -0.5673, -0.0341,  0.1010]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6104, -0.3717, -0.0321, -0.2022]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 327 ] state=tensor([[-1.6104, -0.3717, -0.0321, -0.2022]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6178, -0.5663, -0.0361,  0.0802]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 328 ] state=tensor([[-1.6178, -0.5663, -0.0361,  0.0802]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6291, -0.7609, -0.0345,  0.3612]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 329 ] state=tensor([[-1.6291, -0.7609, -0.0345,  0.3612]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6443, -0.5653, -0.0273,  0.0579]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 330 ] state=tensor([[-1.6443, -0.5653, -0.0273,  0.0579]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6556, -0.3698, -0.0261, -0.2433]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 331 ] state=tensor([[-1.6556, -0.3698, -0.0261, -0.2433]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6630, -0.1743, -0.0310, -0.5441]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 332 ] state=tensor([[-1.6630, -0.1743, -0.0310, -0.5441]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6665, -0.3690, -0.0419, -0.2613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 333 ] state=tensor([[-1.6665, -0.3690, -0.0419, -0.2613]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6739, -0.1733, -0.0471, -0.5669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 334 ] state=tensor([[-1.6739, -0.1733, -0.0471, -0.5669]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6774, -0.3677, -0.0584, -0.2894]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 335 ] state=tensor([[-1.6774, -0.3677, -0.0584, -0.2894]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6847, -0.5620, -0.0642, -0.0157]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 336 ] state=tensor([[-1.6847, -0.5620, -0.0642, -0.0157]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6960, -0.7561, -0.0645,  0.2560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 337 ] state=tensor([[-1.6960, -0.7561, -0.0645,  0.2560]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7111, -0.9503, -0.0594,  0.5276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 338 ] state=tensor([[-1.7111, -0.9503, -0.0594,  0.5276]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7301, -1.1445, -0.0489,  0.8010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 339 ] state=tensor([[-1.7301, -1.1445, -0.0489,  0.8010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7530, -1.3389, -0.0328,  1.0779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 340 ] state=tensor([[-1.7530, -1.3389, -0.0328,  1.0779]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7798, -1.5336, -0.0113,  1.3601]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 131 ][ timestamp 341 ] state=tensor([[-1.7798, -1.5336, -0.0113,  1.3601]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8104, -1.3383,  0.0159,  1.0640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 342 ] state=tensor([[-1.8104, -1.3383,  0.0159,  1.0640]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8372, -1.5337,  0.0372,  1.3616]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 343 ] state=tensor([[-1.8372, -1.5337,  0.0372,  1.3616]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8679, -1.3390,  0.0644,  1.0808]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 344 ] state=tensor([[-1.8679, -1.3390,  0.0644,  1.0808]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8947, -1.5349,  0.0860,  1.3929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 345 ] state=tensor([[-1.8947, -1.5349,  0.0860,  1.3929]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9253, -1.3410,  0.1139,  1.1284]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 346 ] state=tensor([[-1.9253, -1.3410,  0.1139,  1.1284]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9522, -1.5374,  0.1365,  1.4545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 347 ] state=tensor([[-1.9522, -1.5374,  0.1365,  1.4545]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9829, -1.3442,  0.1656,  1.2074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 348 ] state=tensor([[-1.9829, -1.3442,  0.1656,  1.2074]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0098, -1.5410,  0.1897,  1.5470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 131 ][ timestamp 349 ] state=tensor([[-2.0098, -1.5410,  0.1897,  1.5470]]), action=tensor([[0]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 131: Exploration_rate=0.05. Score=349.\n",
      "[ episode 132 ] state=tensor([[ 0.0057, -0.0370, -0.0324, -0.0287]])\n",
      "[ episode 132 ][ timestamp 1 ] state=tensor([[ 0.0057, -0.0370, -0.0324, -0.0287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0049,  0.1586, -0.0330, -0.3314]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 2 ] state=tensor([[ 0.0049,  0.1586, -0.0330, -0.3314]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0081, -0.0360, -0.0396, -0.0493]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 3 ] state=tensor([[ 0.0081, -0.0360, -0.0396, -0.0493]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0074,  0.1596, -0.0406, -0.3543]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 4 ] state=tensor([[ 0.0074,  0.1596, -0.0406, -0.3543]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0106, -0.0349, -0.0477, -0.0747]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 5 ] state=tensor([[ 0.0106, -0.0349, -0.0477, -0.0747]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0099,  0.1609, -0.0492, -0.3820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 6 ] state=tensor([[ 0.0099,  0.1609, -0.0492, -0.3820]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0131, -0.0335, -0.0568, -0.1052]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 7 ] state=tensor([[ 0.0131, -0.0335, -0.0568, -0.1052]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0124,  0.1624, -0.0590, -0.4153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 8 ] state=tensor([[ 0.0124,  0.1624, -0.0590, -0.4153]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0157, -0.0318, -0.0673, -0.1418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 9 ] state=tensor([[ 0.0157, -0.0318, -0.0673, -0.1418]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0150, -0.2259, -0.0701,  0.1290]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 10 ] state=tensor([[ 0.0150, -0.2259, -0.0701,  0.1290]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0105, -0.4200, -0.0675,  0.3987]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 11 ] state=tensor([[ 0.0105, -0.4200, -0.0675,  0.3987]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0021, -0.2240, -0.0595,  0.0855]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 12 ] state=tensor([[ 0.0021, -0.2240, -0.0595,  0.0855]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0024, -0.4182, -0.0578,  0.3589]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 13 ] state=tensor([[-0.0024, -0.4182, -0.0578,  0.3589]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0107, -0.6125, -0.0507,  0.6328]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 14 ] state=tensor([[-0.0107, -0.6125, -0.0507,  0.6328]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0230, -0.4167, -0.0380,  0.3246]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 15 ] state=tensor([[-0.0230, -0.4167, -0.0380,  0.3246]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0313, -0.2210, -0.0315,  0.0201]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 16 ] state=tensor([[-0.0313, -0.2210, -0.0315,  0.0201]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0357, -0.4157, -0.0311,  0.3027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 17 ] state=tensor([[-0.0357, -0.4157, -0.0311,  0.3027]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0441, -0.6103, -0.0250,  0.5854]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 18 ] state=tensor([[-0.0441, -0.6103, -0.0250,  0.5854]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0563, -0.4149, -0.0133,  0.2850]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 19 ] state=tensor([[-0.0563, -0.4149, -0.0133,  0.2850]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0646, -0.2196, -0.0076, -0.0119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 20 ] state=tensor([[-0.0646, -0.2196, -0.0076, -0.0119]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0689, -0.4146, -0.0079,  0.2784]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 21 ] state=tensor([[-0.0689, -0.4146, -0.0079,  0.2784]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0772, -0.2193, -0.0023, -0.0168]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 22 ] state=tensor([[-0.0772, -0.2193, -0.0023, -0.0168]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0816, -0.4144, -0.0026,  0.2752]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 23 ] state=tensor([[-0.0816, -0.4144, -0.0026,  0.2752]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0899, -0.6095,  0.0029,  0.5670]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 24 ] state=tensor([[-0.0899, -0.6095,  0.0029,  0.5670]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1021, -0.4144,  0.0142,  0.2752]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 25 ] state=tensor([[-0.1021, -0.4144,  0.0142,  0.2752]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1104, -0.2195,  0.0197, -0.0129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 26 ] state=tensor([[-0.1104, -0.2195,  0.0197, -0.0129]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1148, -0.0247,  0.0194, -0.2993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 27 ] state=tensor([[-0.1148, -0.0247,  0.0194, -0.2993]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1153, -0.2201,  0.0135, -0.0006]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 28 ] state=tensor([[-0.1153, -0.2201,  0.0135, -0.0006]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1197, -0.0252,  0.0134, -0.2890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 29 ] state=tensor([[-0.1197, -0.0252,  0.0134, -0.2890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1202,  0.1698,  0.0077, -0.5774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 30 ] state=tensor([[-0.1202,  0.1698,  0.0077, -0.5774]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1168, -0.0255, -0.0039, -0.2823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 31 ] state=tensor([[-0.1168, -0.0255, -0.0039, -0.2823]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1173,  0.1697, -0.0095, -0.5762]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 32 ] state=tensor([[-0.1173,  0.1697, -0.0095, -0.5762]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1139, -0.0253, -0.0211, -0.2866]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 33 ] state=tensor([[-0.1139, -0.0253, -0.0211, -0.2866]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1144,  0.1702, -0.0268, -0.5858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 34 ] state=tensor([[-0.1144,  0.1702, -0.0268, -0.5858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1110, -0.0246, -0.0385, -0.3017]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 35 ] state=tensor([[-0.1110, -0.0246, -0.0385, -0.3017]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1115, -0.2191, -0.0445, -0.0214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 36 ] state=tensor([[-0.1115, -0.2191, -0.0445, -0.0214]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1159, -0.4136, -0.0450,  0.2569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 37 ] state=tensor([[-0.1159, -0.4136, -0.0450,  0.2569]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1241, -0.2179, -0.0398, -0.0496]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 38 ] state=tensor([[-0.1241, -0.2179, -0.0398, -0.0496]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1285, -0.4124, -0.0408,  0.2303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 39 ] state=tensor([[-0.1285, -0.4124, -0.0408,  0.2303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1368, -0.6069, -0.0362,  0.5098]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 40 ] state=tensor([[-0.1368, -0.6069, -0.0362,  0.5098]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1489, -0.4113, -0.0260,  0.2059]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 41 ] state=tensor([[-0.1489, -0.4113, -0.0260,  0.2059]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1571, -0.2158, -0.0219, -0.0948]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 42 ] state=tensor([[-0.1571, -0.2158, -0.0219, -0.0948]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1614, -0.4106, -0.0238,  0.1909]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 43 ] state=tensor([[-0.1614, -0.4106, -0.0238,  0.1909]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1696, -0.2152, -0.0200, -0.1092]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 44 ] state=tensor([[-0.1696, -0.2152, -0.0200, -0.1092]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1739, -0.4100, -0.0222,  0.1771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 45 ] state=tensor([[-0.1739, -0.4100, -0.0222,  0.1771]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1821, -0.2146, -0.0186, -0.1225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 46 ] state=tensor([[-0.1821, -0.2146, -0.0186, -0.1225]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1864, -0.0192, -0.0211, -0.4210]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 47 ] state=tensor([[-0.1864, -0.0192, -0.0211, -0.4210]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1868,  0.1762, -0.0295, -0.7203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 48 ] state=tensor([[-0.1868,  0.1762, -0.0295, -0.7203]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1833, -0.0185, -0.0439, -0.4370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 49 ] state=tensor([[-0.1833, -0.0185, -0.0439, -0.4370]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1837, -0.2129, -0.0526, -0.1585]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 50 ] state=tensor([[-0.1837, -0.2129, -0.0526, -0.1585]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1879, -0.4073, -0.0558,  0.1172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 51 ] state=tensor([[-0.1879, -0.4073, -0.0558,  0.1172]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1961, -0.6015, -0.0535,  0.3917]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 52 ] state=tensor([[-0.1961, -0.6015, -0.0535,  0.3917]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2081, -0.7959, -0.0456,  0.6671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 53 ] state=tensor([[-0.2081, -0.7959, -0.0456,  0.6671]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2240, -0.9903, -0.0323,  0.9451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 54 ] state=tensor([[-0.2240, -0.9903, -0.0323,  0.9451]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2438, -1.1850, -0.0134,  1.2274]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 55 ] state=tensor([[-0.2438, -1.1850, -0.0134,  1.2274]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2675, -0.9897,  0.0112,  0.9306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 56 ] state=tensor([[-0.2675, -0.9897,  0.0112,  0.9306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2873, -0.7947,  0.0298,  0.6414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 57 ] state=tensor([[-0.2873, -0.7947,  0.0298,  0.6414]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3032, -0.6000,  0.0426,  0.3583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 58 ] state=tensor([[-0.3032, -0.6000,  0.0426,  0.3583]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3152, -0.4056,  0.0498,  0.0793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 59 ] state=tensor([[-0.3152, -0.4056,  0.0498,  0.0793]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3233, -0.2112,  0.0514, -0.1972]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 60 ] state=tensor([[-0.3233, -0.2112,  0.0514, -0.1972]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3276, -0.0168,  0.0474, -0.4733]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 61 ] state=tensor([[-0.3276, -0.0168,  0.0474, -0.4733]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3279,  0.1776,  0.0379, -0.7507]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 62 ] state=tensor([[-0.3279,  0.1776,  0.0379, -0.7507]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3243, -0.0180,  0.0229, -0.4463]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 132 ][ timestamp 63 ] state=tensor([[-0.3243, -0.0180,  0.0229, -0.4463]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3247,  0.1768,  0.0140, -0.7317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 64 ] state=tensor([[-0.3247,  0.1768,  0.0140, -0.7317]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3212, -0.0186, -0.0006, -0.4346]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 65 ] state=tensor([[-0.3212, -0.0186, -0.0006, -0.4346]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3215, -0.2137, -0.0093, -0.1421]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 66 ] state=tensor([[-0.3215, -0.2137, -0.0093, -0.1421]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3258, -0.0184, -0.0122, -0.4377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 67 ] state=tensor([[-0.3258, -0.0184, -0.0122, -0.4377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3262, -0.2134, -0.0209, -0.1489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 68 ] state=tensor([[-0.3262, -0.2134, -0.0209, -0.1489]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3304, -0.4082, -0.0239,  0.1371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 69 ] state=tensor([[-0.3304, -0.4082, -0.0239,  0.1371]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3386, -0.2127, -0.0211, -0.1630]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 70 ] state=tensor([[-0.3386, -0.2127, -0.0211, -0.1630]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3429, -0.4075, -0.0244,  0.1229]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 71 ] state=tensor([[-0.3429, -0.4075, -0.0244,  0.1229]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3510, -0.2121, -0.0219, -0.1773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 72 ] state=tensor([[-0.3510, -0.2121, -0.0219, -0.1773]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3552, -0.0166, -0.0255, -0.4769]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 73 ] state=tensor([[-0.3552, -0.0166, -0.0255, -0.4769]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3556, -0.2114, -0.0350, -0.1923]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 74 ] state=tensor([[-0.3556, -0.2114, -0.0350, -0.1923]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3598, -0.4060, -0.0389,  0.0891]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 75 ] state=tensor([[-0.3598, -0.4060, -0.0389,  0.0891]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3679, -0.6005, -0.0371,  0.3693]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 76 ] state=tensor([[-0.3679, -0.6005, -0.0371,  0.3693]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3799, -0.7951, -0.0297,  0.6500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 77 ] state=tensor([[-0.3799, -0.7951, -0.0297,  0.6500]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3958, -0.5996, -0.0167,  0.3481]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 78 ] state=tensor([[-0.3958, -0.5996, -0.0167,  0.3481]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4078, -0.7945, -0.0097,  0.6355]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 79 ] state=tensor([[-0.4078, -0.7945, -0.0097,  0.6355]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4237, -0.5992,  0.0030,  0.3398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 80 ] state=tensor([[-0.4237, -0.5992,  0.0030,  0.3398]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4357, -0.4041,  0.0098,  0.0480]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 81 ] state=tensor([[-0.4357, -0.4041,  0.0098,  0.0480]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4438, -0.2092,  0.0107, -0.2416]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 82 ] state=tensor([[-0.4438, -0.2092,  0.0107, -0.2416]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4480, -0.4044,  0.0059,  0.0545]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 83 ] state=tensor([[-0.4480, -0.4044,  0.0059,  0.0545]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4561, -0.5996,  0.0070,  0.3490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 84 ] state=tensor([[-0.4561, -0.5996,  0.0070,  0.3490]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4681, -0.4046,  0.0140,  0.0585]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 85 ] state=tensor([[-0.4681, -0.4046,  0.0140,  0.0585]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4761, -0.5999,  0.0151,  0.3556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 86 ] state=tensor([[-0.4761, -0.5999,  0.0151,  0.3556]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4881, -0.7953,  0.0222,  0.6530]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 87 ] state=tensor([[-0.4881, -0.7953,  0.0222,  0.6530]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5041, -0.6005,  0.0353,  0.3674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 88 ] state=tensor([[-0.5041, -0.6005,  0.0353,  0.3674]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5161, -0.4059,  0.0426,  0.0861]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 89 ] state=tensor([[-0.5161, -0.4059,  0.0426,  0.0861]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5242, -0.2114,  0.0444, -0.1929]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 90 ] state=tensor([[-0.5242, -0.2114,  0.0444, -0.1929]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5284, -0.4071,  0.0405,  0.1135]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 91 ] state=tensor([[-0.5284, -0.4071,  0.0405,  0.1135]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5365, -0.2126,  0.0428, -0.1661]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 92 ] state=tensor([[-0.5365, -0.2126,  0.0428, -0.1661]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5408, -0.4083,  0.0395,  0.1397]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 93 ] state=tensor([[-0.5408, -0.4083,  0.0395,  0.1397]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5490, -0.2138,  0.0423, -0.1403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 94 ] state=tensor([[-0.5490, -0.2138,  0.0423, -0.1403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5532, -0.4095,  0.0394,  0.1655]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 95 ] state=tensor([[-0.5532, -0.4095,  0.0394,  0.1655]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5614, -0.2149,  0.0428, -0.1145]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 96 ] state=tensor([[-0.5614, -0.2149,  0.0428, -0.1145]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5657, -0.4106,  0.0405,  0.1913]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 97 ] state=tensor([[-0.5657, -0.4106,  0.0405,  0.1913]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5739, -0.6063,  0.0443,  0.4965]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 98 ] state=tensor([[-0.5739, -0.6063,  0.0443,  0.4965]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5861, -0.4118,  0.0542,  0.2181]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 99 ] state=tensor([[-0.5861, -0.4118,  0.0542,  0.2181]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5943, -0.2175,  0.0586, -0.0570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 100 ] state=tensor([[-0.5943, -0.2175,  0.0586, -0.0570]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5987, -0.0233,  0.0574, -0.3306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 101 ] state=tensor([[-0.5987, -0.0233,  0.0574, -0.3306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5991,  0.1710,  0.0508, -0.6047]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 102 ] state=tensor([[-0.5991,  0.1710,  0.0508, -0.6047]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5957, -0.0248,  0.0387, -0.2964]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 103 ] state=tensor([[-0.5957, -0.0248,  0.0387, -0.2964]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5962,  0.1697,  0.0328, -0.5766]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 104 ] state=tensor([[-0.5962,  0.1697,  0.0328, -0.5766]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5928,  0.3644,  0.0213, -0.8588]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 105 ] state=tensor([[-0.5928,  0.3644,  0.0213, -0.8588]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5855,  0.5592,  0.0041, -1.1447]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 106 ] state=tensor([[-0.5855,  0.5592,  0.0041, -1.1447]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5743,  0.3640, -0.0188, -0.8507]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 107 ] state=tensor([[-0.5743,  0.3640, -0.0188, -0.8507]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5671,  0.1692, -0.0358, -0.5640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 108 ] state=tensor([[-0.5671,  0.1692, -0.0358, -0.5640]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5637, -0.0254, -0.0471, -0.2828]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 109 ] state=tensor([[-0.5637, -0.0254, -0.0471, -0.2828]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5642, -0.2199, -0.0527, -0.0054]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 110 ] state=tensor([[-0.5642, -0.2199, -0.0527, -0.0054]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5686, -0.4142, -0.0528,  0.2702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 111 ] state=tensor([[-0.5686, -0.4142, -0.0528,  0.2702]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5769, -0.6085, -0.0474,  0.5458]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 112 ] state=tensor([[-0.5769, -0.6085, -0.0474,  0.5458]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5890, -0.8029, -0.0365,  0.8231]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 113 ] state=tensor([[-0.5890, -0.8029, -0.0365,  0.8231]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6051, -0.6073, -0.0201,  0.5192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 114 ] state=tensor([[-0.6051, -0.6073, -0.0201,  0.5192]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6172, -0.4119, -0.0097,  0.2203]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 115 ] state=tensor([[-0.6172, -0.4119, -0.0097,  0.2203]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6255, -0.6069, -0.0053,  0.5099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 116 ] state=tensor([[-0.6255, -0.6069, -0.0053,  0.5099]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6376, -0.8020,  0.0049,  0.8009]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 117 ] state=tensor([[-0.6376, -0.8020,  0.0049,  0.8009]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6537, -0.6069,  0.0209,  0.5097]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 118 ] state=tensor([[-0.6537, -0.6069,  0.0209,  0.5097]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6658, -0.4121,  0.0311,  0.2237]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 119 ] state=tensor([[-0.6658, -0.4121,  0.0311,  0.2237]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6740, -0.2174,  0.0356, -0.0590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 120 ] state=tensor([[-0.6740, -0.2174,  0.0356, -0.0590]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6784, -0.0228,  0.0344, -0.3402]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 121 ] state=tensor([[-0.6784, -0.0228,  0.0344, -0.3402]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6788, -0.2184,  0.0276, -0.0369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 122 ] state=tensor([[-0.6788, -0.2184,  0.0276, -0.0369]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6832, -0.0237,  0.0269, -0.3207]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 123 ] state=tensor([[-0.6832, -0.0237,  0.0269, -0.3207]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6837, -0.2192,  0.0205, -0.0197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 124 ] state=tensor([[-0.6837, -0.2192,  0.0205, -0.0197]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6881, -0.0244,  0.0201, -0.3058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 125 ] state=tensor([[-0.6881, -0.0244,  0.0201, -0.3058]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6886, -0.2198,  0.0140, -0.0069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 126 ] state=tensor([[-0.6886, -0.2198,  0.0140, -0.0069]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6929, -0.4151,  0.0138,  0.2902]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 127 ] state=tensor([[-0.6929, -0.4151,  0.0138,  0.2902]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7012, -0.2202,  0.0196,  0.0019]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 128 ] state=tensor([[-0.7012, -0.2202,  0.0196,  0.0019]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7057, -0.0254,  0.0197, -0.2845]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 129 ] state=tensor([[-0.7057, -0.0254,  0.0197, -0.2845]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7062, -0.2208,  0.0140,  0.0143]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 130 ] state=tensor([[-0.7062, -0.2208,  0.0140,  0.0143]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7106, -0.0258,  0.0143, -0.2740]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 131 ] state=tensor([[-0.7106, -0.0258,  0.0143, -0.2740]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7111, -0.2212,  0.0088,  0.0232]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 132 ] state=tensor([[-0.7111, -0.2212,  0.0088,  0.0232]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7155, -0.0262,  0.0092, -0.2667]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 133 ] state=tensor([[-0.7155, -0.0262,  0.0092, -0.2667]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7160, -0.2214,  0.0039,  0.0289]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 134 ] state=tensor([[-0.7160, -0.2214,  0.0039,  0.0289]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7205, -0.0263,  0.0045, -0.2626]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 132 ][ timestamp 135 ] state=tensor([[-0.7205, -0.0263,  0.0045, -0.2626]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7210, -0.2215, -0.0008,  0.0315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 136 ] state=tensor([[-0.7210, -0.2215, -0.0008,  0.0315]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.2542e-01, -2.6401e-02, -1.3381e-04, -2.6141e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 137 ] state=tensor([[-7.2542e-01, -2.6401e-02, -1.3381e-04, -2.6141e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7260, -0.2215, -0.0054,  0.0312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 138 ] state=tensor([[-0.7260, -0.2215, -0.0054,  0.0312]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7304, -0.0263, -0.0047, -0.2631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 139 ] state=tensor([[-0.7304, -0.0263, -0.0047, -0.2631]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7309, -0.2214, -0.0100,  0.0280]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 140 ] state=tensor([[-0.7309, -0.2214, -0.0100,  0.0280]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7353, -0.0261, -0.0094, -0.2678]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 141 ] state=tensor([[-0.7353, -0.0261, -0.0094, -0.2678]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7359, -0.2211, -0.0148,  0.0219]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 142 ] state=tensor([[-0.7359, -0.2211, -0.0148,  0.0219]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7403, -0.4160, -0.0144,  0.3099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 143 ] state=tensor([[-0.7403, -0.4160, -0.0144,  0.3099]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7486, -0.2207, -0.0082,  0.0127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 144 ] state=tensor([[-0.7486, -0.2207, -0.0082,  0.0127]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7530, -0.0254, -0.0079, -0.2825]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 145 ] state=tensor([[-0.7530, -0.0254, -0.0079, -0.2825]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7535, -0.2205, -0.0136,  0.0077]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 146 ] state=tensor([[-0.7535, -0.2205, -0.0136,  0.0077]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7579, -0.0251, -0.0134, -0.2893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 147 ] state=tensor([[-0.7579, -0.0251, -0.0134, -0.2893]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7584, -0.2201, -0.0192, -0.0008]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 148 ] state=tensor([[-0.7584, -0.2201, -0.0192, -0.0008]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7628, -0.0247, -0.0192, -0.2995]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 149 ] state=tensor([[-0.7628, -0.0247, -0.0192, -0.2995]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7633, -0.2195, -0.0252, -0.0130]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 150 ] state=tensor([[-0.7633, -0.2195, -0.0252, -0.0130]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7677, -0.4143, -0.0255,  0.2717]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 151 ] state=tensor([[-0.7677, -0.4143, -0.0255,  0.2717]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7760, -0.2188, -0.0200, -0.0289]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 152 ] state=tensor([[-0.7760, -0.2188, -0.0200, -0.0289]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7804, -0.4136, -0.0206,  0.2574]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 153 ] state=tensor([[-0.7804, -0.4136, -0.0206,  0.2574]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7887, -0.2182, -0.0155, -0.0417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 154 ] state=tensor([[-0.7887, -0.2182, -0.0155, -0.0417]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7930, -0.4131, -0.0163,  0.2460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 155 ] state=tensor([[-0.7930, -0.4131, -0.0163,  0.2460]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8013, -0.2178, -0.0114, -0.0517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 156 ] state=tensor([[-0.8013, -0.2178, -0.0114, -0.0517]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8056, -0.4127, -0.0124,  0.2373]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 157 ] state=tensor([[-0.8056, -0.4127, -0.0124,  0.2373]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8139, -0.6077, -0.0077,  0.5261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 158 ] state=tensor([[-0.8139, -0.6077, -0.0077,  0.5261]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8260, -0.4124,  0.0029,  0.2310]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 159 ] state=tensor([[-0.8260, -0.4124,  0.0029,  0.2310]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8343, -0.2173,  0.0075, -0.0608]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 160 ] state=tensor([[-0.8343, -0.2173,  0.0075, -0.0608]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8386, -0.0223,  0.0063, -0.3511]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 161 ] state=tensor([[-0.8386, -0.0223,  0.0063, -0.3511]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-8.3909e-01, -2.1755e-01, -7.4874e-04, -5.6438e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 162 ] state=tensor([[-8.3909e-01, -2.1755e-01, -7.4874e-04, -5.6438e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8434, -0.0224, -0.0019, -0.3494]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 163 ] state=tensor([[-0.8434, -0.0224, -0.0019, -0.3494]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8439, -0.2175, -0.0089, -0.0573]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 164 ] state=tensor([[-0.8439, -0.2175, -0.0089, -0.0573]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8482, -0.4125, -0.0100,  0.2326]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 165 ] state=tensor([[-0.8482, -0.4125, -0.0100,  0.2326]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8565, -0.2172, -0.0054, -0.0632]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 166 ] state=tensor([[-0.8565, -0.2172, -0.0054, -0.0632]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8608, -0.4123, -0.0066,  0.2278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 167 ] state=tensor([[-0.8608, -0.4123, -0.0066,  0.2278]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8691, -0.2171, -0.0021, -0.0670]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 168 ] state=tensor([[-0.8691, -0.2171, -0.0021, -0.0670]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8734, -0.0219, -0.0034, -0.3603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 169 ] state=tensor([[-0.8734, -0.0219, -0.0034, -0.3603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8739, -0.2170, -0.0106, -0.0687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 170 ] state=tensor([[-0.8739, -0.2170, -0.0106, -0.0687]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8782, -0.4120, -0.0120,  0.2206]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 171 ] state=tensor([[-0.8782, -0.4120, -0.0120,  0.2206]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8864, -0.6069, -0.0076,  0.5095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 172 ] state=tensor([[-0.8864, -0.6069, -0.0076,  0.5095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8986, -0.4117,  0.0026,  0.2144]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 173 ] state=tensor([[-0.8986, -0.4117,  0.0026,  0.2144]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9068, -0.2166,  0.0069, -0.0774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 174 ] state=tensor([[-0.9068, -0.2166,  0.0069, -0.0774]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9111, -0.0216,  0.0054, -0.3679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 175 ] state=tensor([[-0.9111, -0.0216,  0.0054, -0.3679]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9116, -0.2168, -0.0020, -0.0736]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 176 ] state=tensor([[-0.9116, -0.2168, -0.0020, -0.0736]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9159, -0.0216, -0.0035, -0.3669]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 177 ] state=tensor([[-0.9159, -0.0216, -0.0035, -0.3669]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9163,  0.1736, -0.0108, -0.6607]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 178 ] state=tensor([[-0.9163,  0.1736, -0.0108, -0.6607]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9129, -0.0214, -0.0240, -0.3714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 179 ] state=tensor([[-0.9129, -0.0214, -0.0240, -0.3714]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9133, -0.2162, -0.0315, -0.0864]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 180 ] state=tensor([[-0.9133, -0.2162, -0.0315, -0.0864]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9176, -0.4108, -0.0332,  0.1962]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 181 ] state=tensor([[-0.9176, -0.4108, -0.0332,  0.1962]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9258, -0.6055, -0.0293,  0.4782]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 182 ] state=tensor([[-0.9258, -0.6055, -0.0293,  0.4782]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9379, -0.8002, -0.0197,  0.7615]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 183 ] state=tensor([[-0.9379, -0.8002, -0.0197,  0.7615]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9539, -0.6048, -0.0045,  0.4627]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 184 ] state=tensor([[-0.9539, -0.6048, -0.0045,  0.4627]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9660, -0.4096,  0.0048,  0.1686]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 185 ] state=tensor([[-0.9660, -0.4096,  0.0048,  0.1686]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9742, -0.2146,  0.0082, -0.1225]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 186 ] state=tensor([[-0.9742, -0.2146,  0.0082, -0.1225]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9785, -0.4098,  0.0057,  0.1727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 187 ] state=tensor([[-0.9785, -0.4098,  0.0057,  0.1727]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9867, -0.2147,  0.0092, -0.1182]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 188 ] state=tensor([[-0.9867, -0.2147,  0.0092, -0.1182]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9910, -0.0198,  0.0068, -0.4079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 189 ] state=tensor([[-0.9910, -0.0198,  0.0068, -0.4079]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9914, -0.2150, -0.0014, -0.1131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 190 ] state=tensor([[-0.9914, -0.2150, -0.0014, -0.1131]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9957, -0.0198, -0.0036, -0.4062]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 191 ] state=tensor([[-0.9957, -0.0198, -0.0036, -0.4062]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9961, -0.2149, -0.0117, -0.1147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 192 ] state=tensor([[-0.9961, -0.2149, -0.0117, -0.1147]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0004, -0.4099, -0.0140,  0.1743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 193 ] state=tensor([[-1.0004, -0.4099, -0.0140,  0.1743]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0086, -0.6048, -0.0106,  0.4625]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 194 ] state=tensor([[-1.0086, -0.6048, -0.0106,  0.4625]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0207, -0.4095, -0.0013,  0.1665]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 195 ] state=tensor([[-1.0207, -0.4095, -0.0013,  0.1665]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0289, -0.2144,  0.0020, -0.1266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 196 ] state=tensor([[-1.0289, -0.2144,  0.0020, -0.1266]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0332e+00, -1.9274e-02, -5.0270e-04, -4.1863e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 197 ] state=tensor([[-1.0332e+00, -1.9274e-02, -5.0270e-04, -4.1863e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0336,  0.1759, -0.0089, -0.7115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 198 ] state=tensor([[-1.0336,  0.1759, -0.0089, -0.7115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0300, -0.0191, -0.0231, -0.4216]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 199 ] state=tensor([[-1.0300, -0.0191, -0.0231, -0.4216]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0304, -0.2139, -0.0315, -0.1363]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 200 ] state=tensor([[-1.0304, -0.2139, -0.0315, -0.1363]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0347, -0.4086, -0.0343,  0.1463]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 201 ] state=tensor([[-1.0347, -0.4086, -0.0343,  0.1463]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0429, -0.2130, -0.0313, -0.1570]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 202 ] state=tensor([[-1.0429, -0.2130, -0.0313, -0.1570]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0471, -0.4077, -0.0345,  0.1256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 203 ] state=tensor([[-1.0471, -0.4077, -0.0345,  0.1256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0553, -0.2121, -0.0320, -0.1777]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 204 ] state=tensor([[-1.0553, -0.2121, -0.0320, -0.1777]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0595, -0.4067, -0.0355,  0.1047]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 205 ] state=tensor([[-1.0595, -0.4067, -0.0355,  0.1047]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0677, -0.6013, -0.0334,  0.3860]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 132 ][ timestamp 206 ] state=tensor([[-1.0677, -0.6013, -0.0334,  0.3860]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0797, -0.7959, -0.0257,  0.6679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 207 ] state=tensor([[-1.0797, -0.7959, -0.0257,  0.6679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0956, -0.6005, -0.0123,  0.3673]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 208 ] state=tensor([[-1.0956, -0.6005, -0.0123,  0.3673]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1076, -0.7954, -0.0050,  0.6560]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 209 ] state=tensor([[-1.1076, -0.7954, -0.0050,  0.6560]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1235, -0.6002,  0.0081,  0.3618]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 210 ] state=tensor([[-1.1235, -0.6002,  0.0081,  0.3618]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1355, -0.4052,  0.0154,  0.0717]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 211 ] state=tensor([[-1.1355, -0.4052,  0.0154,  0.0717]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1436, -0.2103,  0.0168, -0.2161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 212 ] state=tensor([[-1.1436, -0.2103,  0.0168, -0.2161]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1478, -0.0154,  0.0125, -0.5035]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 213 ] state=tensor([[-1.1478, -0.0154,  0.0125, -0.5035]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1482,  0.1795,  0.0024, -0.7922]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 214 ] state=tensor([[-1.1482,  0.1795,  0.0024, -0.7922]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1446, -0.0156, -0.0134, -0.4988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 215 ] state=tensor([[-1.1446, -0.0156, -0.0134, -0.4988]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1449, -0.2106, -0.0234, -0.2104]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 216 ] state=tensor([[-1.1449, -0.2106, -0.0234, -0.2104]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1491, -0.4054, -0.0276,  0.0748]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 217 ] state=tensor([[-1.1491, -0.4054, -0.0276,  0.0748]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1572, -0.6001, -0.0261,  0.3587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 218 ] state=tensor([[-1.1572, -0.6001, -0.0261,  0.3587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1692, -0.7948, -0.0190,  0.6430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 219 ] state=tensor([[-1.1692, -0.7948, -0.0190,  0.6430]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1851, -0.5994, -0.0061,  0.3444]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 220 ] state=tensor([[-1.1851, -0.5994, -0.0061,  0.3444]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1971e+00, -4.0422e-01,  7.8773e-04,  4.9820e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 221 ] state=tensor([[-1.1971e+00, -4.0422e-01,  7.8773e-04,  4.9820e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2052, -0.2091,  0.0018, -0.2426]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 222 ] state=tensor([[-1.2052, -0.2091,  0.0018, -0.2426]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2093, -0.0140, -0.0031, -0.5347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 223 ] state=tensor([[-1.2093, -0.0140, -0.0031, -0.5347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2096, -0.2091, -0.0138, -0.2430]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 224 ] state=tensor([[-1.2096, -0.2091, -0.0138, -0.2430]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2138, -0.4040, -0.0186,  0.0453]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 225 ] state=tensor([[-1.2138, -0.4040, -0.0186,  0.0453]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2219, -0.2086, -0.0177, -0.2532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 226 ] state=tensor([[-1.2219, -0.2086, -0.0177, -0.2532]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2261, -0.4035, -0.0228,  0.0338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 227 ] state=tensor([[-1.2261, -0.4035, -0.0228,  0.0338]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2341, -0.5983, -0.0221,  0.3192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 228 ] state=tensor([[-1.2341, -0.5983, -0.0221,  0.3192]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2461, -0.4029, -0.0157,  0.0197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 229 ] state=tensor([[-1.2461, -0.4029, -0.0157,  0.0197]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2542, -0.2075, -0.0153, -0.2779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 230 ] state=tensor([[-1.2542, -0.2075, -0.0153, -0.2779]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2583, -0.0122, -0.0209, -0.5754]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 231 ] state=tensor([[-1.2583, -0.0122, -0.0209, -0.5754]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2585, -0.2070, -0.0324, -0.2894]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 232 ] state=tensor([[-1.2585, -0.2070, -0.0324, -0.2894]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2627, -0.0114, -0.0382, -0.5921]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 233 ] state=tensor([[-1.2627, -0.0114, -0.0382, -0.5921]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2629, -0.2060, -0.0500, -0.3117]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 234 ] state=tensor([[-1.2629, -0.2060, -0.0500, -0.3117]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2670, -0.4004, -0.0563, -0.0352]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 235 ] state=tensor([[-1.2670, -0.4004, -0.0563, -0.0352]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2750, -0.5946, -0.0570,  0.2392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 236 ] state=tensor([[-1.2750, -0.5946, -0.0570,  0.2392]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2869, -0.7889, -0.0522,  0.5134]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 237 ] state=tensor([[-1.2869, -0.7889, -0.0522,  0.5134]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3027, -0.5931, -0.0419,  0.2048]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 238 ] state=tensor([[-1.3027, -0.5931, -0.0419,  0.2048]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3146, -0.7876, -0.0378,  0.4839]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 239 ] state=tensor([[-1.3146, -0.7876, -0.0378,  0.4839]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3303, -0.5919, -0.0281,  0.1796]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 240 ] state=tensor([[-1.3303, -0.5919, -0.0281,  0.1796]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3422, -0.7867, -0.0245,  0.4633]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 241 ] state=tensor([[-1.3422, -0.7867, -0.0245,  0.4633]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3579, -0.5912, -0.0153,  0.1629]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 242 ] state=tensor([[-1.3579, -0.5912, -0.0153,  0.1629]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3697, -0.7861, -0.0120,  0.4508]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 243 ] state=tensor([[-1.3697, -0.7861, -0.0120,  0.4508]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3854, -0.5908, -0.0030,  0.1543]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 244 ] state=tensor([[-1.3854, -0.5908, -0.0030,  0.1543]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3973e+00, -3.9564e-01,  8.3375e-05, -1.3931e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 245 ] state=tensor([[-1.3973e+00, -3.9564e-01,  8.3375e-05, -1.3931e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4052, -0.5908, -0.0027,  0.1534]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 246 ] state=tensor([[-1.4052, -0.5908, -0.0027,  0.1534]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4170e+00, -3.9561e-01,  3.6502e-04, -1.4014e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 247 ] state=tensor([[-1.4170e+00, -3.9561e-01,  3.6502e-04, -1.4014e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4249, -0.5907, -0.0024,  0.1527]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 248 ] state=tensor([[-1.4249, -0.5907, -0.0024,  0.1527]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4367e+00, -7.8582e-01,  6.1543e-04,  4.4457e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 249 ] state=tensor([[-1.4367e+00, -7.8582e-01,  6.1543e-04,  4.4457e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4524, -0.9809,  0.0095,  0.7374]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 250 ] state=tensor([[-1.4524, -0.9809,  0.0095,  0.7374]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4721, -0.7860,  0.0243,  0.4478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 251 ] state=tensor([[-1.4721, -0.7860,  0.0243,  0.4478]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4878, -0.5912,  0.0332,  0.1628]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 252 ] state=tensor([[-1.4878, -0.5912,  0.0332,  0.1628]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4996, -0.3966,  0.0365, -0.1192]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 253 ] state=tensor([[-1.4996, -0.3966,  0.0365, -0.1192]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5075, -0.5922,  0.0341,  0.1848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 254 ] state=tensor([[-1.5075, -0.5922,  0.0341,  0.1848]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5194, -0.3976,  0.0378, -0.0970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 255 ] state=tensor([[-1.5194, -0.3976,  0.0378, -0.0970]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5273, -0.2030,  0.0358, -0.3775]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 256 ] state=tensor([[-1.5273, -0.2030,  0.0358, -0.3775]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5314, -0.0084,  0.0283, -0.6587]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 257 ] state=tensor([[-1.5314, -0.0084,  0.0283, -0.6587]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5315, -0.2039,  0.0151, -0.3572]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 258 ] state=tensor([[-1.5315, -0.2039,  0.0151, -0.3572]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5356, -0.3992,  0.0080, -0.0598]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 259 ] state=tensor([[-1.5356, -0.3992,  0.0080, -0.0598]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5436, -0.2042,  0.0068, -0.3500]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 260 ] state=tensor([[-1.5436, -0.2042,  0.0068, -0.3500]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5477e+00, -3.9946e-01, -2.2222e-04, -5.5144e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 261 ] state=tensor([[-1.5477e+00, -3.9946e-01, -2.2222e-04, -5.5144e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5557e+00, -2.0433e-01, -1.3251e-03, -3.4790e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 262 ] state=tensor([[-1.5557e+00, -2.0433e-01, -1.3251e-03, -3.4790e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5598, -0.3994, -0.0083, -0.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 263 ] state=tensor([[-1.5598, -0.3994, -0.0083, -0.0556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5678, -0.2042, -0.0094, -0.3509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 264 ] state=tensor([[-1.5678, -0.2042, -0.0094, -0.3509]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5718, -0.0089, -0.0164, -0.6465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 265 ] state=tensor([[-1.5718, -0.0089, -0.0164, -0.6465]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5720, -0.2038, -0.0293, -0.3591]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 266 ] state=tensor([[-1.5720, -0.2038, -0.0293, -0.3591]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5761, -0.3985, -0.0365, -0.0758]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 267 ] state=tensor([[-1.5761, -0.3985, -0.0365, -0.0758]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5841, -0.2029, -0.0380, -0.3798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 268 ] state=tensor([[-1.5841, -0.2029, -0.0380, -0.3798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5881, -0.3975, -0.0456, -0.0993]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 269 ] state=tensor([[-1.5881, -0.3975, -0.0456, -0.0993]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5961, -0.5919, -0.0476,  0.1786]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 270 ] state=tensor([[-1.5961, -0.5919, -0.0476,  0.1786]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6079, -0.7863, -0.0441,  0.4559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 271 ] state=tensor([[-1.6079, -0.7863, -0.0441,  0.4559]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6236, -0.5906, -0.0349,  0.1497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 272 ] state=tensor([[-1.6236, -0.5906, -0.0349,  0.1497]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6355, -0.7852, -0.0319,  0.4311]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 273 ] state=tensor([[-1.6355, -0.7852, -0.0319,  0.4311]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6512, -0.9799, -0.0233,  0.7136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 274 ] state=tensor([[-1.6512, -0.9799, -0.0233,  0.7136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6708, -0.7844, -0.0090,  0.4136]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 275 ] state=tensor([[-1.6708, -0.7844, -0.0090,  0.4136]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6864e+00, -5.8917e-01, -7.7309e-04,  1.1813e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 276 ] state=tensor([[-1.6864e+00, -5.8917e-01, -7.7309e-04,  1.1813e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6982e+00, -3.9403e-01,  1.5894e-03, -1.7480e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 277 ] state=tensor([[-1.6982e+00, -3.9403e-01,  1.5894e-03, -1.7480e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7061, -0.1989, -0.0019, -0.4670]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 132 ][ timestamp 278 ] state=tensor([[-1.7061, -0.1989, -0.0019, -0.4670]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7101, -0.0038, -0.0112, -0.7603]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 279 ] state=tensor([[-1.7101, -0.0038, -0.0112, -0.7603]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7102, -0.1988, -0.0265, -0.4711]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 280 ] state=tensor([[-1.7102, -0.1988, -0.0265, -0.4711]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7141, -0.3935, -0.0359, -0.1869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 281 ] state=tensor([[-1.7141, -0.3935, -0.0359, -0.1869]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7220, -0.1979, -0.0396, -0.4907]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 282 ] state=tensor([[-1.7220, -0.1979, -0.0396, -0.4907]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7260, -0.3924, -0.0494, -0.2108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 283 ] state=tensor([[-1.7260, -0.3924, -0.0494, -0.2108]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7338, -0.5868, -0.0536,  0.0659]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 284 ] state=tensor([[-1.7338, -0.5868, -0.0536,  0.0659]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7455, -0.3909, -0.0523, -0.2432]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 285 ] state=tensor([[-1.7455, -0.3909, -0.0523, -0.2432]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7534, -0.1951, -0.0572, -0.5519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 286 ] state=tensor([[-1.7534, -0.1951, -0.0572, -0.5519]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7573, -0.3894, -0.0682, -0.2778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 287 ] state=tensor([[-1.7573, -0.3894, -0.0682, -0.2778]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7651, -0.5835, -0.0738, -0.0074]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 288 ] state=tensor([[-1.7651, -0.5835, -0.0738, -0.0074]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7767, -0.7775, -0.0739,  0.2612]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 289 ] state=tensor([[-1.7767, -0.7775, -0.0739,  0.2612]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7923, -0.9715, -0.0687,  0.5297]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 290 ] state=tensor([[-1.7923, -0.9715, -0.0687,  0.5297]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8117, -0.7754, -0.0581,  0.2161]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 291 ] state=tensor([[-1.8117, -0.7754, -0.0581,  0.2161]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8272, -0.9697, -0.0538,  0.4899]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 292 ] state=tensor([[-1.8272, -0.9697, -0.0538,  0.4899]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8466, -1.1640, -0.0440,  0.7652]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 293 ] state=tensor([[-1.8466, -1.1640, -0.0440,  0.7652]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8699, -0.9683, -0.0287,  0.4590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 294 ] state=tensor([[-1.8699, -0.9683, -0.0287,  0.4590]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8893, -0.7728, -0.0195,  0.1574]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 295 ] state=tensor([[-1.8893, -0.7728, -0.0195,  0.1574]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9047, -0.5774, -0.0164, -0.1414]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 296 ] state=tensor([[-1.9047, -0.5774, -0.0164, -0.1414]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9163, -0.3821, -0.0192, -0.4392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 297 ] state=tensor([[-1.9163, -0.3821, -0.0192, -0.4392]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9239, -0.5769, -0.0280, -0.1526]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 298 ] state=tensor([[-1.9239, -0.5769, -0.0280, -0.1526]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9354, -0.3814, -0.0310, -0.4540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 299 ] state=tensor([[-1.9354, -0.3814, -0.0310, -0.4540]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9431, -0.5761, -0.0401, -0.1712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 300 ] state=tensor([[-1.9431, -0.5761, -0.0401, -0.1712]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9546, -0.3804, -0.0435, -0.4763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 301 ] state=tensor([[-1.9546, -0.3804, -0.0435, -0.4763]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9622, -0.5749, -0.0530, -0.1976]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 302 ] state=tensor([[-1.9622, -0.5749, -0.0530, -0.1976]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9737, -0.7692, -0.0570,  0.0779]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 303 ] state=tensor([[-1.9737, -0.7692, -0.0570,  0.0779]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9891, -0.5733, -0.0554, -0.2322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 304 ] state=tensor([[-1.9891, -0.5733, -0.0554, -0.2322]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0005, -0.7676, -0.0601,  0.0425]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 305 ] state=tensor([[-2.0005, -0.7676, -0.0601,  0.0425]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0159, -0.5717, -0.0592, -0.2686]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 306 ] state=tensor([[-2.0159, -0.5717, -0.0592, -0.2686]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0273, -0.3757, -0.0646, -0.5793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 307 ] state=tensor([[-2.0273, -0.3757, -0.0646, -0.5793]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0348, -0.5699, -0.0762, -0.3077]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 308 ] state=tensor([[-2.0348, -0.5699, -0.0762, -0.3077]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0462, -0.7639, -0.0823, -0.0399]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 309 ] state=tensor([[-2.0462, -0.7639, -0.0823, -0.0399]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0615, -0.9577, -0.0831,  0.2257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 310 ] state=tensor([[-2.0615, -0.9577, -0.0831,  0.2257]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0807, -1.1516, -0.0786,  0.4910]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 311 ] state=tensor([[-2.0807, -1.1516, -0.0786,  0.4910]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1037, -0.9554, -0.0688,  0.1746]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 312 ] state=tensor([[-2.1037, -0.9554, -0.0688,  0.1746]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1228, -0.7594, -0.0653, -0.1390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 313 ] state=tensor([[-2.1228, -0.7594, -0.0653, -0.1390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1380, -0.5634, -0.0681, -0.4515]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 314 ] state=tensor([[-2.1380, -0.5634, -0.0681, -0.4515]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1493, -0.7575, -0.0771, -0.1811]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 315 ] state=tensor([[-2.1493, -0.7575, -0.0771, -0.1811]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1644, -0.5613, -0.0808, -0.4970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 316 ] state=tensor([[-2.1644, -0.5613, -0.0808, -0.4970]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1756, -0.3652, -0.0907, -0.8140]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 317 ] state=tensor([[-2.1756, -0.3652, -0.0907, -0.8140]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1829, -0.1689, -0.1070, -1.1338]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 318 ] state=tensor([[-2.1829, -0.1689, -0.1070, -1.1338]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1863,  0.0274, -0.1296, -1.4580]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 319 ] state=tensor([[-2.1863,  0.0274, -0.1296, -1.4580]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1858,  0.2239, -0.1588, -1.7883]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 320 ] state=tensor([[-2.1858,  0.2239, -0.1588, -1.7883]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1813,  0.4204, -0.1946, -2.1258]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 132 ][ timestamp 321 ] state=tensor([[-2.1813,  0.4204, -0.1946, -2.1258]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 132: Exploration_rate=0.05. Score=321.\n",
      "[ episode 133 ] state=tensor([[-0.0396,  0.0409,  0.0097,  0.0186]])\n",
      "[ episode 133 ][ timestamp 1 ] state=tensor([[-0.0396,  0.0409,  0.0097,  0.0186]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0388,  0.2359,  0.0100, -0.2710]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 2 ] state=tensor([[-0.0388,  0.2359,  0.0100, -0.2710]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0341,  0.4309,  0.0046, -0.5605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 3 ] state=tensor([[-0.0341,  0.4309,  0.0046, -0.5605]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0255,  0.6260, -0.0066, -0.8517]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 4 ] state=tensor([[-0.0255,  0.6260, -0.0066, -0.8517]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0129,  0.4309, -0.0236, -0.5611]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 5 ] state=tensor([[-0.0129,  0.4309, -0.0236, -0.5611]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0043,  0.2361, -0.0349, -0.2760]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 6 ] state=tensor([[-0.0043,  0.2361, -0.0349, -0.2760]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0004,  0.0415, -0.0404,  0.0055]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 7 ] state=tensor([[ 0.0004,  0.0415, -0.0404,  0.0055]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0012,  0.2372, -0.0403, -0.2996]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 8 ] state=tensor([[ 0.0012,  0.2372, -0.0403, -0.2996]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0060,  0.0427, -0.0463, -0.0199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 9 ] state=tensor([[ 0.0060,  0.0427, -0.0463, -0.0199]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0068,  0.2384, -0.0467, -0.3268]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 10 ] state=tensor([[ 0.0068,  0.2384, -0.0467, -0.3268]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0116,  0.0440, -0.0532, -0.0492]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 11 ] state=tensor([[ 0.0116,  0.0440, -0.0532, -0.0492]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0125,  0.2399, -0.0542, -0.3582]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 12 ] state=tensor([[ 0.0125,  0.2399, -0.0542, -0.3582]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0173,  0.0455, -0.0613, -0.0831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 13 ] state=tensor([[ 0.0173,  0.0455, -0.0613, -0.0831]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0182,  0.2415, -0.0630, -0.3945]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 14 ] state=tensor([[ 0.0182,  0.2415, -0.0630, -0.3945]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0230,  0.0473, -0.0709, -0.1223]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 15 ] state=tensor([[ 0.0230,  0.0473, -0.0709, -0.1223]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[ 0.0240,  0.2434, -0.0733, -0.4365]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 16 ] state=tensor([[ 0.0240,  0.2434, -0.0733, -0.4365]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0288,  0.0494, -0.0821, -0.1678]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 17 ] state=tensor([[ 0.0288,  0.0494, -0.0821, -0.1678]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0298, -0.1445, -0.0854,  0.0979]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 18 ] state=tensor([[ 0.0298, -0.1445, -0.0854,  0.0979]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0269, -0.3383, -0.0835,  0.3625]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 19 ] state=tensor([[ 0.0269, -0.3383, -0.0835,  0.3625]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0202, -0.5321, -0.0762,  0.6277]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 20 ] state=tensor([[ 0.0202, -0.5321, -0.0762,  0.6277]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[ 0.0095, -0.7261, -0.0637,  0.8955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 21 ] state=tensor([[ 0.0095, -0.7261, -0.0637,  0.8955]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0050, -0.9203, -0.0458,  1.1675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 22 ] state=tensor([[-0.0050, -0.9203, -0.0458,  1.1675]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0234, -1.1148, -0.0224,  1.4455]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 23 ] state=tensor([[-0.0234, -1.1148, -0.0224,  1.4455]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0457, -0.9194,  0.0065,  1.1459]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 24 ] state=tensor([[-0.0457, -0.9194,  0.0065,  1.1459]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0641, -0.7244,  0.0294,  0.8552]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 25 ] state=tensor([[-0.0641, -0.7244,  0.0294,  0.8552]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0786, -0.5297,  0.0465,  0.5719]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 26 ] state=tensor([[-0.0786, -0.5297,  0.0465,  0.5719]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0892, -0.3352,  0.0580,  0.2943]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 27 ] state=tensor([[-0.0892, -0.3352,  0.0580,  0.2943]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0959, -0.1410,  0.0639,  0.0204]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 28 ] state=tensor([[-0.0959, -0.1410,  0.0639,  0.0204]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0987,  0.0532,  0.0643, -0.2515]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 29 ] state=tensor([[-0.0987,  0.0532,  0.0643, -0.2515]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0976,  0.2473,  0.0592, -0.5232]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 30 ] state=tensor([[-0.0976,  0.2473,  0.0592, -0.5232]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0927,  0.4416,  0.0488, -0.7966]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 31 ] state=tensor([[-0.0927,  0.4416,  0.0488, -0.7966]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0839,  0.2458,  0.0328, -0.4890]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 32 ] state=tensor([[-0.0839,  0.2458,  0.0328, -0.4890]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0789,  0.4404,  0.0231, -0.7712]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 33 ] state=tensor([[-0.0789,  0.4404,  0.0231, -0.7712]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0701,  0.6352,  0.0076, -1.0565]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 34 ] state=tensor([[-0.0701,  0.6352,  0.0076, -1.0565]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0574,  0.4400, -0.0135, -0.7615]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 35 ] state=tensor([[-0.0574,  0.4400, -0.0135, -0.7615]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0486,  0.2451, -0.0287, -0.4731]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 36 ] state=tensor([[-0.0486,  0.2451, -0.0287, -0.4731]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0437,  0.0504, -0.0382, -0.1896]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 37 ] state=tensor([[-0.0437,  0.0504, -0.0382, -0.1896]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0427, -0.1442, -0.0420,  0.0908]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 38 ] state=tensor([[-0.0427, -0.1442, -0.0420,  0.0908]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0456, -0.3387, -0.0402,  0.3700]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 39 ] state=tensor([[-0.0456, -0.3387, -0.0402,  0.3700]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0524, -0.5332, -0.0328,  0.6497]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 40 ] state=tensor([[-0.0524, -0.5332, -0.0328,  0.6497]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0630, -0.3376, -0.0198,  0.3469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 41 ] state=tensor([[-0.0630, -0.3376, -0.0198,  0.3469]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0698, -0.1422, -0.0128,  0.0481]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 42 ] state=tensor([[-0.0698, -0.1422, -0.0128,  0.0481]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0726,  0.0531, -0.0119, -0.2486]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 43 ] state=tensor([[-0.0726,  0.0531, -0.0119, -0.2486]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0716,  0.2484, -0.0168, -0.5451]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 44 ] state=tensor([[-0.0716,  0.2484, -0.0168, -0.5451]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0666,  0.0535, -0.0277, -0.2577]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 45 ] state=tensor([[-0.0666,  0.0535, -0.0277, -0.2577]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0655, -0.1412, -0.0329,  0.0261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 46 ] state=tensor([[-0.0655, -0.1412, -0.0329,  0.0261]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0684,  0.0543, -0.0324, -0.2768]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 47 ] state=tensor([[-0.0684,  0.0543, -0.0324, -0.2768]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0673,  0.2499, -0.0379, -0.5795]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 48 ] state=tensor([[-0.0673,  0.2499, -0.0379, -0.5795]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0623,  0.0553, -0.0495, -0.2990]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 49 ] state=tensor([[-0.0623,  0.0553, -0.0495, -0.2990]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0612,  0.2511, -0.0555, -0.6069]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 50 ] state=tensor([[-0.0612,  0.2511, -0.0555, -0.6069]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0561,  0.0568, -0.0676, -0.3322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 51 ] state=tensor([[-0.0561,  0.0568, -0.0676, -0.3322]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0550, -0.1373, -0.0743, -0.0616]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 52 ] state=tensor([[-0.0550, -0.1373, -0.0743, -0.0616]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0578, -0.3313, -0.0755,  0.2068]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 53 ] state=tensor([[-0.0578, -0.3313, -0.0755,  0.2068]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0644, -0.1351, -0.0714, -0.1087]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 54 ] state=tensor([[-0.0644, -0.1351, -0.0714, -0.1087]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0671, -0.3292, -0.0735,  0.1606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 55 ] state=tensor([[-0.0671, -0.3292, -0.0735,  0.1606]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0737, -0.5232, -0.0703,  0.4292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 56 ] state=tensor([[-0.0737, -0.5232, -0.0703,  0.4292]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.0841, -0.3271, -0.0617,  0.1152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 57 ] state=tensor([[-0.0841, -0.3271, -0.0617,  0.1152]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.0907, -0.5213, -0.0594,  0.3878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 58 ] state=tensor([[-0.0907, -0.5213, -0.0594,  0.3878]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1011, -0.7155, -0.0517,  0.6612]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 59 ] state=tensor([[-0.1011, -0.7155, -0.0517,  0.6612]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1154, -0.9099, -0.0385,  0.9371]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 60 ] state=tensor([[-0.1154, -0.9099, -0.0385,  0.9371]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1336, -1.1045, -0.0197,  1.2175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 61 ] state=tensor([[-0.1336, -1.1045, -0.0197,  1.2175]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1557, -1.2994,  0.0046,  1.5039]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 62 ] state=tensor([[-0.1557, -1.2994,  0.0046,  1.5039]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1817, -1.1043,  0.0347,  1.2127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 63 ] state=tensor([[-0.1817, -1.1043,  0.0347,  1.2127]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2038, -0.9096,  0.0590,  0.9311]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 64 ] state=tensor([[-0.2038, -0.9096,  0.0590,  0.9311]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2220, -1.1055,  0.0776,  1.2417]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 65 ] state=tensor([[-0.2220, -1.1055,  0.0776,  1.2417]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2441, -0.9115,  0.1024,  0.9743]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 66 ] state=tensor([[-0.2441, -0.9115,  0.1024,  0.9743]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2623, -0.7178,  0.1219,  0.7155]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 67 ] state=tensor([[-0.2623, -0.7178,  0.1219,  0.7155]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2767, -0.5246,  0.1362,  0.4635]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 68 ] state=tensor([[-0.2767, -0.5246,  0.1362,  0.4635]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2872, -0.3316,  0.1455,  0.2167]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 69 ] state=tensor([[-0.2872, -0.3316,  0.1455,  0.2167]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2938, -0.1389,  0.1498, -0.0268]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 70 ] state=tensor([[-0.2938, -0.1389,  0.1498, -0.0268]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2966,  0.0538,  0.1493, -0.2687]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 71 ] state=tensor([[-0.2966,  0.0538,  0.1493, -0.2687]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2955,  0.2465,  0.1439, -0.5108]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 72 ] state=tensor([[-0.2955,  0.2465,  0.1439, -0.5108]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2906,  0.4394,  0.1337, -0.7549]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 73 ] state=tensor([[-0.2906,  0.4394,  0.1337, -0.7549]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2818,  0.6324,  0.1186, -1.0027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 74 ] state=tensor([[-0.2818,  0.6324,  0.1186, -1.0027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2691,  0.8258,  0.0985, -1.2559]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 75 ] state=tensor([[-0.2691,  0.8258,  0.0985, -1.2559]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2526,  1.0195,  0.0734, -1.5162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 76 ] state=tensor([[-0.2526,  1.0195,  0.0734, -1.5162]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2322,  0.8236,  0.0431, -1.2015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 77 ] state=tensor([[-0.2322,  0.8236,  0.0431, -1.2015]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2157,  1.0181,  0.0191, -1.4804]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 78 ] state=tensor([[-0.2157,  1.0181,  0.0191, -1.4804]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1954,  0.8228, -0.0105, -1.1818]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 79 ] state=tensor([[-0.1954,  0.8228, -0.0105, -1.1818]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1789,  0.6278, -0.0342, -0.8925]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 80 ] state=tensor([[-0.1789,  0.6278, -0.0342, -0.8925]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1664,  0.4331, -0.0520, -0.6107]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 81 ] state=tensor([[-0.1664,  0.4331, -0.0520, -0.6107]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1577,  0.6290, -0.0642, -0.9193]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 82 ] state=tensor([[-0.1577,  0.6290, -0.0642, -0.9193]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1451,  0.4348, -0.0826, -0.6475]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 83 ] state=tensor([[-0.1451,  0.4348, -0.0826, -0.6475]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1364,  0.2409, -0.0956, -0.3820]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 84 ] state=tensor([[-0.1364,  0.2409, -0.0956, -0.3820]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1316,  0.0472, -0.1032, -0.1209]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 85 ] state=tensor([[-0.1316,  0.0472, -0.1032, -0.1209]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1307, -0.1463, -0.1056,  0.1375]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 86 ] state=tensor([[-0.1307, -0.1463, -0.1056,  0.1375]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1336, -0.3397, -0.1029,  0.3951]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 87 ] state=tensor([[-0.1336, -0.3397, -0.1029,  0.3951]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1404, -0.5333, -0.0950,  0.6537]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 88 ] state=tensor([[-0.1404, -0.5333, -0.0950,  0.6537]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1511, -0.3370, -0.0819,  0.3327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 89 ] state=tensor([[-0.1511, -0.3370, -0.0819,  0.3327]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1578, -0.1408, -0.0753,  0.0153]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 90 ] state=tensor([[-0.1578, -0.1408, -0.0753,  0.0153]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1606, -0.3347, -0.0749,  0.2833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 91 ] state=tensor([[-0.1606, -0.3347, -0.0749,  0.2833]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.1673, -0.1386, -0.0693, -0.0320]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 92 ] state=tensor([[-0.1673, -0.1386, -0.0693, -0.0320]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1701, -0.3327, -0.0699,  0.2380]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 93 ] state=tensor([[-0.1701, -0.3327, -0.0699,  0.2380]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1767, -0.5267, -0.0652,  0.5079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 94 ] state=tensor([[-0.1767, -0.5267, -0.0652,  0.5079]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.1873, -0.7209, -0.0550,  0.7793]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 95 ] state=tensor([[-0.1873, -0.7209, -0.0550,  0.7793]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2017, -0.5251, -0.0394,  0.4699]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 96 ] state=tensor([[-0.2017, -0.5251, -0.0394,  0.4699]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2122, -0.3294, -0.0300,  0.1650]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 97 ] state=tensor([[-0.2122, -0.3294, -0.0300,  0.1650]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2188, -0.5241, -0.0267,  0.4481]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 98 ] state=tensor([[-0.2188, -0.5241, -0.0267,  0.4481]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2293, -0.7188, -0.0178,  0.7322]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 99 ] state=tensor([[-0.2293, -0.7188, -0.0178,  0.7322]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2436, -0.5235, -0.0031,  0.4340]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 100 ] state=tensor([[-0.2436, -0.5235, -0.0031,  0.4340]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2541, -0.3283,  0.0056,  0.1403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 101 ] state=tensor([[-0.2541, -0.3283,  0.0056,  0.1403]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2607, -0.5235,  0.0084,  0.4348]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 102 ] state=tensor([[-0.2607, -0.5235,  0.0084,  0.4348]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2711, -0.3285,  0.0171,  0.1448]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 103 ] state=tensor([[-0.2711, -0.3285,  0.0171,  0.1448]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2777, -0.5238,  0.0200,  0.4428]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 104 ] state=tensor([[-0.2777, -0.5238,  0.0200,  0.4428]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2882, -0.3290,  0.0288,  0.1564]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 105 ] state=tensor([[-0.2882, -0.3290,  0.0288,  0.1564]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2948, -0.1343,  0.0319, -0.1270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 106 ] state=tensor([[-0.2948, -0.1343,  0.0319, -0.1270]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2974,  0.0603,  0.0294, -0.4094]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 107 ] state=tensor([[-0.2974,  0.0603,  0.0294, -0.4094]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.2962,  0.2550,  0.0212, -0.6927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 108 ] state=tensor([[-0.2962,  0.2550,  0.0212, -0.6927]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2911,  0.0596,  0.0074, -0.3934]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 109 ] state=tensor([[-0.2911,  0.0596,  0.0074, -0.3934]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2899, -0.1356, -0.0005, -0.0984]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 110 ] state=tensor([[-0.2899, -0.1356, -0.0005, -0.0984]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2927, -0.3307, -0.0025,  0.1941]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 111 ] state=tensor([[-0.2927, -0.3307, -0.0025,  0.1941]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.2993, -0.5258,  0.0014,  0.4860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 112 ] state=tensor([[-0.2993, -0.5258,  0.0014,  0.4860]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3098, -0.3307,  0.0111,  0.1938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 113 ] state=tensor([[-0.3098, -0.3307,  0.0111,  0.1938]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3164, -0.1357,  0.0150, -0.0954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 114 ] state=tensor([[-0.3164, -0.1357,  0.0150, -0.0954]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3191,  0.0592,  0.0131, -0.3833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 115 ] state=tensor([[-0.3191,  0.0592,  0.0131, -0.3833]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3179,  0.2541,  0.0054, -0.6718]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 116 ] state=tensor([[-0.3179,  0.2541,  0.0054, -0.6718]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3129,  0.0589, -0.0080, -0.3774]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 117 ] state=tensor([[-0.3129,  0.0589, -0.0080, -0.3774]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3117,  0.2541, -0.0156, -0.6726]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 118 ] state=tensor([[-0.3117,  0.2541, -0.0156, -0.6726]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3066,  0.0592, -0.0290, -0.3849]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 119 ] state=tensor([[-0.3066,  0.0592, -0.0290, -0.3849]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3054, -0.1355, -0.0367, -0.1015]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 120 ] state=tensor([[-0.3054, -0.1355, -0.0367, -0.1015]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3081, -0.3301, -0.0387,  0.1794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 121 ] state=tensor([[-0.3081, -0.3301, -0.0387,  0.1794]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3147, -0.1344, -0.0351, -0.1252]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 122 ] state=tensor([[-0.3147, -0.1344, -0.0351, -0.1252]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3174, -0.3290, -0.0376,  0.1561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 123 ] state=tensor([[-0.3174, -0.3290, -0.0376,  0.1561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3240, -0.5236, -0.0345,  0.4367]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 124 ] state=tensor([[-0.3240, -0.5236, -0.0345,  0.4367]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3345, -0.7182, -0.0258,  0.7183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 125 ] state=tensor([[-0.3345, -0.7182, -0.0258,  0.7183]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3488, -0.5227, -0.0114,  0.4176]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 126 ] state=tensor([[-0.3488, -0.5227, -0.0114,  0.4176]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3593, -0.3274, -0.0031,  0.1214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 127 ] state=tensor([[-0.3593, -0.3274, -0.0031,  0.1214]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3658, -0.5225, -0.0006,  0.4131]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 128 ] state=tensor([[-0.3658, -0.5225, -0.0006,  0.4131]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3763, -0.3274,  0.0076,  0.1202]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 129 ] state=tensor([[-0.3763, -0.3274,  0.0076,  0.1202]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3828, -0.1324,  0.0100, -0.1701]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 130 ] state=tensor([[-0.3828, -0.1324,  0.0100, -0.1701]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3855,  0.0626,  0.0066, -0.4596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 131 ] state=tensor([[-0.3855,  0.0626,  0.0066, -0.4596]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3842, -0.1326, -0.0026, -0.1648]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 132 ] state=tensor([[-0.3842, -0.1326, -0.0026, -0.1648]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3869,  0.0626, -0.0059, -0.4583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 133 ] state=tensor([[-0.3869,  0.0626, -0.0059, -0.4583]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3856, -0.1325, -0.0150, -0.1675]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 134 ] state=tensor([[-0.3856, -0.1325, -0.0150, -0.1675]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.3883,  0.0629, -0.0184, -0.4649]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 135 ] state=tensor([[-0.3883,  0.0629, -0.0184, -0.4649]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3870, -0.1320, -0.0277, -0.1780]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 136 ] state=tensor([[-0.3870, -0.1320, -0.0277, -0.1780]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3897, -0.3267, -0.0312,  0.1058]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 137 ] state=tensor([[-0.3897, -0.3267, -0.0312,  0.1058]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.3962, -0.5214, -0.0291,  0.3884]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 138 ] state=tensor([[-0.3962, -0.5214, -0.0291,  0.3884]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4066, -0.7161, -0.0214,  0.6718]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 139 ] state=tensor([[-0.4066, -0.7161, -0.0214,  0.6718]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4209, -0.5207, -0.0079,  0.3725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 140 ] state=tensor([[-0.4209, -0.5207, -0.0079,  0.3725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4314, -0.3254, -0.0005,  0.0773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 141 ] state=tensor([[-0.4314, -0.3254, -0.0005,  0.0773]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4379, -0.5205,  0.0011,  0.3698]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 142 ] state=tensor([[-0.4379, -0.5205,  0.0011,  0.3698]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4483, -0.3254,  0.0085,  0.0775]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 143 ] state=tensor([[-0.4483, -0.3254,  0.0085,  0.0775]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4548, -0.5207,  0.0100,  0.3728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 144 ] state=tensor([[-0.4548, -0.5207,  0.0100,  0.3728]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4652, -0.3257,  0.0175,  0.0833]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 145 ] state=tensor([[-0.4652, -0.3257,  0.0175,  0.0833]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4717, -0.5211,  0.0191,  0.3815]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 146 ] state=tensor([[-0.4717, -0.5211,  0.0191,  0.3815]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4821, -0.3262,  0.0268,  0.0949]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 147 ] state=tensor([[-0.4821, -0.3262,  0.0268,  0.0949]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.4887, -0.1315,  0.0287, -0.1892]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 148 ] state=tensor([[-0.4887, -0.1315,  0.0287, -0.1892]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4913, -0.3270,  0.0249,  0.1124]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 149 ] state=tensor([[-0.4913, -0.3270,  0.0249,  0.1124]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.4978, -0.5225,  0.0271,  0.4128]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 150 ] state=tensor([[-0.4978, -0.5225,  0.0271,  0.4128]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5083, -0.3278,  0.0354,  0.1288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 151 ] state=tensor([[-0.5083, -0.3278,  0.0354,  0.1288]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5148, -0.5234,  0.0380,  0.4324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 152 ] state=tensor([[-0.5148, -0.5234,  0.0380,  0.4324]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5253, -0.3288,  0.0466,  0.1519]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 153 ] state=tensor([[-0.5253, -0.3288,  0.0466,  0.1519]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5319, -0.1344,  0.0497, -0.1257]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 154 ] state=tensor([[-0.5319, -0.1344,  0.0497, -0.1257]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5346,  0.0600,  0.0471, -0.4023]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 155 ] state=tensor([[-0.5346,  0.0600,  0.0471, -0.4023]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5334,  0.2544,  0.0391, -0.6798]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 156 ] state=tensor([[-0.5334,  0.2544,  0.0391, -0.6798]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5283,  0.0588,  0.0255, -0.3750]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 157 ] state=tensor([[-0.5283,  0.0588,  0.0255, -0.3750]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5271,  0.2535,  0.0180, -0.6596]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 158 ] state=tensor([[-0.5271,  0.2535,  0.0180, -0.6596]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5220,  0.0582,  0.0048, -0.3613]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 159 ] state=tensor([[-0.5220,  0.0582,  0.0048, -0.3613]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5209,  0.2532, -0.0024, -0.6524]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 160 ] state=tensor([[-0.5209,  0.2532, -0.0024, -0.6524]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5158,  0.0581, -0.0155, -0.3605]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 161 ] state=tensor([[-0.5158,  0.0581, -0.0155, -0.3605]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5146, -0.1368, -0.0227, -0.0727]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 162 ] state=tensor([[-0.5146, -0.1368, -0.0227, -0.0727]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5174, -0.3316, -0.0241,  0.2127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 163 ] state=tensor([[-0.5174, -0.3316, -0.0241,  0.2127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5240, -0.5263, -0.0199,  0.4977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 164 ] state=tensor([[-0.5240, -0.5263, -0.0199,  0.4977]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5345, -0.3309, -0.0099,  0.1988]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 165 ] state=tensor([[-0.5345, -0.3309, -0.0099,  0.1988]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5411, -0.1357, -0.0060, -0.0970]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 166 ] state=tensor([[-0.5411, -0.1357, -0.0060, -0.0970]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5439, -0.3307, -0.0079,  0.1938]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 167 ] state=tensor([[-0.5439, -0.3307, -0.0079,  0.1938]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5505, -0.1355, -0.0040, -0.1014]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 168 ] state=tensor([[-0.5505, -0.1355, -0.0040, -0.1014]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5532, -0.3305, -0.0060,  0.1900]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 169 ] state=tensor([[-0.5532, -0.3305, -0.0060,  0.1900]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5598, -0.1353, -0.0022, -0.1045]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 170 ] state=tensor([[-0.5598, -0.1353, -0.0022, -0.1045]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5625, -0.3304, -0.0043,  0.1874]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 171 ] state=tensor([[-0.5625, -0.3304, -0.0043,  0.1874]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5691, -0.1352, -0.0006, -0.1066]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 172 ] state=tensor([[-0.5691, -0.1352, -0.0006, -0.1066]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.5718,  0.0599, -0.0027, -0.3995]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 173 ] state=tensor([[-0.5718,  0.0599, -0.0027, -0.3995]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5706, -0.1352, -0.0107, -0.1076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 174 ] state=tensor([[-0.5706, -0.1352, -0.0107, -0.1076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5733, -0.3302, -0.0129,  0.1816]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 175 ] state=tensor([[-0.5733, -0.3302, -0.0129,  0.1816]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5799, -0.5251, -0.0092,  0.4702]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 176 ] state=tensor([[-0.5799, -0.5251, -0.0092,  0.4702]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-5.9043e-01, -3.2984e-01,  1.8055e-04,  1.7466e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 177 ] state=tensor([[-5.9043e-01, -3.2984e-01,  1.8055e-04,  1.7466e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.5970, -0.5250,  0.0037,  0.4674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 178 ] state=tensor([[-0.5970, -0.5250,  0.0037,  0.4674]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6075, -0.3299,  0.0130,  0.1759]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 179 ] state=tensor([[-0.6075, -0.3299,  0.0130,  0.1759]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6141, -0.1350,  0.0165, -0.1127]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 180 ] state=tensor([[-0.6141, -0.1350,  0.0165, -0.1127]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6168, -0.3303,  0.0143,  0.1852]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 181 ] state=tensor([[-0.6168, -0.3303,  0.0143,  0.1852]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6234, -0.1354,  0.0180, -0.1030]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 182 ] state=tensor([[-0.6234, -0.1354,  0.0180, -0.1030]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6261, -0.3308,  0.0159,  0.1954]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 183 ] state=tensor([[-0.6261, -0.3308,  0.0159,  0.1954]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6328, -0.5261,  0.0198,  0.4930]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 184 ] state=tensor([[-0.6328, -0.5261,  0.0198,  0.4930]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6433, -0.3313,  0.0297,  0.2067]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 185 ] state=tensor([[-0.6433, -0.3313,  0.0297,  0.2067]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6499, -0.1366,  0.0338, -0.0765]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 186 ] state=tensor([[-0.6499, -0.1366,  0.0338, -0.0765]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6526,  0.0580,  0.0323, -0.3583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 187 ] state=tensor([[-0.6526,  0.0580,  0.0323, -0.3583]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6515, -0.1376,  0.0251, -0.0556]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 188 ] state=tensor([[-0.6515, -0.1376,  0.0251, -0.0556]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6542,  0.0572,  0.0240, -0.3403]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 189 ] state=tensor([[-0.6542,  0.0572,  0.0240, -0.3403]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6531,  0.2520,  0.0172, -0.6253]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 190 ] state=tensor([[-0.6531,  0.2520,  0.0172, -0.6253]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6480,  0.0566,  0.0047, -0.3273]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 191 ] state=tensor([[-0.6480,  0.0566,  0.0047, -0.3273]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6469, -0.1386, -0.0018, -0.0331]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 192 ] state=tensor([[-0.6469, -0.1386, -0.0018, -0.0331]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6497, -0.3337, -0.0025,  0.2590]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 193 ] state=tensor([[-0.6497, -0.3337, -0.0025,  0.2590]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6564, -0.1385,  0.0027, -0.0345]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 194 ] state=tensor([[-0.6564, -0.1385,  0.0027, -0.0345]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6591,  0.0566,  0.0020, -0.3263]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 195 ] state=tensor([[-0.6591,  0.0566,  0.0020, -0.3263]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6580, -0.1386, -0.0045, -0.0330]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 196 ] state=tensor([[-0.6580, -0.1386, -0.0045, -0.0330]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6608,  0.0566, -0.0052, -0.3271]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 197 ] state=tensor([[-0.6608,  0.0566, -0.0052, -0.3271]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6596, -0.1384, -0.0117, -0.0360]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 198 ] state=tensor([[-0.6596, -0.1384, -0.0117, -0.0360]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6624, -0.3334, -0.0125,  0.2529]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 199 ] state=tensor([[-0.6624, -0.3334, -0.0125,  0.2529]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6691, -0.5283, -0.0074,  0.5416]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 200 ] state=tensor([[-0.6691, -0.5283, -0.0074,  0.5416]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6796, -0.3331,  0.0034,  0.2466]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 201 ] state=tensor([[-0.6796, -0.3331,  0.0034,  0.2466]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6863, -0.1380,  0.0084, -0.0450]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 202 ] state=tensor([[-0.6863, -0.1380,  0.0084, -0.0450]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6891, -0.3333,  0.0075,  0.2503]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 203 ] state=tensor([[-0.6891, -0.3333,  0.0075,  0.2503]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.6957, -0.1383,  0.0125, -0.0400]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 204 ] state=tensor([[-0.6957, -0.1383,  0.0125, -0.0400]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.6985, -0.3336,  0.0117,  0.2566]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 205 ] state=tensor([[-0.6985, -0.3336,  0.0117,  0.2566]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7052, -0.1386,  0.0168, -0.0324]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 206 ] state=tensor([[-0.7052, -0.1386,  0.0168, -0.0324]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7079,  0.0563,  0.0162, -0.3197]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 207 ] state=tensor([[-0.7079,  0.0563,  0.0162, -0.3197]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7068, -0.1391,  0.0098, -0.0220]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 208 ] state=tensor([[-0.7068, -0.1391,  0.0098, -0.0220]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7096,  0.0559,  0.0093, -0.3115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 209 ] state=tensor([[-0.7096,  0.0559,  0.0093, -0.3115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7085, -0.1394,  0.0031, -0.0159]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 210 ] state=tensor([[-0.7085, -0.1394,  0.0031, -0.0159]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7113,  0.0557,  0.0028, -0.3076]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 211 ] state=tensor([[-0.7113,  0.0557,  0.0028, -0.3076]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7101, -0.1394, -0.0034, -0.0141]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 212 ] state=tensor([[-0.7101, -0.1394, -0.0034, -0.0141]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7129, -0.3345, -0.0037,  0.2775]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 213 ] state=tensor([[-0.7129, -0.3345, -0.0037,  0.2775]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7196, -0.1393,  0.0019, -0.0163]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 214 ] state=tensor([[-0.7196, -0.1393,  0.0019, -0.0163]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7224, -0.3345,  0.0016,  0.2770]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 215 ] state=tensor([[-0.7224, -0.3345,  0.0016,  0.2770]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7291, -0.1394,  0.0071, -0.0152]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 216 ] state=tensor([[-0.7291, -0.1394,  0.0071, -0.0152]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7319, -0.3346,  0.0068,  0.2797]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 217 ] state=tensor([[-0.7319, -0.3346,  0.0068,  0.2797]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7386, -0.5298,  0.0124,  0.5745]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 218 ] state=tensor([[-0.7386, -0.5298,  0.0124,  0.5745]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7492, -0.3349,  0.0239,  0.2858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 219 ] state=tensor([[-0.7492, -0.3349,  0.0239,  0.2858]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7559, -0.5303,  0.0296,  0.5859]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 220 ] state=tensor([[-0.7559, -0.5303,  0.0296,  0.5859]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7665, -0.3356,  0.0413,  0.3027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 221 ] state=tensor([[-0.7665, -0.3356,  0.0413,  0.3027]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7732, -0.1411,  0.0474,  0.0233]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 222 ] state=tensor([[-0.7732, -0.1411,  0.0474,  0.0233]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7760,  0.0533,  0.0478, -0.2540]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 223 ] state=tensor([[-0.7760,  0.0533,  0.0478, -0.2540]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7749,  0.2477,  0.0428, -0.5312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 224 ] state=tensor([[-0.7749,  0.2477,  0.0428, -0.5312]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7700,  0.4422,  0.0321, -0.8101]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 225 ] state=tensor([[-0.7700,  0.4422,  0.0321, -0.8101]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7611,  0.2466,  0.0159, -0.5075]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 226 ] state=tensor([[-0.7611,  0.2466,  0.0159, -0.5075]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7562,  0.0513,  0.0058, -0.2099]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 227 ] state=tensor([[-0.7562,  0.0513,  0.0058, -0.2099]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7552, -0.1439,  0.0016,  0.0846]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 228 ] state=tensor([[-0.7552, -0.1439,  0.0016,  0.0846]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7581, -0.3391,  0.0033,  0.3778]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 229 ] state=tensor([[-0.7581, -0.3391,  0.0033,  0.3778]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7648, -0.1440,  0.0108,  0.0862]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 230 ] state=tensor([[-0.7648, -0.1440,  0.0108,  0.0862]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7677, -0.3393,  0.0126,  0.3823]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 231 ] state=tensor([[-0.7677, -0.3393,  0.0126,  0.3823]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7745, -0.1443,  0.0202,  0.0936]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 232 ] state=tensor([[-0.7745, -0.1443,  0.0202,  0.0936]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7774,  0.0505,  0.0221, -0.1927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 233 ] state=tensor([[-0.7774,  0.0505,  0.0221, -0.1927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7764,  0.2453,  0.0182, -0.4783]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 234 ] state=tensor([[-0.7764,  0.2453,  0.0182, -0.4783]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7715,  0.0499,  0.0087, -0.1799]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 235 ] state=tensor([[-0.7715,  0.0499,  0.0087, -0.1799]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7705,  0.2449,  0.0051, -0.4699]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 236 ] state=tensor([[-0.7705,  0.2449,  0.0051, -0.4699]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7656,  0.0497, -0.0043, -0.1756]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 237 ] state=tensor([[-0.7656,  0.0497, -0.0043, -0.1756]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7646, -0.1453, -0.0078,  0.1157]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 238 ] state=tensor([[-0.7646, -0.1453, -0.0078,  0.1157]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7675, -0.3403, -0.0055,  0.4059]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 239 ] state=tensor([[-0.7675, -0.3403, -0.0055,  0.4059]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7743, -0.1451,  0.0026,  0.1115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 240 ] state=tensor([[-0.7743, -0.1451,  0.0026,  0.1115]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7772,  0.0500,  0.0048, -0.1804]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 241 ] state=tensor([[-0.7772,  0.0500,  0.0048, -0.1804]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7762,  0.2450,  0.0012, -0.4715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 242 ] state=tensor([[-0.7762,  0.2450,  0.0012, -0.4715]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7713,  0.0499, -0.0082, -0.1785]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 243 ] state=tensor([[-0.7713,  0.0499, -0.0082, -0.1785]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7703,  0.2451, -0.0118, -0.4737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 244 ] state=tensor([[-0.7703,  0.2451, -0.0118, -0.4737]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7654,  0.0502, -0.0213, -0.1848]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 245 ] state=tensor([[-0.7654,  0.0502, -0.0213, -0.1848]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7644, -0.1447, -0.0250,  0.1011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 246 ] state=tensor([[-0.7644, -0.1447, -0.0250,  0.1011]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7673, -0.3394, -0.0229,  0.3858]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 247 ] state=tensor([[-0.7673, -0.3394, -0.0229,  0.3858]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7741, -0.1440, -0.0152,  0.0860]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 248 ] state=tensor([[-0.7741, -0.1440, -0.0152,  0.0860]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7770,  0.0514, -0.0135, -0.2115]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 249 ] state=tensor([[-0.7770,  0.0514, -0.0135, -0.2115]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7759, -0.1436, -0.0177,  0.0769]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 250 ] state=tensor([[-0.7759, -0.1436, -0.0177,  0.0769]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7788, -0.3384, -0.0162,  0.3640]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 251 ] state=tensor([[-0.7788, -0.3384, -0.0162,  0.3640]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.7856, -0.1431, -0.0089,  0.0662]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 252 ] state=tensor([[-0.7856, -0.1431, -0.0089,  0.0662]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7884, -0.3381, -0.0076,  0.3561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 253 ] state=tensor([[-0.7884, -0.3381, -0.0076,  0.3561]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-7.9520e-01, -1.4284e-01, -4.7380e-04,  6.0992e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 254 ] state=tensor([[-7.9520e-01, -1.4284e-01, -4.7380e-04,  6.0992e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-7.9806e-01, -3.3795e-01,  7.4605e-04,  3.5353e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 255 ] state=tensor([[-7.9806e-01, -3.3795e-01,  7.4605e-04,  3.5353e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8048, -0.1428,  0.0078,  0.0611]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 256 ] state=tensor([[-0.8048, -0.1428,  0.0078,  0.0611]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8077, -0.3381,  0.0090,  0.3562]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 257 ] state=tensor([[-0.8077, -0.3381,  0.0090,  0.3562]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8144, -0.1431,  0.0162,  0.0664]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 258 ] state=tensor([[-0.8144, -0.1431,  0.0162,  0.0664]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8173,  0.0518,  0.0175, -0.2211]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 259 ] state=tensor([[-0.8173,  0.0518,  0.0175, -0.2211]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8163,  0.2467,  0.0131, -0.5083]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 260 ] state=tensor([[-0.8163,  0.2467,  0.0131, -0.5083]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8113,  0.4416,  0.0029, -0.7968]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 261 ] state=tensor([[-0.8113,  0.4416,  0.0029, -0.7968]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8025,  0.2464, -0.0130, -0.5032]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 262 ] state=tensor([[-0.8025,  0.2464, -0.0130, -0.5032]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7976,  0.0515, -0.0231, -0.2147]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 263 ] state=tensor([[-0.7976,  0.0515, -0.0231, -0.2147]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7965, -0.1433, -0.0274,  0.0707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 264 ] state=tensor([[-0.7965, -0.1433, -0.0274,  0.0707]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.7994, -0.3380, -0.0260,  0.3546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 265 ] state=tensor([[-0.7994, -0.3380, -0.0260,  0.3546]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8062, -0.5327, -0.0189,  0.6390]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 266 ] state=tensor([[-0.8062, -0.5327, -0.0189,  0.6390]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8168, -0.3374, -0.0061,  0.3404]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 267 ] state=tensor([[-0.8168, -0.3374, -0.0061,  0.3404]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.2356e-01, -1.4215e-01,  7.0078e-04,  4.5780e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 268 ] state=tensor([[-8.2356e-01, -1.4215e-01,  7.0078e-04,  4.5780e-02]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8264, -0.3373,  0.0016,  0.3387]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 269 ] state=tensor([[-0.8264, -0.3373,  0.0016,  0.3387]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8331, -0.1422,  0.0084,  0.0465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 270 ] state=tensor([[-0.8331, -0.1422,  0.0084,  0.0465]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8360,  0.0528,  0.0093, -0.2435]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 271 ] state=tensor([[-0.8360,  0.0528,  0.0093, -0.2435]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8349,  0.2478,  0.0045, -0.5332]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 272 ] state=tensor([[-0.8349,  0.2478,  0.0045, -0.5332]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8300,  0.0526, -0.0062, -0.2392]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 273 ] state=tensor([[-0.8300,  0.0526, -0.0062, -0.2392]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8289, -0.1424, -0.0110,  0.0516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 274 ] state=tensor([[-0.8289, -0.1424, -0.0110,  0.0516]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8318, -0.3374, -0.0100,  0.3407]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 275 ] state=tensor([[-0.8318, -0.3374, -0.0100,  0.3407]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8385, -0.5324, -0.0032,  0.6303]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 276 ] state=tensor([[-0.8385, -0.5324, -0.0032,  0.6303]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8492, -0.7274,  0.0095,  0.9220]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 277 ] state=tensor([[-0.8492, -0.7274,  0.0095,  0.9220]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8637, -0.5324,  0.0279,  0.6323]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 278 ] state=tensor([[-0.8637, -0.5324,  0.0279,  0.6323]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8744, -0.3377,  0.0405,  0.3485]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 279 ] state=tensor([[-0.8744, -0.3377,  0.0405,  0.3485]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8811, -0.1432,  0.0475,  0.0689]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 280 ] state=tensor([[-0.8811, -0.1432,  0.0475,  0.0689]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8840,  0.0512,  0.0489, -0.2085]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 281 ] state=tensor([[-0.8840,  0.0512,  0.0489, -0.2085]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8830,  0.2456,  0.0447, -0.4853]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 282 ] state=tensor([[-0.8830,  0.2456,  0.0447, -0.4853]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8780,  0.4401,  0.0350, -0.7636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 283 ] state=tensor([[-0.8780,  0.4401,  0.0350, -0.7636]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8692,  0.2445,  0.0197, -0.4601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 284 ] state=tensor([[-0.8692,  0.2445,  0.0197, -0.4601]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8644,  0.4393,  0.0105, -0.7465]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 285 ] state=tensor([[-0.8644,  0.4393,  0.0105, -0.7465]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8556,  0.2441, -0.0044, -0.4505]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 286 ] state=tensor([[-0.8556,  0.2441, -0.0044, -0.4505]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8507,  0.4392, -0.0134, -0.7446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 287 ] state=tensor([[-0.8507,  0.4392, -0.0134, -0.7446]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8419,  0.2443, -0.0283, -0.4561]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 288 ] state=tensor([[-0.8419,  0.2443, -0.0283, -0.4561]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8370,  0.0496, -0.0374, -0.1725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 289 ] state=tensor([[-0.8370,  0.0496, -0.0374, -0.1725]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8360, -0.1450, -0.0409,  0.1081]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 290 ] state=tensor([[-0.8360, -0.1450, -0.0409,  0.1081]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8389, -0.3395, -0.0387,  0.3876]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 291 ] state=tensor([[-0.8389, -0.3395, -0.0387,  0.3876]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8457, -0.5340, -0.0310,  0.6679]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 292 ] state=tensor([[-0.8457, -0.5340, -0.0310,  0.6679]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8564, -0.3385, -0.0176,  0.3656]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 293 ] state=tensor([[-0.8564, -0.3385, -0.0176,  0.3656]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8632, -0.1431, -0.0103,  0.0674]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 294 ] state=tensor([[-0.8632, -0.1431, -0.0103,  0.0674]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8660, -0.3381, -0.0089,  0.3569]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 295 ] state=tensor([[-0.8660, -0.3381, -0.0089,  0.3569]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8728, -0.1429, -0.0018,  0.0614]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 296 ] state=tensor([[-0.8728, -0.1429, -0.0018,  0.0614]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-8.7565e-01,  5.2293e-02, -5.7081e-04, -2.3188e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 297 ] state=tensor([[-8.7565e-01,  5.2293e-02, -5.7081e-04, -2.3188e-01]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8746, -0.1428, -0.0052,  0.0606]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 298 ] state=tensor([[-0.8746, -0.1428, -0.0052,  0.0606]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8775,  0.0524, -0.0040, -0.2337]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 299 ] state=tensor([[-0.8775,  0.0524, -0.0040, -0.2337]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8764,  0.2476, -0.0087, -0.5276]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 300 ] state=tensor([[-0.8764,  0.2476, -0.0087, -0.5276]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8715,  0.0526, -0.0192, -0.2377]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 301 ] state=tensor([[-0.8715,  0.0526, -0.0192, -0.2377]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8704, -0.1423, -0.0240,  0.0489]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 302 ] state=tensor([[-0.8704, -0.1423, -0.0240,  0.0489]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8733, -0.3371, -0.0230,  0.3339]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 303 ] state=tensor([[-0.8733, -0.3371, -0.0230,  0.3339]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8800, -0.1416, -0.0163,  0.0340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 304 ] state=tensor([[-0.8800, -0.1416, -0.0163,  0.0340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8828, -0.3365, -0.0156,  0.3215]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 305 ] state=tensor([[-0.8828, -0.3365, -0.0156,  0.3215]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8896, -0.1412, -0.0092,  0.0239]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 306 ] state=tensor([[-0.8896, -0.1412, -0.0092,  0.0239]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8924,  0.0541, -0.0087, -0.2716]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 307 ] state=tensor([[-0.8924,  0.0541, -0.0087, -0.2716]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8913, -0.1409, -0.0142,  0.0183]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 308 ] state=tensor([[-0.8913, -0.1409, -0.0142,  0.0183]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8941,  0.0544, -0.0138, -0.2788]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 309 ] state=tensor([[-0.8941,  0.0544, -0.0138, -0.2788]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8930, -0.1405, -0.0194,  0.0095]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 310 ] state=tensor([[-0.8930, -0.1405, -0.0194,  0.0095]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.8958,  0.0549, -0.0192, -0.2893]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 311 ] state=tensor([[-0.8958,  0.0549, -0.0192, -0.2893]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8947, -0.1400, -0.0250, -0.0027]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 312 ] state=tensor([[-0.8947, -0.1400, -0.0250, -0.0027]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8975, -0.3347, -0.0250,  0.2820]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 313 ] state=tensor([[-0.8975, -0.3347, -0.0250,  0.2820]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9042, -0.1392, -0.0194, -0.0185]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 314 ] state=tensor([[-0.9042, -0.1392, -0.0194, -0.0185]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9070,  0.0562, -0.0198, -0.3172]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 315 ] state=tensor([[-0.9070,  0.0562, -0.0198, -0.3172]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9059,  0.2516, -0.0261, -0.6160]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 316 ] state=tensor([[-0.9059,  0.2516, -0.0261, -0.6160]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9009,  0.0568, -0.0384, -0.3317]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 317 ] state=tensor([[-0.9009,  0.0568, -0.0384, -0.3317]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.8997, -0.1377, -0.0451, -0.0514]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 318 ] state=tensor([[-0.8997, -0.1377, -0.0451, -0.0514]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9025, -0.3322, -0.0461,  0.2268]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 319 ] state=tensor([[-0.9025, -0.3322, -0.0461,  0.2268]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9091, -0.5266, -0.0415,  0.5046]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 320 ] state=tensor([[-0.9091, -0.5266, -0.0415,  0.5046]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9197, -0.3309, -0.0315,  0.1991]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 321 ] state=tensor([[-0.9197, -0.3309, -0.0315,  0.1991]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9263, -0.1354, -0.0275, -0.1034]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 322 ] state=tensor([[-0.9263, -0.1354, -0.0275, -0.1034]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9290, -0.3301, -0.0295,  0.1805]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 323 ] state=tensor([[-0.9290, -0.3301, -0.0295,  0.1805]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9356, -0.1346, -0.0259, -0.1213]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 324 ] state=tensor([[-0.9356, -0.1346, -0.0259, -0.1213]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9383, -0.3293, -0.0284,  0.1631]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 325 ] state=tensor([[-0.9383, -0.3293, -0.0284,  0.1631]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9449, -0.5240, -0.0251,  0.4467]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 326 ] state=tensor([[-0.9449, -0.5240, -0.0251,  0.4467]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9553, -0.3286, -0.0162,  0.1462]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 327 ] state=tensor([[-0.9553, -0.3286, -0.0162,  0.1462]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9619, -0.1332, -0.0132, -0.1516]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 328 ] state=tensor([[-0.9619, -0.1332, -0.0132, -0.1516]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9646, -0.3281, -0.0163,  0.1369]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 329 ] state=tensor([[-0.9646, -0.3281, -0.0163,  0.1369]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9711, -0.5230, -0.0135,  0.4244]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 330 ] state=tensor([[-0.9711, -0.5230, -0.0135,  0.4244]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-0.9816, -0.7179, -0.0050,  0.7128]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 331 ] state=tensor([[-0.9816, -0.7179, -0.0050,  0.7128]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-0.9960, -0.5228,  0.0092,  0.4186]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 332 ] state=tensor([[-0.9960, -0.5228,  0.0092,  0.4186]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0064, -0.3278,  0.0176,  0.1288]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 333 ] state=tensor([[-1.0064, -0.3278,  0.0176,  0.1288]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0130, -0.1329,  0.0202, -0.1583]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 334 ] state=tensor([[-1.0130, -0.1329,  0.0202, -0.1583]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0156,  0.0619,  0.0170, -0.4446]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 335 ] state=tensor([[-1.0156,  0.0619,  0.0170, -0.4446]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0144,  0.2568,  0.0081, -0.7318]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 336 ] state=tensor([[-1.0144,  0.2568,  0.0081, -0.7318]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0093,  0.0616, -0.0065, -0.4366]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 337 ] state=tensor([[-1.0093,  0.0616, -0.0065, -0.4366]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0080, -0.1335, -0.0153, -0.1460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 338 ] state=tensor([[-1.0080, -0.1335, -0.0153, -0.1460]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0107, -0.3284, -0.0182,  0.1418]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 339 ] state=tensor([[-1.0107, -0.3284, -0.0182,  0.1418]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0173, -0.5232, -0.0153,  0.4287]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 340 ] state=tensor([[-1.0173, -0.5232, -0.0153,  0.4287]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0277, -0.3279, -0.0068,  0.1312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 341 ] state=tensor([[-1.0277, -0.3279, -0.0068,  0.1312]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0343, -0.1327, -0.0041, -0.1636]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 342 ] state=tensor([[-1.0343, -0.1327, -0.0041, -0.1636]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0369, -0.3277, -0.0074,  0.1278]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 343 ] state=tensor([[-1.0369, -0.3277, -0.0074,  0.1278]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0435, -0.1325, -0.0049, -0.1672]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 344 ] state=tensor([[-1.0435, -0.1325, -0.0049, -0.1672]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0461,  0.0627, -0.0082, -0.4614]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 345 ] state=tensor([[-1.0461,  0.0627, -0.0082, -0.4614]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0449, -0.1323, -0.0174, -0.1713]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 346 ] state=tensor([[-1.0449, -0.1323, -0.0174, -0.1713]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0475, -0.3272, -0.0209,  0.1158]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 347 ] state=tensor([[-1.0475, -0.3272, -0.0209,  0.1158]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0541, -0.5220, -0.0185,  0.4018]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 348 ] state=tensor([[-1.0541, -0.5220, -0.0185,  0.4018]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0645, -0.3266, -0.0105,  0.1033]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 349 ] state=tensor([[-1.0645, -0.3266, -0.0105,  0.1033]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0711, -0.5216, -0.0084,  0.3927]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 350 ] state=tensor([[-1.0711, -0.5216, -0.0084,  0.3927]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0815e+00, -3.2634e-01, -5.9032e-04,  9.7364e-02]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 351 ] state=tensor([[-1.0815e+00, -3.2634e-01, -5.9032e-04,  9.7364e-02]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0880, -0.1312,  0.0014, -0.1955]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 352 ] state=tensor([[-1.0880, -0.1312,  0.0014, -0.1955]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0906,  0.0639, -0.0026, -0.4878]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 353 ] state=tensor([[-1.0906,  0.0639, -0.0026, -0.4878]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0894, -0.1312, -0.0123, -0.1959]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 354 ] state=tensor([[-1.0894, -0.1312, -0.0123, -0.1959]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.0920,  0.0641, -0.0162, -0.4924]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 355 ] state=tensor([[-1.0920,  0.0641, -0.0162, -0.4924]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0907, -0.1308, -0.0261, -0.2049]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 356 ] state=tensor([[-1.0907, -0.1308, -0.0261, -0.2049]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0933, -0.3255, -0.0302,  0.0794]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 357 ] state=tensor([[-1.0933, -0.3255, -0.0302,  0.0794]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.0998, -0.5202, -0.0286,  0.3625]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 358 ] state=tensor([[-1.0998, -0.5202, -0.0286,  0.3625]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1102, -0.3247, -0.0213,  0.0609]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 359 ] state=tensor([[-1.1102, -0.3247, -0.0213,  0.0609]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1167, -0.5195, -0.0201,  0.3468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 360 ] state=tensor([[-1.1167, -0.5195, -0.0201,  0.3468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1271, -0.3241, -0.0132,  0.0478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 361 ] state=tensor([[-1.1271, -0.3241, -0.0132,  0.0478]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1336, -0.1288, -0.0122, -0.2490]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 362 ] state=tensor([[-1.1336, -0.1288, -0.0122, -0.2490]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1362, -0.3237, -0.0172,  0.0398]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 363 ] state=tensor([[-1.1362, -0.3237, -0.0172,  0.0398]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1426, -0.5186, -0.0164,  0.3270]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 364 ] state=tensor([[-1.1426, -0.5186, -0.0164,  0.3270]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1530, -0.3233, -0.0099,  0.0292]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 365 ] state=tensor([[-1.1530, -0.3233, -0.0099,  0.0292]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1595, -0.5182, -0.0093,  0.3188]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 366 ] state=tensor([[-1.1595, -0.5182, -0.0093,  0.3188]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1698, -0.3230, -0.0029,  0.0232]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 367 ] state=tensor([[-1.1698, -0.3230, -0.0029,  0.0232]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1763, -0.1278, -0.0024, -0.2704]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 368 ] state=tensor([[-1.1763, -0.1278, -0.0024, -0.2704]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1789,  0.0673, -0.0079, -0.5639]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 369 ] state=tensor([[-1.1789,  0.0673, -0.0079, -0.5639]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1775, -0.1277, -0.0191, -0.2737]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 370 ] state=tensor([[-1.1775, -0.1277, -0.0191, -0.2737]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1801, -0.3225, -0.0246,  0.0129]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 371 ] state=tensor([[-1.1801, -0.3225, -0.0246,  0.0129]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.1865, -0.5173, -0.0243,  0.2977]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 372 ] state=tensor([[-1.1865, -0.5173, -0.0243,  0.2977]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.1969, -0.3218, -0.0184, -0.0025]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 373 ] state=tensor([[-1.1969, -0.3218, -0.0184, -0.0025]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2033, -0.1264, -0.0184, -0.3010]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 374 ] state=tensor([[-1.2033, -0.1264, -0.0184, -0.3010]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2058, -0.3213, -0.0245, -0.0142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 375 ] state=tensor([[-1.2058, -0.3213, -0.0245, -0.0142]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2123, -0.5160, -0.0247,  0.2707]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 376 ] state=tensor([[-1.2123, -0.5160, -0.0247,  0.2707]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2226, -0.7108, -0.0193,  0.5555]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 377 ] state=tensor([[-1.2226, -0.7108, -0.0193,  0.5555]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2368, -0.5154, -0.0082,  0.2568]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 378 ] state=tensor([[-1.2368, -0.5154, -0.0082,  0.2568]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2471, -0.7104, -0.0031,  0.5468]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 379 ] state=tensor([[-1.2471, -0.7104, -0.0031,  0.5468]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2613, -0.5153,  0.0078,  0.2532]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 380 ] state=tensor([[-1.2613, -0.5153,  0.0078,  0.2532]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2716, -0.3202,  0.0129, -0.0370]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 381 ] state=tensor([[-1.2716, -0.3202,  0.0129, -0.0370]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2780, -0.1253,  0.0122, -0.3256]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 382 ] state=tensor([[-1.2780, -0.1253,  0.0122, -0.3256]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2805,  0.0696,  0.0057, -0.6144]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 383 ] state=tensor([[-1.2805,  0.0696,  0.0057, -0.6144]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2791, -0.1256, -0.0066, -0.3199]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 384 ] state=tensor([[-1.2791, -0.1256, -0.0066, -0.3199]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2816, -0.3206, -0.0130, -0.0294]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 385 ] state=tensor([[-1.2816, -0.3206, -0.0130, -0.0294]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2881, -0.1253, -0.0136, -0.3261]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 386 ] state=tensor([[-1.2881, -0.1253, -0.0136, -0.3261]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.2906,  0.0700, -0.0201, -0.6231]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 387 ] state=tensor([[-1.2906,  0.0700, -0.0201, -0.6231]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2892, -0.1248, -0.0326, -0.3368]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 388 ] state=tensor([[-1.2892, -0.1248, -0.0326, -0.3368]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2917, -0.3195, -0.0393, -0.0546]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 389 ] state=tensor([[-1.2917, -0.3195, -0.0393, -0.0546]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.2980, -0.5140, -0.0404,  0.2255]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 390 ] state=tensor([[-1.2980, -0.5140, -0.0404,  0.2255]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3083, -0.7085, -0.0359,  0.5051]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 391 ] state=tensor([[-1.3083, -0.7085, -0.0359,  0.5051]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3225, -0.5129, -0.0258,  0.2013]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 392 ] state=tensor([[-1.3225, -0.5129, -0.0258,  0.2013]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3328, -0.3174, -0.0218, -0.0994]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 393 ] state=tensor([[-1.3328, -0.3174, -0.0218, -0.0994]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3391, -0.5122, -0.0238,  0.1863]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 394 ] state=tensor([[-1.3391, -0.5122, -0.0238,  0.1863]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3493, -0.7070, -0.0200,  0.4714]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 395 ] state=tensor([[-1.3493, -0.7070, -0.0200,  0.4714]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3635, -0.5116, -0.0106,  0.1725]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 396 ] state=tensor([[-1.3635, -0.5116, -0.0106,  0.1725]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3737, -0.3163, -0.0072, -0.1235]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 397 ] state=tensor([[-1.3737, -0.3163, -0.0072, -0.1235]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3800, -0.1211, -0.0096, -0.4184]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 398 ] state=tensor([[-1.3800, -0.1211, -0.0096, -0.4184]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.3825,  0.0742, -0.0180, -0.7142]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 399 ] state=tensor([[-1.3825,  0.0742, -0.0180, -0.7142]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3810, -0.1207, -0.0323, -0.4272]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 400 ] state=tensor([[-1.3810, -0.1207, -0.0323, -0.4272]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3834, -0.3154, -0.0408, -0.1449]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 401 ] state=tensor([[-1.3834, -0.3154, -0.0408, -0.1449]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3897, -0.5099, -0.0437,  0.1347]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 402 ] state=tensor([[-1.3897, -0.5099, -0.0437,  0.1347]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.3999, -0.7043, -0.0410,  0.4132]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 403 ] state=tensor([[-1.3999, -0.7043, -0.0410,  0.4132]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4140, -0.5087, -0.0328,  0.1079]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 404 ] state=tensor([[-1.4140, -0.5087, -0.0328,  0.1079]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4242, -0.7033, -0.0306,  0.3901]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 405 ] state=tensor([[-1.4242, -0.7033, -0.0306,  0.3901]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4382, -0.5078, -0.0228,  0.0879]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 406 ] state=tensor([[-1.4382, -0.5078, -0.0228,  0.0879]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4484, -0.3123, -0.0211, -0.2119]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 407 ] state=tensor([[-1.4484, -0.3123, -0.0211, -0.2119]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4546, -0.1169, -0.0253, -0.5111]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 408 ] state=tensor([[-1.4546, -0.1169, -0.0253, -0.5111]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4570, -0.3117, -0.0355, -0.2265]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 409 ] state=tensor([[-1.4570, -0.3117, -0.0355, -0.2265]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.4632, -0.1161, -0.0400, -0.5302]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 410 ] state=tensor([[-1.4632, -0.1161, -0.0400, -0.5302]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4655, -0.3106, -0.0507, -0.2504]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 411 ] state=tensor([[-1.4655, -0.3106, -0.0507, -0.2504]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4717, -0.5050, -0.0557,  0.0259]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 412 ] state=tensor([[-1.4717, -0.5050, -0.0557,  0.0259]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4818, -0.6992, -0.0551,  0.3005]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 413 ] state=tensor([[-1.4818, -0.6992, -0.0551,  0.3005]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.4958, -0.8935, -0.0491,  0.5753]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 414 ] state=tensor([[-1.4958, -0.8935, -0.0491,  0.5753]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5137, -1.0879, -0.0376,  0.8521]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 415 ] state=tensor([[-1.5137, -1.0879, -0.0376,  0.8521]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5354, -0.8923, -0.0206,  0.5478]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 416 ] state=tensor([[-1.5354, -0.8923, -0.0206,  0.5478]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5533, -0.6969, -0.0096,  0.2487]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 417 ] state=tensor([[-1.5533, -0.6969, -0.0096,  0.2487]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5672, -0.5016, -0.0047, -0.0470]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 418 ] state=tensor([[-1.5672, -0.5016, -0.0047, -0.0470]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.5773, -0.3065, -0.0056, -0.3411]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 419 ] state=tensor([[-1.5773, -0.3065, -0.0056, -0.3411]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5834, -0.5015, -0.0124, -0.0502]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 420 ] state=tensor([[-1.5834, -0.5015, -0.0124, -0.0502]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.5934, -0.6964, -0.0134,  0.2385]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 421 ] state=tensor([[-1.5934, -0.6964, -0.0134,  0.2385]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6074, -0.8914, -0.0087,  0.5269]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 422 ] state=tensor([[-1.6074, -0.8914, -0.0087,  0.5269]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6252, -0.6961,  0.0019,  0.2315]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 423 ] state=tensor([[-1.6252, -0.6961,  0.0019,  0.2315]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6391, -0.8913,  0.0065,  0.5248]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 424 ] state=tensor([[-1.6391, -0.8913,  0.0065,  0.5248]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6569, -0.6962,  0.0170,  0.2342]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 425 ] state=tensor([[-1.6569, -0.6962,  0.0170,  0.2342]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6709, -0.5014,  0.0217, -0.0531]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 426 ] state=tensor([[-1.6709, -0.5014,  0.0217, -0.0531]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6809, -0.3066,  0.0206, -0.3388]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 427 ] state=tensor([[-1.6809, -0.3066,  0.0206, -0.3388]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6870, -0.1117,  0.0139, -0.6249]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 428 ] state=tensor([[-1.6870, -0.1117,  0.0139, -0.6249]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6892e+00, -3.0706e-01,  1.3634e-03, -3.2791e-01]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 429 ] state=tensor([[-1.6892e+00, -3.0706e-01,  1.3634e-03, -3.2791e-01]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.6954, -0.1120, -0.0052, -0.6202]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 430 ] state=tensor([[-1.6954, -0.1120, -0.0052, -0.6202]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.6976, -0.3070, -0.0176, -0.3291]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 431 ] state=tensor([[-1.6976, -0.3070, -0.0176, -0.3291]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7038, -0.5019, -0.0242, -0.0420]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 432 ] state=tensor([[-1.7038, -0.5019, -0.0242, -0.0420]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7138, -0.3064, -0.0250, -0.3423]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 433 ] state=tensor([[-1.7138, -0.3064, -0.0250, -0.3423]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7199, -0.5012, -0.0319, -0.0576]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 434 ] state=tensor([[-1.7199, -0.5012, -0.0319, -0.0576]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7300, -0.3056, -0.0330, -0.3601]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 435 ] state=tensor([[-1.7300, -0.3056, -0.0330, -0.3601]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7361, -0.5002, -0.0402, -0.0780]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 436 ] state=tensor([[-1.7361, -0.5002, -0.0402, -0.0780]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7461, -0.3046, -0.0418, -0.3831]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 437 ] state=tensor([[-1.7461, -0.3046, -0.0418, -0.3831]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7522, -0.4991, -0.0494, -0.1039]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 438 ] state=tensor([[-1.7522, -0.4991, -0.0494, -0.1039]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7621, -0.6935, -0.0515,  0.1728]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 439 ] state=tensor([[-1.7621, -0.6935, -0.0515,  0.1728]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.7760, -0.8878, -0.0481,  0.4488]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 440 ] state=tensor([[-1.7760, -0.8878, -0.0481,  0.4488]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.7938, -0.6920, -0.0391,  0.1413]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 441 ] state=tensor([[-1.7938, -0.6920, -0.0391,  0.1413]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8076, -0.8866, -0.0363,  0.4214]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 442 ] state=tensor([[-1.8076, -0.8866, -0.0363,  0.4214]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8253, -0.6910, -0.0278,  0.1175]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 443 ] state=tensor([[-1.8253, -0.6910, -0.0278,  0.1175]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8392, -0.4954, -0.0255, -0.1838]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 444 ] state=tensor([[-1.8392, -0.4954, -0.0255, -0.1838]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8491, -0.3000, -0.0292, -0.4844]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 445 ] state=tensor([[-1.8491, -0.3000, -0.0292, -0.4844]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8551, -0.4947, -0.0388, -0.2011]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 446 ] state=tensor([[-1.8551, -0.4947, -0.0388, -0.2011]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8650, -0.2990, -0.0429, -0.5057]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 447 ] state=tensor([[-1.8650, -0.2990, -0.0429, -0.5057]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8709, -0.4935, -0.0530, -0.2269]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 448 ] state=tensor([[-1.8709, -0.4935, -0.0530, -0.2269]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.8808, -0.2977, -0.0575, -0.5358]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 449 ] state=tensor([[-1.8808, -0.2977, -0.0575, -0.5358]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8868, -0.4919, -0.0682, -0.2618]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 450 ] state=tensor([[-1.8868, -0.4919, -0.0682, -0.2618]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.8966, -0.6860, -0.0735,  0.0086]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 451 ] state=tensor([[-1.8966, -0.6860, -0.0735,  0.0086]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9103, -0.8800, -0.0733,  0.2773]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 452 ] state=tensor([[-1.9103, -0.8800, -0.0733,  0.2773]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9279, -1.0740, -0.0678,  0.5460]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 453 ] state=tensor([[-1.9279, -1.0740, -0.0678,  0.5460]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-1.9494, -0.8780, -0.0568,  0.2327]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 454 ] state=tensor([[-1.9494, -0.8780, -0.0568,  0.2327]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9670, -1.0723, -0.0522,  0.5069]])\n",
      "[ Experience replay ] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ episode 133 ][ timestamp 455 ] state=tensor([[-1.9670, -1.0723, -0.0522,  0.5069]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-1.9884, -1.2666, -0.0420,  0.7827]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 456 ] state=tensor([[-1.9884, -1.2666, -0.0420,  0.7827]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0137, -1.0710, -0.0264,  0.4771]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 457 ] state=tensor([[-2.0137, -1.0710, -0.0264,  0.4771]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0352, -0.8755, -0.0168,  0.1763]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 458 ] state=tensor([[-2.0352, -0.8755, -0.0168,  0.1763]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0527, -0.6801, -0.0133, -0.1217]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 459 ] state=tensor([[-2.0527, -0.6801, -0.0133, -0.1217]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0663, -0.4848, -0.0158, -0.4186]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 460 ] state=tensor([[-2.0663, -0.4848, -0.0158, -0.4186]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.0760, -0.2895, -0.0241, -0.7162]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 461 ] state=tensor([[-2.0760, -0.2895, -0.0241, -0.7162]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0818, -0.4842, -0.0384, -0.4312]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 462 ] state=tensor([[-2.0818, -0.4842, -0.0384, -0.4312]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.0915, -0.6788, -0.0471, -0.1509]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 463 ] state=tensor([[-2.0915, -0.6788, -0.0471, -0.1509]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1050, -0.8732, -0.0501,  0.1266]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 464 ] state=tensor([[-2.1050, -0.8732, -0.0501,  0.1266]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1225, -1.0676, -0.0476,  0.4031]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 465 ] state=tensor([[-2.1225, -1.0676, -0.0476,  0.4031]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1438, -0.8718, -0.0395,  0.0958]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 466 ] state=tensor([[-2.1438, -0.8718, -0.0395,  0.0958]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1613, -0.6762, -0.0376, -0.2091]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 467 ] state=tensor([[-2.1613, -0.6762, -0.0376, -0.2091]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.1748, -0.8707, -0.0418,  0.0715]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 468 ] state=tensor([[-2.1748, -0.8707, -0.0418,  0.0715]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.1922, -0.6750, -0.0403, -0.2340]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 469 ] state=tensor([[-2.1922, -0.6750, -0.0403, -0.2340]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2057, -0.8696, -0.0450,  0.0456]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 470 ] state=tensor([[-2.2057, -0.8696, -0.0450,  0.0456]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2231, -0.6738, -0.0441, -0.2609]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 471 ] state=tensor([[-2.2231, -0.6738, -0.0441, -0.2609]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2366, -0.4781, -0.0493, -0.5671]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 472 ] state=tensor([[-2.2366, -0.4781, -0.0493, -0.5671]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2461, -0.6725, -0.0607, -0.2904]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 473 ] state=tensor([[-2.2461, -0.6725, -0.0607, -0.2904]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2596, -0.4766, -0.0665, -0.6016]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 474 ] state=tensor([[-2.2596, -0.4766, -0.0665, -0.6016]]), action=tensor([[0]]), reward=1.0, next_state=tensor([[-2.2691, -0.6707, -0.0785, -0.3306]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 475 ] state=tensor([[-2.2691, -0.6707, -0.0785, -0.3306]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2825, -0.4745, -0.0851, -0.6469]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 476 ] state=tensor([[-2.2825, -0.4745, -0.0851, -0.6469]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2920, -0.2783, -0.0980, -0.9651]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 477 ] state=tensor([[-2.2920, -0.2783, -0.0980, -0.9651]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2976, -0.0821, -0.1174, -1.2869]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 478 ] state=tensor([[-2.2976, -0.0821, -0.1174, -1.2869]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2992,  0.1144, -0.1431, -1.6139]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 479 ] state=tensor([[-2.2992,  0.1144, -0.1431, -1.6139]]), action=tensor([[1]]), reward=1.0, next_state=tensor([[-2.2970,  0.3108, -0.1754, -1.9476]])\n",
      "[ Experience replay ] starts\n",
      "[ episode 133 ][ timestamp 480 ] state=tensor([[-2.2970,  0.3108, -0.1754, -1.9476]]), action=tensor([[1]]), reward=1.0, next_state=None\n",
      "[ Experience replay ] starts\n",
      "[ Ended! ] Episode 133: Exploration_rate=0.05. Score=480.\n",
      "[ Solved! ] Score is now 480\n"
     ]
    }
   ],
   "source": [
    "# create policy\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "policy_net = DQN(observation_space, action_space)\n",
    "target_net = DQN(observation_space, action_space)\n",
    "\n",
    "# create agent\n",
    "agent = Agent(policy_net, target_net)\n",
    "\n",
    "# play game\n",
    "game_durations = []\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor([state]).float()\n",
    "    print(\"[ episode {} ] state={}\".format(i_episode, state))\n",
    "    for t in range(1, 10000):\n",
    "        action = agent.select_action(state)\n",
    "        state_next, reward, done, _ = env.step(action.item())\n",
    "        if done:\n",
    "            state_next = None\n",
    "        else:\n",
    "            state_next = torch.tensor([state_next]).float()\n",
    "        agent.remember(state, action, torch.tensor([[reward]]).float(), state_next)\n",
    "        print(\"[ episode {} ][ timestamp {} ] state={}, action={}, reward={}, next_state={}\".format(i_episode, t, state, action, reward, state_next))\n",
    "        state = state_next\n",
    "        agent.experience_replay()\n",
    "        if done:\n",
    "            game_durations.append(t)\n",
    "            break\n",
    "    print(\"[ Ended! ] Episode {}: Exploration_rate={}. Score={}.\".format(i_episode, agent.exploration_rate, t))\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    agent.update_target_net(i_episode)\n",
    "\n",
    "    # end game criteria\n",
    "    if t > env.spec.reward_threshold:\n",
    "        print(\"[ Solved! ] Score is now {}\".format(t))\n",
    "        break\n",
    "    elif i_episode > 500:\n",
    "        print(\"[ Failed! ] took more than 500 episodes\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.scatter(range(i_episode), game_durations)\n",
    "plt.title('Game Duration Over Time')\n",
    "plt.xlabel('game episode')\n",
    "plt.ylabel('game duration')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pytorch_p36': conda)",
   "language": "python",
   "name": "python361064bitpytorchp36conda143b13e29122453f97130b8bdfe91e87"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
